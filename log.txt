nohup: ignoring input
pip install pipenv --upgrade
Requirement already satisfied: pipenv in /home/rajarshi/.venv/lib/python3.8/site-packages (2021.11.9)
Requirement already satisfied: pip>=18.0 in /home/rajarshi/.venv/lib/python3.8/site-packages (from pipenv) (21.3.1)
Requirement already satisfied: setuptools>=36.2.1 in /home/rajarshi/.venv/lib/python3.8/site-packages (from pipenv) (58.5.3)
Requirement already satisfied: virtualenv in /home/rajarshi/.venv/lib/python3.8/site-packages (from pipenv) (20.10.0)
Requirement already satisfied: virtualenv-clone>=0.2.5 in /home/rajarshi/.venv/lib/python3.8/site-packages (from pipenv) (0.5.7)
Requirement already satisfied: certifi in /home/rajarshi/.venv/lib/python3.8/site-packages (from pipenv) (2021.10.8)
Requirement already satisfied: backports.entry-points-selectable>=1.0.4 in /home/rajarshi/.venv/lib/python3.8/site-packages (from virtualenv->pipenv) (1.1.1)
Requirement already satisfied: six<2,>=1.9.0 in /home/rajarshi/.venv/lib/python3.8/site-packages (from virtualenv->pipenv) (1.16.0)
Requirement already satisfied: filelock<4,>=3.2 in /home/rajarshi/.venv/lib/python3.8/site-packages (from virtualenv->pipenv) (3.3.2)
Requirement already satisfied: platformdirs<3,>=2 in /home/rajarshi/.venv/lib/python3.8/site-packages (from virtualenv->pipenv) (2.4.0)
Requirement already satisfied: distlib<1,>=0.3.1 in /home/rajarshi/.venv/lib/python3.8/site-packages (from virtualenv->pipenv) (0.3.3)
pipenv run pip install --upgrade pip
Courtesy Notice: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set PIPENV_IGNORE_VIRTUALENVS=1 to force pipenv to ignore that environment and create its own instead. You can set PIPENV_VERBOSITY=-1 to suppress this warning.
Requirement already satisfied: pip in /home/rajarshi/.venv/lib/python3.8/site-packages (21.3.1)
pipenv install
Courtesy Notice: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set PIPENV_IGNORE_VIRTUALENVS=1 to force pipenv to ignore that environment and create its own instead. You can set PIPENV_VERBOSITY=-1 to suppress this warning.
Installing dependencies from Pipfile.lock (f724bd)...
cp Pipfile ./spark/processing/3.0/py3
cp Pipfile.lock ./spark/processing/3.0/py3
cp setup.py ./spark/processing/3.0/py3
pipenv run safety check  # https://github.com/pyupio/safety
Courtesy Notice: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set PIPENV_IGNORE_VIRTUALENVS=1 to force pipenv to ignore that environment and create its own instead. You can set PIPENV_VERBOSITY=-1 to suppress this warning.
+==============================================================================+
|                                                                              |
|                               /$$$$$$            /$$                         |
|                              /$$__  $$          | $$                         |
|           /$$$$$$$  /$$$$$$ | $$  \__//$$$$$$  /$$$$$$   /$$   /$$           |
|          /$$_____/ |____  $$| $$$$   /$$__  $$|_  $$_/  | $$  | $$           |
|         |  $$$$$$   /$$$$$$$| $$_/  | $$$$$$$$  | $$    | $$  | $$           |
|          \____  $$ /$$__  $$| $$    | $$_____/  | $$ /$$| $$  | $$           |
|          /$$$$$$$/|  $$$$$$$| $$    |  $$$$$$$  |  $$$$/|  $$$$$$$           |
|         |_______/  \_______/|__/     \_______/   \___/   \____  $$           |
|                                                          /$$  | $$           |
|                                                         |  $$$$$$/           |
|  by pyup.io                                              \______/            |
|                                                                              |
+==============================================================================+
| REPORT                                                                       |
| checked 93 packages, using default DB                                        |
+==============================================================================+
| No known security vulnerabilities found.                                     |
+==============================================================================+
cd test/resources/code/scala/hello-scala-spark; sbt package
[info] welcome to sbt 1.5.5 (Ubuntu Java 11.0.11)
[info] loading project definition from /home/rajarshi/sagemaker-spark/test/resources/code/scala/hello-scala-spark/project
[info] loading settings for project hello-scala-spark from hello-scala-spark.sbt ...
[info] set current project to hello-scala-spark (in build file:/home/rajarshi/sagemaker-spark/test/resources/code/scala/hello-scala-spark/)
[success] Total time: 1 s, completed Nov 14, 2021, 4:53:42 PM
cd test/resources/code/java/hello-java-spark; mvn package
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$1 (file:/usr/share/maven/lib/guice.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$1
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[[1;34mINFO[m] Scanning for projects...
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--------< [0;36mcom.amazonaws.sagemaker.spark.test:hello-java-spark[0;1m >---------[m
[[1;34mINFO[m] [1mBuilding hello-java-spark 1.0-SNAPSHOT[m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mhello-java-spark[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/rajarshi/sagemaker-spark/test/resources/code/java/hello-java-spark/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.1:compile[m [1m(default-compile)[m @ [36mhello-java-spark[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mhello-java-spark[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/rajarshi/sagemaker-spark/test/resources/code/java/hello-java-spark/src/test/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.1:testCompile[m [1m(default-testCompile)[m @ [36mhello-java-spark[0;1m ---[m
[[1;34mINFO[m] No sources to compile
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.12.4:test[m [1m(default-test)[m @ [36mhello-java-spark[0;1m ---[m
[[1;34mINFO[m] No tests to run.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:2.4:jar[m [1m(default-jar)[m @ [36mhello-java-spark[0;1m ---[m
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1;32mBUILD SUCCESS[m
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] Total time:  1.215 s
[[1;34mINFO[m] Finished at: 2021-11-14T16:53:44Z
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
pipenv run python -m pytest -s -vv test/integration/local --repo=sagemaker-spark --tag=latest --role=arn:aws:iam::533753950585:role/arn:aws:iam::533753950585:role/service-role/AmazonSageMaker-ExecutionRole-20210715T004780 --durations=0
Courtesy Notice: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set PIPENV_IGNORE_VIRTUALENVS=1 to force pipenv to ignore that environment and create its own instead. You can set PIPENV_VERBOSITY=-1 to suppress this warning.
============================= test session starts ==============================
platform linux -- Python 3.8.10, pytest-5.4.3, py-1.11.0, pluggy-0.13.1 -- /home/rajarshi/.venv/bin/python
cachedir: .pytest_cache
rootdir: /home/rajarshi/sagemaker-spark
plugins: rerunfailures-10.2, forked-1.3.0, xdist-1.32.0, parallel-0.1.0, cov-2.10.0
collecting ... collected 3 items

test/integration/local/test_multinode_container.py::test_pyspark_multinode CMD='--py-files /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py --verbose /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data' docker-compose up --force-recreate
Creating network "spark-network" with the default driver
Creating algo-1 ... 
Creating algo-2 ... 
[2A[2K
Creating algo-1 ... [32mdone[0m
[2B[1A[2K
Creating algo-2 ... [32mdone[0m
[1BAttaching to algo-1, algo-2
[33malgo-2    |[0m 11-14 16:53 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--py-files', '/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py', '--verbose', '/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py', '--input', 'file:///opt/ml/processing/input/data/data.jsonl', '--output', 'file:///opt/ml/processing/output/data']
[33malgo-2    |[0m 11-14 16:53 smspark.cli  INFO     Raw spark options before processing: {'py_files': '/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py', 'verbose': True, 'class_': None, 'jars': None, 'files': None}
[33malgo-2    |[0m 11-14 16:53 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py', '--input', 'file:///opt/ml/processing/input/data/data.jsonl', '--output', 'file:///opt/ml/processing/output/data']
[36malgo-1    |[0m 11-14 16:53 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--py-files', '/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py', '--verbose', '/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py', '--input', 'file:///opt/ml/processing/input/data/data.jsonl', '--output', 'file:///opt/ml/processing/output/data']
[36malgo-1    |[0m 11-14 16:53 smspark.cli  INFO     Raw spark options before processing: {'py_files': '/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py', 'verbose': True, 'class_': None, 'jars': None, 'files': None}
[36malgo-1    |[0m 11-14 16:53 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py', '--input', 'file:///opt/ml/processing/input/data/data.jsonl', '--output', 'file:///opt/ml/processing/output/data']
[36malgo-1    |[0m 11-14 16:53 smspark.cli  INFO     Rendered spark options: {'py_files': '/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py', 'verbose': True, 'class_': None, 'jars': None, 'files': None}
[36malgo-1    |[0m 11-14 16:53 smspark.cli  INFO     Initializing processing job.
[33malgo-2    |[0m 11-14 16:53 smspark.cli  INFO     Rendered spark options: {'py_files': '/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py', 'verbose': True, 'class_': None, 'jars': None, 'files': None}
[33malgo-2    |[0m 11-14 16:53 smspark.cli  INFO     Initializing processing job.
[36malgo-1    |[0m 11-14 16:53 smspark-submit INFO     {'current_host': 'algo-1', 'hosts': ['algo-1', 'algo-2']}
[33malgo-2    |[0m 11-14 16:53 smspark-submit INFO     {'current_host': 'algo-2', 'hosts': ['algo-1', 'algo-2']}
[36malgo-1    |[0m 11-14 16:53 smspark-submit INFO     {'ProcessingJobArn': 'processing_job_arn', 'ProcessingJobName': 'processing_job_name', 'AppSpecification': {'ImageUri': 'image_uri'}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30}}, 'RoleArn': 'iam_role'}
[36malgo-1    |[0m 11-14 16:53 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client --py-files /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py --verbose /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data
[33malgo-2    |[0m 11-14 16:53 smspark-submit INFO     {'ProcessingJobArn': 'processing_job_arn', 'ProcessingJobName': 'processing_job_name', 'AppSpecification': {'ImageUri': 'image_uri'}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30}}, 'RoleArn': 'iam_role'}
[36malgo-1    |[0m 11-14 16:53 smspark-submit INFO     waiting for hosts
[36malgo-1    |[0m 11-14 16:53 smspark-submit INFO     starting status server
[33malgo-2    |[0m 11-14 16:53 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client --py-files /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py --verbose /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data
[33malgo-2    |[0m 11-14 16:53 smspark-submit INFO     waiting for hosts
[36malgo-1    |[0m 11-14 16:53 smspark-submit INFO     Status server listening on algo-1:5555
[36malgo-1    |[0m 11-14 16:53 smspark-submit INFO     bootstrapping cluster
[36malgo-1    |[0m 11-14 16:53 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING
[33malgo-2    |[0m 11-14 16:53 smspark-submit INFO     starting status server
[36malgo-1    |[0m 11-14 16:53 smspark-submit INFO     copying aws jars
[33malgo-2    |[0m 11-14 16:53 smspark-submit INFO     Status server listening on algo-2:5555
[33malgo-2    |[0m 11-14 16:53 smspark-submit INFO     bootstrapping cluster
[33malgo-2    |[0m 11-14 16:53 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING
[33malgo-2    |[0m 11-14 16:53 smspark-submit INFO     copying aws jars
[36malgo-1    |[0m Serving on http://algo-1:5555
[33malgo-2    |[0m Serving on http://algo-2:5555
[36malgo-1    |[0m 11-14 16:53 smspark-submit INFO     Found hadoop jar hadoop-aws.jar
[33malgo-2    |[0m 11-14 16:53 smspark-submit INFO     Found hadoop jar hadoop-aws.jar
[36malgo-1    |[0m 11-14 16:53 smspark-submit INFO     Optional jar jets3t-0.9.0.jar in /usr/lib/hadoop/lib does not exist
[33malgo-2    |[0m 11-14 16:53 smspark-submit INFO     Optional jar jets3t-0.9.0.jar in /usr/lib/hadoop/lib does not exist
[36malgo-1    |[0m 11-14 16:53 smspark-submit INFO     copying cluster config
[33malgo-2    |[0m 11-14 16:53 smspark-submit INFO     copying cluster config
[36malgo-1    |[0m 11-14 16:53 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml
[33malgo-2    |[0m 11-14 16:53 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml
[33malgo-2    |[0m 11-14 16:53 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml
[36malgo-1    |[0m 11-14 16:53 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml
[33malgo-2    |[0m 11-14 16:53 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[36malgo-1    |[0m 11-14 16:53 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[33malgo-2    |[0m 11-14 16:53 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m 11-14 16:53 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 16:53 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh
[36malgo-1    |[0m 11-14 16:53 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh
[33malgo-2    |[0m 11-14 16:53 root         INFO     Detected instance type: m5.xlarge with total memory: 16384M and total cores: 4
[33malgo-2    |[0m 11-14 16:53 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[33malgo-2    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0"?>
[33malgo-2    |[0m <!-- Site specific YARN configuration properties -->
[33malgo-2    |[0m  <configuration>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.resourcemanager.hostname</name>
[33malgo-2    |[0m          <value>172.21.0.2</value>
[33malgo-2    |[0m          <description>The hostname of the RM.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.hostname</name>
[33malgo-2    |[0m          <value>algo-2</value>
[33malgo-2    |[0m          <description>The hostname of the NM.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.webapp.address</name>
[33malgo-2    |[0m          <value>algo-2:8042</value>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.vmem-pmem-ratio</name>
[33malgo-2    |[0m          <value>5</value>
[33malgo-2    |[0m          <description>Ratio between virtual memory to physical memory.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.resourcemanager.am.max-attempts</name>
[33malgo-2    |[0m          <value>1</value>
[33malgo-2    |[0m          <description>The maximum number of application attempts.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.env-whitelist</name>
[33malgo-2    |[0m          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>
[33malgo-2    |[0m          <description>Environment variable whitelist</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m  
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.minimum-allocation-mb</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.maximum-allocation-mb</name>
[33malgo-2    |[0m     <value>15892</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.minimum-allocation-vcores</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.maximum-allocation-vcores</name>
[33malgo-2    |[0m     <value>4</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.nodemanager.resource.memory-mb</name>
[33malgo-2    |[0m     <value>15892</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.nodemanager.resource.cpu-vcores</name>
[33malgo-2    |[0m     <value>4</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:53 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[33malgo-2    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.driver.host=172.21.0.2
[33malgo-2    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[33malgo-2    |[0m 
[33malgo-2    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[33malgo-2    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[33malgo-2    |[0m spark.rpc.askTimeout=300s
[33malgo-2    |[0m spark.driver.memory 2048m
[33malgo-2    |[0m spark.driver.memoryOverhead 204m
[33malgo-2    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[33malgo-2    |[0m spark.executor.memory 12399m
[33malgo-2    |[0m spark.executor.memoryOverhead 1239m
[33malgo-2    |[0m spark.executor.cores 4
[33malgo-2    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[33malgo-2    |[0m spark.executor.instances 2
[33malgo-2    |[0m spark.default.parallelism 16
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:53 root         INFO     Finished Yarn configuration files setup.
[33malgo-2    |[0m 11-14 16:53 root         INFO     reading user configuration from /opt/ml/processing/input/conf/configuration.json
[36malgo-1    |[0m 11-14 16:53 root         INFO     Detected instance type: m5.xlarge with total memory: 16384M and total cores: 4
[36malgo-1    |[0m 11-14 16:53 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[33malgo-2    |[0m 11-14 16:53 root         INFO     User configuration list or dict: [{'Classification': 'spark-defaults', 'Properties': {'spark.executor.memory': '2g', 'spark.executor.cores': '1'}}, {'Classification': 'hadoop-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'core-site', 'Properties': {'spark.executor.memory': '2g', 'spark.executor.cores': '1'}}, {'Classification': 'hadoop-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'hive-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-exec-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-site', 'Properties': {'key': 'value'}}, {'Classification': 'spark-defaults', 'Properties': {'key': 'value'}}, {'Classification': 'spark-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'spark-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'spark-hive-site', 'Properties': {'key': 'value'}}, {'Classification': 'spark-metrics', 'Properties': {'key': 'value'}}, {'Classification': 'yarn-site', 'Properties': {'key': 'value'}}, {'Classification': 'yarn-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}] , type <class 'list'>
[33malgo-2    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[33malgo-2    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.driver.host=172.21.0.2
[33malgo-2    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[33malgo-2    |[0m 
[33malgo-2    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[33malgo-2    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[33malgo-2    |[0m spark.rpc.askTimeout=300s
[33malgo-2    |[0m spark.driver.memory 2048m
[33malgo-2    |[0m spark.driver.memoryOverhead 204m
[33malgo-2    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[33malgo-2    |[0m spark.executor.memory 12399m
[33malgo-2    |[0m spark.executor.memoryOverhead 1239m
[33malgo-2    |[0m spark.executor.cores 4
[33malgo-2    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[33malgo-2    |[0m spark.executor.instances 2
[33malgo-2    |[0m spark.default.parallelism 16
[33malgo-2    |[0m spark.executor.memory 2g
[33malgo-2    |[0m spark.executor.cores 1
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/hadoop-env.sh
[36malgo-1    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0"?>
[36malgo-1    |[0m <!-- Site specific YARN configuration properties -->
[36malgo-1    |[0m  <configuration>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.resourcemanager.hostname</name>
[36malgo-1    |[0m          <value>172.21.0.2</value>
[36malgo-1    |[0m          <description>The hostname of the RM.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.hostname</name>
[36malgo-1    |[0m          <value>algo-1</value>
[36malgo-1    |[0m          <description>The hostname of the NM.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.webapp.address</name>
[36malgo-1    |[0m          <value>algo-1:8042</value>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.vmem-pmem-ratio</name>
[36malgo-1    |[0m          <value>5</value>
[36malgo-1    |[0m          <description>Ratio between virtual memory to physical memory.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.resourcemanager.am.max-attempts</name>
[36malgo-1    |[0m          <value>1</value>
[36malgo-1    |[0m          <description>The maximum number of application attempts.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.env-whitelist</name>
[36malgo-1    |[0m          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>
[36malgo-1    |[0m          <description>Environment variable whitelist</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m  
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.minimum-allocation-mb</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.maximum-allocation-mb</name>
[36malgo-1    |[0m     <value>15892</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.minimum-allocation-vcores</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.maximum-allocation-vcores</name>
[36malgo-1    |[0m     <value>4</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.nodemanager.resource.memory-mb</name>
[36malgo-1    |[0m     <value>15892</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.nodemanager.resource.cpu-vcores</name>
[36malgo-1    |[0m     <value>4</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 
[33malgo-2    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/hadoop-env.sh is: 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[33malgo-2    |[0m # or more contributor license agreements.  See the NOTICE file
[33malgo-2    |[0m # distributed with this work for additional information
[33malgo-2    |[0m # regarding copyright ownership.  The ASF licenses this file
[33malgo-2    |[0m # to you under the Apache License, Version 2.0 (the
[33malgo-2    |[0m # "License"); you may not use this file except in compliance
[33malgo-2    |[0m # with the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m # Set Hadoop-specific environment variables here.
[33malgo-2    |[0m 
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## THIS FILE ACTS AS THE MASTER FILE FOR ALL HADOOP PROJECTS.
[33malgo-2    |[0m ## SETTINGS HERE WILL BE READ BY ALL HADOOP COMMANDS.  THEREFORE,
[33malgo-2    |[0m ## ONE CAN USE THIS FILE TO SET YARN, HDFS, AND MAPREDUCE
[33malgo-2    |[0m ## CONFIGURATION OPTIONS INSTEAD OF xxx-env.sh.
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## Precedence rules:
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## {yarn-env.sh|hdfs-env.sh} > hadoop-env.sh > hard-coded defaults
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## {YARN_xyz|HDFS_xyz} > HADOOP_xyz > hard-coded defaults
[33malgo-2    |[0m ##
[33malgo-2    |[0m 
[33malgo-2    |[0m # Many of the options here are built from the perspective that users
[33malgo-2    |[0m # may want to provide OVERWRITING values on the command line.
[33malgo-2    |[0m # For example:
[33malgo-2    |[0m #
[33malgo-2    |[0m #  JAVA_HOME=/usr/java/testing hdfs dfs -ls
[33malgo-2    |[0m #
[33malgo-2    |[0m # Therefore, the vast majority (BUT NOT ALL!) of these defaults
[33malgo-2    |[0m # are configured for substitution and not append.  If append
[33malgo-2    |[0m # is preferable, modify this file accordingly.
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Generic settings for HADOOP
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Technically, the only required environment variable is JAVA_HOME.
[33malgo-2    |[0m # All others are optional.  However, the defaults are probably not
[33malgo-2    |[0m # preferred.  Many sites configure these options outside of Hadoop,
[33malgo-2    |[0m # such as in /etc/profile.d
[33malgo-2    |[0m 
[33malgo-2    |[0m # The java implementation to use. By default, this environment
[33malgo-2    |[0m # variable is REQUIRED on ALL platforms except OS X!
[33malgo-2    |[0m # export JAVA_HOME=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Location of Hadoop.  By default, Hadoop will attempt to determine
[33malgo-2    |[0m # this location based upon its execution path.
[33malgo-2    |[0m # export HADOOP_HOME=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Location of Hadoop's configuration information.  i.e., where this
[33malgo-2    |[0m # file is living. If this is not defined, Hadoop will attempt to
[33malgo-2    |[0m # locate it based upon its execution path.
[33malgo-2    |[0m #
[33malgo-2    |[0m # NOTE: It is recommend that this variable not be set here but in
[33malgo-2    |[0m # /etc/profile.d or equivalent.  Some options (such as
[33malgo-2    |[0m # --config) may react strangely otherwise.
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
[33malgo-2    |[0m 
[33malgo-2    |[0m # The maximum amount of heap to use (Java -Xmx).  If no unit
[33malgo-2    |[0m # is provided, it will be converted to MB.  Daemons will
[33malgo-2    |[0m # prefer any Xmx setting in their respective _OPT variable.
[33malgo-2    |[0m # There is no default; the JVM will autoscale based upon machine
[33malgo-2    |[0m # memory size.
[33malgo-2    |[0m # export HADOOP_HEAPSIZE_MAX=
[33malgo-2    |[0m 
[33malgo-2    |[0m # The minimum amount of heap to use (Java -Xms).  If no unit
[33malgo-2    |[0m # is provided, it will be converted to MB.  Daemons will
[33malgo-2    |[0m # prefer any Xms setting in their respective _OPT variable.
[33malgo-2    |[0m # There is no default; the JVM will autoscale based upon machine
[33malgo-2    |[0m # memory size.
[33malgo-2    |[0m # export HADOOP_HEAPSIZE_MIN=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Enable extra debugging of Hadoop's JAAS binding, used to set up
[33malgo-2    |[0m # Kerberos security.
[33malgo-2    |[0m # export HADOOP_JAAS_DEBUG=true
[33malgo-2    |[0m 
[33malgo-2    |[0m # Extra Java runtime options for all Hadoop commands. We don't support
[33malgo-2    |[0m # IPv6 yet/still, so by default the preference is set to IPv4.
[33malgo-2    |[0m # export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true"
[33malgo-2    |[0m # For Kerberos debugging, an extended option set logs more information
[33malgo-2    |[0m # export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug"
[33malgo-2    |[0m 
[33malgo-2    |[0m # Some parts of the shell code may do special things dependent upon
[33malgo-2    |[0m # the operating system.  We have to set this here. See the next
[33malgo-2    |[0m # section as to why....
[33malgo-2    |[0m #export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}
[33malgo-2    |[0m 
[33malgo-2    |[0m # Extra Java runtime options for some Hadoop commands
[33malgo-2    |[0m # and clients (i.e., hdfs dfs -blah).  These get appended to HADOOP_OPTS for
[33malgo-2    |[0m # such commands.  In most cases, # this should be left empty and
[33malgo-2    |[0m # let users supply it on the command line.
[33malgo-2    |[0m # export HADOOP_CLIENT_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # A note about classpaths.
[33malgo-2    |[0m #
[33malgo-2    |[0m # By default, Apache Hadoop overrides Java's CLASSPATH
[33malgo-2    |[0m # environment variable.  It is configured such
[33malgo-2    |[0m # that it starts out blank with new entries added after passing
[33malgo-2    |[0m # a series of checks (file/dir exists, not already listed aka
[33malgo-2    |[0m # de-deduplication).  During de-deduplication, wildcards and/or
[33malgo-2    |[0m # directories are *NOT* expanded to keep it simple. Therefore,
[33malgo-2    |[0m # if the computed classpath has two specific mentions of
[33malgo-2    |[0m # awesome-methods-1.0.jar, only the first one added will be seen.
[33malgo-2    |[0m # If two directories are in the classpath that both contain
[33malgo-2    |[0m # awesome-methods-1.0.jar, then Java will pick up both versions.
[33malgo-2    |[0m 
[33malgo-2    |[0m # An additional, custom CLASSPATH. Site-wide configs should be
[33malgo-2    |[0m # handled via the shellprofile functionality, utilizing the
[33malgo-2    |[0m # hadoop_add_classpath function for greater control and much
[33malgo-2    |[0m # harder for apps/end-users to accidentally override.
[33malgo-2    |[0m # Similarly, end users should utilize ${HOME}/.hadooprc .
[33malgo-2    |[0m # This variable should ideally only be used as a short-cut,
[33malgo-2    |[0m # interactive way for temporary additions on the command line.
[33malgo-2    |[0m # export HADOOP_CLASSPATH="/some/cool/path/on/your/machine"
[33malgo-2    |[0m 
[33malgo-2    |[0m # Should HADOOP_CLASSPATH be first in the official CLASSPATH?
[33malgo-2    |[0m # export HADOOP_USER_CLASSPATH_FIRST="yes"
[33malgo-2    |[0m 
[33malgo-2    |[0m # If HADOOP_USE_CLIENT_CLASSLOADER is set, the classpath along
[33malgo-2    |[0m # with the main jar are handled by a separate isolated
[33malgo-2    |[0m # client classloader when 'hadoop jar', 'yarn jar', or 'mapred job'
[33malgo-2    |[0m # is utilized. If it is set, HADOOP_CLASSPATH and
[33malgo-2    |[0m # HADOOP_USER_CLASSPATH_FIRST are ignored.
[33malgo-2    |[0m # export HADOOP_USE_CLIENT_CLASSLOADER=true
[33malgo-2    |[0m 
[33malgo-2    |[0m # HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES overrides the default definition of
[33malgo-2    |[0m # system classes for the client classloader when HADOOP_USE_CLIENT_CLASSLOADER
[33malgo-2    |[0m # is enabled. Names ending in '.' (period) are treated as package names, and
[33malgo-2    |[0m # names starting with a '-' are treated as negative matches. For example,
[33malgo-2    |[0m # export HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES="-org.apache.hadoop.UserClass,java.,javax.,org.apache.hadoop."
[33malgo-2    |[0m 
[33malgo-2    |[0m # Enable optional, bundled Hadoop features
[33malgo-2    |[0m # This is a comma delimited list.  It may NOT be overridden via .hadooprc
[33malgo-2    |[0m # Entries may be added/removed as needed.
[33malgo-2    |[0m # export HADOOP_OPTIONAL_TOOLS="hadoop-azure,hadoop-azure-datalake,hadoop-aws,hadoop-openstack,hadoop-aliyun,hadoop-kafka"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Options for remote shell connectivity
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # There are some optional components of hadoop that allow for
[33malgo-2    |[0m # command and control of remote hosts.  For example,
[33malgo-2    |[0m # start-dfs.sh will attempt to bring up all NNs, DNS, etc.
[33malgo-2    |[0m 
[33malgo-2    |[0m # Options to pass to SSH when one of the "log into a host and
[33malgo-2    |[0m # start/stop daemons" scripts is executed
[33malgo-2    |[0m # export HADOOP_SSH_OPTS="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10s"
[33malgo-2    |[0m 
[33malgo-2    |[0m # The built-in ssh handler will limit itself to 10 simultaneous connections.
[33malgo-2    |[0m # For pdsh users, this sets the fanout size ( -f )
[33malgo-2    |[0m # Change this to increase/decrease as necessary.
[33malgo-2    |[0m # export HADOOP_SSH_PARALLEL=10
[33malgo-2    |[0m 
[33malgo-2    |[0m # Filename which contains all of the hosts for any remote execution
[33malgo-2    |[0m # helper scripts # such as workers.sh, start-dfs.sh, etc.
[33malgo-2    |[0m # export HADOOP_WORKERS="${HADOOP_CONF_DIR}/workers"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Options for all daemons
[33malgo-2    |[0m ###
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Many options may also be specified as Java properties.  It is
[33malgo-2    |[0m # very common, and in many cases, desirable, to hard-set these
[33malgo-2    |[0m # in daemon _OPTS variables.  Where applicable, the appropriate
[33malgo-2    |[0m # Java property is also identified.  Note that many are re-used
[33malgo-2    |[0m # or set differently in certain contexts (e.g., secure vs
[33malgo-2    |[0m # non-secure)
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m # Where (primarily) daemon log files are stored.
[33malgo-2    |[0m # ${HADOOP_HOME}/logs by default.
[33malgo-2    |[0m # Java property: hadoop.log.dir
[33malgo-2    |[0m # export HADOOP_LOG_DIR=${HADOOP_HOME}/logs
[33malgo-2    |[0m 
[33malgo-2    |[0m # A string representing this instance of hadoop. $USER by default.
[33malgo-2    |[0m # This is used in writing log and pid files, so keep that in mind!
[33malgo-2    |[0m # Java property: hadoop.id.str
[33malgo-2    |[0m # export HADOOP_IDENT_STRING=$USER
[33malgo-2    |[0m 
[33malgo-2    |[0m # How many seconds to pause after stopping a daemon
[33malgo-2    |[0m # export HADOOP_STOP_TIMEOUT=5
[33malgo-2    |[0m 
[33malgo-2    |[0m # Where pid files are stored.  /tmp by default.
[33malgo-2    |[0m # export HADOOP_PID_DIR=/tmp
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default log4j setting for interactive commands
[33malgo-2    |[0m # Java property: hadoop.root.logger
[33malgo-2    |[0m # export HADOOP_ROOT_LOGGER=INFO,console
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default log4j setting for daemons spawned explicitly by
[33malgo-2    |[0m # --daemon option of hadoop, hdfs, mapred and yarn command.
[33malgo-2    |[0m # Java property: hadoop.root.logger
[33malgo-2    |[0m # export HADOOP_DAEMON_ROOT_LOGGER=INFO,RFA
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default log level and output location for security-related messages.
[33malgo-2    |[0m # You will almost certainly want to change this on a per-daemon basis via
[33malgo-2    |[0m # the Java property (i.e., -Dhadoop.security.logger=foo). (Note that the
[33malgo-2    |[0m # defaults for the NN and 2NN override this by default.)
[33malgo-2    |[0m # Java property: hadoop.security.logger
[33malgo-2    |[0m # export HADOOP_SECURITY_LOGGER=INFO,NullAppender
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default process priority level
[33malgo-2    |[0m # Note that sub-processes will also run at this level!
[33malgo-2    |[0m # export HADOOP_NICENESS=0
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default name for the service level authorization file
[33malgo-2    |[0m # Java property: hadoop.policy.file
[33malgo-2    |[0m # export HADOOP_POLICYFILE="hadoop-policy.xml"
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # NOTE: this is not used by default!  <-----
[33malgo-2    |[0m # You can define variables right here and then re-use them later on.
[33malgo-2    |[0m # For example, it is common to use the same garbage collection settings
[33malgo-2    |[0m # for all the daemons.  So one could define:
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HADOOP_GC_SETTINGS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps"
[33malgo-2    |[0m #
[33malgo-2    |[0m # .. and then use it as per the b option under the namenode.
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Secure/privileged execution
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Out of the box, Hadoop uses jsvc from Apache Commons to launch daemons
[33malgo-2    |[0m # on privileged ports.  This functionality can be replaced by providing
[33malgo-2    |[0m # custom functions.  See hadoop-functions.sh for more information.
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m # The jsvc implementation to use. Jsvc is required to run secure datanodes
[33malgo-2    |[0m # that bind to privileged ports to provide authentication of data transfer
[33malgo-2    |[0m # protocol.  Jsvc is not required if SASL is configured for authentication of
[33malgo-2    |[0m # data transfer protocol using non-privileged ports.
[33malgo-2    |[0m # export JSVC_HOME=/usr/bin
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # This directory contains pids for secure and privileged processes.
[33malgo-2    |[0m #export HADOOP_SECURE_PID_DIR=${HADOOP_PID_DIR}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # This directory contains the logs for secure and privileged processes.
[33malgo-2    |[0m # Java property: hadoop.log.dir
[33malgo-2    |[0m # export HADOOP_SECURE_LOG=${HADOOP_LOG_DIR}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # When running a secure daemon, the default value of HADOOP_IDENT_STRING
[33malgo-2    |[0m # ends up being a bit bogus.  Therefore, by default, the code will
[33malgo-2    |[0m # replace HADOOP_IDENT_STRING with HADOOP_xx_SECURE_USER.  If one wants
[33malgo-2    |[0m # to keep HADOOP_IDENT_STRING untouched, then uncomment this line.
[33malgo-2    |[0m # export HADOOP_SECURE_IDENT_PRESERVE="true"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # NameNode specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default log level and output location for file system related change
[33malgo-2    |[0m # messages. For non-namenode daemons, the Java property must be set in
[33malgo-2    |[0m # the appropriate _OPTS if one wants something other than INFO,NullAppender
[33malgo-2    |[0m # Java property: hdfs.audit.logger
[33malgo-2    |[0m # export HDFS_AUDIT_LOGGER=INFO,NullAppender
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the NameNode.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # a) Set JMX options
[33malgo-2    |[0m # export HDFS_NAMENODE_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
[33malgo-2    |[0m #
[33malgo-2    |[0m # b) Set garbage collection logs
[33malgo-2    |[0m # export HDFS_NAMENODE_OPTS="${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[33malgo-2    |[0m #
[33malgo-2    |[0m # c) ... or set them directly
[33malgo-2    |[0m # export HDFS_NAMENODE_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[33malgo-2    |[0m 
[33malgo-2    |[0m # this is the default:
[33malgo-2    |[0m # export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # SecondaryNameNode specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the SecondaryNameNode.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # This is the default:
[33malgo-2    |[0m # export HDFS_SECONDARYNAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # DataNode specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the DataNode.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # This is the default:
[33malgo-2    |[0m # export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS"
[33malgo-2    |[0m 
[33malgo-2    |[0m # On secure datanodes, user to run the datanode as after dropping privileges.
[33malgo-2    |[0m # This **MUST** be uncommented to enable secure HDFS if using privileged ports
[33malgo-2    |[0m # to provide authentication of data transfer protocol.  This **MUST NOT** be
[33malgo-2    |[0m # defined if SASL is configured for authentication of data transfer protocol
[33malgo-2    |[0m # using non-privileged ports.
[33malgo-2    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[33malgo-2    |[0m # export HDFS_DATANODE_SECURE_USER=hdfs
[33malgo-2    |[0m 
[33malgo-2    |[0m # Supplemental options for secure datanodes
[33malgo-2    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[33malgo-2    |[0m # server jvm.
[33malgo-2    |[0m # export HDFS_DATANODE_SECURE_EXTRA_OPTS="-jvm server"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # NFS3 Gateway specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the NFS3 Gateway.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_NFS3_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the Hadoop portmapper.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_PORTMAP_OPTS="-Xmx512m"
[33malgo-2    |[0m 
[33malgo-2    |[0m # Supplemental options for priviliged gateways
[33malgo-2    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[33malgo-2    |[0m # server jvm.
[33malgo-2    |[0m # export HDFS_NFS3_SECURE_EXTRA_OPTS="-jvm server"
[33malgo-2    |[0m 
[33malgo-2    |[0m # On privileged gateways, user to run the gateway as after dropping privileges
[33malgo-2    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[33malgo-2    |[0m # export HDFS_NFS3_SECURE_USER=nfsserver
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # ZKFailoverController specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the ZKFailoverController.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_ZKFC_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # QuorumJournalNode specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the QuorumJournalNode.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_JOURNALNODE_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # HDFS Balancer specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the HDFS Balancer.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_BALANCER_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # HDFS Mover specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the HDFS Mover.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_MOVER_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Router-based HDFS Federation specific parameters
[33malgo-2    |[0m # Specify the JVM options to be used when starting the RBF Routers.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_DFSROUTER_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # HDFS StorageContainerManager specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the HDFS Storage Container Manager.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_STORAGECONTAINERMANAGER_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Advanced Users Only!
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # When building Hadoop, one can add the class paths to the commands
[33malgo-2    |[0m # via this special env var:
[33malgo-2    |[0m # export HADOOP_ENABLE_BUILD_PATHS="true"
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # To prevent accidents, shell commands be (superficially) locked
[33malgo-2    |[0m # to only allow certain users to execute certain subcommands.
[33malgo-2    |[0m # It uses the format of (command)_(subcommand)_USER.
[33malgo-2    |[0m #
[33malgo-2    |[0m # For example, to limit who can execute the namenode command,
[33malgo-2    |[0m # export HDFS_NAMENODE_USER=hdfs
[33malgo-2    |[0m export SPARK_MASTER_HOST=172.21.0.2
[33malgo-2    |[0m export AWS_CONTAINER_CREDENTIALS_RELATIVE_URI=
[33malgo-2    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[33malgo-2    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/core-site.xml
[36malgo-1    |[0m 11-14 16:53 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/core-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0" encoding="UTF-8"?>
[33malgo-2    |[0m  <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[33malgo-2    |[0m  <!-- Put site-specific property overrides in this file. -->
[33malgo-2    |[0m 
[33malgo-2    |[0m  <configuration>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>fs.defaultFS</name>
[33malgo-2    |[0m          <value>hdfs://172.21.0.2/</value>
[33malgo-2    |[0m          <description>NameNode URI</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>fs.s3a.aws.credentials.provider</name>
[33malgo-2    |[0m          <value>com.amazonaws.auth.DefaultAWSCredentialsProviderChain</value>
[33malgo-2    |[0m          <description>AWS S3 credential provider</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>fs.s3.impl</name>
[33malgo-2    |[0m          <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
[33malgo-2    |[0m          <description>s3a filesystem implementation</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>fs.AbstractFileSystem.s3a.imp</name>
[33malgo-2    |[0m          <value>org.apache.hadoop.fs.s3a.S3A</value>
[33malgo-2    |[0m          <description>s3a filesystem implementation</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m  
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>spark.executor.memory</name>
[33malgo-2    |[0m     <value>2g</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>spark.executor.cores</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 
[36malgo-1    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[36malgo-1    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.driver.host=172.21.0.2
[36malgo-1    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[36malgo-1    |[0m 
[36malgo-1    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[36malgo-1    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[36malgo-1    |[0m spark.rpc.askTimeout=300s
[36malgo-1    |[0m spark.driver.memory 2048m
[36malgo-1    |[0m spark.driver.memoryOverhead 204m
[36malgo-1    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[36malgo-1    |[0m spark.executor.memory 12399m
[36malgo-1    |[0m spark.executor.memoryOverhead 1239m
[36malgo-1    |[0m spark.executor.cores 4
[36malgo-1    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[36malgo-1    |[0m spark.executor.instances 2
[36malgo-1    |[0m spark.default.parallelism 16
[36malgo-1    |[0m 
[33malgo-2    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/log4j.properties
[36malgo-1    |[0m 11-14 16:53 root         INFO     Finished Yarn configuration files setup.
[36malgo-1    |[0m 11-14 16:53 root         INFO     reading user configuration from /opt/ml/processing/input/conf/configuration.json
[33malgo-2    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/log4j.properties is: 
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[33malgo-2    |[0m # or more contributor license agreements.  See the NOTICE file
[33malgo-2    |[0m # distributed with this work for additional information
[33malgo-2    |[0m # regarding copyright ownership.  The ASF licenses this file
[33malgo-2    |[0m # to you under the Apache License, Version 2.0 (the
[33malgo-2    |[0m # "License"); you may not use this file except in compliance
[33malgo-2    |[0m # with the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m # Define some default values that can be overridden by system properties
[33malgo-2    |[0m hadoop.root.logger=INFO,console
[33malgo-2    |[0m hadoop.log.dir=.
[33malgo-2    |[0m hadoop.log.file=hadoop.log
[33malgo-2    |[0m 
[33malgo-2    |[0m # Define the root logger to the system property "hadoop.root.logger".
[33malgo-2    |[0m log4j.rootLogger=${hadoop.root.logger}, EventCounter
[33malgo-2    |[0m 
[33malgo-2    |[0m # Logging Threshold
[33malgo-2    |[0m log4j.threshold=ALL
[33malgo-2    |[0m 
[33malgo-2    |[0m # Null Appender
[33malgo-2    |[0m log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Rolling File Appender - cap space usage at 5gb.
[33malgo-2    |[0m #
[33malgo-2    |[0m hadoop.log.maxfilesize=256MB
[33malgo-2    |[0m hadoop.log.maxbackupindex=20
[33malgo-2    |[0m log4j.appender.RFA=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.RFA.MaxFileSize=${hadoop.log.maxfilesize}
[33malgo-2    |[0m log4j.appender.RFA.MaxBackupIndex=${hadoop.log.maxbackupindex}
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.RFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m 
[33malgo-2    |[0m # Pattern format: Date LogLevel LoggerName LogMessage
[33malgo-2    |[0m log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[33malgo-2    |[0m # Debugging Pattern format
[33malgo-2    |[0m #log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Daily Rolling File Appender
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.DRFA.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m 
[33malgo-2    |[0m # Rollver at midnight
[33malgo-2    |[0m #log4j.appender.DRFA.DatePattern=.yyyy-MM-dd
[33malgo-2    |[0m # Rollver at every hour
[33malgo-2    |[0m log4j.appender.DRFA.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m 
[33malgo-2    |[0m # Pattern format: Date LogLevel LoggerName LogMessage
[33malgo-2    |[0m log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m # Debugging Pattern format
[33malgo-2    |[0m #log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # console
[33malgo-2    |[0m # Add "console" to rootlogger above if you want to use this
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.console=org.apache.log4j.ConsoleAppender
[33malgo-2    |[0m log4j.appender.console.target=System.err
[33malgo-2    |[0m log4j.appender.console.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # TaskLog Appender
[33malgo-2    |[0m #
[33malgo-2    |[0m log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.TLA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # HDFS block state change log from block manager
[33malgo-2    |[0m #
[33malgo-2    |[0m # Uncomment the following to log normal block state change
[33malgo-2    |[0m # messages from BlockManager in NameNode.
[33malgo-2    |[0m #log4j.logger.BlockStateChange=DEBUG
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m #Security appender
[33malgo-2    |[0m #
[33malgo-2    |[0m hadoop.security.logger=INFO,NullAppender
[33malgo-2    |[0m hadoop.security.log.maxfilesize=256MB
[33malgo-2    |[0m hadoop.security.log.maxbackupindex=20
[33malgo-2    |[0m log4j.category.SecurityLogger=${hadoop.security.logger}
[33malgo-2    |[0m hadoop.security.log.file=SecurityAuth-${user.name}.audit
[33malgo-2    |[0m log4j.appender.RFAS=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.RFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
[33malgo-2    |[0m log4j.appender.RFAS.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[33malgo-2    |[0m log4j.appender.RFAS.MaxFileSize=${hadoop.security.log.maxfilesize}
[33malgo-2    |[0m log4j.appender.RFAS.MaxBackupIndex=${hadoop.security.log.maxbackupindex}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Daily Rolling Security appender
[33malgo-2    |[0m #
[33malgo-2    |[0m log4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.DRFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
[33malgo-2    |[0m log4j.appender.DRFAS.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.DRFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[33malgo-2    |[0m log4j.appender.DRFAS.DatePattern=.yyyy-MM-dd
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # hadoop configuration logging
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m # Uncomment the following line to turn off configuration deprecation warnings.
[33malgo-2    |[0m # log4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # hdfs audit logging
[33malgo-2    |[0m #
[33malgo-2    |[0m hdfs.audit.logger=INFO,NullAppender
[33malgo-2    |[0m hdfs.audit.log.maxfilesize=256MB
[33malgo-2    |[0m hdfs.audit.log.maxbackupindex=20
[33malgo-2    |[0m log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}
[33malgo-2    |[0m log4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false
[33malgo-2    |[0m log4j.appender.RFAAUDIT=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.RFAAUDIT.File=${hadoop.log.dir}/hdfs-audit.log
[33malgo-2    |[0m log4j.appender.RFAAUDIT.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.RFAAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[33malgo-2    |[0m log4j.appender.RFAAUDIT.MaxFileSize=${hdfs.audit.log.maxfilesize}
[33malgo-2    |[0m log4j.appender.RFAAUDIT.MaxBackupIndex=${hdfs.audit.log.maxbackupindex}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # NameNode metrics logging.
[33malgo-2    |[0m # The default is to retain two namenode-metrics.log files up to 64MB each.
[33malgo-2    |[0m #
[33malgo-2    |[0m namenode.metrics.logger=INFO,NullAppender
[33malgo-2    |[0m log4j.logger.NameNodeMetricsLog=${namenode.metrics.logger}
[33malgo-2    |[0m log4j.additivity.NameNodeMetricsLog=false
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.File=${hadoop.log.dir}/namenode-metrics.log
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.MaxBackupIndex=1
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.MaxFileSize=64MB
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # DataNode metrics logging.
[33malgo-2    |[0m # The default is to retain two datanode-metrics.log files up to 64MB each.
[33malgo-2    |[0m #
[33malgo-2    |[0m datanode.metrics.logger=INFO,NullAppender
[33malgo-2    |[0m log4j.logger.DataNodeMetricsLog=${datanode.metrics.logger}
[33malgo-2    |[0m log4j.additivity.DataNodeMetricsLog=false
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.File=${hadoop.log.dir}/datanode-metrics.log
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.MaxBackupIndex=1
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.MaxFileSize=64MB
[33malgo-2    |[0m 
[33malgo-2    |[0m # Custom Logging levels
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=DEBUG
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m # AWS SDK & S3A FileSystem
[33malgo-2    |[0m #log4j.logger.com.amazonaws=ERROR
[33malgo-2    |[0m log4j.logger.com.amazonaws.http.AmazonHttpClient=ERROR
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.fs.s3a.S3AFileSystem=WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Event Counter Appender
[33malgo-2    |[0m # Sends counts of logging messages at different severity levels to Hadoop Metrics.
[33malgo-2    |[0m #
[33malgo-2    |[0m log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Job Summary Appender
[33malgo-2    |[0m #
[33malgo-2    |[0m # Use following logger to send summary to separate file defined by
[33malgo-2    |[0m # hadoop.mapreduce.jobsummary.log.file :
[33malgo-2    |[0m # hadoop.mapreduce.jobsummary.logger=INFO,JSA
[33malgo-2    |[0m # 
[33malgo-2    |[0m hadoop.mapreduce.jobsummary.logger=${hadoop.root.logger}
[33malgo-2    |[0m hadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log
[33malgo-2    |[0m hadoop.mapreduce.jobsummary.log.maxfilesize=256MB
[33malgo-2    |[0m hadoop.mapreduce.jobsummary.log.maxbackupindex=20
[33malgo-2    |[0m log4j.appender.JSA=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.JSA.File=${hadoop.log.dir}/${hadoop.mapreduce.jobsummary.log.file}
[33malgo-2    |[0m log4j.appender.JSA.MaxFileSize=${hadoop.mapreduce.jobsummary.log.maxfilesize}
[33malgo-2    |[0m log4j.appender.JSA.MaxBackupIndex=${hadoop.mapreduce.jobsummary.log.maxbackupindex}
[33malgo-2    |[0m log4j.appender.JSA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.JSA.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
[33malgo-2    |[0m log4j.appender.JSA.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.logger.org.apache.hadoop.mapred.JobInProgress$JobSummary=${hadoop.mapreduce.jobsummary.logger}
[33malgo-2    |[0m log4j.additivity.org.apache.hadoop.mapred.JobInProgress$JobSummary=false
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # shuffle connection log from shuffleHandler
[33malgo-2    |[0m # Uncomment the following line to enable logging of shuffle connections
[33malgo-2    |[0m # log4j.logger.org.apache.hadoop.mapred.ShuffleHandler.audit=DEBUG
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Yarn ResourceManager Application Summary Log
[33malgo-2    |[0m #
[33malgo-2    |[0m # Set the ResourceManager summary log filename
[33malgo-2    |[0m yarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log
[33malgo-2    |[0m # Set the ResourceManager summary log level and appender
[33malgo-2    |[0m yarn.server.resourcemanager.appsummary.logger=${hadoop.root.logger}
[33malgo-2    |[0m #yarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY
[33malgo-2    |[0m 
[33malgo-2    |[0m # To enable AppSummaryLogging for the RM,
[33malgo-2    |[0m # set yarn.server.resourcemanager.appsummary.logger to
[33malgo-2    |[0m # <LEVEL>,RMSUMMARY in hadoop-env.sh
[33malgo-2    |[0m 
[33malgo-2    |[0m # Appender for ResourceManager Application Summary Log
[33malgo-2    |[0m # Requires the following properties to be set
[33malgo-2    |[0m #    - hadoop.log.dir (Hadoop Log directory)
[33malgo-2    |[0m #    - yarn.server.resourcemanager.appsummary.log.file (resource manager app summary log filename)
[33malgo-2    |[0m #    - yarn.server.resourcemanager.appsummary.logger (resource manager app summary log level and appender)
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=${yarn.server.resourcemanager.appsummary.logger}
[33malgo-2    |[0m log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=false
[33malgo-2    |[0m log4j.appender.RMSUMMARY=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.RMSUMMARY.File=${hadoop.log.dir}/${yarn.server.resourcemanager.appsummary.log.file}
[33malgo-2    |[0m log4j.appender.RMSUMMARY.MaxFileSize=256MB
[33malgo-2    |[0m log4j.appender.RMSUMMARY.MaxBackupIndex=20
[33malgo-2    |[0m log4j.appender.RMSUMMARY.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.RMSUMMARY.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m # HS audit log configs
[33malgo-2    |[0m #mapreduce.hs.audit.logger=INFO,HSAUDIT
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=${mapreduce.hs.audit.logger}
[33malgo-2    |[0m #log4j.additivity.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=false
[33malgo-2    |[0m #log4j.appender.HSAUDIT=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m #log4j.appender.HSAUDIT.File=${hadoop.log.dir}/hs-audit.log
[33malgo-2    |[0m #log4j.appender.HSAUDIT.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m #log4j.appender.HSAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[33malgo-2    |[0m #log4j.appender.HSAUDIT.DatePattern=.yyyy-MM-dd
[33malgo-2    |[0m 
[33malgo-2    |[0m # Http Server Request Logs
[33malgo-2    |[0m #log4j.logger.http.requests.namenode=INFO,namenoderequestlog
[33malgo-2    |[0m #log4j.appender.namenoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.namenoderequestlog.Filename=${hadoop.log.dir}/jetty-namenode-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.namenoderequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.http.requests.datanode=INFO,datanoderequestlog
[33malgo-2    |[0m #log4j.appender.datanoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.datanoderequestlog.Filename=${hadoop.log.dir}/jetty-datanode-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.datanoderequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.http.requests.resourcemanager=INFO,resourcemanagerrequestlog
[33malgo-2    |[0m #log4j.appender.resourcemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.resourcemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-resourcemanager-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.resourcemanagerrequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.http.requests.jobhistory=INFO,jobhistoryrequestlog
[33malgo-2    |[0m #log4j.appender.jobhistoryrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.jobhistoryrequestlog.Filename=${hadoop.log.dir}/jetty-jobhistory-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.jobhistoryrequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.http.requests.nodemanager=INFO,nodemanagerrequestlog
[33malgo-2    |[0m #log4j.appender.nodemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.nodemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-nodemanager-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.nodemanagerrequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m # WebHdfs request log on datanodes
[33malgo-2    |[0m # Specify -Ddatanode.webhdfs.logger=INFO,HTTPDRFA on datanode startup to
[33malgo-2    |[0m # direct the log to a separate file.
[33malgo-2    |[0m #datanode.webhdfs.logger=INFO,console
[33malgo-2    |[0m #log4j.logger.datanode.webhdfs=${datanode.webhdfs.logger}
[33malgo-2    |[0m #log4j.appender.HTTPDRFA=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m #log4j.appender.HTTPDRFA.File=${hadoop.log.dir}/hadoop-datanode-webhdfs.log
[33malgo-2    |[0m #log4j.appender.HTTPDRFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m #log4j.appender.HTTPDRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[33malgo-2    |[0m #log4j.appender.HTTPDRFA.DatePattern=.yyyy-MM-dd
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m # Appender for viewing information for errors and warnings
[33malgo-2    |[0m yarn.ewma.cleanupInterval=300
[33malgo-2    |[0m yarn.ewma.messageAgeLimitSeconds=86400
[33malgo-2    |[0m yarn.ewma.maxUniqueMessages=250
[33malgo-2    |[0m log4j.appender.EWMA=org.apache.hadoop.yarn.util.Log4jWarningErrorMetricsAppender
[33malgo-2    |[0m log4j.appender.EWMA.cleanupInterval=${yarn.ewma.cleanupInterval}
[33malgo-2    |[0m log4j.appender.EWMA.messageAgeLimitSeconds=${yarn.ewma.messageAgeLimitSeconds}
[33malgo-2    |[0m log4j.appender.EWMA.maxUniqueMessages=${yarn.ewma.maxUniqueMessages}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Fair scheduler state dump
[33malgo-2    |[0m #
[33malgo-2    |[0m # Use following logger to dump the state to a separate file
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=DEBUG,FSSTATEDUMP
[33malgo-2    |[0m #log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=false
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.File=${hadoop.log.dir}/fairscheduler-statedump.log
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.MaxFileSize=${hadoop.log.maxfilesize}
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.MaxBackupIndex=${hadoop.log.maxbackupindex}
[33malgo-2    |[0m 
[33malgo-2    |[0m # Log levels of third-party libraries
[33malgo-2    |[0m log4j.logger.org.apache.commons.beanutils=WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m #AWS SDK Logging
[33malgo-2    |[0m log4j.logger.com.amazonaws=WARN
[33malgo-2    |[0m log4j.logger.org.apache.zookeeper=ERROR
[33malgo-2    |[0m log4j.logger.org.apache.hadoop.hbase=WARN
[33malgo-2    |[0m log4j.logger.amazon.emr.metrics=WARN
[33malgo-2    |[0m log4j.logger.emr=INFO
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.HADOOP=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.HADOOP.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m log4j.appender.HADOOP.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.appender.HADOOP.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.HADOOP.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.JHS=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.JHS.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m log4j.appender.JHS.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.appender.JHS.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.JHS.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.MAPRED=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.MAPRED.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m log4j.appender.MAPRED.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.appender.MAPRED.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.MAPRED.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.YARN=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.YARN.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m log4j.appender.YARN.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.appender.YARN.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.YARN.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/hive/conf/hive-env.sh
[36malgo-1    |[0m 11-14 16:53 root         INFO     User configuration list or dict: [{'Classification': 'spark-defaults', 'Properties': {'spark.executor.memory': '2g', 'spark.executor.cores': '1'}}, {'Classification': 'hadoop-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'core-site', 'Properties': {'spark.executor.memory': '2g', 'spark.executor.cores': '1'}}, {'Classification': 'hadoop-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'hive-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-exec-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-site', 'Properties': {'key': 'value'}}, {'Classification': 'spark-defaults', 'Properties': {'key': 'value'}}, {'Classification': 'spark-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'spark-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'spark-hive-site', 'Properties': {'key': 'value'}}, {'Classification': 'spark-metrics', 'Properties': {'key': 'value'}}, {'Classification': 'yarn-site', 'Properties': {'key': 'value'}}, {'Classification': 'yarn-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}] , type <class 'list'>
[36malgo-1    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[36malgo-1    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.driver.host=172.21.0.2
[36malgo-1    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[36malgo-1    |[0m 
[36malgo-1    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[36malgo-1    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[36malgo-1    |[0m spark.rpc.askTimeout=300s
[36malgo-1    |[0m spark.driver.memory 2048m
[36malgo-1    |[0m spark.driver.memoryOverhead 204m
[36malgo-1    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[36malgo-1    |[0m spark.executor.memory 12399m
[36malgo-1    |[0m spark.executor.memoryOverhead 1239m
[36malgo-1    |[0m spark.executor.cores 4
[36malgo-1    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[36malgo-1    |[0m spark.executor.instances 2
[36malgo-1    |[0m spark.default.parallelism 16
[36malgo-1    |[0m spark.executor.memory 2g
[36malgo-1    |[0m spark.executor.cores 1
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/hadoop-env.sh
[33malgo-2    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/hive/conf/hive-env.sh is: 
[33malgo-2    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[33malgo-2    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/hive/conf/hive-log4j2.properties
[36malgo-1    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/hadoop-env.sh is: 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[36malgo-1    |[0m # or more contributor license agreements.  See the NOTICE file
[36malgo-1    |[0m # distributed with this work for additional information
[36malgo-1    |[0m # regarding copyright ownership.  The ASF licenses this file
[36malgo-1    |[0m # to you under the Apache License, Version 2.0 (the
[36malgo-1    |[0m # "License"); you may not use this file except in compliance
[36malgo-1    |[0m # with the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m # Set Hadoop-specific environment variables here.
[36malgo-1    |[0m 
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## THIS FILE ACTS AS THE MASTER FILE FOR ALL HADOOP PROJECTS.
[36malgo-1    |[0m ## SETTINGS HERE WILL BE READ BY ALL HADOOP COMMANDS.  THEREFORE,
[36malgo-1    |[0m ## ONE CAN USE THIS FILE TO SET YARN, HDFS, AND MAPREDUCE
[36malgo-1    |[0m ## CONFIGURATION OPTIONS INSTEAD OF xxx-env.sh.
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## Precedence rules:
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## {yarn-env.sh|hdfs-env.sh} > hadoop-env.sh > hard-coded defaults
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## {YARN_xyz|HDFS_xyz} > HADOOP_xyz > hard-coded defaults
[36malgo-1    |[0m ##
[36malgo-1    |[0m 
[36malgo-1    |[0m # Many of the options here are built from the perspective that users
[36malgo-1    |[0m # may want to provide OVERWRITING values on the command line.
[36malgo-1    |[0m # For example:
[36malgo-1    |[0m #
[36malgo-1    |[0m #  JAVA_HOME=/usr/java/testing hdfs dfs -ls
[36malgo-1    |[0m #
[36malgo-1    |[0m # Therefore, the vast majority (BUT NOT ALL!) of these defaults
[36malgo-1    |[0m # are configured for substitution and not append.  If append
[36malgo-1    |[0m # is preferable, modify this file accordingly.
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Generic settings for HADOOP
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Technically, the only required environment variable is JAVA_HOME.
[36malgo-1    |[0m # All others are optional.  However, the defaults are probably not
[36malgo-1    |[0m # preferred.  Many sites configure these options outside of Hadoop,
[36malgo-1    |[0m # such as in /etc/profile.d
[36malgo-1    |[0m 
[36malgo-1    |[0m # The java implementation to use. By default, this environment
[36malgo-1    |[0m # variable is REQUIRED on ALL platforms except OS X!
[36malgo-1    |[0m # export JAVA_HOME=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Location of Hadoop.  By default, Hadoop will attempt to determine
[36malgo-1    |[0m # this location based upon its execution path.
[36malgo-1    |[0m # export HADOOP_HOME=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Location of Hadoop's configuration information.  i.e., where this
[36malgo-1    |[0m # file is living. If this is not defined, Hadoop will attempt to
[36malgo-1    |[0m # locate it based upon its execution path.
[36malgo-1    |[0m #
[36malgo-1    |[0m # NOTE: It is recommend that this variable not be set here but in
[36malgo-1    |[0m # /etc/profile.d or equivalent.  Some options (such as
[36malgo-1    |[0m # --config) may react strangely otherwise.
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
[36malgo-1    |[0m 
[36malgo-1    |[0m # The maximum amount of heap to use (Java -Xmx).  If no unit
[36malgo-1    |[0m # is provided, it will be converted to MB.  Daemons will
[36malgo-1    |[0m # prefer any Xmx setting in their respective _OPT variable.
[36malgo-1    |[0m # There is no default; the JVM will autoscale based upon machine
[36malgo-1    |[0m # memory size.
[36malgo-1    |[0m # export HADOOP_HEAPSIZE_MAX=
[36malgo-1    |[0m 
[36malgo-1    |[0m # The minimum amount of heap to use (Java -Xms).  If no unit
[36malgo-1    |[0m # is provided, it will be converted to MB.  Daemons will
[36malgo-1    |[0m # prefer any Xms setting in their respective _OPT variable.
[36malgo-1    |[0m # There is no default; the JVM will autoscale based upon machine
[36malgo-1    |[0m # memory size.
[36malgo-1    |[0m # export HADOOP_HEAPSIZE_MIN=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Enable extra debugging of Hadoop's JAAS binding, used to set up
[36malgo-1    |[0m # Kerberos security.
[36malgo-1    |[0m # export HADOOP_JAAS_DEBUG=true
[36malgo-1    |[0m 
[36malgo-1    |[0m # Extra Java runtime options for all Hadoop commands. We don't support
[36malgo-1    |[0m # IPv6 yet/still, so by default the preference is set to IPv4.
[36malgo-1    |[0m # export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true"
[36malgo-1    |[0m # For Kerberos debugging, an extended option set logs more information
[36malgo-1    |[0m # export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug"
[36malgo-1    |[0m 
[36malgo-1    |[0m # Some parts of the shell code may do special things dependent upon
[36malgo-1    |[0m # the operating system.  We have to set this here. See the next
[36malgo-1    |[0m # section as to why....
[36malgo-1    |[0m #export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}
[36malgo-1    |[0m 
[36malgo-1    |[0m # Extra Java runtime options for some Hadoop commands
[36malgo-1    |[0m # and clients (i.e., hdfs dfs -blah).  These get appended to HADOOP_OPTS for
[36malgo-1    |[0m # such commands.  In most cases, # this should be left empty and
[36malgo-1    |[0m # let users supply it on the command line.
[36malgo-1    |[0m # export HADOOP_CLIENT_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # A note about classpaths.
[36malgo-1    |[0m #
[36malgo-1    |[0m # By default, Apache Hadoop overrides Java's CLASSPATH
[36malgo-1    |[0m # environment variable.  It is configured such
[36malgo-1    |[0m # that it starts out blank with new entries added after passing
[36malgo-1    |[0m # a series of checks (file/dir exists, not already listed aka
[36malgo-1    |[0m # de-deduplication).  During de-deduplication, wildcards and/or
[36malgo-1    |[0m # directories are *NOT* expanded to keep it simple. Therefore,
[36malgo-1    |[0m # if the computed classpath has two specific mentions of
[36malgo-1    |[0m # awesome-methods-1.0.jar, only the first one added will be seen.
[36malgo-1    |[0m # If two directories are in the classpath that both contain
[36malgo-1    |[0m # awesome-methods-1.0.jar, then Java will pick up both versions.
[36malgo-1    |[0m 
[36malgo-1    |[0m # An additional, custom CLASSPATH. Site-wide configs should be
[36malgo-1    |[0m # handled via the shellprofile functionality, utilizing the
[36malgo-1    |[0m # hadoop_add_classpath function for greater control and much
[36malgo-1    |[0m # harder for apps/end-users to accidentally override.
[36malgo-1    |[0m # Similarly, end users should utilize ${HOME}/.hadooprc .
[36malgo-1    |[0m # This variable should ideally only be used as a short-cut,
[36malgo-1    |[0m # interactive way for temporary additions on the command line.
[36malgo-1    |[0m # export HADOOP_CLASSPATH="/some/cool/path/on/your/machine"
[36malgo-1    |[0m 
[36malgo-1    |[0m # Should HADOOP_CLASSPATH be first in the official CLASSPATH?
[36malgo-1    |[0m # export HADOOP_USER_CLASSPATH_FIRST="yes"
[36malgo-1    |[0m 
[36malgo-1    |[0m # If HADOOP_USE_CLIENT_CLASSLOADER is set, the classpath along
[36malgo-1    |[0m # with the main jar are handled by a separate isolated
[36malgo-1    |[0m # client classloader when 'hadoop jar', 'yarn jar', or 'mapred job'
[36malgo-1    |[0m # is utilized. If it is set, HADOOP_CLASSPATH and
[36malgo-1    |[0m # HADOOP_USER_CLASSPATH_FIRST are ignored.
[36malgo-1    |[0m # export HADOOP_USE_CLIENT_CLASSLOADER=true
[36malgo-1    |[0m 
[36malgo-1    |[0m # HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES overrides the default definition of
[36malgo-1    |[0m # system classes for the client classloader when HADOOP_USE_CLIENT_CLASSLOADER
[36malgo-1    |[0m # is enabled. Names ending in '.' (period) are treated as package names, and
[36malgo-1    |[0m # names starting with a '-' are treated as negative matches. For example,
[36malgo-1    |[0m # export HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES="-org.apache.hadoop.UserClass,java.,javax.,org.apache.hadoop."
[36malgo-1    |[0m 
[36malgo-1    |[0m # Enable optional, bundled Hadoop features
[36malgo-1    |[0m # This is a comma delimited list.  It may NOT be overridden via .hadooprc
[36malgo-1    |[0m # Entries may be added/removed as needed.
[36malgo-1    |[0m # export HADOOP_OPTIONAL_TOOLS="hadoop-azure,hadoop-azure-datalake,hadoop-aws,hadoop-openstack,hadoop-aliyun,hadoop-kafka"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Options for remote shell connectivity
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # There are some optional components of hadoop that allow for
[36malgo-1    |[0m # command and control of remote hosts.  For example,
[36malgo-1    |[0m # start-dfs.sh will attempt to bring up all NNs, DNS, etc.
[36malgo-1    |[0m 
[36malgo-1    |[0m # Options to pass to SSH when one of the "log into a host and
[36malgo-1    |[0m # start/stop daemons" scripts is executed
[36malgo-1    |[0m # export HADOOP_SSH_OPTS="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10s"
[36malgo-1    |[0m 
[36malgo-1    |[0m # The built-in ssh handler will limit itself to 10 simultaneous connections.
[36malgo-1    |[0m # For pdsh users, this sets the fanout size ( -f )
[36malgo-1    |[0m # Change this to increase/decrease as necessary.
[36malgo-1    |[0m # export HADOOP_SSH_PARALLEL=10
[36malgo-1    |[0m 
[36malgo-1    |[0m # Filename which contains all of the hosts for any remote execution
[36malgo-1    |[0m # helper scripts # such as workers.sh, start-dfs.sh, etc.
[36malgo-1    |[0m # export HADOOP_WORKERS="${HADOOP_CONF_DIR}/workers"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Options for all daemons
[36malgo-1    |[0m ###
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Many options may also be specified as Java properties.  It is
[36malgo-1    |[0m # very common, and in many cases, desirable, to hard-set these
[36malgo-1    |[0m # in daemon _OPTS variables.  Where applicable, the appropriate
[36malgo-1    |[0m # Java property is also identified.  Note that many are re-used
[36malgo-1    |[0m # or set differently in certain contexts (e.g., secure vs
[36malgo-1    |[0m # non-secure)
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m # Where (primarily) daemon log files are stored.
[36malgo-1    |[0m # ${HADOOP_HOME}/logs by default.
[36malgo-1    |[0m # Java property: hadoop.log.dir
[36malgo-1    |[0m # export HADOOP_LOG_DIR=${HADOOP_HOME}/logs
[36malgo-1    |[0m 
[36malgo-1    |[0m # A string representing this instance of hadoop. $USER by default.
[36malgo-1    |[0m # This is used in writing log and pid files, so keep that in mind!
[36malgo-1    |[0m # Java property: hadoop.id.str
[36malgo-1    |[0m # export HADOOP_IDENT_STRING=$USER
[36malgo-1    |[0m 
[36malgo-1    |[0m # How many seconds to pause after stopping a daemon
[36malgo-1    |[0m # export HADOOP_STOP_TIMEOUT=5
[36malgo-1    |[0m 
[36malgo-1    |[0m # Where pid files are stored.  /tmp by default.
[36malgo-1    |[0m # export HADOOP_PID_DIR=/tmp
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default log4j setting for interactive commands
[36malgo-1    |[0m # Java property: hadoop.root.logger
[36malgo-1    |[0m # export HADOOP_ROOT_LOGGER=INFO,console
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default log4j setting for daemons spawned explicitly by
[36malgo-1    |[0m # --daemon option of hadoop, hdfs, mapred and yarn command.
[36malgo-1    |[0m # Java property: hadoop.root.logger
[36malgo-1    |[0m # export HADOOP_DAEMON_ROOT_LOGGER=INFO,RFA
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default log level and output location for security-related messages.
[36malgo-1    |[0m # You will almost certainly want to change this on a per-daemon basis via
[36malgo-1    |[0m # the Java property (i.e., -Dhadoop.security.logger=foo). (Note that the
[36malgo-1    |[0m # defaults for the NN and 2NN override this by default.)
[36malgo-1    |[0m # Java property: hadoop.security.logger
[36malgo-1    |[0m # export HADOOP_SECURITY_LOGGER=INFO,NullAppender
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default process priority level
[36malgo-1    |[0m # Note that sub-processes will also run at this level!
[36malgo-1    |[0m # export HADOOP_NICENESS=0
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default name for the service level authorization file
[36malgo-1    |[0m # Java property: hadoop.policy.file
[36malgo-1    |[0m # export HADOOP_POLICYFILE="hadoop-policy.xml"
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # NOTE: this is not used by default!  <-----
[36malgo-1    |[0m # You can define variables right here and then re-use them later on.
[36malgo-1    |[0m # For example, it is common to use the same garbage collection settings
[36malgo-1    |[0m # for all the daemons.  So one could define:
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HADOOP_GC_SETTINGS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps"
[36malgo-1    |[0m #
[36malgo-1    |[0m # .. and then use it as per the b option under the namenode.
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Secure/privileged execution
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Out of the box, Hadoop uses jsvc from Apache Commons to launch daemons
[36malgo-1    |[0m # on privileged ports.  This functionality can be replaced by providing
[36malgo-1    |[0m # custom functions.  See hadoop-functions.sh for more information.
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m # The jsvc implementation to use. Jsvc is required to run secure datanodes
[36malgo-1    |[0m # that bind to privileged ports to provide authentication of data transfer
[36malgo-1    |[0m # protocol.  Jsvc is not required if SASL is configured for authentication of
[36malgo-1    |[0m # data transfer protocol using non-privileged ports.
[36malgo-1    |[0m # export JSVC_HOME=/usr/bin
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # This directory contains pids for secure and privileged processes.
[36malgo-1    |[0m #export HADOOP_SECURE_PID_DIR=${HADOOP_PID_DIR}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # This directory contains the logs for secure and privileged processes.
[36malgo-1    |[0m # Java property: hadoop.log.dir
[36malgo-1    |[0m # export HADOOP_SECURE_LOG=${HADOOP_LOG_DIR}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # When running a secure daemon, the default value of HADOOP_IDENT_STRING
[36malgo-1    |[0m # ends up being a bit bogus.  Therefore, by default, the code will
[36malgo-1    |[0m # replace HADOOP_IDENT_STRING with HADOOP_xx_SECURE_USER.  If one wants
[36malgo-1    |[0m # to keep HADOOP_IDENT_STRING untouched, then uncomment this line.
[36malgo-1    |[0m # export HADOOP_SECURE_IDENT_PRESERVE="true"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # NameNode specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default log level and output location for file system related change
[36malgo-1    |[0m # messages. For non-namenode daemons, the Java property must be set in
[36malgo-1    |[0m # the appropriate _OPTS if one wants something other than INFO,NullAppender
[36malgo-1    |[0m # Java property: hdfs.audit.logger
[36malgo-1    |[0m # export HDFS_AUDIT_LOGGER=INFO,NullAppender
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the NameNode.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # a) Set JMX options
[36malgo-1    |[0m # export HDFS_NAMENODE_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
[36malgo-1    |[0m #
[36malgo-1    |[0m # b) Set garbage collection logs
[36malgo-1    |[0m # export HDFS_NAMENODE_OPTS="${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[36malgo-1    |[0m #
[36malgo-1    |[0m # c) ... or set them directly
[36malgo-1    |[0m # export HDFS_NAMENODE_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[36malgo-1    |[0m 
[36malgo-1    |[0m # this is the default:
[36malgo-1    |[0m # export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # SecondaryNameNode specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the SecondaryNameNode.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # This is the default:
[36malgo-1    |[0m # export HDFS_SECONDARYNAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # DataNode specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the DataNode.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # This is the default:
[36malgo-1    |[0m # export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS"
[36malgo-1    |[0m 
[36malgo-1    |[0m # On secure datanodes, user to run the datanode as after dropping privileges.
[36malgo-1    |[0m # This **MUST** be uncommented to enable secure HDFS if using privileged ports
[36malgo-1    |[0m # to provide authentication of data transfer protocol.  This **MUST NOT** be
[36malgo-1    |[0m # defined if SASL is configured for authentication of data transfer protocol
[36malgo-1    |[0m # using non-privileged ports.
[36malgo-1    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[36malgo-1    |[0m # export HDFS_DATANODE_SECURE_USER=hdfs
[36malgo-1    |[0m 
[36malgo-1    |[0m # Supplemental options for secure datanodes
[36malgo-1    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[36malgo-1    |[0m # server jvm.
[36malgo-1    |[0m # export HDFS_DATANODE_SECURE_EXTRA_OPTS="-jvm server"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # NFS3 Gateway specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the NFS3 Gateway.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_NFS3_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the Hadoop portmapper.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_PORTMAP_OPTS="-Xmx512m"
[36malgo-1    |[0m 
[36malgo-1    |[0m # Supplemental options for priviliged gateways
[36malgo-1    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[36malgo-1    |[0m # server jvm.
[36malgo-1    |[0m # export HDFS_NFS3_SECURE_EXTRA_OPTS="-jvm server"
[36malgo-1    |[0m 
[36malgo-1    |[0m # On privileged gateways, user to run the gateway as after dropping privileges
[36malgo-1    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[36malgo-1    |[0m # export HDFS_NFS3_SECURE_USER=nfsserver
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # ZKFailoverController specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the ZKFailoverController.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_ZKFC_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # QuorumJournalNode specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the QuorumJournalNode.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_JOURNALNODE_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # HDFS Balancer specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the HDFS Balancer.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_BALANCER_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # HDFS Mover specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the HDFS Mover.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_MOVER_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Router-based HDFS Federation specific parameters
[36malgo-1    |[0m # Specify the JVM options to be used when starting the RBF Routers.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_DFSROUTER_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # HDFS StorageContainerManager specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the HDFS Storage Container Manager.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_STORAGECONTAINERMANAGER_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Advanced Users Only!
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # When building Hadoop, one can add the class paths to the commands
[36malgo-1    |[0m # via this special env var:
[36malgo-1    |[0m # export HADOOP_ENABLE_BUILD_PATHS="true"
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # To prevent accidents, shell commands be (superficially) locked
[36malgo-1    |[0m # to only allow certain users to execute certain subcommands.
[36malgo-1    |[0m # It uses the format of (command)_(subcommand)_USER.
[36malgo-1    |[0m #
[36malgo-1    |[0m # For example, to limit who can execute the namenode command,
[36malgo-1    |[0m # export HDFS_NAMENODE_USER=hdfs
[36malgo-1    |[0m export SPARK_MASTER_HOST=172.21.0.2
[36malgo-1    |[0m export AWS_CONTAINER_CREDENTIALS_RELATIVE_URI=
[36malgo-1    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[36malgo-1    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/core-site.xml
[36malgo-1    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/core-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0" encoding="UTF-8"?>
[36malgo-1    |[0m  <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[36malgo-1    |[0m  <!-- Put site-specific property overrides in this file. -->
[36malgo-1    |[0m 
[36malgo-1    |[0m  <configuration>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>fs.defaultFS</name>
[36malgo-1    |[0m          <value>hdfs://172.21.0.2/</value>
[36malgo-1    |[0m          <description>NameNode URI</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>fs.s3a.aws.credentials.provider</name>
[36malgo-1    |[0m          <value>com.amazonaws.auth.DefaultAWSCredentialsProviderChain</value>
[36malgo-1    |[0m          <description>AWS S3 credential provider</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>fs.s3.impl</name>
[36malgo-1    |[0m          <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
[36malgo-1    |[0m          <description>s3a filesystem implementation</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>fs.AbstractFileSystem.s3a.imp</name>
[36malgo-1    |[0m          <value>org.apache.hadoop.fs.s3a.S3A</value>
[36malgo-1    |[0m          <description>s3a filesystem implementation</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m  
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>spark.executor.memory</name>
[36malgo-1    |[0m     <value>2g</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>spark.executor.cores</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 
[33malgo-2    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/hive/conf/hive-log4j2.properties is: 
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[33malgo-2    |[0m # or more contributor license agreements.  See the NOTICE file
[33malgo-2    |[0m # distributed with this work for additional information
[33malgo-2    |[0m # regarding copyright ownership.  The ASF licenses this file
[33malgo-2    |[0m # to you under the Apache License, Version 2.0 (the
[33malgo-2    |[0m # "License"); you may not use this file except in compliance
[33malgo-2    |[0m # with the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m status = INFO
[33malgo-2    |[0m name = HiveLog4j2
[33malgo-2    |[0m packages = org.apache.hadoop.hive.ql.log
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of properties
[33malgo-2    |[0m property.hive.log.level = INFO
[33malgo-2    |[0m property.hive.root.logger = DRFA
[33malgo-2    |[0m property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
[33malgo-2    |[0m property.hive.log.file = hive.log
[33malgo-2    |[0m property.hive.perflogger.log.level = INFO
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of all appenders
[33malgo-2    |[0m appenders = console, DRFA
[33malgo-2    |[0m 
[33malgo-2    |[0m # console appender
[33malgo-2    |[0m appender.console.type = Console
[33malgo-2    |[0m appender.console.name = console
[33malgo-2    |[0m appender.console.target = SYSTEM_ERR
[33malgo-2    |[0m appender.console.layout.type = PatternLayout
[33malgo-2    |[0m appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m # daily rolling file appender
[33malgo-2    |[0m appender.DRFA.type = RollingRandomAccessFile
[33malgo-2    |[0m appender.DRFA.name = DRFA
[33malgo-2    |[0m appender.DRFA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
[33malgo-2    |[0m # Use %pid in the filePattern to append <process-id>@<host-name> to the filename if you want separate log files for different CLI session
[33malgo-2    |[0m appender.DRFA.filePattern = ${sys:hive.log.dir}/${sys:hive.log.file}.%d{yyyy-MM-dd}
[33malgo-2    |[0m appender.DRFA.layout.type = PatternLayout
[33malgo-2    |[0m appender.DRFA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[33malgo-2    |[0m appender.DRFA.policies.type = Policies
[33malgo-2    |[0m appender.DRFA.policies.time.type = TimeBasedTriggeringPolicy
[33malgo-2    |[0m appender.DRFA.policies.time.interval = 1
[33malgo-2    |[0m appender.DRFA.policies.time.modulate = true
[33malgo-2    |[0m appender.DRFA.strategy.type = DefaultRolloverStrategy
[33malgo-2    |[0m appender.DRFA.strategy.max = 30
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of all loggers
[33malgo-2    |[0m loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, PerfLogger, AmazonAws, ApacheHttp
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
[33malgo-2    |[0m logger.NIOServerCnxn.level = WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
[33malgo-2    |[0m logger.ClientCnxnSocketNIO.level = WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.DataNucleus.name = DataNucleus
[33malgo-2    |[0m logger.DataNucleus.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.Datastore.name = Datastore
[33malgo-2    |[0m logger.Datastore.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.JPOX.name = JPOX
[33malgo-2    |[0m logger.JPOX.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.AmazonAws.name=com.amazonaws
[33malgo-2    |[0m logger.AmazonAws.level = INFO
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.ApacheHttp.name=org.apache.http
[33malgo-2    |[0m logger.ApacheHttp.level = INFO
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.PerfLogger.name = org.apache.hadoop.hive.ql.log.PerfLogger
[33malgo-2    |[0m logger.PerfLogger.level = ${sys:hive.perflogger.log.level}
[33malgo-2    |[0m 
[33malgo-2    |[0m # root logger
[33malgo-2    |[0m rootLogger.level = ${sys:hive.log.level}
[33malgo-2    |[0m rootLogger.appenderRefs = root
[33malgo-2    |[0m rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[36malgo-1    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/log4j.properties
[33malgo-2    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/hive/conf/hive-exec-log4j2.properties
[36malgo-1    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/log4j.properties is: 
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[36malgo-1    |[0m # or more contributor license agreements.  See the NOTICE file
[36malgo-1    |[0m # distributed with this work for additional information
[36malgo-1    |[0m # regarding copyright ownership.  The ASF licenses this file
[36malgo-1    |[0m # to you under the Apache License, Version 2.0 (the
[36malgo-1    |[0m # "License"); you may not use this file except in compliance
[36malgo-1    |[0m # with the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m # Define some default values that can be overridden by system properties
[36malgo-1    |[0m hadoop.root.logger=INFO,console
[36malgo-1    |[0m hadoop.log.dir=.
[36malgo-1    |[0m hadoop.log.file=hadoop.log
[36malgo-1    |[0m 
[36malgo-1    |[0m # Define the root logger to the system property "hadoop.root.logger".
[36malgo-1    |[0m log4j.rootLogger=${hadoop.root.logger}, EventCounter
[36malgo-1    |[0m 
[36malgo-1    |[0m # Logging Threshold
[36malgo-1    |[0m log4j.threshold=ALL
[36malgo-1    |[0m 
[36malgo-1    |[0m # Null Appender
[36malgo-1    |[0m log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Rolling File Appender - cap space usage at 5gb.
[36malgo-1    |[0m #
[36malgo-1    |[0m hadoop.log.maxfilesize=256MB
[36malgo-1    |[0m hadoop.log.maxbackupindex=20
[36malgo-1    |[0m log4j.appender.RFA=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.RFA.MaxFileSize=${hadoop.log.maxfilesize}
[36malgo-1    |[0m log4j.appender.RFA.MaxBackupIndex=${hadoop.log.maxbackupindex}
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.RFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m 
[36malgo-1    |[0m # Pattern format: Date LogLevel LoggerName LogMessage
[36malgo-1    |[0m log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[36malgo-1    |[0m # Debugging Pattern format
[36malgo-1    |[0m #log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Daily Rolling File Appender
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.DRFA.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m 
[36malgo-1    |[0m # Rollver at midnight
[36malgo-1    |[0m #log4j.appender.DRFA.DatePattern=.yyyy-MM-dd
[36malgo-1    |[0m # Rollver at every hour
[36malgo-1    |[0m log4j.appender.DRFA.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m 
[36malgo-1    |[0m # Pattern format: Date LogLevel LoggerName LogMessage
[36malgo-1    |[0m log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m # Debugging Pattern format
[36malgo-1    |[0m #log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # console
[36malgo-1    |[0m # Add "console" to rootlogger above if you want to use this
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.console=org.apache.log4j.ConsoleAppender
[36malgo-1    |[0m log4j.appender.console.target=System.err
[36malgo-1    |[0m log4j.appender.console.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # TaskLog Appender
[36malgo-1    |[0m #
[36malgo-1    |[0m log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.TLA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # HDFS block state change log from block manager
[36malgo-1    |[0m #
[36malgo-1    |[0m # Uncomment the following to log normal block state change
[36malgo-1    |[0m # messages from BlockManager in NameNode.
[36malgo-1    |[0m #log4j.logger.BlockStateChange=DEBUG
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m #Security appender
[36malgo-1    |[0m #
[36malgo-1    |[0m hadoop.security.logger=INFO,NullAppender
[36malgo-1    |[0m hadoop.security.log.maxfilesize=256MB
[36malgo-1    |[0m hadoop.security.log.maxbackupindex=20
[36malgo-1    |[0m log4j.category.SecurityLogger=${hadoop.security.logger}
[36malgo-1    |[0m hadoop.security.log.file=SecurityAuth-${user.name}.audit
[36malgo-1    |[0m log4j.appender.RFAS=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.RFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
[36malgo-1    |[0m log4j.appender.RFAS.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[36malgo-1    |[0m log4j.appender.RFAS.MaxFileSize=${hadoop.security.log.maxfilesize}
[36malgo-1    |[0m log4j.appender.RFAS.MaxBackupIndex=${hadoop.security.log.maxbackupindex}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Daily Rolling Security appender
[36malgo-1    |[0m #
[36malgo-1    |[0m log4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.DRFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
[36malgo-1    |[0m log4j.appender.DRFAS.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.DRFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[36malgo-1    |[0m log4j.appender.DRFAS.DatePattern=.yyyy-MM-dd
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # hadoop configuration logging
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m # Uncomment the following line to turn off configuration deprecation warnings.
[36malgo-1    |[0m # log4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # hdfs audit logging
[36malgo-1    |[0m #
[36malgo-1    |[0m hdfs.audit.logger=INFO,NullAppender
[36malgo-1    |[0m hdfs.audit.log.maxfilesize=256MB
[36malgo-1    |[0m hdfs.audit.log.maxbackupindex=20
[36malgo-1    |[0m log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}
[36malgo-1    |[0m log4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false
[36malgo-1    |[0m log4j.appender.RFAAUDIT=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.RFAAUDIT.File=${hadoop.log.dir}/hdfs-audit.log
[36malgo-1    |[0m log4j.appender.RFAAUDIT.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.RFAAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[36malgo-1    |[0m log4j.appender.RFAAUDIT.MaxFileSize=${hdfs.audit.log.maxfilesize}
[36malgo-1    |[0m log4j.appender.RFAAUDIT.MaxBackupIndex=${hdfs.audit.log.maxbackupindex}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # NameNode metrics logging.
[36malgo-1    |[0m # The default is to retain two namenode-metrics.log files up to 64MB each.
[36malgo-1    |[0m #
[36malgo-1    |[0m namenode.metrics.logger=INFO,NullAppender
[36malgo-1    |[0m log4j.logger.NameNodeMetricsLog=${namenode.metrics.logger}
[36malgo-1    |[0m log4j.additivity.NameNodeMetricsLog=false
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.File=${hadoop.log.dir}/namenode-metrics.log
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.MaxBackupIndex=1
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.MaxFileSize=64MB
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # DataNode metrics logging.
[36malgo-1    |[0m # The default is to retain two datanode-metrics.log files up to 64MB each.
[36malgo-1    |[0m #
[36malgo-1    |[0m datanode.metrics.logger=INFO,NullAppender
[36malgo-1    |[0m log4j.logger.DataNodeMetricsLog=${datanode.metrics.logger}
[36malgo-1    |[0m log4j.additivity.DataNodeMetricsLog=false
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.File=${hadoop.log.dir}/datanode-metrics.log
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.MaxBackupIndex=1
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.MaxFileSize=64MB
[36malgo-1    |[0m 
[36malgo-1    |[0m # Custom Logging levels
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=DEBUG
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m # AWS SDK & S3A FileSystem
[36malgo-1    |[0m #log4j.logger.com.amazonaws=ERROR
[36malgo-1    |[0m log4j.logger.com.amazonaws.http.AmazonHttpClient=ERROR
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.fs.s3a.S3AFileSystem=WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Event Counter Appender
[36malgo-1    |[0m # Sends counts of logging messages at different severity levels to Hadoop Metrics.
[36malgo-1    |[0m #
[36malgo-1    |[0m log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Job Summary Appender
[36malgo-1    |[0m #
[36malgo-1    |[0m # Use following logger to send summary to separate file defined by
[36malgo-1    |[0m # hadoop.mapreduce.jobsummary.log.file :
[36malgo-1    |[0m # hadoop.mapreduce.jobsummary.logger=INFO,JSA
[36malgo-1    |[0m # 
[36malgo-1    |[0m hadoop.mapreduce.jobsummary.logger=${hadoop.root.logger}
[36malgo-1    |[0m hadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log
[36malgo-1    |[0m hadoop.mapreduce.jobsummary.log.maxfilesize=256MB
[36malgo-1    |[0m hadoop.mapreduce.jobsummary.log.maxbackupindex=20
[36malgo-1    |[0m log4j.appender.JSA=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.JSA.File=${hadoop.log.dir}/${hadoop.mapreduce.jobsummary.log.file}
[36malgo-1    |[0m log4j.appender.JSA.MaxFileSize=${hadoop.mapreduce.jobsummary.log.maxfilesize}
[36malgo-1    |[0m log4j.appender.JSA.MaxBackupIndex=${hadoop.mapreduce.jobsummary.log.maxbackupindex}
[36malgo-1    |[0m log4j.appender.JSA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.JSA.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
[36malgo-1    |[0m log4j.appender.JSA.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.logger.org.apache.hadoop.mapred.JobInProgress$JobSummary=${hadoop.mapreduce.jobsummary.logger}
[36malgo-1    |[0m log4j.additivity.org.apache.hadoop.mapred.JobInProgress$JobSummary=false
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # shuffle connection log from shuffleHandler
[36malgo-1    |[0m # Uncomment the following line to enable logging of shuffle connections
[36malgo-1    |[0m # log4j.logger.org.apache.hadoop.mapred.ShuffleHandler.audit=DEBUG
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Yarn ResourceManager Application Summary Log
[36malgo-1    |[0m #
[36malgo-1    |[0m # Set the ResourceManager summary log filename
[36malgo-1    |[0m yarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log
[36malgo-1    |[0m # Set the ResourceManager summary log level and appender
[36malgo-1    |[0m yarn.server.resourcemanager.appsummary.logger=${hadoop.root.logger}
[36malgo-1    |[0m #yarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY
[36malgo-1    |[0m 
[36malgo-1    |[0m # To enable AppSummaryLogging for the RM,
[36malgo-1    |[0m # set yarn.server.resourcemanager.appsummary.logger to
[36malgo-1    |[0m # <LEVEL>,RMSUMMARY in hadoop-env.sh
[36malgo-1    |[0m 
[36malgo-1    |[0m # Appender for ResourceManager Application Summary Log
[36malgo-1    |[0m # Requires the following properties to be set
[36malgo-1    |[0m #    - hadoop.log.dir (Hadoop Log directory)
[36malgo-1    |[0m #    - yarn.server.resourcemanager.appsummary.log.file (resource manager app summary log filename)
[36malgo-1    |[0m #    - yarn.server.resourcemanager.appsummary.logger (resource manager app summary log level and appender)
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=${yarn.server.resourcemanager.appsummary.logger}
[36malgo-1    |[0m log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=false
[36malgo-1    |[0m log4j.appender.RMSUMMARY=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.RMSUMMARY.File=${hadoop.log.dir}/${yarn.server.resourcemanager.appsummary.log.file}
[36malgo-1    |[0m log4j.appender.RMSUMMARY.MaxFileSize=256MB
[36malgo-1    |[0m log4j.appender.RMSUMMARY.MaxBackupIndex=20
[36malgo-1    |[0m log4j.appender.RMSUMMARY.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.RMSUMMARY.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m # HS audit log configs
[36malgo-1    |[0m #mapreduce.hs.audit.logger=INFO,HSAUDIT
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=${mapreduce.hs.audit.logger}
[36malgo-1    |[0m #log4j.additivity.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=false
[36malgo-1    |[0m #log4j.appender.HSAUDIT=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m #log4j.appender.HSAUDIT.File=${hadoop.log.dir}/hs-audit.log
[36malgo-1    |[0m #log4j.appender.HSAUDIT.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m #log4j.appender.HSAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[36malgo-1    |[0m #log4j.appender.HSAUDIT.DatePattern=.yyyy-MM-dd
[36malgo-1    |[0m 
[36malgo-1    |[0m # Http Server Request Logs
[36malgo-1    |[0m #log4j.logger.http.requests.namenode=INFO,namenoderequestlog
[36malgo-1    |[0m #log4j.appender.namenoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.namenoderequestlog.Filename=${hadoop.log.dir}/jetty-namenode-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.namenoderequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.http.requests.datanode=INFO,datanoderequestlog
[36malgo-1    |[0m #log4j.appender.datanoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.datanoderequestlog.Filename=${hadoop.log.dir}/jetty-datanode-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.datanoderequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.http.requests.resourcemanager=INFO,resourcemanagerrequestlog
[36malgo-1    |[0m #log4j.appender.resourcemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.resourcemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-resourcemanager-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.resourcemanagerrequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.http.requests.jobhistory=INFO,jobhistoryrequestlog
[36malgo-1    |[0m #log4j.appender.jobhistoryrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.jobhistoryrequestlog.Filename=${hadoop.log.dir}/jetty-jobhistory-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.jobhistoryrequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.http.requests.nodemanager=INFO,nodemanagerrequestlog
[36malgo-1    |[0m #log4j.appender.nodemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.nodemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-nodemanager-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.nodemanagerrequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m # WebHdfs request log on datanodes
[36malgo-1    |[0m # Specify -Ddatanode.webhdfs.logger=INFO,HTTPDRFA on datanode startup to
[36malgo-1    |[0m # direct the log to a separate file.
[36malgo-1    |[0m #datanode.webhdfs.logger=INFO,console
[36malgo-1    |[0m #log4j.logger.datanode.webhdfs=${datanode.webhdfs.logger}
[36malgo-1    |[0m #log4j.appender.HTTPDRFA=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m #log4j.appender.HTTPDRFA.File=${hadoop.log.dir}/hadoop-datanode-webhdfs.log
[36malgo-1    |[0m #log4j.appender.HTTPDRFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m #log4j.appender.HTTPDRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[36malgo-1    |[0m #log4j.appender.HTTPDRFA.DatePattern=.yyyy-MM-dd
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m # Appender for viewing information for errors and warnings
[36malgo-1    |[0m yarn.ewma.cleanupInterval=300
[36malgo-1    |[0m yarn.ewma.messageAgeLimitSeconds=86400
[36malgo-1    |[0m yarn.ewma.maxUniqueMessages=250
[36malgo-1    |[0m log4j.appender.EWMA=org.apache.hadoop.yarn.util.Log4jWarningErrorMetricsAppender
[36malgo-1    |[0m log4j.appender.EWMA.cleanupInterval=${yarn.ewma.cleanupInterval}
[36malgo-1    |[0m log4j.appender.EWMA.messageAgeLimitSeconds=${yarn.ewma.messageAgeLimitSeconds}
[36malgo-1    |[0m log4j.appender.EWMA.maxUniqueMessages=${yarn.ewma.maxUniqueMessages}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Fair scheduler state dump
[36malgo-1    |[0m #
[36malgo-1    |[0m # Use following logger to dump the state to a separate file
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=DEBUG,FSSTATEDUMP
[36malgo-1    |[0m #log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=false
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.File=${hadoop.log.dir}/fairscheduler-statedump.log
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.MaxFileSize=${hadoop.log.maxfilesize}
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.MaxBackupIndex=${hadoop.log.maxbackupindex}
[36malgo-1    |[0m 
[36malgo-1    |[0m # Log levels of third-party libraries
[36malgo-1    |[0m log4j.logger.org.apache.commons.beanutils=WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m #AWS SDK Logging
[36malgo-1    |[0m log4j.logger.com.amazonaws=WARN
[36malgo-1    |[0m log4j.logger.org.apache.zookeeper=ERROR
[36malgo-1    |[0m log4j.logger.org.apache.hadoop.hbase=WARN
[36malgo-1    |[0m log4j.logger.amazon.emr.metrics=WARN
[36malgo-1    |[0m log4j.logger.emr=INFO
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.HADOOP=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.HADOOP.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m log4j.appender.HADOOP.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.appender.HADOOP.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.HADOOP.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.JHS=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.JHS.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m log4j.appender.JHS.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.appender.JHS.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.JHS.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.MAPRED=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.MAPRED.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m log4j.appender.MAPRED.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.appender.MAPRED.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.MAPRED.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.YARN=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.YARN.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m log4j.appender.YARN.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.appender.YARN.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.YARN.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/hive/conf/hive-env.sh
[33malgo-2    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/hive/conf/hive-exec-log4j2.properties is: 
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[33malgo-2    |[0m # or more contributor license agreements.  See the NOTICE file
[33malgo-2    |[0m # distributed with this work for additional information
[33malgo-2    |[0m # regarding copyright ownership.  The ASF licenses this file
[33malgo-2    |[0m # to you under the Apache License, Version 2.0 (the
[33malgo-2    |[0m # "License"); you may not use this file except in compliance
[33malgo-2    |[0m # with the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m status = INFO
[33malgo-2    |[0m name = HiveExecLog4j2
[33malgo-2    |[0m packages = org.apache.hadoop.hive.ql.log
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of properties
[33malgo-2    |[0m property.hive.log.level = INFO
[33malgo-2    |[0m property.hive.root.logger = FA
[33malgo-2    |[0m property.hive.query.id = hadoop
[33malgo-2    |[0m property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
[33malgo-2    |[0m property.hive.log.file = ${sys:hive.query.id}.log
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of all appenders
[33malgo-2    |[0m appenders = console, FA
[33malgo-2    |[0m 
[33malgo-2    |[0m # console appender
[33malgo-2    |[0m appender.console.type = Console
[33malgo-2    |[0m appender.console.name = console
[33malgo-2    |[0m appender.console.target = SYSTEM_ERR
[33malgo-2    |[0m appender.console.layout.type = PatternLayout
[33malgo-2    |[0m appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m # simple file appender
[33malgo-2    |[0m appender.FA.type = RandomAccessFile
[33malgo-2    |[0m appender.FA.name = FA
[33malgo-2    |[0m appender.FA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
[33malgo-2    |[0m appender.FA.layout.type = PatternLayout
[33malgo-2    |[0m appender.FA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of all loggers
[33malgo-2    |[0m loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
[33malgo-2    |[0m logger.NIOServerCnxn.level = WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
[33malgo-2    |[0m logger.ClientCnxnSocketNIO.level = WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.DataNucleus.name = DataNucleus
[33malgo-2    |[0m logger.DataNucleus.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.Datastore.name = Datastore
[33malgo-2    |[0m logger.Datastore.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.JPOX.name = JPOX
[33malgo-2    |[0m logger.JPOX.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m # root logger
[33malgo-2    |[0m rootLogger.level = ${sys:hive.log.level}
[33malgo-2    |[0m rootLogger.appenderRefs = root
[33malgo-2    |[0m rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[36malgo-1    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/hive/conf/hive-env.sh is: 
[36malgo-1    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[36malgo-1    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[36malgo-1    |[0m 
[33malgo-2    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/hive/conf/hive-site.xml
[33malgo-2    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/hive/conf/hive-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0"?>
[33malgo-2    |[0m <!--
[33malgo-2    |[0m   Licensed to the Apache Software Foundation (ASF) under one or more
[33malgo-2    |[0m   contributor license agreements.  See the NOTICE file distributed with
[33malgo-2    |[0m   this work for additional information regarding copyright ownership.
[33malgo-2    |[0m   The ASF licenses this file to You under the Apache License, Version 2.0
[33malgo-2    |[0m   (the "License"); you may not use this file except in compliance with
[33malgo-2    |[0m   the License.  You may obtain a copy of the License at
[33malgo-2    |[0m 
[33malgo-2    |[0m       http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m 
[33malgo-2    |[0m   Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m   distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m   See the License for the specific language governing permissions and
[33malgo-2    |[0m   limitations under the License.
[33malgo-2    |[0m -->
[33malgo-2    |[0m <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[33malgo-2    |[0m 
[33malgo-2    |[0m <configuration>
[33malgo-2    |[0m 
[33malgo-2    |[0m <!-- Hive Configuration can either be stored in this file or in the hadoop configuration files  -->
[33malgo-2    |[0m <!-- that are implied by Hadoop setup variables.                                                -->
[33malgo-2    |[0m <!-- Aside from Hadoop setup variables - this file is provided as a convenience so that Hive    -->
[33malgo-2    |[0m <!-- users do not have to edit hadoop configuration files (that may be managed as a centralized -->
[33malgo-2    |[0m <!-- resource).                                                                                 -->
[33malgo-2    |[0m 
[33malgo-2    |[0m <!-- Hive Execution Parameters -->
[33malgo-2    |[0m 
[33malgo-2    |[0m <property>
[33malgo-2    |[0m   <name>javax.jdo.option.ConnectionURL</name>
[33malgo-2    |[0m   <value>jdbc:derby:;databaseName=/var/lib/hive/metastore/metastore_db;create=true</value>
[33malgo-2    |[0m   <description>JDBC connect string for a JDBC metastore</description>
[33malgo-2    |[0m </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m <property>
[33malgo-2    |[0m   <name>javax.jdo.option.ConnectionDriverName</name>
[33malgo-2    |[0m   <value>org.apache.derby.jdbc.EmbeddedDriver</value>
[33malgo-2    |[0m   <description>Driver class name for a JDBC metastore</description>
[33malgo-2    |[0m </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>key</name>
[33malgo-2    |[0m     <value>value</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 
[36malgo-1    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/hive/conf/hive-log4j2.properties
[33malgo-2    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/hive/conf/hive-log4j2.properties is: 
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[36malgo-1    |[0m # or more contributor license agreements.  See the NOTICE file
[36malgo-1    |[0m # distributed with this work for additional information
[36malgo-1    |[0m # regarding copyright ownership.  The ASF licenses this file
[36malgo-1    |[0m # to you under the Apache License, Version 2.0 (the
[36malgo-1    |[0m # "License"); you may not use this file except in compliance
[36malgo-1    |[0m # with the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m status = INFO
[36malgo-1    |[0m name = HiveLog4j2
[36malgo-1    |[0m packages = org.apache.hadoop.hive.ql.log
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of properties
[36malgo-1    |[0m property.hive.log.level = INFO
[36malgo-1    |[0m property.hive.root.logger = DRFA
[36malgo-1    |[0m property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
[36malgo-1    |[0m property.hive.log.file = hive.log
[36malgo-1    |[0m property.hive.perflogger.log.level = INFO
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of all appenders
[36malgo-1    |[0m appenders = console, DRFA
[36malgo-1    |[0m 
[36malgo-1    |[0m # console appender
[36malgo-1    |[0m appender.console.type = Console
[36malgo-1    |[0m appender.console.name = console
[36malgo-1    |[0m appender.console.target = SYSTEM_ERR
[36malgo-1    |[0m appender.console.layout.type = PatternLayout
[36malgo-1    |[0m appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m # daily rolling file appender
[36malgo-1    |[0m appender.DRFA.type = RollingRandomAccessFile
[36malgo-1    |[0m appender.DRFA.name = DRFA
[36malgo-1    |[0m appender.DRFA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
[36malgo-1    |[0m # Use %pid in the filePattern to append <process-id>@<host-name> to the filename if you want separate log files for different CLI session
[36malgo-1    |[0m appender.DRFA.filePattern = ${sys:hive.log.dir}/${sys:hive.log.file}.%d{yyyy-MM-dd}
[36malgo-1    |[0m appender.DRFA.layout.type = PatternLayout
[36malgo-1    |[0m appender.DRFA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[36malgo-1    |[0m appender.DRFA.policies.type = Policies
[36malgo-1    |[0m appender.DRFA.policies.time.type = TimeBasedTriggeringPolicy
[36malgo-1    |[0m appender.DRFA.policies.time.interval = 1
[36malgo-1    |[0m appender.DRFA.policies.time.modulate = true
[36malgo-1    |[0m appender.DRFA.strategy.type = DefaultRolloverStrategy
[36malgo-1    |[0m appender.DRFA.strategy.max = 30
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of all loggers
[36malgo-1    |[0m loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, PerfLogger, AmazonAws, ApacheHttp
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
[36malgo-1    |[0m logger.NIOServerCnxn.level = WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
[36malgo-1    |[0m logger.ClientCnxnSocketNIO.level = WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.DataNucleus.name = DataNucleus
[36malgo-1    |[0m logger.DataNucleus.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.Datastore.name = Datastore
[36malgo-1    |[0m logger.Datastore.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.JPOX.name = JPOX
[36malgo-1    |[0m logger.JPOX.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.AmazonAws.name=com.amazonaws
[36malgo-1    |[0m logger.AmazonAws.level = INFO
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.ApacheHttp.name=org.apache.http
[36malgo-1    |[0m logger.ApacheHttp.level = INFO
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.PerfLogger.name = org.apache.hadoop.hive.ql.log.PerfLogger
[36malgo-1    |[0m logger.PerfLogger.level = ${sys:hive.perflogger.log.level}
[36malgo-1    |[0m 
[36malgo-1    |[0m # root logger
[36malgo-1    |[0m rootLogger.level = ${sys:hive.log.level}
[36malgo-1    |[0m rootLogger.appenderRefs = root
[36malgo-1    |[0m rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[33malgo-2    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[33malgo-2    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.driver.host=172.21.0.2
[33malgo-2    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[33malgo-2    |[0m 
[33malgo-2    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[33malgo-2    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[33malgo-2    |[0m spark.rpc.askTimeout=300s
[33malgo-2    |[0m spark.driver.memory 2048m
[33malgo-2    |[0m spark.driver.memoryOverhead 204m
[33malgo-2    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[33malgo-2    |[0m spark.executor.memory 12399m
[33malgo-2    |[0m spark.executor.memoryOverhead 1239m
[33malgo-2    |[0m spark.executor.cores 4
[33malgo-2    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[33malgo-2    |[0m spark.executor.instances 2
[33malgo-2    |[0m spark.default.parallelism 16
[33malgo-2    |[0m spark.executor.memory 2g
[33malgo-2    |[0m spark.executor.cores 1
[33malgo-2    |[0m key value
[33malgo-2    |[0m 
[36malgo-1    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/hive/conf/hive-exec-log4j2.properties
[33malgo-2    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/spark/conf/spark-env.sh
[36malgo-1    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/hive/conf/hive-exec-log4j2.properties is: 
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[36malgo-1    |[0m # or more contributor license agreements.  See the NOTICE file
[36malgo-1    |[0m # distributed with this work for additional information
[36malgo-1    |[0m # regarding copyright ownership.  The ASF licenses this file
[36malgo-1    |[0m # to you under the Apache License, Version 2.0 (the
[36malgo-1    |[0m # "License"); you may not use this file except in compliance
[36malgo-1    |[0m # with the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m status = INFO
[36malgo-1    |[0m name = HiveExecLog4j2
[36malgo-1    |[0m packages = org.apache.hadoop.hive.ql.log
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of properties
[36malgo-1    |[0m property.hive.log.level = INFO
[36malgo-1    |[0m property.hive.root.logger = FA
[36malgo-1    |[0m property.hive.query.id = hadoop
[36malgo-1    |[0m property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
[36malgo-1    |[0m property.hive.log.file = ${sys:hive.query.id}.log
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of all appenders
[36malgo-1    |[0m appenders = console, FA
[36malgo-1    |[0m 
[36malgo-1    |[0m # console appender
[36malgo-1    |[0m appender.console.type = Console
[36malgo-1    |[0m appender.console.name = console
[36malgo-1    |[0m appender.console.target = SYSTEM_ERR
[36malgo-1    |[0m appender.console.layout.type = PatternLayout
[36malgo-1    |[0m appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m # simple file appender
[36malgo-1    |[0m appender.FA.type = RandomAccessFile
[36malgo-1    |[0m appender.FA.name = FA
[36malgo-1    |[0m appender.FA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
[36malgo-1    |[0m appender.FA.layout.type = PatternLayout
[36malgo-1    |[0m appender.FA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of all loggers
[36malgo-1    |[0m loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
[36malgo-1    |[0m logger.NIOServerCnxn.level = WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
[36malgo-1    |[0m logger.ClientCnxnSocketNIO.level = WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.DataNucleus.name = DataNucleus
[36malgo-1    |[0m logger.DataNucleus.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.Datastore.name = Datastore
[36malgo-1    |[0m logger.Datastore.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.JPOX.name = JPOX
[36malgo-1    |[0m logger.JPOX.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m # root logger
[36malgo-1    |[0m rootLogger.level = ${sys:hive.log.level}
[36malgo-1    |[0m rootLogger.appenderRefs = root
[36malgo-1    |[0m rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/hive/conf/hive-site.xml
[33malgo-2    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/spark/conf/spark-env.sh is: 
[33malgo-2    |[0m #EMPTY FILE AVOID OVERRIDDING ENV VARS
[33malgo-2    |[0m # Specifically, without copying the empty file, SPARK_HISTORY_OPTS will be overriden, 
[33malgo-2    |[0m # spark.history.ui.port defaults to 18082, and spark.eventLog.dir defaults to local fs
[33malgo-2    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[33malgo-2    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[33malgo-2    |[0m 
[36malgo-1    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/hive/conf/hive-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0"?>
[36malgo-1    |[0m <!--
[36malgo-1    |[0m   Licensed to the Apache Software Foundation (ASF) under one or more
[36malgo-1    |[0m   contributor license agreements.  See the NOTICE file distributed with
[36malgo-1    |[0m   this work for additional information regarding copyright ownership.
[36malgo-1    |[0m   The ASF licenses this file to You under the Apache License, Version 2.0
[36malgo-1    |[0m   (the "License"); you may not use this file except in compliance with
[36malgo-1    |[0m   the License.  You may obtain a copy of the License at
[36malgo-1    |[0m 
[36malgo-1    |[0m       http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m 
[36malgo-1    |[0m   Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m   distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m   See the License for the specific language governing permissions and
[36malgo-1    |[0m   limitations under the License.
[36malgo-1    |[0m -->
[36malgo-1    |[0m <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[36malgo-1    |[0m 
[36malgo-1    |[0m <configuration>
[36malgo-1    |[0m 
[36malgo-1    |[0m <!-- Hive Configuration can either be stored in this file or in the hadoop configuration files  -->
[36malgo-1    |[0m <!-- that are implied by Hadoop setup variables.                                                -->
[36malgo-1    |[0m <!-- Aside from Hadoop setup variables - this file is provided as a convenience so that Hive    -->
[36malgo-1    |[0m <!-- users do not have to edit hadoop configuration files (that may be managed as a centralized -->
[36malgo-1    |[0m <!-- resource).                                                                                 -->
[36malgo-1    |[0m 
[36malgo-1    |[0m <!-- Hive Execution Parameters -->
[36malgo-1    |[0m 
[36malgo-1    |[0m <property>
[36malgo-1    |[0m   <name>javax.jdo.option.ConnectionURL</name>
[36malgo-1    |[0m   <value>jdbc:derby:;databaseName=/var/lib/hive/metastore/metastore_db;create=true</value>
[36malgo-1    |[0m   <description>JDBC connect string for a JDBC metastore</description>
[36malgo-1    |[0m </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m <property>
[36malgo-1    |[0m   <name>javax.jdo.option.ConnectionDriverName</name>
[36malgo-1    |[0m   <value>org.apache.derby.jdbc.EmbeddedDriver</value>
[36malgo-1    |[0m   <description>Driver class name for a JDBC metastore</description>
[36malgo-1    |[0m </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>key</name>
[36malgo-1    |[0m     <value>value</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 
[33malgo-2    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/spark/conf/log4j.properties
[36malgo-1    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[36malgo-1    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.driver.host=172.21.0.2
[36malgo-1    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[36malgo-1    |[0m 
[36malgo-1    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[36malgo-1    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[36malgo-1    |[0m spark.rpc.askTimeout=300s
[36malgo-1    |[0m spark.driver.memory 2048m
[36malgo-1    |[0m spark.driver.memoryOverhead 204m
[36malgo-1    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[36malgo-1    |[0m spark.executor.memory 12399m
[36malgo-1    |[0m spark.executor.memoryOverhead 1239m
[36malgo-1    |[0m spark.executor.cores 4
[36malgo-1    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[36malgo-1    |[0m spark.executor.instances 2
[36malgo-1    |[0m spark.default.parallelism 16
[36malgo-1    |[0m spark.executor.memory 2g
[36malgo-1    |[0m spark.executor.cores 1
[36malgo-1    |[0m key value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/spark/conf/spark-env.sh
[33malgo-2    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/spark/conf/log4j.properties is: 
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[36malgo-1    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/spark/conf/spark-env.sh is: 
[36malgo-1    |[0m #EMPTY FILE AVOID OVERRIDDING ENV VARS
[36malgo-1    |[0m # Specifically, without copying the empty file, SPARK_HISTORY_OPTS will be overriden, 
[36malgo-1    |[0m # spark.history.ui.port defaults to 18082, and spark.eventLog.dir defaults to local fs
[36malgo-1    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[36malgo-1    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/spark/conf/log4j.properties
[33malgo-2    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/spark/conf/hive-site.xml
[36malgo-1    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/spark/conf/log4j.properties is: 
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/spark/conf/hive-site.xml
[33malgo-2    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/spark/conf/hive-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0"?>
[33malgo-2    |[0m <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[33malgo-2    |[0m <configuration>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>key</name>
[33malgo-2    |[0m     <value>value</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m </configuration>
[36malgo-1    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/spark/conf/hive-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0"?>
[36malgo-1    |[0m <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[36malgo-1    |[0m <configuration>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>key</name>
[36malgo-1    |[0m     <value>value</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m </configuration>
[33malgo-2    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/spark/conf/metrics.properties
[36malgo-1    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/spark/conf/metrics.properties
[36malgo-1    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/spark/conf/metrics.properties is: 
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[33malgo-2    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/spark/conf/metrics.properties is: 
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[36malgo-1    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[33malgo-2    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[36malgo-1    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0"?>
[36malgo-1    |[0m <!-- Site specific YARN configuration properties -->
[36malgo-1    |[0m  <configuration>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.resourcemanager.hostname</name>
[36malgo-1    |[0m          <value>172.21.0.2</value>
[36malgo-1    |[0m          <description>The hostname of the RM.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.hostname</name>
[36malgo-1    |[0m          <value>algo-1</value>
[36malgo-1    |[0m          <description>The hostname of the NM.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.webapp.address</name>
[36malgo-1    |[0m          <value>algo-1:8042</value>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.vmem-pmem-ratio</name>
[36malgo-1    |[0m          <value>5</value>
[36malgo-1    |[0m          <description>Ratio between virtual memory to physical memory.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.resourcemanager.am.max-attempts</name>
[36malgo-1    |[0m          <value>1</value>
[36malgo-1    |[0m          <description>The maximum number of application attempts.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.env-whitelist</name>
[36malgo-1    |[0m          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>
[36malgo-1    |[0m          <description>Environment variable whitelist</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m  
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.minimum-allocation-mb</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.maximum-allocation-mb</name>
[36malgo-1    |[0m     <value>15892</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.minimum-allocation-vcores</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.maximum-allocation-vcores</name>
[36malgo-1    |[0m     <value>4</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.nodemanager.resource.memory-mb</name>
[36malgo-1    |[0m     <value>15892</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.nodemanager.resource.cpu-vcores</name>
[36malgo-1    |[0m     <value>4</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>key</name>
[36malgo-1    |[0m     <value>value</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/yarn-env.sh
[36malgo-1    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-env.sh is: 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one or more
[36malgo-1    |[0m # contributor license agreements.  See the NOTICE file distributed with
[36malgo-1    |[0m # this work for additional information regarding copyright ownership.
[36malgo-1    |[0m # The ASF licenses this file to You under the Apache License, Version 2.0
[36malgo-1    |[0m # (the "License"); you may not use this file except in compliance with
[36malgo-1    |[0m # the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## THIS FILE ACTS AS AN OVERRIDE FOR hadoop-env.sh FOR ALL
[36malgo-1    |[0m ## WORK DONE BY THE yarn AND RELATED COMMANDS.
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## Precedence rules:
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## yarn-env.sh > hadoop-env.sh > hard-coded defaults
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## YARN_xyz > HADOOP_xyz > hard-coded defaults
[36malgo-1    |[0m ##
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Resource Manager specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the max heapsize for the ResourceManager.  If no units are
[36malgo-1    |[0m # given, it will be assumed to be in MB.
[36malgo-1    |[0m # This value will be overridden by an Xmx setting specified in either
[36malgo-1    |[0m # HADOOP_OPTS and/or YARN_RESOURCEMANAGER_OPTS.
[36malgo-1    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX
[36malgo-1    |[0m #export YARN_RESOURCEMANAGER_HEAPSIZE=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the ResourceManager.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # Examples for a Sun/Oracle JDK:
[36malgo-1    |[0m # a) override the appsummary log file:
[36malgo-1    |[0m # export YARN_RESOURCEMANAGER_OPTS="-Dyarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log -Dyarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY"
[36malgo-1    |[0m #
[36malgo-1    |[0m # b) Set JMX options
[36malgo-1    |[0m # export YARN_RESOURCEMANAGER_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
[36malgo-1    |[0m #
[36malgo-1    |[0m # c) Set garbage collection logs from hadoop-env.sh
[36malgo-1    |[0m # export YARN_RESOURCE_MANAGER_OPTS="${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[36malgo-1    |[0m #
[36malgo-1    |[0m # d) ... or set them directly
[36malgo-1    |[0m # export YARN_RESOURCEMANAGER_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[36malgo-1    |[0m #
[36malgo-1    |[0m #
[36malgo-1    |[0m # export YARN_RESOURCEMANAGER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Node Manager specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the max heapsize for the NodeManager.  If no units are
[36malgo-1    |[0m # given, it will be assumed to be in MB.
[36malgo-1    |[0m # This value will be overridden by an Xmx setting specified in either
[36malgo-1    |[0m # HADOOP_OPTS and/or YARN_NODEMANAGER_OPTS.
[36malgo-1    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[36malgo-1    |[0m #export YARN_NODEMANAGER_HEAPSIZE=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the NodeManager.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_NODEMANAGER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # TimeLineServer specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the max heapsize for the timelineserver.  If no units are
[36malgo-1    |[0m # given, it will be assumed to be in MB.
[36malgo-1    |[0m # This value will be overridden by an Xmx setting specified in either
[36malgo-1    |[0m # HADOOP_OPTS and/or YARN_TIMELINESERVER_OPTS.
[36malgo-1    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[36malgo-1    |[0m #export YARN_TIMELINE_HEAPSIZE=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the TimeLineServer.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_TIMELINESERVER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # TimeLineReader specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the TimeLineReader.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_TIMELINEREADER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Web App Proxy Server specifc parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the max heapsize for the web app proxy server.  If no units are
[36malgo-1    |[0m # given, it will be assumed to be in MB.
[36malgo-1    |[0m # This value will be overridden by an Xmx setting specified in either
[36malgo-1    |[0m # HADOOP_OPTS and/or YARN_PROXYSERVER_OPTS.
[36malgo-1    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[36malgo-1    |[0m #export YARN_PROXYSERVER_HEAPSIZE=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the proxy server.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_PROXYSERVER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Shared Cache Manager specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the
[36malgo-1    |[0m # shared cache manager server.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_SHAREDCACHEMANAGER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Router specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the Router.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_ROUTER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Registry DNS specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # For privileged registry DNS, user to run as after dropping privileges
[36malgo-1    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[36malgo-1    |[0m # export YARN_REGISTRYDNS_SECURE_USER=yarn
[36malgo-1    |[0m 
[36malgo-1    |[0m # Supplemental options for privileged registry DNS
[36malgo-1    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[36malgo-1    |[0m # server jvm.
[36malgo-1    |[0m # export YARN_REGISTRYDNS_SECURE_EXTRA_OPTS="-jvm server"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # YARN Services parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Directory containing service examples
[36malgo-1    |[0m # export YARN_SERVICE_EXAMPLES_DIR = $HADOOP_YARN_HOME/share/hadoop/yarn/yarn-service-examples
[36malgo-1    |[0m # export YARN_CONTAINER_RUNTIME_DOCKER_RUN_OVERRIDE_DISABLE=true
[36malgo-1    |[0m export YARN_LOG_DIR=/var/log/yarn/export HADOOP_DATANODE_HEAPSIZE=2048
[36malgo-1    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[36malgo-1    |[0m 
[33malgo-2    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0"?>
[33malgo-2    |[0m <!-- Site specific YARN configuration properties -->
[33malgo-2    |[0m  <configuration>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.resourcemanager.hostname</name>
[33malgo-2    |[0m          <value>172.21.0.2</value>
[33malgo-2    |[0m          <description>The hostname of the RM.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.hostname</name>
[33malgo-2    |[0m          <value>algo-2</value>
[33malgo-2    |[0m          <description>The hostname of the NM.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.webapp.address</name>
[33malgo-2    |[0m          <value>algo-2:8042</value>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.vmem-pmem-ratio</name>
[33malgo-2    |[0m          <value>5</value>
[33malgo-2    |[0m          <description>Ratio between virtual memory to physical memory.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.resourcemanager.am.max-attempts</name>
[33malgo-2    |[0m          <value>1</value>
[33malgo-2    |[0m          <description>The maximum number of application attempts.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.env-whitelist</name>
[33malgo-2    |[0m          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>
[33malgo-2    |[0m          <description>Environment variable whitelist</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m  
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.minimum-allocation-mb</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.maximum-allocation-mb</name>
[33malgo-2    |[0m     <value>15892</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.minimum-allocation-vcores</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.maximum-allocation-vcores</name>
[33malgo-2    |[0m     <value>4</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.nodemanager.resource.memory-mb</name>
[33malgo-2    |[0m     <value>15892</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.nodemanager.resource.cpu-vcores</name>
[33malgo-2    |[0m     <value>4</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>key</name>
[33malgo-2    |[0m     <value>value</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:53 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/yarn-env.sh
[33malgo-2    |[0m 11-14 16:53 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-env.sh is: 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one or more
[33malgo-2    |[0m # contributor license agreements.  See the NOTICE file distributed with
[33malgo-2    |[0m # this work for additional information regarding copyright ownership.
[33malgo-2    |[0m # The ASF licenses this file to You under the Apache License, Version 2.0
[33malgo-2    |[0m # (the "License"); you may not use this file except in compliance with
[33malgo-2    |[0m # the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## THIS FILE ACTS AS AN OVERRIDE FOR hadoop-env.sh FOR ALL
[33malgo-2    |[0m ## WORK DONE BY THE yarn AND RELATED COMMANDS.
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## Precedence rules:
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## yarn-env.sh > hadoop-env.sh > hard-coded defaults
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## YARN_xyz > HADOOP_xyz > hard-coded defaults
[33malgo-2    |[0m ##
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Resource Manager specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the max heapsize for the ResourceManager.  If no units are
[33malgo-2    |[0m # given, it will be assumed to be in MB.
[33malgo-2    |[0m # This value will be overridden by an Xmx setting specified in either
[33malgo-2    |[0m # HADOOP_OPTS and/or YARN_RESOURCEMANAGER_OPTS.
[33malgo-2    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX
[33malgo-2    |[0m #export YARN_RESOURCEMANAGER_HEAPSIZE=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the ResourceManager.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # Examples for a Sun/Oracle JDK:
[33malgo-2    |[0m # a) override the appsummary log file:
[33malgo-2    |[0m # export YARN_RESOURCEMANAGER_OPTS="-Dyarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log -Dyarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY"
[33malgo-2    |[0m #
[33malgo-2    |[0m # b) Set JMX options
[33malgo-2    |[0m # export YARN_RESOURCEMANAGER_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
[33malgo-2    |[0m #
[33malgo-2    |[0m # c) Set garbage collection logs from hadoop-env.sh
[33malgo-2    |[0m # export YARN_RESOURCE_MANAGER_OPTS="${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[33malgo-2    |[0m #
[33malgo-2    |[0m # d) ... or set them directly
[33malgo-2    |[0m # export YARN_RESOURCEMANAGER_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[33malgo-2    |[0m #
[33malgo-2    |[0m #
[33malgo-2    |[0m # export YARN_RESOURCEMANAGER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Node Manager specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the max heapsize for the NodeManager.  If no units are
[33malgo-2    |[0m # given, it will be assumed to be in MB.
[33malgo-2    |[0m # This value will be overridden by an Xmx setting specified in either
[33malgo-2    |[0m # HADOOP_OPTS and/or YARN_NODEMANAGER_OPTS.
[33malgo-2    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[33malgo-2    |[0m #export YARN_NODEMANAGER_HEAPSIZE=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the NodeManager.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_NODEMANAGER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # TimeLineServer specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the max heapsize for the timelineserver.  If no units are
[33malgo-2    |[0m # given, it will be assumed to be in MB.
[33malgo-2    |[0m # This value will be overridden by an Xmx setting specified in either
[33malgo-2    |[0m # HADOOP_OPTS and/or YARN_TIMELINESERVER_OPTS.
[33malgo-2    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[33malgo-2    |[0m #export YARN_TIMELINE_HEAPSIZE=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the TimeLineServer.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_TIMELINESERVER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # TimeLineReader specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the TimeLineReader.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_TIMELINEREADER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Web App Proxy Server specifc parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the max heapsize for the web app proxy server.  If no units are
[33malgo-2    |[0m # given, it will be assumed to be in MB.
[33malgo-2    |[0m # This value will be overridden by an Xmx setting specified in either
[33malgo-2    |[0m # HADOOP_OPTS and/or YARN_PROXYSERVER_OPTS.
[33malgo-2    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[33malgo-2    |[0m #export YARN_PROXYSERVER_HEAPSIZE=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the proxy server.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_PROXYSERVER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Shared Cache Manager specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the
[33malgo-2    |[0m # shared cache manager server.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_SHAREDCACHEMANAGER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Router specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the Router.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_ROUTER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Registry DNS specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # For privileged registry DNS, user to run as after dropping privileges
[33malgo-2    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[33malgo-2    |[0m # export YARN_REGISTRYDNS_SECURE_USER=yarn
[33malgo-2    |[0m 
[33malgo-2    |[0m # Supplemental options for privileged registry DNS
[33malgo-2    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[33malgo-2    |[0m # server jvm.
[33malgo-2    |[0m # export YARN_REGISTRYDNS_SECURE_EXTRA_OPTS="-jvm server"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # YARN Services parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Directory containing service examples
[33malgo-2    |[0m # export YARN_SERVICE_EXAMPLES_DIR = $HADOOP_YARN_HOME/share/hadoop/yarn/yarn-service-examples
[33malgo-2    |[0m # export YARN_CONTAINER_RUNTIME_DOCKER_RUN_OVERRIDE_DISABLE=true
[33malgo-2    |[0m export YARN_LOG_DIR=/var/log/yarn/export HADOOP_DATANODE_HEAPSIZE=2048
[33malgo-2    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:53 smspark-submit INFO     waiting for cluster to be up
[33malgo-2    |[0m WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.
[36malgo-1    |[0m WARNING: HADOOP_NAMENODE_OPTS has been replaced by HDFS_NAMENODE_OPTS. Using value of HADOOP_NAMENODE_OPTS.
[33malgo-2    |[0m WARNING: /usr/lib/hadoop/logs does not exist. Creating.
[33malgo-2    |[0m WARNING: /var/log/yarn/export does not exist. Creating.
[36malgo-1    |[0m WARNING: /usr/lib/hadoop/logs does not exist. Creating.
[33malgo-2    |[0m 2021-11-14 16:53:48,405 INFO nodemanager.NodeManager: STARTUP_MSG: 
[33malgo-2    |[0m /************************************************************
[33malgo-2    |[0m STARTUP_MSG: Starting NodeManager
[33malgo-2    |[0m STARTUP_MSG:   host = algo-2/172.21.0.3
[33malgo-2    |[0m STARTUP_MSG:   args = []
[33malgo-2    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[33malgo-2    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar
[33malgo-2    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[33malgo-2    |[0m STARTUP_MSG:   java = 1.8.0_302
[33malgo-2    |[0m ************************************************************/
[33malgo-2    |[0m 2021-11-14 16:53:48,414 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 16:53:48,441 INFO namenode.NameNode: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting NameNode
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/172.21.0.2
[36malgo-1    |[0m STARTUP_MSG:   args = [-format, -force]
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 16:53:48,449 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
[33malgo-2    |[0m 2021-11-14 16:53:48,464 INFO datanode.DataNode: STARTUP_MSG: 
[33malgo-2    |[0m /************************************************************
[33malgo-2    |[0m STARTUP_MSG: Starting DataNode
[33malgo-2    |[0m STARTUP_MSG:   host = algo-2/172.21.0.3
[33malgo-2    |[0m STARTUP_MSG:   args = []
[33malgo-2    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[33malgo-2    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar
[33malgo-2    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[33malgo-2    |[0m STARTUP_MSG:   java = 1.8.0_302
[33malgo-2    |[0m ************************************************************/
[33malgo-2    |[0m 2021-11-14 16:53:48,472 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 16:53:48,522 INFO namenode.NameNode: createNameNode [-format, -force]
[33malgo-2    |[0m 2021-11-14 16:53:48,787 INFO resourceplugin.ResourcePluginManager: No Resource plugins found from configuration!
[33malgo-2    |[0m 2021-11-14 16:53:48,787 INFO resourceplugin.ResourcePluginManager: Found Resource plugins from configuration: null
[33malgo-2    |[0m 2021-11-14 16:53:48,843 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode
[33malgo-2    |[0m 2021-11-14 16:53:48,855 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.
[33malgo-2    |[0m 2021-11-14 16:53:48,906 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher
[33malgo-2    |[0m 2021-11-14 16:53:48,907 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher
[33malgo-2    |[0m 2021-11-14 16:53:48,908 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper
[33malgo-2    |[0m 2021-11-14 16:53:48,909 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices
[36malgo-1    |[0m Formatting using clusterid: CID-6c9e34c9-2f47-48db-b81c-b360d8566205
[33malgo-2    |[0m 2021-11-14 16:53:48,909 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl
[33malgo-2    |[0m 2021-11-14 16:53:48,910 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher
[33malgo-2    |[0m 2021-11-14 16:53:48,910 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler
[33malgo-2    |[0m 2021-11-14 16:53:48,912 INFO tracker.NMLogAggregationStatusTracker: the rolling interval seconds for the NodeManager Cached Log aggregation status is 600
[33malgo-2    |[0m 2021-11-14 16:53:48,930 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl
[33malgo-2    |[0m 2021-11-14 16:53:48,930 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager
[33malgo-2    |[0m 2021-11-14 16:53:48,931 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m 2021-11-14 16:53:48,947 INFO namenode.FSEditLog: Edit logging is async:true
[36malgo-1    |[0m 2021-11-14 16:53:48,961 INFO namenode.FSNamesystem: KeyProvider: null
[36malgo-1    |[0m 2021-11-14 16:53:48,962 INFO namenode.FSNamesystem: fsLock is fair: true
[36malgo-1    |[0m 2021-11-14 16:53:48,963 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
[36malgo-1    |[0m 2021-11-14 16:53:48,968 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 16:53:48,968 INFO namenode.FSNamesystem: supergroup          = supergroup
[36malgo-1    |[0m 2021-11-14 16:53:48,968 INFO namenode.FSNamesystem: isPermissionEnabled = true
[36malgo-1    |[0m 2021-11-14 16:53:48,968 INFO namenode.FSNamesystem: HA Enabled: false
[33malgo-2    |[0m 2021-11-14 16:53:48,984 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[33malgo-2    |[0m 2021-11-14 16:53:49,007 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[33malgo-2    |[0m 2021-11-14 16:53:49,007 INFO impl.MetricsSystemImpl: DataNode metrics system started
[36malgo-1    |[0m 2021-11-14 16:53:49,016 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[36malgo-1    |[0m 2021-11-14 16:53:49,028 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
[36malgo-1    |[0m 2021-11-14 16:53:49,029 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
[36malgo-1    |[0m 2021-11-14 16:53:49,033 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
[36malgo-1    |[0m 2021-11-14 16:53:49,033 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Nov 14 16:53:49
[36malgo-1    |[0m 2021-11-14 16:53:49,035 INFO util.GSet: Computing capacity for map BlocksMap
[36malgo-1    |[0m 2021-11-14 16:53:49,035 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 16:53:49,036 INFO util.GSet: 2.0% max memory 6.5 GB = 133.6 MB
[36malgo-1    |[0m 2021-11-14 16:53:49,036 INFO util.GSet: capacity      = 2^24 = 16777216 entries
[33malgo-2    |[0m 2021-11-14 16:53:49,058 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[33malgo-2    |[0m 2021-11-14 16:53:49,058 INFO impl.MetricsSystemImpl: NodeManager metrics system started
[33malgo-2    |[0m 2021-11-14 16:53:49,075 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.
[36malgo-1    |[0m 2021-11-14 16:53:49,081 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
[36malgo-1    |[0m 2021-11-14 16:53:49,081 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
[33malgo-2    |[0m 2021-11-14 16:53:49,084 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.
[36malgo-1    |[0m 2021-11-14 16:53:49,089 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
[36malgo-1    |[0m 2021-11-14 16:53:49,089 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
[36malgo-1    |[0m 2021-11-14 16:53:49,089 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
[36malgo-1    |[0m 2021-11-14 16:53:49,089 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
[36malgo-1    |[0m 2021-11-14 16:53:49,090 INFO blockmanagement.BlockManager: defaultReplication         = 3
[36malgo-1    |[0m 2021-11-14 16:53:49,090 INFO blockmanagement.BlockManager: maxReplication             = 512
[36malgo-1    |[0m 2021-11-14 16:53:49,090 INFO blockmanagement.BlockManager: minReplication             = 1
[36malgo-1    |[0m 2021-11-14 16:53:49,090 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
[36malgo-1    |[0m 2021-11-14 16:53:49,090 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
[36malgo-1    |[0m 2021-11-14 16:53:49,090 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
[36malgo-1    |[0m 2021-11-14 16:53:49,090 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
[33malgo-2    |[0m 2021-11-14 16:53:49,116 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@4c60d6e9
[33malgo-2    |[0m 2021-11-14 16:53:49,118 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler
[36malgo-1    |[0m 2021-11-14 16:53:49,118 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
[36malgo-1    |[0m 2021-11-14 16:53:49,118 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 16:53:49,118 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 16:53:49,118 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
[33malgo-2    |[0m 2021-11-14 16:53:49,119 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService
[33malgo-2    |[0m 2021-11-14 16:53:49,119 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled
[33malgo-2    |[0m 2021-11-14 16:53:49,119 INFO localizer.ResourceLocalizationService: per directory file limit = 8192
[36malgo-1    |[0m 2021-11-14 16:53:49,132 INFO util.GSet: Computing capacity for map INodeMap
[36malgo-1    |[0m 2021-11-14 16:53:49,132 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 16:53:49,132 INFO util.GSet: 1.0% max memory 6.5 GB = 66.8 MB
[36malgo-1    |[0m 2021-11-14 16:53:49,132 INFO util.GSet: capacity      = 2^23 = 8388608 entries
[33malgo-2    |[0m 2021-11-14 16:53:49,140 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker
[33malgo-2    |[0m 2021-11-14 16:53:49,147 INFO resources.ResourceHandlerModule: Using traffic control bandwidth handler
[33malgo-2    |[0m 2021-11-14 16:53:49,151 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@221a3fa4
[33malgo-2    |[0m 2021-11-14 16:53:49,151 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null
[36malgo-1    |[0m 2021-11-14 16:53:49,152 INFO namenode.FSDirectory: ACLs enabled? false
[36malgo-1    |[0m 2021-11-14 16:53:49,152 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
[36malgo-1    |[0m 2021-11-14 16:53:49,152 INFO namenode.FSDirectory: XAttrs enabled? true
[36malgo-1    |[0m 2021-11-14 16:53:49,152 INFO namenode.NameNode: Caching file names occurring more than 10 times
[33malgo-2    |[0m 2021-11-14 16:53:49,153 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true
[33malgo-2    |[0m 2021-11-14 16:53:49,153 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true
[33malgo-2    |[0m 2021-11-14 16:53:49,153 INFO monitor.ContainersMonitorImpl: Elastic memory control enabled: false
[33malgo-2    |[0m 2021-11-14 16:53:49,153 INFO monitor.ContainersMonitorImpl: Strict memory control enabled: true
[33malgo-2    |[0m 2021-11-14 16:53:49,153 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true
[33malgo-2    |[0m 2021-11-14 16:53:49,156 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.
[36malgo-1    |[0m 2021-11-14 16:53:49,157 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
[36malgo-1    |[0m 2021-11-14 16:53:49,159 INFO snapshot.SnapshotManager: SkipList is disabled
[36malgo-1    |[0m 2021-11-14 16:53:49,163 INFO util.GSet: Computing capacity for map cachedBlocks
[36malgo-1    |[0m 2021-11-14 16:53:49,164 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 16:53:49,164 INFO util.GSet: 0.25% max memory 6.5 GB = 16.7 MB
[36malgo-1    |[0m 2021-11-14 16:53:49,164 INFO util.GSet: capacity      = 2^21 = 2097152 entries
[33malgo-2    |[0m 2021-11-14 16:53:49,172 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[36malgo-1    |[0m 2021-11-14 16:53:49,172 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
[36malgo-1    |[0m 2021-11-14 16:53:49,172 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
[36malgo-1    |[0m 2021-11-14 16:53:49,172 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
[33malgo-2    |[0m 2021-11-14 16:53:49,175 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
[36malgo-1    |[0m 2021-11-14 16:53:49,176 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
[36malgo-1    |[0m 2021-11-14 16:53:49,176 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
[36malgo-1    |[0m 2021-11-14 16:53:49,177 INFO util.GSet: Computing capacity for map NameNodeRetryCache
[36malgo-1    |[0m 2021-11-14 16:53:49,177 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 16:53:49,178 INFO util.GSet: 0.029999999329447746% max memory 6.5 GB = 2.0 MB
[36malgo-1    |[0m 2021-11-14 16:53:49,178 INFO util.GSet: capacity      = 2^18 = 262144 entries
[33malgo-2    |[0m 2021-11-14 16:53:49,181 INFO datanode.DataNode: Configured hostname is algo-2
[33malgo-2    |[0m 2021-11-14 16:53:49,181 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[33malgo-2    |[0m 2021-11-14 16:53:49,186 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0
[33malgo-2    |[0m 2021-11-14 16:53:49,190 INFO conf.Configuration: resource-types.xml not found
[33malgo-2    |[0m 2021-11-14 16:53:49,191 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
[33malgo-2    |[0m 2021-11-14 16:53:49,196 INFO conf.Configuration: node-resources.xml not found
[33malgo-2    |[0m 2021-11-14 16:53:49,196 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.
[33malgo-2    |[0m 2021-11-14 16:53:49,198 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:15892, vCores:4>
[33malgo-2    |[0m 2021-11-14 16:53:49,202 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=15892 virtual-memory=79460 virtual-cores=4
[36malgo-1    |[0m 2021-11-14 16:53:49,205 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1213576767-172.21.0.2-1636908829197
[33malgo-2    |[0m 2021-11-14 16:53:49,205 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866
[33malgo-2    |[0m 2021-11-14 16:53:49,207 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s
[33malgo-2    |[0m 2021-11-14 16:53:49,207 INFO datanode.DataNode: Number threads for balancing is 50
[36malgo-1    |[0m 2021-11-14 16:53:49,222 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.
[33malgo-2    |[0m 2021-11-14 16:53:49,238 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[33malgo-2    |[0m 2021-11-14 16:53:49,245 INFO util.log: Logging initialized @1264ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m 2021-11-14 16:53:49,249 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression
[33malgo-2    |[0m 2021-11-14 16:53:49,256 INFO ipc.Server: Starting Socket Reader #1 for port 0
[36malgo-1    |[0m 2021-11-14 16:53:49,334 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .
[36malgo-1    |[0m 2021-11-14 16:53:49,345 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
[36malgo-1    |[0m 2021-11-14 16:53:49,350 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
[36malgo-1    |[0m 2021-11-14 16:53:49,350 INFO namenode.NameNode: SHUTDOWN_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m SHUTDOWN_MSG: Shutting down NameNode at algo-1/172.21.0.2
[36malgo-1    |[0m ************************************************************/
[33malgo-2    |[0m 2021-11-14 16:53:49,378 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[33malgo-2    |[0m 2021-11-14 16:53:49,386 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined
[33malgo-2    |[0m 2021-11-14 16:53:49,392 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[33malgo-2    |[0m 2021-11-14 16:53:49,393 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
[33malgo-2    |[0m 2021-11-14 16:53:49,393 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[33malgo-2    |[0m 2021-11-14 16:53:49,393 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 11-14 16:53 smspark-submit INFO     waiting for cluster to be up
[33malgo-2    |[0m 2021-11-14 16:53:49,420 INFO http.HttpServer2: Jetty bound to port 41931
[33malgo-2    |[0m 2021-11-14 16:53:49,421 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[33malgo-2    |[0m 2021-11-14 16:53:49,443 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server
[33malgo-2    |[0m 2021-11-14 16:53:49,444 INFO ipc.Server: IPC Server Responder: starting
[33malgo-2    |[0m 2021-11-14 16:53:49,444 INFO ipc.Server: IPC Server listener on 0: starting
[33malgo-2    |[0m 2021-11-14 16:53:49,444 INFO server.session: DefaultSessionIdManager workerName=node0
[33malgo-2    |[0m 2021-11-14 16:53:49,444 INFO server.session: No SessionScavenger set, using defaults
[33malgo-2    |[0m 2021-11-14 16:53:49,446 INFO server.session: node0 Scavenging every 600000ms
[33malgo-2    |[0m 2021-11-14 16:53:49,450 INFO security.NMContainerTokenSecretManager: Updating node address : algo-2:44783
[36malgo-1    |[0m WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.
[33malgo-2    |[0m 2021-11-14 16:53:49,456 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 500, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[33malgo-2    |[0m 2021-11-14 16:53:49,457 INFO ipc.Server: Starting Socket Reader #1 for port 8040
[33malgo-2    |[0m 2021-11-14 16:53:49,457 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a3793c7{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}
[33malgo-2    |[0m 2021-11-14 16:53:49,457 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@bb9e6dc{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}
[36malgo-1    |[0m WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.
[33malgo-2    |[0m 2021-11-14 16:53:49,459 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server
[33malgo-2    |[0m 2021-11-14 16:53:49,460 INFO ipc.Server: IPC Server Responder: starting
[33malgo-2    |[0m 2021-11-14 16:53:49,460 INFO ipc.Server: IPC Server listener on 8040: starting
[33malgo-2    |[0m 2021-11-14 16:53:49,461 INFO localizer.ResourceLocalizationService: Localizer started on port 8040
[33malgo-2    |[0m 2021-11-14 16:53:49,463 INFO containermanager.ContainerManagerImpl: ContainerManager started at /172.21.0.3:44783
[33malgo-2    |[0m 2021-11-14 16:53:49,463 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-2/172.21.0.3:0
[33malgo-2    |[0m 2021-11-14 16:53:49,463 WARN tracker.NMLogAggregationStatusTracker: Log Aggregation is disabled.So is the LogAggregationStatusTracker.
[33malgo-2    |[0m 2021-11-14 16:53:49,467 INFO webapp.WebServer: Instantiating NMWebApp at algo-2:8042
[36malgo-1    |[0m WARNING: HADOOP_NAMENODE_OPTS has been replaced by HDFS_NAMENODE_OPTS. Using value of HADOOP_NAMENODE_OPTS.
[36malgo-1    |[0m WARNING: /var/log/yarn/export does not exist. Creating.
[33malgo-2    |[0m 2021-11-14 16:53:49,494 INFO util.log: Logging initialized @1513ms to org.eclipse.jetty.util.log.Slf4jLog
[33malgo-2    |[0m 2021-11-14 16:53:49,512 INFO util.TypeUtil: JVM Runtime does not support Modules
[33malgo-2    |[0m 2021-11-14 16:53:49,521 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@338c99c8{datanode,/,file:///usr/lib/hadoop-hdfs/webapps/datanode/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/datanode}
[33malgo-2    |[0m 2021-11-14 16:53:49,528 INFO server.AbstractConnector: Started ServerConnector@6c372fe6{HTTP/1.1,[http/1.1]}{localhost:41931}
[33malgo-2    |[0m 2021-11-14 16:53:49,528 INFO server.Server: Started @1547ms
[33malgo-2    |[0m 2021-11-14 16:53:49,586 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[33malgo-2    |[0m 2021-11-14 16:53:49,590 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined
[33malgo-2    |[0m 2021-11-14 16:53:49,597 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[33malgo-2    |[0m 2021-11-14 16:53:49,599 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node
[33malgo-2    |[0m 2021-11-14 16:53:49,599 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[33malgo-2    |[0m 2021-11-14 16:53:49,599 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[33malgo-2    |[0m 2021-11-14 16:53:49,600 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context node
[33malgo-2    |[0m 2021-11-14 16:53:49,600 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
[33malgo-2    |[0m 2021-11-14 16:53:49,600 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
[33malgo-2    |[0m 2021-11-14 16:53:49,602 INFO http.HttpServer2: adding path spec: /node/*
[33malgo-2    |[0m 2021-11-14 16:53:49,602 INFO http.HttpServer2: adding path spec: /ws/*
[33malgo-2    |[0m 2021-11-14 16:53:49,667 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864
[33malgo-2    |[0m 2021-11-14 16:53:49,672 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[33malgo-2    |[0m 2021-11-14 16:53:49,673 INFO datanode.DataNode: dnUserName = root
[33malgo-2    |[0m 2021-11-14 16:53:49,673 INFO datanode.DataNode: supergroup = supergroup
[33malgo-2    |[0m 2021-11-14 16:53:49,716 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[33malgo-2    |[0m 2021-11-14 16:53:49,732 INFO ipc.Server: Starting Socket Reader #1 for port 9867
[33malgo-2    |[0m 2021-11-14 16:53:49,970 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:9867
[33malgo-2    |[0m 2021-11-14 16:53:49,972 INFO webapp.WebApps: Registered webapp guice modules
[33malgo-2    |[0m 2021-11-14 16:53:49,976 INFO http.HttpServer2: Jetty bound to port 8042
[33malgo-2    |[0m 2021-11-14 16:53:49,977 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[33malgo-2    |[0m 2021-11-14 16:53:49,991 INFO datanode.DataNode: Refresh request received for nameservices: null
[33malgo-2    |[0m 2021-11-14 16:53:50,002 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>
[33malgo-2    |[0m 2021-11-14 16:53:50,007 INFO server.session: DefaultSessionIdManager workerName=node0
[33malgo-2    |[0m 2021-11-14 16:53:50,007 INFO server.session: No SessionScavenger set, using defaults
[33malgo-2    |[0m 2021-11-14 16:53:50,009 INFO server.session: node0 Scavenging every 660000ms
[33malgo-2    |[0m 2021-11-14 16:53:50,013 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1.spark-network/172.21.0.2:8020 starting to offer service
[36malgo-1    |[0m 2021-11-14 16:53:50,013 INFO resourcemanager.ResourceManager: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting ResourceManager
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/172.21.0.2
[36malgo-1    |[0m STARTUP_MSG:   args = []
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[33malgo-2    |[0m 2021-11-14 16:53:50,020 INFO ipc.Server: IPC Server Responder: starting
[33malgo-2    |[0m 2021-11-14 16:53:50,020 INFO ipc.Server: IPC Server listener on 9867: starting
[33malgo-2    |[0m 2021-11-14 16:53:50,025 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 16:53:50,028 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]
[33malgo-2    |[0m 2021-11-14 16:53:50,030 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4bff64c2{logs,/logs,file:///var/log/yarn/export/,AVAILABLE}
[33malgo-2    |[0m 2021-11-14 16:53:50,031 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3cc41abc{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/static,AVAILABLE}
[33malgo-2    |[0m 2021-11-14 16:53:50,045 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException
[36malgo-1    |[0m 2021-11-14 16:53:50,052 INFO nodemanager.NodeManager: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting NodeManager
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/172.21.0.2
[36malgo-1    |[0m STARTUP_MSG:   args = []
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 16:53:50,063 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 16:53:50,063 INFO namenode.NameNode: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting NameNode
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/172.21.0.2
[36malgo-1    |[0m STARTUP_MSG:   args = []
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 16:53:50,075 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 16:53:50,099 INFO datanode.DataNode: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting DataNode
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/172.21.0.2
[36malgo-1    |[0m STARTUP_MSG:   args = []
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 16:53:50,107 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 16:53:50,158 INFO namenode.NameNode: createNameNode []
[33malgo-2    |[0m 2021-11-14 16:53:50,175 INFO util.TypeUtil: JVM Runtime does not support Modules
[33malgo-2    |[0m Nov 14, 2021 4:53:50 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[33malgo-2    |[0m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class
[33malgo-2    |[0m Nov 14, 2021 4:53:50 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[33malgo-2    |[0m INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
[33malgo-2    |[0m Nov 14, 2021 4:53:50 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[33malgo-2    |[0m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class
[33malgo-2    |[0m Nov 14, 2021 4:53:50 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
[33malgo-2    |[0m INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'
[36malgo-1    |[0m 2021-11-14 16:53:50,314 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[33malgo-2    |[0m Nov 14, 2021 4:53:50 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[33malgo-2    |[0m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 16:53:50,427 INFO conf.Configuration: found resource core-site.xml at file:/etc/hadoop/conf.empty/core-site.xml
[36malgo-1    |[0m 2021-11-14 16:53:50,442 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m 2021-11-14 16:53:50,442 INFO impl.MetricsSystemImpl: NameNode metrics system started
[36malgo-1    |[0m 2021-11-14 16:53:50,469 INFO conf.Configuration: resource-types.xml not found
[36malgo-1    |[0m 2021-11-14 16:53:50,470 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
[36malgo-1    |[0m 2021-11-14 16:53:50,471 INFO namenode.NameNodeUtils: fs.defaultFS is hdfs://172.21.0.2/
[36malgo-1    |[0m 2021-11-14 16:53:50,513 INFO resourceplugin.ResourcePluginManager: No Resource plugins found from configuration!
[36malgo-1    |[0m 2021-11-14 16:53:50,513 INFO resourceplugin.ResourcePluginManager: Found Resource plugins from configuration: null
[36malgo-1    |[0m 2021-11-14 16:53:50,515 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode
[36malgo-1    |[0m 2021-11-14 16:53:50,523 INFO conf.Configuration: found resource yarn-site.xml at file:/etc/hadoop/conf.empty/yarn-site.xml
[36malgo-1    |[0m 2021-11-14 16:53:50,539 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher
[33malgo-2    |[0m Nov 14, 2021 4:53:50 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[33malgo-2    |[0m INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 16:53:50,585 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms
[36malgo-1    |[0m 2021-11-14 16:53:50,590 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms
[36malgo-1    |[0m 2021-11-14 16:53:50,597 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms
[36malgo-1    |[0m 2021-11-14 16:53:50,599 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.
[36malgo-1    |[0m 2021-11-14 16:53:50,623 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 16:53:50,626 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m 2021-11-14 16:53:50,653 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:9870
[36malgo-1    |[0m 2021-11-14 16:53:50,656 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher
[36malgo-1    |[0m 2021-11-14 16:53:50,657 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler
[36malgo-1    |[0m 2021-11-14 16:53:50,657 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher
[36malgo-1    |[0m 2021-11-14 16:53:50,658 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper
[36malgo-1    |[0m 2021-11-14 16:53:50,659 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices
[36malgo-1    |[0m 2021-11-14 16:53:50,659 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl
[36malgo-1    |[0m 2021-11-14 16:53:50,660 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher
[36malgo-1    |[0m 2021-11-14 16:53:50,660 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler
[36malgo-1    |[0m 2021-11-14 16:53:50,662 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager
[36malgo-1    |[0m 2021-11-14 16:53:50,662 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
[36malgo-1    |[0m 2021-11-14 16:53:50,663 INFO tracker.NMLogAggregationStatusTracker: the rolling interval seconds for the NodeManager Cached Log aggregation status is 600
[36malgo-1    |[0m 2021-11-14 16:53:50,669 INFO util.log: Logging initialized @1178ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m 2021-11-14 16:53:50,687 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl
[36malgo-1    |[0m 2021-11-14 16:53:50,687 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager
[36malgo-1    |[0m 2021-11-14 16:53:50,714 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher
[36malgo-1    |[0m 2021-11-14 16:53:50,715 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher
[36malgo-1    |[0m 2021-11-14 16:53:50,717 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher
[36malgo-1    |[0m 2021-11-14 16:53:50,719 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher
[36malgo-1    |[0m 2021-11-14 16:53:50,733 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m 2021-11-14 16:53:50,734 INFO impl.MetricsSystemImpl: DataNode metrics system started
[36malgo-1    |[0m 2021-11-14 16:53:50,761 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m 2021-11-14 16:53:50,816 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 16:53:50,819 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m 2021-11-14 16:53:50,838 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined
[36malgo-1    |[0m 2021-11-14 16:53:50,850 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[36malgo-1    |[0m 2021-11-14 16:53:50,852 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
[36malgo-1    |[0m 2021-11-14 16:53:50,853 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[36malgo-1    |[0m 2021-11-14 16:53:50,853 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 2021-11-14 16:53:50,867 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m 2021-11-14 16:53:50,867 INFO impl.MetricsSystemImpl: NodeManager metrics system started
[36malgo-1    |[0m 2021-11-14 16:53:50,893 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.
[36malgo-1    |[0m 2021-11-14 16:53:50,901 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
[36malgo-1    |[0m 2021-11-14 16:53:50,902 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
[36malgo-1    |[0m 2021-11-14 16:53:50,904 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.
[36malgo-1    |[0m 2021-11-14 16:53:50,914 INFO http.HttpServer2: Jetty bound to port 9870
[33malgo-2    |[0m Nov 14, 2021 4:53:50 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[33malgo-2    |[0m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 16:53:50,916 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[36malgo-1    |[0m 2021-11-14 16:53:50,948 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@4c60d6e9
[36malgo-1    |[0m 2021-11-14 16:53:50,951 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler
[36malgo-1    |[0m 2021-11-14 16:53:50,952 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService
[36malgo-1    |[0m 2021-11-14 16:53:50,953 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled
[36malgo-1    |[0m 2021-11-14 16:53:50,953 INFO localizer.ResourceLocalizationService: per directory file limit = 8192
[36malgo-1    |[0m 2021-11-14 16:53:50,953 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m 2021-11-14 16:53:50,953 INFO impl.MetricsSystemImpl: ResourceManager metrics system started
[33malgo-2    |[0m 2021-11-14 16:53:50,956 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5d5b5fa7{node,/,file:///tmp/jetty-algo-2-8042-_-any-4092756198828658432.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/node}
[33malgo-2    |[0m 2021-11-14 16:53:50,968 INFO server.AbstractConnector: Started ServerConnector@3f390d63{HTTP/1.1,[http/1.1]}{algo-2:8042}
[33malgo-2    |[0m 2021-11-14 16:53:50,968 INFO server.Server: Started @2987ms
[33malgo-2    |[0m 2021-11-14 16:53:50,969 INFO webapp.WebApps: Web app node started at 8042
[36malgo-1    |[0m 2021-11-14 16:53:50,978 INFO server.session: DefaultSessionIdManager workerName=node0
[36malgo-1    |[0m 2021-11-14 16:53:50,978 INFO server.session: No SessionScavenger set, using defaults
[36malgo-1    |[0m 2021-11-14 16:53:50,980 INFO server.session: node0 Scavenging every 600000ms
[33malgo-2    |[0m 2021-11-14 16:53:50,981 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-2:44783
[33malgo-2    |[0m 2021-11-14 16:53:50,982 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 16:53:50,984 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker
[33malgo-2    |[0m 2021-11-14 16:53:50,989 INFO client.RMProxy: Connecting to ResourceManager at /172.21.0.2:8031
[36malgo-1    |[0m 2021-11-14 16:53:50,990 INFO resources.ResourceHandlerModule: Using traffic control bandwidth handler
[36malgo-1    |[0m 2021-11-14 16:53:50,994 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@221a3fa4
[36malgo-1    |[0m 2021-11-14 16:53:50,994 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6d60fe40{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 16:53:50,995 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@73eb439a{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 16:53:50,995 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null
[36malgo-1    |[0m 2021-11-14 16:53:50,996 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true
[36malgo-1    |[0m 2021-11-14 16:53:50,996 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true
[36malgo-1    |[0m 2021-11-14 16:53:50,996 INFO monitor.ContainersMonitorImpl: Elastic memory control enabled: false
[36malgo-1    |[0m 2021-11-14 16:53:50,997 INFO monitor.ContainersMonitorImpl: Strict memory control enabled: true
[36malgo-1    |[0m 2021-11-14 16:53:50,997 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true
[36malgo-1    |[0m 2021-11-14 16:53:51,000 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.
[36malgo-1    |[0m 2021-11-14 16:53:51,006 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[36malgo-1    |[0m 2021-11-14 16:53:51,009 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
[36malgo-1    |[0m 2021-11-14 16:53:51,015 INFO datanode.DataNode: Configured hostname is algo-1
[36malgo-1    |[0m 2021-11-14 16:53:51,016 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[36malgo-1    |[0m 2021-11-14 16:53:51,012 INFO security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.
[36malgo-1    |[0m 2021-11-14 16:53:51,020 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0
[36malgo-1    |[0m 2021-11-14 16:53:51,021 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager
[36malgo-1    |[0m 2021-11-14 16:53:51,030 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher
[36malgo-1    |[0m 2021-11-14 16:53:51,033 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean
[36malgo-1    |[0m 2021-11-14 16:53:51,033 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.
[36malgo-1    |[0m 2021-11-14 16:53:51,040 INFO placement.MultiNodeSortingManager: Initializing NodeSortingService=MultiNodeSortingManager
[36malgo-1    |[0m 2021-11-14 16:53:51,041 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866
[36malgo-1    |[0m 2021-11-14 16:53:51,043 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list
[36malgo-1    |[0m 2021-11-14 16:53:51,044 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s
[36malgo-1    |[0m 2021-11-14 16:53:51,044 INFO datanode.DataNode: Number threads for balancing is 50
[36malgo-1    |[0m 2021-11-14 16:53:51,049 INFO conf.Configuration: resource-types.xml not found
[36malgo-1    |[0m 2021-11-14 16:53:51,050 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
[36malgo-1    |[0m 2021-11-14 16:53:51,051 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/etc/hadoop/conf.empty/capacity-scheduler.xml
[33malgo-2    |[0m 2021-11-14 16:53:51,053 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []
[36malgo-1    |[0m 2021-11-14 16:53:51,058 INFO conf.Configuration: node-resources.xml not found
[36malgo-1    |[0m 2021-11-14 16:53:51,058 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.
[36malgo-1    |[0m 2021-11-14 16:53:51,060 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 16:53:51,066 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=15892 virtual-memory=79460 virtual-cores=4
[33malgo-2    |[0m 2021-11-14 16:53:51,068 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]
[36malgo-1    |[0m 2021-11-14 16:53:51,069 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1, vCores:1>
[36malgo-1    |[0m 2021-11-14 16:53:51,070 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 16:53:51,072 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m 2021-11-14 16:53:51,091 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4dd6fd0a{hdfs,/,file:///usr/lib/hadoop-hdfs/webapps/hdfs/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/hdfs}
[36malgo-1    |[0m 2021-11-14 16:53:51,094 INFO util.log: Logging initialized @1604ms to org.eclipse.jetty.util.log.Slf4jLog
[33malgo-2    |[0m 2021-11-14 16:53:51,096 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/172.21.0.2:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 16:53:51,111 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined
[36malgo-1    |[0m 2021-11-14 16:53:51,111 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined
[36malgo-1    |[0m 2021-11-14 16:53:51,129 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=ADMINISTER_QUEUE:*SUBMIT_APP:*, labels=*,
[36malgo-1    |[0m , reservationsContinueLooking=true, orderingPolicy=utilization, priority=0
[36malgo-1    |[0m 2021-11-14 16:53:51,129 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root
[36malgo-1    |[0m 2021-11-14 16:53:51,133 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 16:53:51,138 INFO server.AbstractConnector: Started ServerConnector@4d14b6c2{HTTP/1.1,[http/1.1]}{0.0.0.0:9870}
[36malgo-1    |[0m 2021-11-14 16:53:51,138 INFO server.Server: Started @1648ms
[36malgo-1    |[0m 2021-11-14 16:53:51,149 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined
[36malgo-1    |[0m 2021-11-14 16:53:51,149 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined
[36malgo-1    |[0m 2021-11-14 16:53:51,153 INFO capacity.LeafQueue: Initializing default
[36malgo-1    |[0m capacity = 1.0 [= (float) configuredCapacity / 100 ]
[36malgo-1    |[0m absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]
[36malgo-1    |[0m maxCapacity = 1.0 [= configuredMaxCapacity ]
[36malgo-1    |[0m absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]
[36malgo-1    |[0m effectiveMinResource=<memory:0, vCores:0>
[36malgo-1    |[0m  , effectiveMaxResource=<memory:0, vCores:0>
[36malgo-1    |[0m userLimit = 100 [= configuredUserLimit ]
[36malgo-1    |[0m userLimitFactor = 1.0 [= configuredUserLimitFactor ]
[36malgo-1    |[0m maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]
[36malgo-1    |[0m maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]
[36malgo-1    |[0m usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]
[36malgo-1    |[0m absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]
[36malgo-1    |[0m maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]
[36malgo-1    |[0m minimumAllocationFactor = 0.99993706 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]
[36malgo-1    |[0m maximumAllocation = <memory:15892, vCores:4> [= configuredMaxAllocation ]
[36malgo-1    |[0m numContainers = 0 [= currentNumContainers ]
[36malgo-1    |[0m state = RUNNING [= configuredState ]
[36malgo-1    |[0m acls = ADMINISTER_QUEUE:*SUBMIT_APP:* [= configuredAcls ]
[36malgo-1    |[0m nodeLocalityDelay = 40
[36malgo-1    |[0m rackLocalityAdditionalDelay = -1
[36malgo-1    |[0m labels=*,
[36malgo-1    |[0m reservationsContinueLooking = true
[36malgo-1    |[0m preemptionDisabled = true
[36malgo-1    |[0m defaultAppPriorityPerQueue = 0
[36malgo-1    |[0m priority = 0
[36malgo-1    |[0m maxLifetime = -1 seconds
[36malgo-1    |[0m defaultLifetime = -1 seconds
[36malgo-1    |[0m 2021-11-14 16:53:51,154 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>
[36malgo-1    |[0m 2021-11-14 16:53:51,154 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
[36malgo-1    |[0m 2021-11-14 16:53:51,156 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
[36malgo-1    |[0m 2021-11-14 16:53:51,157 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false
[36malgo-1    |[0m 2021-11-14 16:53:51,157 INFO ipc.Server: Starting Socket Reader #1 for port 0
[36malgo-1    |[0m 2021-11-14 16:53:51,158 INFO placement.MultiNodeSortingManager: MultiNode scheduling is 'false', and configured policies are 
[36malgo-1    |[0m 2021-11-14 16:53:51,158 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1, vCores:1>>, maximumAllocation=<<memory:15892, vCores:4>>, asynchronousScheduling=false, asyncScheduleInterval=5ms,multiNodePlacementEnabled=false
[36malgo-1    |[0m 2021-11-14 16:53:51,162 INFO conf.Configuration: dynamic-resources.xml not found
[36malgo-1    |[0m 2021-11-14 16:53:51,164 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].
[36malgo-1    |[0m 2021-11-14 16:53:51,164 INFO resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.
[36malgo-1    |[0m 2021-11-14 16:53:51,166 INFO resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. 
[36malgo-1    |[0m 2021-11-14 16:53:51,172 INFO resourcemanager.ResourceManager: TimelineServicePublisher is not configured
[36malgo-1    |[0m 2021-11-14 16:53:51,218 INFO util.log: Logging initialized @1725ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m 2021-11-14 16:53:51,339 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 16:53:51,339 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 16:53:51,345 INFO http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined
[33malgo-2    |[0m 11-14 16:53 smspark-submit INFO     cluster is up
[33malgo-2    |[0m 11-14 16:53 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING
[33malgo-2    |[0m 11-14 16:53 smspark-submit INFO     starting executor logs watcher
[33malgo-2    |[0m 11-14 16:53 smspark-submit INFO     waiting for the primary to come up
[33malgo-2    |[0m Starting executor logs watcher on log_dir: /var/log/yarn
[36malgo-1    |[0m 2021-11-14 16:53:51,352 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined
[36malgo-1    |[0m 2021-11-14 16:53:51,353 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[33malgo-2    |[0m 11-14 16:53 smspark-submit INFO     waiting for the primary to go down
[36malgo-1    |[0m 2021-11-14 16:53:51,356 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster
[36malgo-1    |[0m 2021-11-14 16:53:51,357 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs
[36malgo-1    |[0m 2021-11-14 16:53:51,357 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static
[36malgo-1    |[0m 2021-11-14 16:53:51,358 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster
[36malgo-1    |[0m 2021-11-14 16:53:51,358 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[36malgo-1    |[0m 2021-11-14 16:53:51,358 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 2021-11-14 16:53:51,360 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[36malgo-1    |[0m 2021-11-14 16:53:51,360 INFO http.HttpServer2: adding path spec: /cluster/*
[36malgo-1    |[0m 2021-11-14 16:53:51,360 INFO http.HttpServer2: adding path spec: /ws/*
[36malgo-1    |[0m 2021-11-14 16:53:51,360 INFO http.HttpServer2: adding path spec: /app/*
[36malgo-1    |[0m 2021-11-14 16:53:51,362 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
[36malgo-1    |[0m 2021-11-14 16:53:51,362 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[36malgo-1    |[0m 2021-11-14 16:53:51,362 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 2021-11-14 16:53:51,398 INFO http.HttpServer2: Jetty bound to port 43541
[36malgo-1    |[0m 2021-11-14 16:53:51,399 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[36malgo-1    |[0m 2021-11-14 16:53:51,431 INFO server.session: DefaultSessionIdManager workerName=node0
[36malgo-1    |[0m 2021-11-14 16:53:51,431 INFO server.session: No SessionScavenger set, using defaults
[36malgo-1    |[0m 2021-11-14 16:53:51,433 INFO server.session: node0 Scavenging every 660000ms
[36malgo-1    |[0m 2021-11-14 16:53:51,448 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a3793c7{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 16:53:51,448 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@bb9e6dc{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 16:53:51,450 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
[36malgo-1    |[0m 2021-11-14 16:53:51,450 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
[36malgo-1    |[0m 2021-11-14 16:53:51,453 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 16:53:51,458 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 16:53:51,458 INFO ipc.Server: IPC Server listener on 0: starting
[36malgo-1    |[0m 2021-11-14 16:53:51,467 INFO security.NMContainerTokenSecretManager: Updating node address : algo-1:40223
[36malgo-1    |[0m 2021-11-14 16:53:51,475 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 500, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 16:53:51,475 INFO ipc.Server: Starting Socket Reader #1 for port 8040
[36malgo-1    |[0m 2021-11-14 16:53:51,485 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 16:53:51,486 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 16:53:51,486 INFO ipc.Server: IPC Server listener on 8040: starting
[36malgo-1    |[0m 2021-11-14 16:53:51,486 INFO localizer.ResourceLocalizationService: Localizer started on port 8040
[36malgo-1    |[0m 2021-11-14 16:53:51,488 INFO containermanager.ContainerManagerImpl: ContainerManager started at /172.21.0.2:40223
[36malgo-1    |[0m 2021-11-14 16:53:51,489 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-1/172.21.0.2:0
[36malgo-1    |[0m 2021-11-14 16:53:51,489 WARN tracker.NMLogAggregationStatusTracker: Log Aggregation is disabled.So is the LogAggregationStatusTracker.
[36malgo-1    |[0m 2021-11-14 16:53:51,495 INFO webapp.WebServer: Instantiating NMWebApp at algo-1:8042
[36malgo-1    |[0m 2021-11-14 16:53:51,509 INFO namenode.FSEditLog: Edit logging is async:true
[36malgo-1    |[0m 2021-11-14 16:53:51,514 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m 2021-11-14 16:53:51,521 INFO util.log: Logging initialized @2028ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m 2021-11-14 16:53:51,525 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@338c99c8{datanode,/,file:///usr/lib/hadoop-hdfs/webapps/datanode/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/datanode}
[36malgo-1    |[0m 2021-11-14 16:53:51,526 INFO namenode.FSNamesystem: KeyProvider: null
[36malgo-1    |[0m 2021-11-14 16:53:51,528 INFO namenode.FSNamesystem: fsLock is fair: true
[36malgo-1    |[0m 2021-11-14 16:53:51,528 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
[36malgo-1    |[0m 2021-11-14 16:53:51,533 INFO server.AbstractConnector: Started ServerConnector@6c372fe6{HTTP/1.1,[http/1.1]}{localhost:43541}
[36malgo-1    |[0m 2021-11-14 16:53:51,533 INFO server.Server: Started @2043ms
[36malgo-1    |[0m 2021-11-14 16:53:51,535 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 16:53:51,535 INFO namenode.FSNamesystem: supergroup          = supergroup
[36malgo-1    |[0m 2021-11-14 16:53:51,535 INFO namenode.FSNamesystem: isPermissionEnabled = true
[36malgo-1    |[0m 2021-11-14 16:53:51,535 INFO namenode.FSNamesystem: HA Enabled: false
[36malgo-1    |[0m 2021-11-14 16:53:51,579 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[36malgo-1    |[0m 2021-11-14 16:53:51,591 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
[36malgo-1    |[0m 2021-11-14 16:53:51,591 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
[36malgo-1    |[0m 2021-11-14 16:53:51,596 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
[36malgo-1    |[0m 2021-11-14 16:53:51,597 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Nov 14 16:53:51
[36malgo-1    |[0m 2021-11-14 16:53:51,599 INFO util.GSet: Computing capacity for map BlocksMap
[36malgo-1    |[0m 2021-11-14 16:53:51,599 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 16:53:51,601 INFO util.GSet: 2.0% max memory 6.5 GB = 133.6 MB
[36malgo-1    |[0m 2021-11-14 16:53:51,601 INFO util.GSet: capacity      = 2^24 = 16777216 entries
[36malgo-1    |[0m 2021-11-14 16:53:51,609 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 16:53:51,612 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined
[36malgo-1    |[0m 2021-11-14 16:53:51,618 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[36malgo-1    |[0m 2021-11-14 16:53:51,619 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node
[36malgo-1    |[0m 2021-11-14 16:53:51,619 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 2021-11-14 16:53:51,619 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[36malgo-1    |[0m 2021-11-14 16:53:51,619 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context node
[36malgo-1    |[0m 2021-11-14 16:53:51,619 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
[36malgo-1    |[0m 2021-11-14 16:53:51,620 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
[36malgo-1    |[0m 2021-11-14 16:53:51,621 INFO http.HttpServer2: adding path spec: /node/*
[36malgo-1    |[0m 2021-11-14 16:53:51,621 INFO http.HttpServer2: adding path spec: /ws/*
[36malgo-1    |[0m 2021-11-14 16:53:51,625 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
[36malgo-1    |[0m 2021-11-14 16:53:51,625 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
[36malgo-1    |[0m 2021-11-14 16:53:51,633 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
[36malgo-1    |[0m 2021-11-14 16:53:51,633 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
[36malgo-1    |[0m 2021-11-14 16:53:51,633 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
[36malgo-1    |[0m 2021-11-14 16:53:51,634 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
[36malgo-1    |[0m 2021-11-14 16:53:51,634 INFO blockmanagement.BlockManager: defaultReplication         = 3
[36malgo-1    |[0m 2021-11-14 16:53:51,634 INFO blockmanagement.BlockManager: maxReplication             = 512
[36malgo-1    |[0m 2021-11-14 16:53:51,634 INFO blockmanagement.BlockManager: minReplication             = 1
[36malgo-1    |[0m 2021-11-14 16:53:51,634 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
[36malgo-1    |[0m 2021-11-14 16:53:51,634 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
[36malgo-1    |[0m 2021-11-14 16:53:51,635 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
[36malgo-1    |[0m 2021-11-14 16:53:51,635 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
[36malgo-1    |[0m 2021-11-14 16:53:51,661 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
[36malgo-1    |[0m 2021-11-14 16:53:51,661 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 16:53:51,661 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 16:53:51,661 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 16:53:51,665 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864
[36malgo-1    |[0m 2021-11-14 16:53:51,670 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 16:53:51,671 INFO datanode.DataNode: dnUserName = root
[36malgo-1    |[0m 2021-11-14 16:53:51,671 INFO datanode.DataNode: supergroup = supergroup
[36malgo-1    |[0m 2021-11-14 16:53:51,674 INFO util.GSet: Computing capacity for map INodeMap
[36malgo-1    |[0m 2021-11-14 16:53:51,674 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 16:53:51,675 INFO util.GSet: 1.0% max memory 6.5 GB = 66.8 MB
[36malgo-1    |[0m 2021-11-14 16:53:51,675 INFO util.GSet: capacity      = 2^23 = 8388608 entries
[36malgo-1    |[0m 2021-11-14 16:53:51,715 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 16:53:51,744 INFO ipc.Server: Starting Socket Reader #1 for port 9867
[36malgo-1    |[0m 2021-11-14 16:53:51,806 INFO namenode.FSDirectory: ACLs enabled? false
[36malgo-1    |[0m 2021-11-14 16:53:51,807 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
[36malgo-1    |[0m 2021-11-14 16:53:51,807 INFO namenode.FSDirectory: XAttrs enabled? true
[36malgo-1    |[0m 2021-11-14 16:53:51,807 INFO namenode.NameNode: Caching file names occurring more than 10 times
[36malgo-1    |[0m 2021-11-14 16:53:51,812 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
[36malgo-1    |[0m 2021-11-14 16:53:51,813 INFO snapshot.SnapshotManager: SkipList is disabled
[36malgo-1    |[0m 2021-11-14 16:53:51,818 INFO util.GSet: Computing capacity for map cachedBlocks
[36malgo-1    |[0m 2021-11-14 16:53:51,818 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 16:53:51,818 INFO util.GSet: 0.25% max memory 6.5 GB = 16.7 MB
[36malgo-1    |[0m 2021-11-14 16:53:51,818 INFO util.GSet: capacity      = 2^21 = 2097152 entries
[36malgo-1    |[0m 2021-11-14 16:53:51,823 INFO webapp.WebApps: Registered webapp guice modules
[36malgo-1    |[0m 2021-11-14 16:53:51,826 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
[36malgo-1    |[0m 2021-11-14 16:53:51,826 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
[36malgo-1    |[0m 2021-11-14 16:53:51,826 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
[36malgo-1    |[0m 2021-11-14 16:53:51,829 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
[36malgo-1    |[0m 2021-11-14 16:53:51,829 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
[36malgo-1    |[0m 2021-11-14 16:53:51,830 INFO http.HttpServer2: Jetty bound to port 8088
[36malgo-1    |[0m 2021-11-14 16:53:51,831 INFO util.GSet: Computing capacity for map NameNodeRetryCache
[36malgo-1    |[0m 2021-11-14 16:53:51,831 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 16:53:51,831 INFO util.GSet: 0.029999999329447746% max memory 6.5 GB = 2.0 MB
[36malgo-1    |[0m 2021-11-14 16:53:51,831 INFO util.GSet: capacity      = 2^18 = 262144 entries
[36malgo-1    |[0m 2021-11-14 16:53:51,831 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[36malgo-1    |[0m 2021-11-14 16:53:51,850 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/namenode/in_use.lock acquired by nodename 134@algo-1
[36malgo-1    |[0m 2021-11-14 16:53:51,857 INFO server.session: DefaultSessionIdManager workerName=node0
[36malgo-1    |[0m 2021-11-14 16:53:51,857 INFO server.session: No SessionScavenger set, using defaults
[36malgo-1    |[0m 2021-11-14 16:53:51,859 INFO server.session: node0 Scavenging every 660000ms
[36malgo-1    |[0m 2021-11-14 16:53:51,869 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 16:53:51,871 INFO namenode.FileJournalManager: Recovering unfinalized segments in /opt/amazon/hadoop/hdfs/namenode/current
[36malgo-1    |[0m 2021-11-14 16:53:51,872 INFO namenode.FSImage: No edit log streams selected.
[36malgo-1    |[0m 2021-11-14 16:53:51,872 INFO namenode.FSImage: Planning to load image: FSImageFile(file=/opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
[36malgo-1    |[0m 2021-11-14 16:53:51,877 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
[36malgo-1    |[0m 2021-11-14 16:53:51,879 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
[36malgo-1    |[0m 2021-11-14 16:53:51,879 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
[36malgo-1    |[0m 2021-11-14 16:53:51,898 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5a9f4771{logs,/logs,file:///var/log/yarn/export/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 16:53:51,899 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@68d279ec{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/static,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 16:53:51,908 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException
[36malgo-1    |[0m 2021-11-14 16:53:51,930 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.
[36malgo-1    |[0m 2021-11-14 16:53:51,944 INFO webapp.WebApps: Registered webapp guice modules
[36malgo-1    |[0m 2021-11-14 16:53:51,946 INFO http.HttpServer2: Jetty bound to port 8042
[36malgo-1    |[0m 2021-11-14 16:53:51,947 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[36malgo-1    |[0m 2021-11-14 16:53:51,954 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
[36malgo-1    |[0m 2021-11-14 16:53:51,954 INFO namenode.FSImage: Loaded image for txid 0 from /opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000
[36malgo-1    |[0m 2021-11-14 16:53:51,958 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
[36malgo-1    |[0m 2021-11-14 16:53:51,959 INFO namenode.FSEditLog: Starting log segment at 1
[36malgo-1    |[0m 2021-11-14 16:53:51,977 INFO server.session: DefaultSessionIdManager workerName=node0
[36malgo-1    |[0m 2021-11-14 16:53:51,977 INFO server.session: No SessionScavenger set, using defaults
[36malgo-1    |[0m 2021-11-14 16:53:51,979 INFO server.session: node0 Scavenging every 660000ms
[36malgo-1    |[0m 2021-11-14 16:53:51,985 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:9867
[36malgo-1    |[0m 2021-11-14 16:53:51,990 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 16:53:51,992 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4bff64c2{logs,/logs,file:///var/log/yarn/export/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 16:53:51,993 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3cc41abc{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/static,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 16:53:51,998 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m 2021-11-14 16:53:52,003 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException
[36malgo-1    |[0m 2021-11-14 16:53:52,004 INFO datanode.DataNode: Refresh request received for nameservices: null
[36malgo-1    |[0m 2021-11-14 16:53:52,015 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>
[36malgo-1    |[0m 2021-11-14 16:53:52,027 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1/172.21.0.2:8020 starting to offer service
[36malgo-1    |[0m 2021-11-14 16:53:52,033 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 16:53:52,033 INFO ipc.Server: IPC Server listener on 9867: starting
[36malgo-1    |[0m Nov 14, 2021 4:53:52 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class
[36malgo-1    |[0m 2021-11-14 16:53:52,076 INFO namenode.NameCache: initialized with 0 entries 0 lookups
[36malgo-1    |[0m Nov 14, 2021 4:53:52 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class
[36malgo-1    |[0m 2021-11-14 16:53:52,076 INFO namenode.FSNamesystem: Finished loading FSImage in 241 msecs
[36malgo-1    |[0m Nov 14, 2021 4:53:52 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
[36malgo-1    |[0m Nov 14, 2021 4:53:52 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
[36malgo-1    |[0m INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'
[36malgo-1    |[0m 2021-11-14 16:53:52,092 INFO util.TypeUtil: JVM Runtime does not support Modules
[33malgo-2    |[0m 2021-11-14 16:53:52,097 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/172.21.0.2:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m Nov 14, 2021 4:53:52 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m Nov 14, 2021 4:53:52 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class
[36malgo-1    |[0m Nov 14, 2021 4:53:52 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
[36malgo-1    |[0m Nov 14, 2021 4:53:52 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class
[33malgo-2    |[0m 2021-11-14 16:53:52,179 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/172.21.0.2:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m Nov 14, 2021 4:53:52 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
[36malgo-1    |[0m INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'
[36malgo-1    |[0m Nov 14, 2021 4:53:52 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 16:53:52,401 INFO namenode.NameNode: RPC server is binding to algo-1:8020
[36malgo-1    |[0m 2021-11-14 16:53:52,434 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 16:53:52,447 INFO ipc.Server: Starting Socket Reader #1 for port 8020
[36malgo-1    |[0m Nov 14, 2021 4:53:52 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m Nov 14, 2021 4:53:52 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 16:53:52,633 INFO namenode.NameNode: Clients are to use algo-1:8020 to access this namenode/service.
[36malgo-1    |[0m 2021-11-14 16:53:52,635 INFO namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
[36malgo-1    |[0m 2021-11-14 16:53:52,665 INFO namenode.LeaseManager: Number of blocks under construction: 0
[36malgo-1    |[0m 2021-11-14 16:53:52,691 INFO blockmanagement.BlockManager: initializing replication queues
[36malgo-1    |[0m 2021-11-14 16:53:52,692 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs
[36malgo-1    |[0m 2021-11-14 16:53:52,692 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
[36malgo-1    |[0m 2021-11-14 16:53:52,692 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
[36malgo-1    |[0m 2021-11-14 16:53:52,713 INFO blockmanagement.BlockManager: Total number of blocks            = 0
[36malgo-1    |[0m 2021-11-14 16:53:52,713 INFO blockmanagement.BlockManager: Number of invalid blocks          = 0
[36malgo-1    |[0m 2021-11-14 16:53:52,714 INFO blockmanagement.BlockManager: Number of under-replicated blocks = 0
[36malgo-1    |[0m 2021-11-14 16:53:52,714 INFO blockmanagement.BlockManager: Number of  over-replicated blocks = 0
[36malgo-1    |[0m 2021-11-14 16:53:52,714 INFO blockmanagement.BlockManager: Number of blocks being written    = 0
[36malgo-1    |[0m 2021-11-14 16:53:52,714 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 22 msec
[36malgo-1    |[0m 2021-11-14 16:53:52,728 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 16:53:52,729 INFO ipc.Server: IPC Server listener on 8020: starting
[36malgo-1    |[0m 2021-11-14 16:53:52,750 INFO namenode.NameNode: NameNode RPC up at: algo-1/172.21.0.2:8020
[36malgo-1    |[0m 2021-11-14 16:53:52,784 INFO namenode.FSNamesystem: Starting services required for active state
[36malgo-1    |[0m 2021-11-14 16:53:52,784 INFO namenode.FSDirectory: Initializing quota with 4 thread(s)
[36malgo-1    |[0m 2021-11-14 16:53:52,809 INFO namenode.FSDirectory: Quota initialization completed in 24 milliseconds
[36malgo-1    |[0m name space=1
[36malgo-1    |[0m storage space=0
[36malgo-1    |[0m storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
[36malgo-1    |[0m 2021-11-14 16:53:52,820 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
[36malgo-1    |[0m Nov 14, 2021 4:53:53 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 16:53:53,058 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5d5b5fa7{node,/,file:///tmp/jetty-algo-1-8042-_-any-4219380653401750098.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/node}
[36malgo-1    |[0m Nov 14, 2021 4:53:53 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 16:53:53,081 INFO server.AbstractConnector: Started ServerConnector@3f390d63{HTTP/1.1,[http/1.1]}{algo-1:8042}
[36malgo-1    |[0m 2021-11-14 16:53:53,081 INFO server.Server: Started @3588ms
[36malgo-1    |[0m 2021-11-14 16:53:53,081 INFO webapp.WebApps: Web app node started at 8042
[36malgo-1    |[0m 2021-11-14 16:53:53,092 INFO ipc.Client: Retrying connect to server: algo-1/172.21.0.2:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 16:53:53,093 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-1:40223
[33malgo-2    |[0m 2021-11-14 16:53:53,098 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/172.21.0.2:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 16:53:53,109 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 16:53:53,110 INFO client.RMProxy: Connecting to ResourceManager at /172.21.0.2:8031
[36malgo-1    |[0m 2021-11-14 16:53:53,128 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4efc25fc{cluster,/,file:///tmp/jetty-172_21_0_2-8088-_-any-5348647159654919324.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/cluster}
[36malgo-1    |[0m 2021-11-14 16:53:53,140 INFO server.AbstractConnector: Started ServerConnector@73d983ea{HTTP/1.1,[http/1.1]}{172.21.0.2:8088}
[36malgo-1    |[0m 2021-11-14 16:53:53,140 INFO server.Server: Started @3646ms
[36malgo-1    |[0m 2021-11-14 16:53:53,140 INFO webapp.WebApps: Web app cluster started at 8088
[33malgo-2    |[0m 2021-11-14 16:53:53,180 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/172.21.0.2:8031. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 16:53:53,211 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []
[36malgo-1    |[0m 2021-11-14 16:53:53,216 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 16:53:53,228 INFO ipc.Server: Starting Socket Reader #1 for port 8033
[36malgo-1    |[0m 2021-11-14 16:53:53,231 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]
[36malgo-1    |[0m 2021-11-14 16:53:53,317 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1/172.21.0.2:8020
[33malgo-2    |[0m 2021-11-14 16:53:53,318 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1.spark-network/172.21.0.2:8020
[36malgo-1    |[0m 2021-11-14 16:53:53,319 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
[33malgo-2    |[0m 2021-11-14 16:53:53,321 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
[36malgo-1    |[0m 2021-11-14 16:53:53,327 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 135@algo-1
[36malgo-1    |[0m 2021-11-14 16:53:53,328 INFO common.Storage: Storage directory with location [DISK]file:/opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 276945154. Formatting...
[33malgo-2    |[0m 2021-11-14 16:53:53,328 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 18@algo-2
[36malgo-1    |[0m 2021-11-14 16:53:53,329 INFO common.Storage: Generated new storageID DS-f8630eca-cd7a-4774-b0b8-790e17551b10 for directory /opt/amazon/hadoop/hdfs/datanode 
[33malgo-2    |[0m 2021-11-14 16:53:53,330 INFO common.Storage: Storage directory with location [DISK]file:/opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 276945154. Formatting...
[33malgo-2    |[0m 2021-11-14 16:53:53,330 INFO common.Storage: Generated new storageID DS-105cb5e2-7a33-4b76-9bae-8af49e3e7c73 for directory /opt/amazon/hadoop/hdfs/datanode 
[33malgo-2    |[0m 2021-11-14 16:53:53,361 INFO common.Storage: Analyzing storage directories for bpid BP-1213576767-172.21.0.2-1636908829197
[33malgo-2    |[0m 2021-11-14 16:53:53,361 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-1213576767-172.21.0.2-1636908829197
[33malgo-2    |[0m 2021-11-14 16:53:53,362 INFO common.Storage: Block pool storage directory for location [DISK]file:/opt/amazon/hadoop/hdfs/datanode and block pool id BP-1213576767-172.21.0.2-1636908829197 is not formatted. Formatting ...
[33malgo-2    |[0m 2021-11-14 16:53:53,362 INFO common.Storage: Formatting block pool BP-1213576767-172.21.0.2-1636908829197 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-1213576767-172.21.0.2-1636908829197/current
[36malgo-1    |[0m 2021-11-14 16:53:53,363 INFO common.Storage: Analyzing storage directories for bpid BP-1213576767-172.21.0.2-1636908829197
[36malgo-1    |[0m 2021-11-14 16:53:53,363 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-1213576767-172.21.0.2-1636908829197
[36malgo-1    |[0m 2021-11-14 16:53:53,364 INFO common.Storage: Block pool storage directory for location [DISK]file:/opt/amazon/hadoop/hdfs/datanode and block pool id BP-1213576767-172.21.0.2-1636908829197 is not formatted. Formatting ...
[36malgo-1    |[0m 2021-11-14 16:53:53,364 INFO common.Storage: Formatting block pool BP-1213576767-172.21.0.2-1636908829197 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-1213576767-172.21.0.2-1636908829197/current
[33malgo-2    |[0m 2021-11-14 16:53:53,368 INFO datanode.DataNode: Setting up storage: nsid=276945154;bpid=BP-1213576767-172.21.0.2-1636908829197;lv=-57;nsInfo=lv=-65;cid=CID-6c9e34c9-2f47-48db-b81c-b360d8566205;nsid=276945154;c=1636908829197;bpid=BP-1213576767-172.21.0.2-1636908829197;dnuuid=null
[36malgo-1    |[0m 2021-11-14 16:53:53,370 INFO datanode.DataNode: Setting up storage: nsid=276945154;bpid=BP-1213576767-172.21.0.2-1636908829197;lv=-57;nsInfo=lv=-65;cid=CID-6c9e34c9-2f47-48db-b81c-b360d8566205;nsid=276945154;c=1636908829197;bpid=BP-1213576767-172.21.0.2-1636908829197;dnuuid=null
[33malgo-2    |[0m 2021-11-14 16:53:53,371 INFO datanode.DataNode: Generated and persisted new Datanode UUID 08e5fc0e-f287-45de-a6c5-b7ae1baa6fd1
[36malgo-1    |[0m 2021-11-14 16:53:53,372 INFO datanode.DataNode: Generated and persisted new Datanode UUID 48a11371-81fe-4fe1-b56e-7da2a0ec46bd
[36malgo-1    |[0m 2021-11-14 16:53:53,409 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 16:53:53,409 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 16:53:53,435 INFO ipc.Server: IPC Server listener on 8033: starting
[36malgo-1    |[0m 2021-11-14 16:53:53,441 INFO resourcemanager.ResourceManager: Transitioning to active state
[36malgo-1    |[0m 2021-11-14 16:53:53,464 INFO recovery.RMStateStore: Updating AMRMToken
[36malgo-1    |[0m 2021-11-14 16:53:53,464 INFO security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
[36malgo-1    |[0m 2021-11-14 16:53:53,465 INFO security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
[36malgo-1    |[0m 2021-11-14 16:53:53,465 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
[36malgo-1    |[0m 2021-11-14 16:53:53,465 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 1
[36malgo-1    |[0m 2021-11-14 16:53:53,465 INFO recovery.RMStateStore: Storing RMDTMasterKey.
[36malgo-1    |[0m 2021-11-14 16:53:53,465 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
[36malgo-1    |[0m 2021-11-14 16:53:53,466 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
[36malgo-1    |[0m 2021-11-14 16:53:53,466 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 2
[36malgo-1    |[0m 2021-11-14 16:53:53,466 INFO recovery.RMStateStore: Storing RMDTMasterKey.
[36malgo-1    |[0m 2021-11-14 16:53:53,468 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler
[36malgo-1    |[0m 2021-11-14 16:53:53,470 INFO impl.FsDatasetImpl: Added new volume: DS-f8630eca-cd7a-4774-b0b8-790e17551b10
[36malgo-1    |[0m 2021-11-14 16:53:53,470 INFO impl.FsDatasetImpl: Added volume - [DISK]file:/opt/amazon/hadoop/hdfs/datanode, StorageType: DISK
[36malgo-1    |[0m 2021-11-14 16:53:53,474 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean
[36malgo-1    |[0m 2021-11-14 16:53:53,482 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode
[36malgo-1    |[0m 11-14 16:53 smspark-submit INFO     cluster is up
[36malgo-1    |[0m 11-14 16:53 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING
[36malgo-1    |[0m 11-14 16:53 smspark-submit INFO     starting executor logs watcher
[36malgo-1    |[0m 11-14 16:53 smspark-submit INFO     start log event log publisher
[36malgo-1    |[0m Starting executor logs watcher on log_dir: /var/log/yarn
[33malgo-2    |[0m 2021-11-14 16:53:53,491 INFO impl.FsDatasetImpl: Added new volume: DS-105cb5e2-7a33-4b76-9bae-8af49e3e7c73
[33malgo-2    |[0m 2021-11-14 16:53:53,491 INFO impl.FsDatasetImpl: Added volume - [DISK]file:/opt/amazon/hadoop/hdfs/datanode, StorageType: DISK
[36malgo-1    |[0m 11-14 16:53 sagemaker-spark-event-logs-publisher INFO     Spark event log not enabled.
[36malgo-1    |[0m 11-14 16:53 smspark-submit INFO     Waiting for hosts to bootstrap: ['algo-1', 'algo-2']
[36malgo-1    |[0m 2021-11-14 16:53:53,494 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode
[36malgo-1    |[0m 2021-11-14 16:53:53,495 INFO impl.FsDatasetImpl: Adding block pool BP-1213576767-172.21.0.2-1636908829197
[33malgo-2    |[0m 2021-11-14 16:53:53,495 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean
[36malgo-1    |[0m 2021-11-14 16:53:53,496 INFO impl.FsDatasetImpl: Scanning block pool BP-1213576767-172.21.0.2-1636908829197 on volume /opt/amazon/hadoop/hdfs/datanode...
[33malgo-2    |[0m 2021-11-14 16:53:53,502 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode
[36malgo-1    |[0m 11-14 16:53 smspark-submit INFO     Received host statuses: dict_items([('algo-1', StatusMessage(status='WAITING', timestamp='2021-11-14T16:53:53.496371')), ('algo-2', StatusMessage(status='WAITING', timestamp='2021-11-14T16:53:53.501083'))])
[33malgo-2    |[0m 2021-11-14 16:53:53,510 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode
[33malgo-2    |[0m 2021-11-14 16:53:53,512 INFO impl.FsDatasetImpl: Adding block pool BP-1213576767-172.21.0.2-1636908829197
[33malgo-2    |[0m 2021-11-14 16:53:53,513 INFO impl.FsDatasetImpl: Scanning block pool BP-1213576767-172.21.0.2-1636908829197 on volume /opt/amazon/hadoop/hdfs/datanode...
[36malgo-1    |[0m 2021-11-14 16:53:53,531 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-1213576767-172.21.0.2-1636908829197 on /opt/amazon/hadoop/hdfs/datanode: 35ms
[36malgo-1    |[0m 2021-11-14 16:53:53,531 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1213576767-172.21.0.2-1636908829197: 36ms
[36malgo-1    |[0m 2021-11-14 16:53:53,533 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-1213576767-172.21.0.2-1636908829197 on volume /opt/amazon/hadoop/hdfs/datanode...
[36malgo-1    |[0m 2021-11-14 16:53:53,533 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-1213576767-172.21.0.2-1636908829197/current/replicas doesn't exist 
[36malgo-1    |[0m 2021-11-14 16:53:53,534 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1213576767-172.21.0.2-1636908829197 on volume /opt/amazon/hadoop/hdfs/datanode: 2ms
[36malgo-1    |[0m 2021-11-14 16:53:53,534 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-1213576767-172.21.0.2-1636908829197: 2ms
[36malgo-1    |[0m 2021-11-14 16:53:53,536 INFO datanode.VolumeScanner: Now scanning bpid BP-1213576767-172.21.0.2-1636908829197 on volume /opt/amazon/hadoop/hdfs/datanode
[36malgo-1    |[0m 2021-11-14 16:53:53,539 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-f8630eca-cd7a-4774-b0b8-790e17551b10): finished scanning block pool BP-1213576767-172.21.0.2-1636908829197
[33malgo-2    |[0m 2021-11-14 16:53:53,550 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-1213576767-172.21.0.2-1636908829197 on /opt/amazon/hadoop/hdfs/datanode: 36ms
[33malgo-2    |[0m 2021-11-14 16:53:53,550 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1213576767-172.21.0.2-1636908829197: 37ms
[36malgo-1    |[0m 2021-11-14 16:53:53,551 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-f8630eca-cd7a-4774-b0b8-790e17551b10): no suitable block pools found to scan.  Waiting 1814399985 ms.
[36malgo-1    |[0m 2021-11-14 16:53:53,556 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 11/14/21 9:37 PM with interval of 21600000ms
[33malgo-2    |[0m 2021-11-14 16:53:53,556 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-1213576767-172.21.0.2-1636908829197 on volume /opt/amazon/hadoop/hdfs/datanode...
[33malgo-2    |[0m 2021-11-14 16:53:53,556 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-1213576767-172.21.0.2-1636908829197/current/replicas doesn't exist 
[33malgo-2    |[0m 2021-11-14 16:53:53,558 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1213576767-172.21.0.2-1636908829197 on volume /opt/amazon/hadoop/hdfs/datanode: 2ms
[33malgo-2    |[0m 2021-11-14 16:53:53,558 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-1213576767-172.21.0.2-1636908829197: 7ms
[33malgo-2    |[0m 2021-11-14 16:53:53,560 INFO datanode.VolumeScanner: Now scanning bpid BP-1213576767-172.21.0.2-1636908829197 on volume /opt/amazon/hadoop/hdfs/datanode
[36malgo-1    |[0m 2021-11-14 16:53:53,561 INFO datanode.DataNode: Block pool BP-1213576767-172.21.0.2-1636908829197 (Datanode Uuid 48a11371-81fe-4fe1-b56e-7da2a0ec46bd) service to algo-1/172.21.0.2:8020 beginning handshake with NN
[33malgo-2    |[0m 2021-11-14 16:53:53,562 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-105cb5e2-7a33-4b76-9bae-8af49e3e7c73): finished scanning block pool BP-1213576767-172.21.0.2-1636908829197
[33malgo-2    |[0m 2021-11-14 16:53:53,572 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-105cb5e2-7a33-4b76-9bae-8af49e3e7c73): no suitable block pools found to scan.  Waiting 1814399988 ms.
[33malgo-2    |[0m 2021-11-14 16:53:53,573 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 11/14/21 6:47 PM with interval of 21600000ms
[33malgo-2    |[0m 2021-11-14 16:53:53,578 INFO datanode.DataNode: Block pool BP-1213576767-172.21.0.2-1636908829197 (Datanode Uuid 08e5fc0e-f287-45de-a6c5-b7ae1baa6fd1) service to algo-1.spark-network/172.21.0.2:8020 beginning handshake with NN
[36malgo-1    |[0m 2021-11-14 16:53:53,594 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.21.0.2:9866, datanodeUuid=48a11371-81fe-4fe1-b56e-7da2a0ec46bd, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-6c9e34c9-2f47-48db-b81c-b360d8566205;nsid=276945154;c=1636908829197) storage 48a11371-81fe-4fe1-b56e-7da2a0ec46bd
[36malgo-1    |[0m 2021-11-14 16:53:53,596 INFO net.NetworkTopology: Adding a new node: /default-rack/172.21.0.2:9866
[36malgo-1    |[0m 2021-11-14 16:53:53,596 INFO blockmanagement.BlockReportLeaseManager: Registered DN 48a11371-81fe-4fe1-b56e-7da2a0ec46bd (172.21.0.2:9866).
[36malgo-1    |[0m 2021-11-14 16:53:53,600 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.21.0.3:9866, datanodeUuid=08e5fc0e-f287-45de-a6c5-b7ae1baa6fd1, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-6c9e34c9-2f47-48db-b81c-b360d8566205;nsid=276945154;c=1636908829197) storage 08e5fc0e-f287-45de-a6c5-b7ae1baa6fd1
[36malgo-1    |[0m 2021-11-14 16:53:53,600 INFO net.NetworkTopology: Adding a new node: /default-rack/172.21.0.3:9866
[36malgo-1    |[0m 2021-11-14 16:53:53,600 INFO blockmanagement.BlockReportLeaseManager: Registered DN 08e5fc0e-f287-45de-a6c5-b7ae1baa6fd1 (172.21.0.3:9866).
[36malgo-1    |[0m 2021-11-14 16:53:53,608 INFO datanode.DataNode: Block pool Block pool BP-1213576767-172.21.0.2-1636908829197 (Datanode Uuid 48a11371-81fe-4fe1-b56e-7da2a0ec46bd) service to algo-1/172.21.0.2:8020 successfully registered with NN
[36malgo-1    |[0m 2021-11-14 16:53:53,608 INFO datanode.DataNode: For namenode algo-1/172.21.0.2:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
[33malgo-2    |[0m 2021-11-14 16:53:53,609 INFO datanode.DataNode: Block pool Block pool BP-1213576767-172.21.0.2-1636908829197 (Datanode Uuid 08e5fc0e-f287-45de-a6c5-b7ae1baa6fd1) service to algo-1.spark-network/172.21.0.2:8020 successfully registered with NN
[33malgo-2    |[0m 2021-11-14 16:53:53,609 INFO datanode.DataNode: For namenode algo-1.spark-network/172.21.0.2:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
[36malgo-1    |[0m 2021-11-14 16:53:53,631 INFO store.AbstractFSNodeStore: Created store directory :file:/tmp/hadoop-yarn-root/node-attribute
[36malgo-1    |[0m 2021-11-14 16:53:53,644 INFO store.AbstractFSNodeStore: Finished write mirror at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.mirror
[36malgo-1    |[0m 2021-11-14 16:53:53,644 INFO store.AbstractFSNodeStore: Finished create editlog file at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.editlog
[36malgo-1    |[0m 2021-11-14 16:53:53,654 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl$ForwardingEventHandler
[36malgo-1    |[0m 2021-11-14 16:53:53,654 INFO placement.MultiNodeSortingManager: Starting NodeSortingService=MultiNodeSortingManager
[36malgo-1    |[0m 2021-11-14 16:53:53,664 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 16:53:53,665 INFO ipc.Server: Starting Socket Reader #1 for port 8031
[36malgo-1    |[0m 2021-11-14 16:53:53,667 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server
[36malgo-1    |[0m 2021-11-14 16:53:53,667 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 16:53:53,667 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-f8630eca-cd7a-4774-b0b8-790e17551b10 for DN 172.21.0.2:9866
[36malgo-1    |[0m 2021-11-14 16:53:53,667 INFO ipc.Server: IPC Server listener on 8031: starting
[36malgo-1    |[0m 2021-11-14 16:53:53,669 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-105cb5e2-7a33-4b76-9bae-8af49e3e7c73 for DN 172.21.0.3:9866
[36malgo-1    |[0m 2021-11-14 16:53:53,676 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 16:53:53,687 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 16:53:53,692 INFO ipc.Server: Starting Socket Reader #1 for port 8030
[36malgo-1    |[0m 2021-11-14 16:53:53,700 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 16:53:53,700 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 16:53:53,700 INFO ipc.Server: IPC Server listener on 8030: starting
[36malgo-1    |[0m 2021-11-14 16:53:53,779 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 16:53:53,781 INFO ipc.Server: Starting Socket Reader #1 for port 8032
[36malgo-1    |[0m 2021-11-14 16:53:53,803 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 16:53:53,812 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 16:53:53,816 INFO BlockStateChange: BLOCK* processReport 0xd1f58aa47635315e: Processing first storage report for DS-105cb5e2-7a33-4b76-9bae-8af49e3e7c73 from datanode 08e5fc0e-f287-45de-a6c5-b7ae1baa6fd1
[36malgo-1    |[0m 2021-11-14 16:53:53,816 INFO ipc.Server: IPC Server listener on 8032: starting
[36malgo-1    |[0m 2021-11-14 16:53:53,818 INFO BlockStateChange: BLOCK* processReport 0xd1f58aa47635315e: from storage DS-105cb5e2-7a33-4b76-9bae-8af49e3e7c73 node DatanodeRegistration(172.21.0.3:9866, datanodeUuid=08e5fc0e-f287-45de-a6c5-b7ae1baa6fd1, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-6c9e34c9-2f47-48db-b81c-b360d8566205;nsid=276945154;c=1636908829197), blocks: 0, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0
[36malgo-1    |[0m 2021-11-14 16:53:53,842 INFO BlockStateChange: BLOCK* processReport 0x4f3209dcf5128929: Processing first storage report for DS-f8630eca-cd7a-4774-b0b8-790e17551b10 from datanode 48a11371-81fe-4fe1-b56e-7da2a0ec46bd
[36malgo-1    |[0m 2021-11-14 16:53:53,842 INFO BlockStateChange: BLOCK* processReport 0x4f3209dcf5128929: from storage DS-f8630eca-cd7a-4774-b0b8-790e17551b10 node DatanodeRegistration(172.21.0.2:9866, datanodeUuid=48a11371-81fe-4fe1-b56e-7da2a0ec46bd, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-6c9e34c9-2f47-48db-b81c-b360d8566205;nsid=276945154;c=1636908829197), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
[36malgo-1    |[0m 2021-11-14 16:53:53,847 INFO resourcemanager.ResourceManager: Transitioned to active state
[36malgo-1    |[0m 2021-11-14 16:53:53,858 INFO datanode.DataNode: Successfully sent block report 0x4f3209dcf5128929,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 5 msec to generate and 163 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
[36malgo-1    |[0m 2021-11-14 16:53:53,858 INFO datanode.DataNode: Got finalize command for block pool BP-1213576767-172.21.0.2-1636908829197
[33malgo-2    |[0m 2021-11-14 16:53:53,859 INFO datanode.DataNode: Successfully sent block report 0xd1f58aa47635315e,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 5 msec to generate and 165 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
[33malgo-2    |[0m 2021-11-14 16:53:53,859 INFO datanode.DataNode: Got finalize command for block pool BP-1213576767-172.21.0.2-1636908829197
[33malgo-2    |[0m 2021-11-14 16:53:54,181 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/172.21.0.2:8031. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 16:53:54,344 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-2(cmPort: 44783 httpPort: 8042) registered with capability: <memory:15892, vCores:4>, assigned nodeId algo-2:44783
[36malgo-1    |[0m 2021-11-14 16:53:54,347 INFO rmnode.RMNodeImpl: algo-2:44783 Node Transitioned from NEW to RUNNING
[36malgo-1    |[0m 2021-11-14 16:53:54,356 INFO ipc.Client: Retrying connect to server: algo-1/172.21.0.2:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[33malgo-2    |[0m 2021-11-14 16:53:54,360 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id 909027414
[33malgo-2    |[0m 2021-11-14 16:53:54,361 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id 1295022749
[33malgo-2    |[0m 2021-11-14 16:53:54,361 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-2:44783 with total resource of <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 16:53:54,366 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-1(cmPort: 40223 httpPort: 8042) registered with capability: <memory:15892, vCores:4>, assigned nodeId algo-1:40223
[36malgo-1    |[0m 2021-11-14 16:53:54,366 INFO rmnode.RMNodeImpl: algo-1:40223 Node Transitioned from NEW to RUNNING
[36malgo-1    |[0m 2021-11-14 16:53:54,367 INFO capacity.CapacityScheduler: Added node algo-2:44783 clusterResource: <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 16:53:54,370 INFO capacity.CapacityScheduler: Added node algo-1:40223 clusterResource: <memory:31784, vCores:8>
[36malgo-1    |[0m 2021-11-14 16:53:54,377 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id 909027414
[36malgo-1    |[0m 2021-11-14 16:53:54,378 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id 1295022749
[36malgo-1    |[0m 2021-11-14 16:53:54,378 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-1:40223 with total resource of <memory:15892, vCores:4>
[36malgo-1    |[0m Using properties file: /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m Adding default property: spark.driver.host=172.21.0.2
[36malgo-1    |[0m Adding default property: spark.executor.memoryOverhead=1239m
[36malgo-1    |[0m Adding default property: spark.driver.extraLibraryPath=/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m Adding default property: spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[36malgo-1    |[0m Adding default property: spark.rpc.askTimeout=300s
[36malgo-1    |[0m Adding default property: spark.driver.memory=2048m
[36malgo-1    |[0m Adding default property: spark.executor.instances=2
[36malgo-1    |[0m Adding default property: spark.driver.memoryOverhead=204m
[36malgo-1    |[0m Adding default property: key=value
[36malgo-1    |[0m Adding default property: spark.default.parallelism=16
[36malgo-1    |[0m Adding default property: spark.executor.defaultJavaOptions=-verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3
[36malgo-1    |[0m Adding default property: spark.driver.defaultJavaOptions=-XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[36malgo-1    |[0m Adding default property: spark.executor.extraLibraryPath=/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m Adding default property: spark.executor.memory=2g
[36malgo-1    |[0m Adding default property: spark.driver.extraClassPath=/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m Adding default property: spark.executor.extraClassPath=/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m Adding default property: spark.executor.cores=1
[36malgo-1    |[0m Warning: Ignoring non-Spark config property: key
[36malgo-1    |[0m Parsed arguments:
[36malgo-1    |[0m   master                  yarn
[36malgo-1    |[0m   deployMode              client
[36malgo-1    |[0m   executorMemory          2g
[36malgo-1    |[0m   executorCores           1
[36malgo-1    |[0m   totalExecutorCores      null
[36malgo-1    |[0m   propertiesFile          /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m   driverMemory            2048m
[36malgo-1    |[0m   driverCores             null
[36malgo-1    |[0m   driverExtraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m   driverExtraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m   driverExtraJavaOptions  null
[36malgo-1    |[0m   supervise               false
[36malgo-1    |[0m   queue                   null
[36malgo-1    |[0m   numExecutors            2
[36malgo-1    |[0m   files                   null
[36malgo-1    |[0m   pyFiles                 file:/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py
[36malgo-1    |[0m   archives                null
[36malgo-1    |[0m   mainClass               null
[36malgo-1    |[0m   primaryResource         file:/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py
[36malgo-1    |[0m   name                    hello_py_spark_app.py
[36malgo-1    |[0m   childArgs               [--input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data]
[36malgo-1    |[0m   jars                    null
[36malgo-1    |[0m   packages                null
[36malgo-1    |[0m   packagesExclusions      null
[36malgo-1    |[0m   repositories            null
[36malgo-1    |[0m   verbose                 true
[36malgo-1    |[0m 
[36malgo-1    |[0m Spark properties used, including those specified through
[36malgo-1    |[0m  --conf and those from the properties file /usr/lib/spark/conf/spark-defaults.conf:
[36malgo-1    |[0m   (spark.executor.defaultJavaOptions,-verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3)
[36malgo-1    |[0m   (spark.default.parallelism,16)
[36malgo-1    |[0m   (spark.executor.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)
[36malgo-1    |[0m   (spark.executor.memory,2g)
[36malgo-1    |[0m   (spark.driver.memory,2048m)
[36malgo-1    |[0m   (spark.driver.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)
[36malgo-1    |[0m   (spark.executor.instances,2)
[36malgo-1    |[0m   (spark.executor.memoryOverhead,1239m)
[36malgo-1    |[0m   (spark.rpc.askTimeout,300s)
[36malgo-1    |[0m   (spark.driver.memoryOverhead,204m)
[36malgo-1    |[0m   (spark.driver.host,172.21.0.2)
[36malgo-1    |[0m   (spark.driver.defaultJavaOptions,-XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled)
[36malgo-1    |[0m   (spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version,2)
[36malgo-1    |[0m   (spark.executor.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)
[36malgo-1    |[0m   (spark.executor.cores,1)
[36malgo-1    |[0m   (spark.driver.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)
[36malgo-1    |[0m 
[36malgo-1    |[0m     
[36malgo-1    |[0m Main class:
[36malgo-1    |[0m org.apache.spark.deploy.PythonRunner
[36malgo-1    |[0m Arguments:
[36malgo-1    |[0m file:/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py
[36malgo-1    |[0m file:///opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py
[36malgo-1    |[0m --input
[36malgo-1    |[0m file:///opt/ml/processing/input/data/data.jsonl
[36malgo-1    |[0m --output
[36malgo-1    |[0m file:///opt/ml/processing/output/data
[36malgo-1    |[0m Spark config:
[36malgo-1    |[0m (spark.driver.host,172.21.0.2)
[36malgo-1    |[0m (spark.executor.memoryOverhead,1239m)
[36malgo-1    |[0m (spark.driver.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)
[36malgo-1    |[0m (spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version,2)
[36malgo-1    |[0m (spark.app.name,hello_py_spark_app.py)
[36malgo-1    |[0m (spark.rpc.askTimeout,300s)
[36malgo-1    |[0m (spark.driver.memory,2048m)
[36malgo-1    |[0m (spark.executor.instances,2)
[36malgo-1    |[0m (spark.submit.pyFiles,/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py)
[36malgo-1    |[0m (spark.driver.memoryOverhead,204m)
[36malgo-1    |[0m (spark.default.parallelism,16)
[36malgo-1    |[0m (spark.executor.defaultJavaOptions,-verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3)
[36malgo-1    |[0m (spark.yarn.dist.pyFiles,file:///opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py)
[36malgo-1    |[0m (spark.submit.deployMode,client)
[36malgo-1    |[0m (spark.master,yarn)
[36malgo-1    |[0m (spark.driver.defaultJavaOptions,-XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled)
[36malgo-1    |[0m (spark.executor.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)
[36malgo-1    |[0m (spark.executor.memory,2g)
[36malgo-1    |[0m (spark.driver.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)
[36malgo-1    |[0m (spark.executor.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)
[36malgo-1    |[0m (spark.executor.cores,1)
[36malgo-1    |[0m (spark.yarn.isPython,true)
[36malgo-1    |[0m Classpath elements:
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m Hello World, this is PySpark!
[36malgo-1    |[0m Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[36malgo-1    |[0m 21/11/14 16:54:15 INFO SparkContext: Running Spark version 3.0.0-amzn-0
[36malgo-1    |[0m 21/11/14 16:54:15 INFO ResourceUtils: ==============================================================
[36malgo-1    |[0m 21/11/14 16:54:15 INFO ResourceUtils: Resources for spark.driver:
[36malgo-1    |[0m 
[36malgo-1    |[0m 21/11/14 16:54:15 INFO ResourceUtils: ==============================================================
[36malgo-1    |[0m 21/11/14 16:54:15 INFO SparkContext: Submitted application: SparkContainerTestApp
[36malgo-1    |[0m 21/11/14 16:54:16 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m 21/11/14 16:54:16 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m 21/11/14 16:54:16 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m 21/11/14 16:54:16 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m 21/11/14 16:54:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m 21/11/14 16:54:16 INFO Utils: Successfully started service 'sparkDriver' on port 38805.
[36malgo-1    |[0m 21/11/14 16:54:16 INFO SparkEnv: Registering MapOutputTracker
[36malgo-1    |[0m 21/11/14 16:54:16 INFO SparkEnv: Registering BlockManagerMaster
[36malgo-1    |[0m 21/11/14 16:54:16 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[36malgo-1    |[0m 21/11/14 16:54:16 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[36malgo-1    |[0m 21/11/14 16:54:16 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[36malgo-1    |[0m 21/11/14 16:54:16 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-690ec8a6-7be4-4ce8-85ab-ee978cf861fb
[36malgo-1    |[0m 21/11/14 16:54:16 INFO MemoryStore: MemoryStore started with capacity 1007.8 MiB
[36malgo-1    |[0m 21/11/14 16:54:16 INFO SparkEnv: Registering OutputCommitCoordinator
[36malgo-1    |[0m 21/11/14 16:54:16 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[36malgo-1    |[0m 21/11/14 16:54:16 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.21.0.2:4040
[36malgo-1    |[0m 21/11/14 16:54:17 INFO RMProxy: Connecting to ResourceManager at /172.21.0.2:8032
[36malgo-1    |[0m 21/11/14 16:54:17 INFO Client: Requesting a new application from cluster with 2 NodeManagers
[36malgo-1    |[0m 2021-11-14 16:54:17,375 INFO resourcemanager.ClientRMService: Allocated new applicationId: 1
[36malgo-1    |[0m 21/11/14 16:54:17 INFO Configuration: resource-types.xml not found
[36malgo-1    |[0m 21/11/14 16:54:17 INFO ResourceUtils: Unable to find 'resource-types.xml'.
[36malgo-1    |[0m 21/11/14 16:54:17 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15892 MB per container)
[36malgo-1    |[0m 21/11/14 16:54:17 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
[36malgo-1    |[0m 21/11/14 16:54:17 INFO Client: Setting up container launch context for our AM
[36malgo-1    |[0m 21/11/14 16:54:17 INFO Client: Setting up the launch environment for our AM container
[36malgo-1    |[0m 21/11/14 16:54:17 INFO Client: Preparing resources for our AM container
[36malgo-1    |[0m 21/11/14 16:54:17 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
[36malgo-1    |[0m 21/11/14 16:54:23 INFO Client: Uploading resource file:/tmp/spark-cc65bcd2-9cf6-433a-ae81-58b52cb16609/__spark_libs__4952292821467923690.zip -> hdfs://172.21.0.2/user/root/.sparkStaging/application_1636908833443_0001/__spark_libs__4952292821467923690.zip
[36malgo-1    |[0m 2021-11-14 16:54:23,817 INFO hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=172.21.0.2:9866, 172.21.0.3:9866 for /user/root/.sparkStaging/application_1636908833443_0001/__spark_libs__4952292821467923690.zip
[36malgo-1    |[0m 2021-11-14 16:54:23,945 INFO datanode.DataNode: Receiving BP-1213576767-172.21.0.2-1636908829197:blk_1073741825_1001 src: /172.21.0.2:37718 dest: /172.21.0.2:9866
[33malgo-2    |[0m 2021-11-14 16:54:24,009 INFO datanode.DataNode: Receiving BP-1213576767-172.21.0.2-1636908829197:blk_1073741825_1001 src: /172.21.0.2:36730 dest: /172.21.0.3:9866
[33malgo-2    |[0m 2021-11-14 16:54:24,407 INFO DataNode.clienttrace: src: /172.21.0.2:36730, dest: /172.21.0.3:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_836662780_28, offset: 0, srvID: 08e5fc0e-f287-45de-a6c5-b7ae1baa6fd1, blockid: BP-1213576767-172.21.0.2-1636908829197:blk_1073741825_1001, duration(ns): 375904172
[33malgo-2    |[0m 2021-11-14 16:54:24,407 INFO datanode.DataNode: PacketResponder: BP-1213576767-172.21.0.2-1636908829197:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 16:54:24,411 INFO DataNode.clienttrace: src: /172.21.0.2:37718, dest: /172.21.0.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_836662780_28, offset: 0, srvID: 48a11371-81fe-4fe1-b56e-7da2a0ec46bd, blockid: BP-1213576767-172.21.0.2-1636908829197:blk_1073741825_1001, duration(ns): 374221804
[36malgo-1    |[0m 2021-11-14 16:54:24,411 INFO datanode.DataNode: PacketResponder: BP-1213576767-172.21.0.2-1636908829197:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.21.0.3:9866] terminating
[36malgo-1    |[0m 2021-11-14 16:54:24,417 INFO hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=172.21.0.2:9866, 172.21.0.3:9866 for /user/root/.sparkStaging/application_1636908833443_0001/__spark_libs__4952292821467923690.zip
[36malgo-1    |[0m 2021-11-14 16:54:24,421 INFO datanode.DataNode: Receiving BP-1213576767-172.21.0.2-1636908829197:blk_1073741826_1002 src: /172.21.0.2:37722 dest: /172.21.0.2:9866
[33malgo-2    |[0m 2021-11-14 16:54:24,423 INFO datanode.DataNode: Receiving BP-1213576767-172.21.0.2-1636908829197:blk_1073741826_1002 src: /172.21.0.2:36734 dest: /172.21.0.3:9866
[33malgo-2    |[0m 2021-11-14 16:54:24,753 INFO DataNode.clienttrace: src: /172.21.0.2:36734, dest: /172.21.0.3:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_836662780_28, offset: 0, srvID: 08e5fc0e-f287-45de-a6c5-b7ae1baa6fd1, blockid: BP-1213576767-172.21.0.2-1636908829197:blk_1073741826_1002, duration(ns): 329228620
[33malgo-2    |[0m 2021-11-14 16:54:24,753 INFO datanode.DataNode: PacketResponder: BP-1213576767-172.21.0.2-1636908829197:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 16:54:24,755 INFO DataNode.clienttrace: src: /172.21.0.2:37722, dest: /172.21.0.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_836662780_28, offset: 0, srvID: 48a11371-81fe-4fe1-b56e-7da2a0ec46bd, blockid: BP-1213576767-172.21.0.2-1636908829197:blk_1073741826_1002, duration(ns): 330209334
[36malgo-1    |[0m 2021-11-14 16:54:24,755 INFO datanode.DataNode: PacketResponder: BP-1213576767-172.21.0.2-1636908829197:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.21.0.3:9866] terminating
[36malgo-1    |[0m 2021-11-14 16:54:24,757 INFO hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=172.21.0.2:9866, 172.21.0.3:9866 for /user/root/.sparkStaging/application_1636908833443_0001/__spark_libs__4952292821467923690.zip
[36malgo-1    |[0m 2021-11-14 16:54:24,760 INFO datanode.DataNode: Receiving BP-1213576767-172.21.0.2-1636908829197:blk_1073741827_1003 src: /172.21.0.2:37726 dest: /172.21.0.2:9866
[33malgo-2    |[0m 2021-11-14 16:54:24,762 INFO datanode.DataNode: Receiving BP-1213576767-172.21.0.2-1636908829197:blk_1073741827_1003 src: /172.21.0.2:36738 dest: /172.21.0.3:9866
[33malgo-2    |[0m 2021-11-14 16:54:25,075 INFO DataNode.clienttrace: src: /172.21.0.2:36738, dest: /172.21.0.3:9866, bytes: 128628638, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_836662780_28, offset: 0, srvID: 08e5fc0e-f287-45de-a6c5-b7ae1baa6fd1, blockid: BP-1213576767-172.21.0.2-1636908829197:blk_1073741827_1003, duration(ns): 311337675
[33malgo-2    |[0m 2021-11-14 16:54:25,075 INFO datanode.DataNode: PacketResponder: BP-1213576767-172.21.0.2-1636908829197:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 16:54:25,076 INFO DataNode.clienttrace: src: /172.21.0.2:37726, dest: /172.21.0.2:9866, bytes: 128628638, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_836662780_28, offset: 0, srvID: 48a11371-81fe-4fe1-b56e-7da2a0ec46bd, blockid: BP-1213576767-172.21.0.2-1636908829197:blk_1073741827_1003, duration(ns): 312368112
[36malgo-1    |[0m 2021-11-14 16:54:25,076 INFO datanode.DataNode: PacketResponder: BP-1213576767-172.21.0.2-1636908829197:blk_1073741827_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.21.0.3:9866] terminating
[36malgo-1    |[0m 2021-11-14 16:54:25,081 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1636908833443_0001/__spark_libs__4952292821467923690.zip is closed by DFSClient_NONMAPREDUCE_836662780_28
[36malgo-1    |[0m 21/11/14 16:54:25 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://172.21.0.2/user/root/.sparkStaging/application_1636908833443_0001/pyspark.zip
[36malgo-1    |[0m 2021-11-14 16:54:25,177 INFO hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=172.21.0.2:9866, 172.21.0.3:9866 for /user/root/.sparkStaging/application_1636908833443_0001/pyspark.zip
[36malgo-1    |[0m 2021-11-14 16:54:25,181 INFO datanode.DataNode: Receiving BP-1213576767-172.21.0.2-1636908829197:blk_1073741828_1004 src: /172.21.0.2:37730 dest: /172.21.0.2:9866
[33malgo-2    |[0m 2021-11-14 16:54:25,182 INFO datanode.DataNode: Receiving BP-1213576767-172.21.0.2-1636908829197:blk_1073741828_1004 src: /172.21.0.2:36742 dest: /172.21.0.3:9866
[33malgo-2    |[0m 2021-11-14 16:54:25,188 INFO DataNode.clienttrace: src: /172.21.0.2:36742, dest: /172.21.0.3:9866, bytes: 732492, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_836662780_28, offset: 0, srvID: 08e5fc0e-f287-45de-a6c5-b7ae1baa6fd1, blockid: BP-1213576767-172.21.0.2-1636908829197:blk_1073741828_1004, duration(ns): 3943747
[33malgo-2    |[0m 2021-11-14 16:54:25,188 INFO datanode.DataNode: PacketResponder: BP-1213576767-172.21.0.2-1636908829197:blk_1073741828_1004, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 16:54:25,189 INFO DataNode.clienttrace: src: /172.21.0.2:37730, dest: /172.21.0.2:9866, bytes: 732492, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_836662780_28, offset: 0, srvID: 48a11371-81fe-4fe1-b56e-7da2a0ec46bd, blockid: BP-1213576767-172.21.0.2-1636908829197:blk_1073741828_1004, duration(ns): 4761078
[36malgo-1    |[0m 2021-11-14 16:54:25,189 INFO datanode.DataNode: PacketResponder: BP-1213576767-172.21.0.2-1636908829197:blk_1073741828_1004, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.21.0.3:9866] terminating
[36malgo-1    |[0m 2021-11-14 16:54:25,190 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1636908833443_0001/pyspark.zip is closed by DFSClient_NONMAPREDUCE_836662780_28
[36malgo-1    |[0m 21/11/14 16:54:25 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9-src.zip -> hdfs://172.21.0.2/user/root/.sparkStaging/application_1636908833443_0001/py4j-0.10.9-src.zip
[36malgo-1    |[0m 2021-11-14 16:54:25,203 INFO hdfs.StateChange: BLOCK* allocate blk_1073741829_1005, replicas=172.21.0.2:9866, 172.21.0.3:9866 for /user/root/.sparkStaging/application_1636908833443_0001/py4j-0.10.9-src.zip
[36malgo-1    |[0m 2021-11-14 16:54:25,207 INFO datanode.DataNode: Receiving BP-1213576767-172.21.0.2-1636908829197:blk_1073741829_1005 src: /172.21.0.2:37734 dest: /172.21.0.2:9866
[33malgo-2    |[0m 2021-11-14 16:54:25,208 INFO datanode.DataNode: Receiving BP-1213576767-172.21.0.2-1636908829197:blk_1073741829_1005 src: /172.21.0.2:36746 dest: /172.21.0.3:9866
[33malgo-2    |[0m 2021-11-14 16:54:25,211 INFO DataNode.clienttrace: src: /172.21.0.2:36746, dest: /172.21.0.3:9866, bytes: 41587, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_836662780_28, offset: 0, srvID: 08e5fc0e-f287-45de-a6c5-b7ae1baa6fd1, blockid: BP-1213576767-172.21.0.2-1636908829197:blk_1073741829_1005, duration(ns): 1752600
[33malgo-2    |[0m 2021-11-14 16:54:25,211 INFO datanode.DataNode: PacketResponder: BP-1213576767-172.21.0.2-1636908829197:blk_1073741829_1005, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 16:54:25,212 INFO DataNode.clienttrace: src: /172.21.0.2:37734, dest: /172.21.0.2:9866, bytes: 41587, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_836662780_28, offset: 0, srvID: 48a11371-81fe-4fe1-b56e-7da2a0ec46bd, blockid: BP-1213576767-172.21.0.2-1636908829197:blk_1073741829_1005, duration(ns): 2464082
[36malgo-1    |[0m 2021-11-14 16:54:25,212 INFO datanode.DataNode: PacketResponder: BP-1213576767-172.21.0.2-1636908829197:blk_1073741829_1005, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.21.0.3:9866] terminating
[36malgo-1    |[0m 2021-11-14 16:54:25,213 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1636908833443_0001/py4j-0.10.9-src.zip is closed by DFSClient_NONMAPREDUCE_836662780_28
[36malgo-1    |[0m 21/11/14 16:54:25 INFO Client: Uploading resource file:/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py -> hdfs://172.21.0.2/user/root/.sparkStaging/application_1636908833443_0001/hello_py_spark_udfs.py
[36malgo-1    |[0m 2021-11-14 16:54:25,227 INFO hdfs.StateChange: BLOCK* allocate blk_1073741830_1006, replicas=172.21.0.2:9866, 172.21.0.3:9866 for /user/root/.sparkStaging/application_1636908833443_0001/hello_py_spark_udfs.py
[36malgo-1    |[0m 2021-11-14 16:54:25,230 INFO datanode.DataNode: Receiving BP-1213576767-172.21.0.2-1636908829197:blk_1073741830_1006 src: /172.21.0.2:37738 dest: /172.21.0.2:9866
[33malgo-2    |[0m 2021-11-14 16:54:25,231 INFO datanode.DataNode: Receiving BP-1213576767-172.21.0.2-1636908829197:blk_1073741830_1006 src: /172.21.0.2:36750 dest: /172.21.0.3:9866
[33malgo-2    |[0m 2021-11-14 16:54:25,234 INFO DataNode.clienttrace: src: /172.21.0.2:36750, dest: /172.21.0.3:9866, bytes: 6228, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_836662780_28, offset: 0, srvID: 08e5fc0e-f287-45de-a6c5-b7ae1baa6fd1, blockid: BP-1213576767-172.21.0.2-1636908829197:blk_1073741830_1006, duration(ns): 1650013
[33malgo-2    |[0m 2021-11-14 16:54:25,234 INFO datanode.DataNode: PacketResponder: BP-1213576767-172.21.0.2-1636908829197:blk_1073741830_1006, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 16:54:25,235 INFO DataNode.clienttrace: src: /172.21.0.2:37738, dest: /172.21.0.2:9866, bytes: 6228, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_836662780_28, offset: 0, srvID: 48a11371-81fe-4fe1-b56e-7da2a0ec46bd, blockid: BP-1213576767-172.21.0.2-1636908829197:blk_1073741830_1006, duration(ns): 2705119
[36malgo-1    |[0m 2021-11-14 16:54:25,235 INFO datanode.DataNode: PacketResponder: BP-1213576767-172.21.0.2-1636908829197:blk_1073741830_1006, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.21.0.3:9866] terminating
[36malgo-1    |[0m 2021-11-14 16:54:25,236 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1636908833443_0001/hello_py_spark_udfs.py is closed by DFSClient_NONMAPREDUCE_836662780_28
[36malgo-1    |[0m 21/11/14 16:54:25 INFO Client: Uploading resource file:/tmp/spark-cc65bcd2-9cf6-433a-ae81-58b52cb16609/__spark_conf__8974100274612962343.zip -> hdfs://172.21.0.2/user/root/.sparkStaging/application_1636908833443_0001/__spark_conf__.zip
[36malgo-1    |[0m 2021-11-14 16:54:25,350 INFO hdfs.StateChange: BLOCK* allocate blk_1073741831_1007, replicas=172.21.0.2:9866, 172.21.0.3:9866 for /user/root/.sparkStaging/application_1636908833443_0001/__spark_conf__.zip
[36malgo-1    |[0m 2021-11-14 16:54:25,353 INFO datanode.DataNode: Receiving BP-1213576767-172.21.0.2-1636908829197:blk_1073741831_1007 src: /172.21.0.2:37742 dest: /172.21.0.2:9866
[33malgo-2    |[0m 2021-11-14 16:54:25,354 INFO datanode.DataNode: Receiving BP-1213576767-172.21.0.2-1636908829197:blk_1073741831_1007 src: /172.21.0.2:36754 dest: /172.21.0.3:9866
[33malgo-2    |[0m 2021-11-14 16:54:25,358 INFO DataNode.clienttrace: src: /172.21.0.2:36754, dest: /172.21.0.3:9866, bytes: 252572, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_836662780_28, offset: 0, srvID: 08e5fc0e-f287-45de-a6c5-b7ae1baa6fd1, blockid: BP-1213576767-172.21.0.2-1636908829197:blk_1073741831_1007, duration(ns): 2114378
[33malgo-2    |[0m 2021-11-14 16:54:25,358 INFO datanode.DataNode: PacketResponder: BP-1213576767-172.21.0.2-1636908829197:blk_1073741831_1007, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 16:54:25,359 INFO DataNode.clienttrace: src: /172.21.0.2:37742, dest: /172.21.0.2:9866, bytes: 252572, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_836662780_28, offset: 0, srvID: 48a11371-81fe-4fe1-b56e-7da2a0ec46bd, blockid: BP-1213576767-172.21.0.2-1636908829197:blk_1073741831_1007, duration(ns): 3236749
[36malgo-1    |[0m 2021-11-14 16:54:25,359 INFO datanode.DataNode: PacketResponder: BP-1213576767-172.21.0.2-1636908829197:blk_1073741831_1007, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.21.0.3:9866] terminating
[36malgo-1    |[0m 2021-11-14 16:54:25,360 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1636908833443_0001/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_836662780_28
[36malgo-1    |[0m 21/11/14 16:54:25 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m 21/11/14 16:54:25 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m 21/11/14 16:54:25 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m 21/11/14 16:54:25 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m 21/11/14 16:54:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m 21/11/14 16:54:25 INFO Client: Submitting application application_1636908833443_0001 to ResourceManager
[36malgo-1    |[0m 2021-11-14 16:54:25,482 INFO capacity.CapacityScheduler: Application 'application_1636908833443_0001' is submitted without priority hence considering default queue/cluster priority: 0
[36malgo-1    |[0m 2021-11-14 16:54:25,482 INFO capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1636908833443_0001
[36malgo-1    |[0m 2021-11-14 16:54:25,499 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 1]. Use the global max attempts instead.
[36malgo-1    |[0m 2021-11-14 16:54:25,500 INFO resourcemanager.ClientRMService: Application with id 1 submitted by user root
[36malgo-1    |[0m 2021-11-14 16:54:25,501 INFO rmapp.RMAppImpl: Storing application with id application_1636908833443_0001
[36malgo-1    |[0m 2021-11-14 16:54:25,502 INFO resourcemanager.RMAuditLogger: USER=root	IP=172.21.0.2	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1636908833443_0001	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 16:54:25,508 INFO recovery.RMStateStore: Storing info for app: application_1636908833443_0001
[36malgo-1    |[0m 2021-11-14 16:54:25,508 INFO rmapp.RMAppImpl: application_1636908833443_0001 State change from NEW to NEW_SAVING on event = START
[36malgo-1    |[0m 2021-11-14 16:54:25,509 INFO rmapp.RMAppImpl: application_1636908833443_0001 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
[36malgo-1    |[0m 2021-11-14 16:54:25,510 INFO capacity.ParentQueue: Application added - appId: application_1636908833443_0001 user: root leaf-queue of parent: root #applications: 1
[36malgo-1    |[0m 2021-11-14 16:54:25,511 INFO capacity.CapacityScheduler: Accepted application application_1636908833443_0001 from user: root, in queue: default
[36malgo-1    |[0m 2021-11-14 16:54:25,522 INFO rmapp.RMAppImpl: application_1636908833443_0001 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
[36malgo-1    |[0m 2021-11-14 16:54:25,549 INFO resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1636908833443_0001_000001
[36malgo-1    |[0m 2021-11-14 16:54:25,550 INFO attempt.RMAppAttemptImpl: appattempt_1636908833443_0001_000001 State change from NEW to SUBMITTED on event = START
[36malgo-1    |[0m 2021-11-14 16:54:25,599 INFO capacity.LeafQueue: Application application_1636908833443_0001 from user: root activated in queue: default
[36malgo-1    |[0m 2021-11-14 16:54:25,599 INFO capacity.LeafQueue: Application added - appId: application_1636908833443_0001 user: root, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
[36malgo-1    |[0m 2021-11-14 16:54:25,599 INFO capacity.CapacityScheduler: Added Application Attempt appattempt_1636908833443_0001_000001 to scheduler from user root in queue default
[36malgo-1    |[0m 21/11/14 16:54:25 INFO YarnClientImpl: Submitted application application_1636908833443_0001
[36malgo-1    |[0m 2021-11-14 16:54:25,609 INFO attempt.RMAppAttemptImpl: appattempt_1636908833443_0001_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
[36malgo-1    |[0m 2021-11-14 16:54:26,508 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1636908833443_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=
[36malgo-1    |[0m 2021-11-14 16:54:26,512 INFO rmcontainer.RMContainerImpl: container_1636908833443_0001_01_000001 Container Transitioned from NEW to ALLOCATED
[36malgo-1    |[0m 2021-11-14 16:54:26,513 INFO fica.FiCaSchedulerNode: Assigned container container_1636908833443_0001_01_000001 of capacity <memory:896, vCores:1> on host algo-2:44783, which has 1 containers, <memory:896, vCores:1> used and <memory:14996, vCores:3> available after allocation
[36malgo-1    |[0m 2021-11-14 16:54:26,513 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636908833443_0001	CONTAINERID=container_1636908833443_0001_01_000001	RESOURCE=<memory:896, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 16:54:26,531 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-2:44783 for container : container_1636908833443_0001_01_000001
[36malgo-1    |[0m 2021-11-14 16:54:26,538 INFO rmcontainer.RMContainerImpl: container_1636908833443_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
[36malgo-1    |[0m 2021-11-14 16:54:26,539 INFO security.NMTokenSecretManagerInRM: Clear node set for appattempt_1636908833443_0001_000001
[36malgo-1    |[0m 2021-11-14 16:54:26,539 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.028190285 absoluteUsedCapacity=0.028190285 used=<memory:896, vCores:1> cluster=<memory:31784, vCores:8>
[36malgo-1    |[0m 2021-11-14 16:54:26,539 INFO attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1636908833443_0001 AttemptId: appattempt_1636908833443_0001_000001 MasterContainer: Container: [ContainerId: container_1636908833443_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-2:44783, NodeHttpAddress: algo-2:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 172.21.0.3:44783 }, ExecutionType: GUARANTEED, ]
[36malgo-1    |[0m 2021-11-14 16:54:26,539 INFO capacity.CapacityScheduler: Allocation proposal accepted
[36malgo-1    |[0m 2021-11-14 16:54:26,550 INFO attempt.RMAppAttemptImpl: appattempt_1636908833443_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
[36malgo-1    |[0m 2021-11-14 16:54:26,553 INFO attempt.RMAppAttemptImpl: appattempt_1636908833443_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
[36malgo-1    |[0m 2021-11-14 16:54:26,565 INFO amlauncher.AMLauncher: Launching masterappattempt_1636908833443_0001_000001
[36malgo-1    |[0m 21/11/14 16:54:26 INFO Client: Application report for application_1636908833443_0001 (state: ACCEPTED)
[36malgo-1    |[0m 21/11/14 16:54:26 INFO Client: 
[36malgo-1    |[0m 	 client token: N/A
[36malgo-1    |[0m 	 diagnostics: [Sun Nov 14 16:54:26 +0000 2021] Scheduler has assigned a container for AM, waiting for AM container to be launched
[36malgo-1    |[0m 	 ApplicationMaster host: N/A
[36malgo-1    |[0m 	 ApplicationMaster RPC port: -1
[36malgo-1    |[0m 	 queue: default
[36malgo-1    |[0m 	 start time: 1636908865499
[36malgo-1    |[0m 	 final status: UNDEFINED
[36malgo-1    |[0m 	 tracking URL: http://algo-1:8088/proxy/application_1636908833443_0001/
[36malgo-1    |[0m 	 user: root
[36malgo-1    |[0m 2021-11-14 16:54:26,624 INFO amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1636908833443_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-2:44783, NodeHttpAddress: algo-2:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 172.21.0.3:44783 }, ExecutionType: GUARANTEED, ] for AM appattempt_1636908833443_0001_000001
[36malgo-1    |[0m 2021-11-14 16:54:26,625 INFO security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1636908833443_0001_000001
[36malgo-1    |[0m 2021-11-14 16:54:26,628 INFO security.AMRMTokenSecretManager: Creating password for appattempt_1636908833443_0001_000001
[33malgo-2    |[0m 2021-11-14 16:54:26,747 INFO ipc.Server: Auth successful for appattempt_1636908833443_0001_000001 (auth:SIMPLE)
[33malgo-2    |[0m 2021-11-14 16:54:26,830 INFO containermanager.ContainerManagerImpl: Start request for container_1636908833443_0001_01_000001 by user root
[33malgo-2    |[0m 2021-11-14 16:54:26,876 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1636908833443_0001
[33malgo-2    |[0m 2021-11-14 16:54:26,884 INFO application.ApplicationImpl: Application application_1636908833443_0001 transitioned from NEW to INITING
[33malgo-2    |[0m 2021-11-14 16:54:26,884 INFO application.ApplicationImpl: Adding container_1636908833443_0001_01_000001 to application application_1636908833443_0001
[33malgo-2    |[0m 2021-11-14 16:54:26,885 INFO nodemanager.NMAuditLogger: USER=root	IP=172.21.0.2	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1636908833443_0001	CONTAINERID=container_1636908833443_0001_01_000001
[33malgo-2    |[0m 2021-11-14 16:54:26,889 INFO application.ApplicationImpl: Application application_1636908833443_0001 transitioned from INITING to RUNNING
[33malgo-2    |[0m 2021-11-14 16:54:26,893 INFO container.ContainerImpl: Container container_1636908833443_0001_01_000001 transitioned from NEW to LOCALIZING
[33malgo-2    |[0m 2021-11-14 16:54:26,893 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1636908833443_0001
[36malgo-1    |[0m 2021-11-14 16:54:26,901 INFO amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1636908833443_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-2:44783, NodeHttpAddress: algo-2:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 172.21.0.3:44783 }, ExecutionType: GUARANTEED, ] for AM appattempt_1636908833443_0001_000001
[36malgo-1    |[0m 2021-11-14 16:54:26,901 INFO attempt.RMAppAttemptImpl: appattempt_1636908833443_0001_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
[36malgo-1    |[0m 2021-11-14 16:54:26,901 INFO rmapp.RMAppImpl: update the launch time for applicationId: application_1636908833443_0001, attemptId: appattempt_1636908833443_0001_000001launchTime: 1636908866901
[36malgo-1    |[0m 2021-11-14 16:54:26,902 INFO recovery.RMStateStore: Updating info for app: application_1636908833443_0001
[33malgo-2    |[0m 2021-11-14 16:54:26,903 INFO localizer.ResourceLocalizationService: Created localizer for container_1636908833443_0001_01_000001
[33malgo-2    |[0m 2021-11-14 16:54:26,967 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1636908833443_0001_01_000001.tokens
[33malgo-2    |[0m 2021-11-14 16:54:26,977 INFO nodemanager.DefaultContainerExecutor: Initializing user root
[33malgo-2    |[0m 2021-11-14 16:54:26,982 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1636908833443_0001_01_000001.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000001.tokens
[33malgo-2    |[0m 2021-11-14 16:54:26,983 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001
[36malgo-1    |[0m 2021-11-14 16:54:27,503 INFO rmcontainer.RMContainerImpl: container_1636908833443_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING
[36malgo-1    |[0m 21/11/14 16:54:27 INFO Client: Application report for application_1636908833443_0001 (state: ACCEPTED)
[36malgo-1    |[0m 21/11/14 16:54:28 INFO Client: Application report for application_1636908833443_0001 (state: ACCEPTED)
[36malgo-1    |[0m 21/11/14 16:54:29 INFO Client: Application report for application_1636908833443_0001 (state: ACCEPTED)
[33malgo-2    |[0m 2021-11-14 16:54:30,288 INFO container.ContainerImpl: Container container_1636908833443_0001_01_000001 transitioned from LOCALIZING to SCHEDULED
[33malgo-2    |[0m 2021-11-14 16:54:30,289 INFO scheduler.ContainerScheduler: Starting container [container_1636908833443_0001_01_000001]
[33malgo-2    |[0m 2021-11-14 16:54:30,313 INFO container.ContainerImpl: Container container_1636908833443_0001_01_000001 transitioned from SCHEDULED to RUNNING
[33malgo-2    |[0m 2021-11-14 16:54:30,314 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1636908833443_0001_01_000001
[33malgo-2    |[0m 2021-11-14 16:54:30,317 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000001/default_container_executor.sh]
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/prelaunch.out
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/prelaunch.err
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/launch_container.sh
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stdout
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr
[36malgo-1    |[0m 21/11/14 16:54:30 INFO Client: Application report for application_1636908833443_0001 (state: ACCEPTED)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551145   20 -r-x------   1 root     root        20409 Nov 14 16:54 ./__spark_libs__/kerb-simplekdc-1.0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551146   48 -r-x------   1 root     root        45781 Nov 14 16:54 ./__spark_libs__/hive-cli-2.3.7-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551147   16 -r-x------   1 root     root        15071 Nov 14 16:54 ./__spark_libs__/jta-1.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551148   16 -r-x------   1 root     root        13312 Nov 14 16:54 ./__spark_libs__/hive-shims-scheduler-2.3.7-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553075 1252 -r-x------   1 root     root      1280300 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-gamelift-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551149  764 -r-x------   1 root     root       780265 Nov 14 16:54 ./__spark_libs__/javassist-3.25.0-GA.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551150    4 -r-x------   1 root     root         2497 Nov 14 16:54 ./__spark_libs__/javax.inject-1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551151  200 -r-x------   1 root     root       203358 Nov 14 16:54 ./__spark_libs__/hk2-locator-2.6.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551152  780 -r-x------   1 root     root       796532 Nov 14 16:54 ./__spark_libs__/bcpkix-jdk15on-1.60.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551153  196 -r-x------   1 root     root       200223 Nov 14 16:54 ./__spark_libs__/hk2-api-2.6.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553076  656 -r-x------   1 root     root       671609 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-imagebuilder-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553077  140 -r-x------   1 root     root       139479 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-mobile-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551154   24 -r-x------   1 root     root        20889 Nov 14 16:54 ./__spark_libs__/metrics-jmx-4.1.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551155  244 -r-x------   1 root     root       246445 Nov 14 16:54 ./__spark_libs__/libthrift-0.12.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551156   24 -r-x------   1 root     root        22042 Nov 14 16:54 ./__spark_libs__/metrics-graphite-4.1.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553078 1632 -r-x------   1 root     root      1668359 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-rds-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553079   96 -r-x------   1 root     root        97765 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-marketplacemeteringservice-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551157  832 -r-x------   1 root     root       848783 Nov 14 16:54 ./__spark_libs__/parquet-encoding-1.10.1-spark-amzn-2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553080 1560 -r-x------   1 root     root      1595675 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-api-gateway-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551158  496 -r-x------   1 root     root       503880 Nov 14 16:54 ./__spark_libs__/commons-lang3-3.9.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551159  428 -r-x------   1 root     root       435365 Nov 14 16:54 ./__spark_libs__/netlib-native_system-linux-i686-1.1-natives.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553081  440 -r-x------   1 root     root       447924 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-groundstation-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551160   20 -r-x------   1 root     root        19827 Nov 14 16:54 ./__spark_libs__/opencsv-2.3.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551161  132 -r-x------   1 root     root       134044 Nov 14 16:54 ./__spark_libs__/aircompressor-0.10.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553082   48 -r-x------   1 root     root        47976 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-test-utils-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553083  260 -r-x------   1 root     root       262707 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-support-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551162 1180 -r-x------   1 root     root      1208080 Nov 14 16:54 ./__spark_libs__/netlib-native_ref-linux-armhf-1.1-natives.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551163 7020 -r-x------   1 root     root      7188024 Nov 14 16:54 ./__spark_libs__/spire_2.12-0.17.0-M1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551164   12 -r-x------   1 root     root         8937 Nov 14 16:54 ./__spark_libs__/hive-shims-2.3.7-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553084  248 -r-x------   1 root     root       252694 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-serverlessapplicationrepository-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551165  196 -r-x------   1 root     root       197176 Nov 14 16:54 ./__spark_libs__/commons-text-1.6.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551166   64 -r-x------   1 root     root        65261 Nov 14 16:54 ./__spark_libs__/oro-2.0.8.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551167   16 -r-x------   1 root     root        15529 Nov 14 16:54 ./__spark_libs__/jackson-jaxrs-json-provider-2.10.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551168   96 -r-x------   1 root     root        95077 Nov 14 16:54 ./__spark_libs__/parquet-common-1.10.1-spark-amzn-2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551169  296 -r-x------   1 root     root       299508 Nov 14 16:54 ./__spark_libs__/nimbus-jose-jwt-4.41.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551170 2860 -r-x------   1 root     root      2927301 Nov 14 16:54 ./__spark_libs__/hadoop-yarn-common-3.2.1-amzn-1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551171  100 -r-x------   1 root     root       100431 Nov 14 16:54 ./__spark_libs__/pyrolite-4.30.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551172  224 -r-x------   1 root     root       225999 Nov 14 16:54 ./__spark_libs__/hadoop-yarn-registry-3.2.1-amzn-1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551173  628 -r-x------   1 root     root       643043 Nov 14 16:54 ./__spark_libs__/joda-time-2.10.5.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551174 4056 -r-x------   1 root     root      4153218 Nov 14 16:54 ./__spark_libs__/netty-all-4.1.47.Final.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551175  316 -r-x------   1 root     root       322915 Nov 14 16:54 ./__spark_libs__/hadoop-yarn-client-3.2.1-amzn-1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553085  136 -r-x------   1 root     root       135896 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-mediatailor-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553086  668 -r-x------   1 root     root       680714 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-iotanalytics-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553087  636 -r-x------   1 root     root       649305 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-workspaces-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553088  224 -r-x------   1 root     root       227528 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-cloudhsm-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553089  620 -r-x------   1 root     root       631376 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-inspector-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553090 1068 -r-x------   1 root     root      1091741 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-cloudfront-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551176   60 -r-x------   1 root     root        60261 Nov 14 16:54 ./__spark_libs__/hadoop-annotations-3.2.1-amzn-1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551177   60 -r-x------   1 root     root        58684 Nov 14 16:54 ./__spark_libs__/chill-java-0.9.5.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553091  428 -r-x------   1 root     root       435660 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-datasync-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553092  512 -r-x------   1 root     root       521647 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-mechanicalturkrequester-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551178   44 -r-x------   1 root     root        43740 Nov 14 16:54 ./__spark_libs__/jackson-module-paranamer-2.10.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551179  232 -r-x------   1 root     root       233745 Nov 14 16:54 ./__spark_libs__/threeten-extra-1.5.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553093  448 -r-x------   1 root     root       455261 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-cloudsearch-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551180 7384 -r-x------   1 root     root      7558789 Nov 14 16:54 ./__spark_libs__/spark-sql_2.12-3.0.0-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553094  432 -r-x------   1 root     root       441811 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-appconfig-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553095    8 -r-x------   1 root     root         7782 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-codegen-maven-plugin-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551181 4112 -r-x------   1 root     root      4210625 Nov 14 16:54 ./__spark_libs__/zstd-jni-1.4.4-3.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553096  176 -r-x------   1 root     root       176990 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-cloud9-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553097  236 -r-x------   1 root     root       238845 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-mediastore-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551182  384 -r-x------   1 root     root       392124 Nov 14 16:54 ./__spark_libs__/velocity-1.5.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551183 1152 -r-x------   1 root     root      1175798 Nov 14 16:54 ./__spark_libs__/JTransforms-3.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553098   24 -r-x------   1 root     root        24085 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-workmailmessageflow-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553099  136 -r-x------   1 root     root       137101 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-lex-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553100  100 -r-x------   1 root     root        99335 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-connectparticipant-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553101  132 -r-x------   1 root     root       133434 Nov 14 16:54 ./__spark_libs__/aws-glue-datacatalog-spark-client-3.0.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551184  436 -r-x------   1 root     root       443231 Nov 14 16:54 ./__spark_libs__/univocity-parsers-2.8.3.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551185  164 -r-x------   1 root     root       164422 Nov 14 16:54 ./__spark_libs__/core-1.1.2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553102  520 -r-x------   1 root     root       528775 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-connect-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551186 1364 -r-x------   1 root     root      1393617 Nov 14 16:54 ./__spark_libs__/hadoop-yarn-server-common-3.2.1-amzn-1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553103 1192 -r-x------   1 root     root      1220227 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-servicecatalog-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553104 1068 -r-x------   1 root     root      1091650 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-opsworks-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553105  180 -r-x------   1 root     root       182585 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-autoscalingplans-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553106 1000 -r-x------   1 root     root      1023417 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-macie2-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551187 2252 -r-x------   1 root     root      2305169 Nov 14 16:54 ./__spark_libs__/netlib-native_ref-win-x86_64-1.1-natives.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551188  544 -r-x------   1 root     root       553993 Nov 14 16:54 ./__spark_libs__/netlib-native_system-osx-x86_64-1.1-natives.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551189   12 -r-x------   1 root     root         9498 Nov 14 16:54 ./__spark_libs__/spark-tags_2.12-3.0.0-amzn-0-tests.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1551190   28 -r-x------   1 root     root        27006 Nov 14 16:54 ./__spark_libs__/aopalliance-repackaged-2.6.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552848  116 -r-x------   1 root     root       116120 Nov 14 16:54 ./__spark_libs__/kerb-crypto-1.0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553107  208 -r-x------   1 root     root       208995 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-iot1clickprojects-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553108 2484 -r-x------   1 root     root      2541217 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-ssm-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552849   64 -r-x------   1 root     root        65464 Nov 14 16:54 ./__spark_libs__/kerb-common-1.0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552850  200 -r-x------   1 root     root       204650 Nov 14 16:54 ./__spark_libs__/kerby-pkix-1.0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553109  716 -r-x------   1 root     root       729294 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-organizations-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552851  504 -r-x------   1 root     root       512742 Nov 14 16:54 ./__spark_libs__/woodstox-core-5.0.3.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552852 2344 -r-x------   1 root     root      2397719 Nov 14 16:54 ./__spark_libs__/spark-network-common_2.12-3.0.0-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552853  360 -r-x------   1 root     root       365552 Nov 14 16:54 ./__spark_libs__/commons-compress-1.8.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553110  804 -r-x------   1 root     root       819646 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-directory-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552854  120 -r-x------   1 root     root       120527 Nov 14 16:54 ./__spark_libs__/hive-shims-common-2.3.7-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553111  344 -r-x------   1 root     root       348636 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-transcribe-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552855 10424 -r-x------   1 root     root     10672015 Nov 14 16:54 ./__spark_libs__/scala-compiler-2.12.10.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552856  656 -r-x------   1 root     root       668235 Nov 14 16:54 ./__spark_libs__/guice-4.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553112 1100 -r-x------   1 root     root      1122498 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-securityhub-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552857 1468 -r-x------   1 root     root      1502280 Nov 14 16:54 ./__spark_libs__/htrace-core4-4.1.0-incubating.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553113  176 -r-x------   1 root     root       177649 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-iot1clickdevices-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552858 3152 -r-x------   1 root     root      3226851 Nov 14 16:54 ./__spark_libs__/cats-kernel_2.12-2.0.0-M4.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552859 3596 -r-x------   1 root     root      3678534 Nov 14 16:54 ./__spark_libs__/scala-reflect-2.12.10.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552860   32 -r-x------   1 root     root        30674 Nov 14 16:54 ./__spark_libs__/kerby-config-1.0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552861  300 -r-x------   1 root     root       305001 Nov 14 16:54 ./__spark_libs__/commons-httpclient-3.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553114  132 -r-x------   1 root     root       131735 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-ioteventsdata-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552862   56 -r-x------   1 root     root        55236 Nov 14 16:54 ./__spark_libs__/geronimo-jcache_1.0_spec-1.0-alpha-1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552863  224 -r-x------   1 root     root       226672 Nov 14 16:54 ./__spark_libs__/kerb-core-1.0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553115  140 -r-x------   1 root     root       139799 Nov 14 16:54 ./__spark_libs__/aws-glue-datacatalog-hive3-client-3.0.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553116   48 -r-x------   1 root     root        47606 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-cloudwatchmetrics-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553117  404 -r-x------   1 root     root       411539 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-servicediscovery-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553118 1876 -r-x------   1 root     root      1918555 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-medialive-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552864 1712 -r-x------   1 root     root      1749371 Nov 14 16:54 ./__spark_libs__/netlib-native_ref-linux-x86_64-1.1-natives.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553119  252 -r-x------   1 root     root       254750 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-cognitosync-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552865 13504 -r-x------   1 root     root     13826799 Nov 14 16:54 ./__spark_libs__/breeze_2.12-1.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552866   96 -r-x------   1 root     root        96221 Nov 14 16:54 ./__spark_libs__/commons-pool-1.5.4.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552867 1688 -r-x------   1 root     root      1726527 Nov 14 16:54 ./__spark_libs__/ehcache-3.3.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553120  392 -r-x------   1 root     root       398329 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-elastictranscoder-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552868   92 -r-x------   1 root     root        91930 Nov 14 16:54 ./__spark_libs__/jakarta.validation-api-2.0.2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552869   28 -r-x------   1 root     root        28355 Nov 14 16:54 ./__spark_libs__/spark-ganglia-lgpl_2.12-3.0.0-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553121  684 -r-x------   1 root     root       699987 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-personalize-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552870   44 -r-x------   1 root     root        44513 Nov 14 16:54 ./__spark_libs__/gmetric4j-1.0.10.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552871   20 -r-x------   1 root     root        19479 Nov 14 16:54 ./__spark_libs__/osgi-resource-locator-1.0.3.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552872  900 -r-x------   1 root     root       918300 Nov 14 16:54 ./__spark_libs__/hive-serde-2.3.7-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553122  192 -r-x------   1 root     root       194406 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-cloudhsmv2-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553123 1040 -r-x------   1 root     root      1061132 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-s3-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552873   16 -r-x------   1 root     root        14395 Nov 14 16:54 ./__spark_libs__/generex-1.0.2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553124  728 -r-x------   1 root     root       743779 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-sesv2-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552874  908 -r-x------   1 root     root       926574 Nov 14 16:54 ./__spark_libs__/janino-3.0.16.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553125  136 -r-x------   1 root     root       138924 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-marketplacecatalog-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553126  488 -r-x------   1 root     root       496063 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-ecr-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553127  248 -r-x------   1 root     root       252611 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-computeoptimizer-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552875   76 -r-x------   1 root     root        76983 Nov 14 16:54 ./__spark_libs__/guice-servlet-4.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553128  616 -r-x------   1 root     root       629711 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-kendra-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552876   12 -r-x------   1 root     root        12211 Nov 14 16:54 ./__spark_libs__/slf4j-log4j12-1.7.30.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553129  764 -r-x------   1 root     root       779663 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-codepipeline-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552877  232 -r-x------   1 root     root       234942 Nov 14 16:54 ./__spark_libs__/hive-storage-api-2.7.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553130  948 -r-x------   1 root     root       966965 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-apigatewayv2-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552878  432 -r-x------   1 root     root       438843 Nov 14 16:54 ./__spark_libs__/hive-common-2.3.7-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552879  200 -r-x------   1 root     root       201965 Nov 14 16:54 ./__spark_libs__/curator-framework-2.13.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553131  368 -r-x------   1 root     root       374969 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-codeguruprofiler-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553132  296 -r-x------   1 root     root       300742 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-mediapackagevod-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552880   56 -r-x------   1 root     root        54509 Nov 14 16:54 ./__spark_libs__/native_system-java-1.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552881  244 -r-x------   1 root     root       249790 Nov 14 16:54 ./__spark_libs__/javax.jdo-3.2.0-m3.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553133  384 -r-x------   1 root     root       393130 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-networkmanager-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553134  104 -r-x------   1 root     root       103083 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-outposts-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552882  576 -r-x------   1 root     root       588337 Nov 14 16:54 ./__spark_libs__/commons-collections-3.2.2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552883   28 -r-x------   1 root     root        25475 Nov 14 16:54 ./__spark_libs__/json-1.8.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553135  620 -r-x------   1 root     root       632485 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-lexmodelbuilding-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553136  228 -r-x------   1 root     root       229423 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-applicationautoscaling-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553137 1092 -r-x------   1 root     root      1117335 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-devicefarm-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553138  240 -r-x------   1 root     root       244715 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-lakeformation-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552884   24 -r-x------   1 root     root        24239 Nov 14 16:54 ./__spark_libs__/commons-daemon-1.0.13.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553139 2800 -r-x------   1 root     root      2864744 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-sagemaker-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552885   72 -r-x------   1 root     root        73349 Nov 14 16:54 ./__spark_libs__/jersey-container-servlet-core-2.30.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553140  552 -r-x------   1 root     root       563962 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-logs-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552886 1988 -r-x------   1 root     root      2035066 Nov 14 16:54 ./__spark_libs__/commons-math3-3.4.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553141  464 -r-x------   1 root     root       473688 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-kinesisvideo-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552887   68 -r-x------   1 root     root        69409 Nov 14 16:54 ./__spark_libs__/activation-1.1.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552888  416 -r-x------   1 root     root       423175 Nov 14 16:54 ./__spark_libs__/okhttp-3.12.6.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553142  776 -r-x------   1 root     root       791754 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-ses-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552889  220 -r-x------   1 root     root       222980 Nov 14 16:54 ./__spark_libs__/scala-parser-combinators_2.12-1.1.2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553143   84 -r-x------   1 root     root        82717 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-pi-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552890  788 -r-x------   1 root     root       805848 Nov 14 16:54 ./__spark_libs__/hadoop-mapreduce-client-common-3.2.1-amzn-1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553144  288 -r-x------   1 root     root       293007 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-licensemanager-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552891   56 -r-x------   1 root     root        54391 Nov 14 16:54 ./__spark_libs__/objenesis-2.5.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553145   56 -r-x------   1 root     root        53254 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-apigatewaymanagementapi-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553146  628 -r-x------   1 root     root       641941 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-stepfunctions-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553147 1808 -r-x------   1 root     root      1848502 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-waf-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553148 1824 -r-x------   1 root     root      1866513 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-lightsail-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553149  396 -r-x------   1 root     root       403163 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-discovery-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553150   96 -r-x------   1 root     root        97822 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-augmentedairuntime-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553151  544 -r-x------   1 root     root       554655 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-elasticloadbalancingv2-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552892   28 -r-x------   1 root     root        27749 Nov 14 16:54 ./__spark_libs__/orc-shims-1.5.10.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552893  320 -r-x------   1 root     root       326874 Nov 14 16:54 ./__spark_libs__/httpcore-4.4.11.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552894  244 -r-x------   1 root     root       246918 Nov 14 16:54 ./__spark_libs__/commons-beanutils-1.9.4.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552895   36 -r-x------   1 root     root        35518 Nov 14 16:54 ./__spark_libs__/zjsonpatch-0.3.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552896  448 -r-x------   1 root     root       458605 Nov 14 16:54 ./__spark_libs__/netlib-native_system-linux-x86_64-1.1-natives.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553152  308 -r-x------   1 root     root       315095 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-s3control-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552897   84 -r-x------   1 root     root        85815 Nov 14 16:54 ./__spark_libs__/jersey-media-jaxb-2.30.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552898  604 -r-x------   1 root     root       616888 Nov 14 16:54 ./__spark_libs__/commons-configuration2-2.1.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553153 1436 -r-x------   1 root     root      1469407 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-chime-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552899   20 -r-x------   1 root     root        20046 Nov 14 16:54 ./__spark_libs__/kerb-identity-1.0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552900   76 -r-x------   1 root     root        76733 Nov 14 16:54 ./__spark_libs__/jersey-hk2-2.30.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553154 40140 -r-x------   1 root     root     41099632 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-models-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552901  100 -r-x------   1 root     root       100636 Nov 14 16:54 ./__spark_libs__/jsp-api-2.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553155 1028 -r-x------   1 root     root      1051487 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-codedeploy-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552902   36 -r-x------   1 root     root        34654 Nov 14 16:54 ./__spark_libs__/paranamer-2.8.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553156  404 -r-x------   1 root     root       413024 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-managedblockchain-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553157  492 -r-x------   1 root     root       501832 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-amplify-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553158   44 -r-x------   1 root     root        43408 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-marketplaceentitlement-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552903   68 -r-x------   1 root     root        67897 Nov 14 16:54 ./__spark_libs__/jackson-annotations-2.10.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552904 1168 -r-x------   1 root     root      1194003 Nov 14 16:54 ./__spark_libs__/arpack_combined_all-0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552905 5752 -r-x------   1 root     root      5886128 Nov 14 16:54 ./__spark_libs__/spark-mllib_2.12-3.0.0-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552906    8 -r-x------   1 root     root         4467 Nov 14 16:54 ./__spark_libs__/aopalliance-1.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552907  624 -r-x------   1 root     root       637428 Nov 14 16:54 ./__spark_libs__/netlib-native_system-win-x86_64-1.1-natives.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552908   28 -r-x------   1 root     root        26514 Nov 14 16:54 ./__spark_libs__/stax-api-1.0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552909   80 -r-x------   1 root     root        79588 Nov 14 16:54 ./__spark_libs__/spire-macros_2.12-0.17.0-M1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552910 4064 -r-x------   1 root     root      4158923 Nov 14 16:54 ./__spark_libs__/hadoop-common-3.2.1-amzn-1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552911  332 -r-x------   1 root     root       336803 Nov 14 16:54 ./__spark_libs__/antlr4-runtime-4.7.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553159  896 -r-x------   1 root     root       913853 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-simpleworkflow-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552912  132 -r-x------   1 root     root       134696 Nov 14 16:54 ./__spark_libs__/breeze-macros_2.12-1.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553160  276 -r-x------   1 root     root       281444 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-codestar-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553161 1464 -r-x------   1 root     root      1498425 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-codecommit-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553162 1644 -r-x------   1 root     root      1683061 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-quicksight-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552913   28 -r-x------   1 root     root        27156 Nov 14 16:54 ./__spark_libs__/istack-commons-runtime-3.0.8.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552914 1112 -r-x------   1 root     root      1137921 Nov 14 16:54 ./__spark_libs__/spark-streaming_2.12-3.0.0-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553163  284 -r-x------   1 root     root       290178 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-datapipeline-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553164  696 -r-x------   1 root     root       711839 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-elasticbeanstalk-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552915  268 -r-x------   1 root     root       273370 Nov 14 16:54 ./__spark_libs__/commons-net-3.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553165  804 -r-x------   1 root     root       822890 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-kinesisanalyticsv2-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552916  228 -r-x------   1 root     root       232248 Nov 14 16:54 ./__spark_libs__/jackson-core-asl-1.9.13.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552917   56 -r-x------   1 root     root        56273 Nov 14 16:54 ./__spark_libs__/hive-shims-0.23-2.3.7-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552918  524 -r-x------   1 root     root       533455 Nov 14 16:54 ./__spark_libs__/protobuf-java-2.5.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552919   92 -r-x------   1 root     root        93210 Nov 14 16:54 ./__spark_libs__/super-csv-2.2.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553166 1008 -r-x------   1 root     root      1029561 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-dms-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552920   84 -r-x------   1 root     root        83632 Nov 14 16:54 ./__spark_libs__/json4s-ast_2.12-3.6.6.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552921 1864 -r-x------   1 root     root      1908681 Nov 14 16:54 ./__spark_libs__/datanucleus-rdbms-4.1.19.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552922  188 -r-x------   1 root     root       190432 Nov 14 16:54 ./__spark_libs__/gson-2.2.4.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553167  408 -r-x------   1 root     root       416480 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-cloudwatch-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552923  552 -r-x------   1 root     root       561459 Nov 14 16:54 ./__spark_libs__/netlib-native_system-win-i686-1.1-natives.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552924 1140 -r-x------   1 root     root      1166647 Nov 14 16:54 ./__spark_libs__/jersey-common-2.30.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553168  400 -r-x------   1 root     root       408589 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-eks-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552925  308 -r-x------   1 root     root       313702 Nov 14 16:54 ./__spark_libs__/libfb303-0.9.3.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553169  360 -r-x------   1 root     root       366549 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-dax-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553170  364 -r-x------   1 root     root       369379 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-worklink-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552926  320 -r-x------   1 root     root       325335 Nov 14 16:54 ./__spark_libs__/RoaringBitmap-0.7.45.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553171 1796 -r-x------   1 root     root      1835621 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-dynamodb-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552927  708 -r-x------   1 root     root       723203 Nov 14 16:54 ./__spark_libs__/parquet-format-2.4.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553172  264 -r-x------   1 root     root       268887 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-opsworkscm-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552928  184 -r-x------   1 root     root       186708 Nov 14 16:54 ./__spark_libs__/avro-mapred-1.8.2-hadoop2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553173   72 -r-x------   1 root     root        70061 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-pricing-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553174  512 -r-x------   1 root     root       520978 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-glacier-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552929   84 -r-x------   1 root     root        85902 Nov 14 16:54 ./__spark_libs__/hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552930  336 -r-x------   1 root     root       341862 Nov 14 16:54 ./__spark_libs__/jackson-module-scala_2.12-2.10.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553175   44 -r-x------   1 root     root        44094 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-marketplacecommerceanalytics-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552931  280 -r-x------   1 root     root       283653 Nov 14 16:54 ./__spark_libs__/curator-recipes-2.13.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553176  888 -r-x------   1 root     root       905885 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-emr-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552932   16 -r-x------   1 root     root        15169 Nov 14 16:54 ./__spark_libs__/spark-tags_2.12-3.0.0-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552933  176 -r-x------   1 root     root       176285 Nov 14 16:54 ./__spark_libs__/automaton-1.11-8.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552934   40 -r-x------   1 root     root        38370 Nov 14 16:54 ./__spark_libs__/hive-vector-code-gen-2.3.7-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552935   36 -r-x------   1 root     root        36175 Nov 14 16:54 ./__spark_libs__/json4s-jackson_2.12-3.6.6.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552936 8008 -r-x------   1 root     root      8198272 Nov 14 16:54 ./__spark_libs__/hive-metastore-2.3.7-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553177  216 -r-x------   1 root     root       218500 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-codegurureviewer-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552937 9292 -r-x------   1 root     root      9511792 Nov 14 16:54 ./__spark_libs__/spark-catalyst_2.12-3.0.0-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552938   60 -r-x------   1 root     root        59890 Nov 14 16:54 ./__spark_libs__/spark-kvstore_2.12-3.0.0-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552939 1144 -r-x------   1 root     root      1168113 Nov 14 16:54 ./__spark_libs__/algebra_2.12-2.0.0-M2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552940  680 -r-x------   1 root     root       693271 Nov 14 16:54 ./__spark_libs__/spark-hive_2.12-3.0.0-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553178  240 -r-x------   1 root     root       245684 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-servicequotas-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553179  112 -r-x------   1 root     root       113976 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-elasticinference-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552941  132 -r-x------   1 root     root       131590 Nov 14 16:54 ./__spark_libs__/hk2-utils-2.6.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552942  164 -r-x------   1 root     root       167761 Nov 14 16:54 ./__spark_libs__/antlr-runtime-3.5.2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552943   72 -r-x------   1 root     root        71626 Nov 14 16:54 ./__spark_libs__/commons-compiler-3.0.16.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552944 1076 -r-x------   1 root     root      1098934 Nov 14 16:54 ./__spark_libs__/parquet-column-1.10.1-spark-amzn-2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553180  700 -r-x------   1 root     root       714238 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-robomaker-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552945  280 -r-x------   1 root     root       286277 Nov 14 16:54 ./__spark_libs__/parquet-hadoop-1.10.1-spark-amzn-2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552946 2140 -r-x------   1 root     root      2189117 Nov 14 16:54 ./__spark_libs__/guava-14.0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552947   36 -r-x------   1 root     root        33786 Nov 14 16:54 ./__spark_libs__/machinist_2.12-0.6.8.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553181  392 -r-x------   1 root     root       398971 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-fms-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552948  112 -r-x------   1 root     root       112235 Nov 14 16:54 ./__spark_libs__/scala-collection-compat_2.12-2.1.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552949  304 -r-x------   1 root     root       310891 Nov 14 16:54 ./__spark_libs__/netlib-native_system-linux-armhf-1.1-natives.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553182  148 -r-x------   1 root     root       149482 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-codestarconnections-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552950  120 -r-x------   1 root     root       120316 Nov 14 16:54 ./__spark_libs__/json-smart-2.3.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552951   20 -r-x------   1 root     root        16537 Nov 14 16:54 ./__spark_libs__/jcl-over-slf4j-1.7.30.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553183  468 -r-x------   1 root     root       477556 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-machinelearning-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552952  908 -r-x------   1 root     root       927721 Nov 14 16:54 ./__spark_libs__/jersey-server-2.30.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552953  100 -r-x------   1 root     root       102174 Nov 14 16:54 ./__spark_libs__/kerby-asn1-1.0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553184  728 -r-x------   1 root     root       741552 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-elasticsearch-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553185  624 -r-x------   1 root     root       637712 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-pinpointemail-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552954  344 -r-x------   1 root     root       348635 Nov 14 16:54 ./__spark_libs__/jackson-core-2.10.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552955 1404 -r-x------   1 root     root      1437215 Nov 14 16:54 ./__spark_libs__/arrow-vector-0.15.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553186  388 -r-x------   1 root     root       396097 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-elasticloadbalancing-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552956   16 -r-x------   1 root     root        15071 Nov 14 16:54 ./__spark_libs__/transaction-api-1.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553187  364 -r-x------   1 root     root       370023 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-ram-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553188  356 -r-x------   1 root     root       364262 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-globalaccelerator-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552957   48 -r-x------   1 root     root        46646 Nov 14 16:54 ./__spark_libs__/jackson-dataformat-yaml-2.10.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1552958  264 -r-x------   1 root     root       268780 Nov 14 16:54 ./__spark_libs__/jline-2.14.6.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553189  204 -r-x------   1 root     root       205056 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-codestarnotifications-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553191    4 drwx------   3 root     root         4096 Nov 14 16:54 ./__spark_conf__
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553220    4 -r-x------   1 root     root          919 Nov 14 16:54 ./__spark_conf__/__spark_dist_cache__.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553193    4 -r-x------   1 root     root           10 Nov 14 16:54 ./__spark_conf__/metrics.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553218  128 -r-x------   1 root     root       130388 Nov 14 16:54 ./__spark_conf__/__spark_hadoop_conf__.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553192    4 -r-x------   1 root     root           10 Nov 14 16:54 ./__spark_conf__/log4j.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553194    4 drwx------   2 root     root         4096 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553206    4 -r-x------   1 root     root         2979 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/hdfs-site.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553202    4 -r-x------   1 root     root         1159 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/core-site.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553211    4 -r-x------   1 root     root         2316 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/ssl-client.xml.example
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553204    4 -r-x------   1 root     root         3593 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/container-log4j.properties.default
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553217    8 -r-x------   1 root     root         4113 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/mapred-queues.xml.template
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553210    4 -r-x------   1 root     root          188 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/hive-site.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553199    4 -r-x------   1 root     root         1973 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/yarn-site.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553198    4 -r-x------   1 root     root         3321 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/hadoop-metrics2.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553207    8 -r-x------   1 root     root         6174 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/yarn-env.sh
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553215    4 -r-x------   1 root     root         2697 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/ssl-server.xml.example
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553212   16 -r-x------   1 root     root        14890 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/log4j.properties.default
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553200   12 -r-x------   1 root     root         8260 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/capacity-scheduler.xml.default
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553197   20 -r-x------   1 root     root        16403 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/hadoop-env.sh
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553209    4 -r-x------   1 root     root         1764 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/mapred-env.sh
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553196    4 -r-x------   1 root     root          758 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/mapred-site.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553195   16 -r-x------   1 root     root        14900 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/log4j.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553203   12 -r-x------   1 root     root         8260 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/capacity-scheduler.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553208    4 -r-x------   1 root     root         1940 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/container-executor.cfg
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/directory.info] 1553216    4 -r-x------   1 root     root         1764 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/mapred-e2021-11-14 16:54:31,469 INFO monitor.ContainersMonitorImpl: container_1636908833443_0001_01_000001's ip = 172.21.0.3, and hostname = algo-2
[33malgo-2    |[0m 2021-11-14 16:54:31,474 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1636908833443_0001_01_000001 since CPU usage is not yet available.
[36malgo-1    |[0m 21/11/14 16:54:31 INFO Client: Application report for application_1636908833443_0001 (state: ACCEPTED)
[36malgo-1    |[0m 2021-11-14 16:54:32,568 INFO ipc.Server: Auth successful for appattempt_1636908833443_0001_000001 (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 16:54:32,607 INFO resourcemanager.DefaultAMSProcessor: AM registration appattempt_1636908833443_0001_000001
[36malgo-1    |[0m 2021-11-14 16:54:32,608 INFO resourcemanager.RMAuditLogger: USER=root	IP=172.21.0.3	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1636908833443_0001	APPATTEMPTID=appattempt_1636908833443_0001_000001
[36malgo-1    |[0m 2021-11-14 16:54:32,608 INFO attempt.RMAppAttemptImpl: appattempt_1636908833443_0001_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
[36malgo-1    |[0m 2021-11-14 16:54:32,608 INFO rmapp.RMAppImpl: application_1636908833443_0001 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
[36malgo-1    |[0m 21/11/14 16:54:32 INFO Client: Application report for application_1636908833443_0001 (state: RUNNING)
[36malgo-1    |[0m 21/11/14 16:54:32 INFO Client: 
[36malgo-1    |[0m 	 client token: N/A
[36malgo-1    |[0m 	 diagnostics: N/A
[36malgo-1    |[0m 	 ApplicationMaster host: 172.21.0.3
[36malgo-1    |[0m 	 ApplicationMaster RPC port: -1
[36malgo-1    |[0m 	 queue: default
[36malgo-1    |[0m 	 start time: 1636908865499
[36malgo-1    |[0m 	 final status: UNDEFINED
[36malgo-1    |[0m 	 tracking URL: http://algo-1:8088/proxy/application_1636908833443_0001/
[36malgo-1    |[0m 	 user: root
[36malgo-1    |[0m 21/11/14 16:54:32 INFO YarnClientSchedulerBackend: Application application_1636908833443_0001 has started running.
[36malgo-1    |[0m 21/11/14 16:54:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41233.
[36malgo-1    |[0m 21/11/14 16:54:32 INFO NettyBlockTransferService: Server created on 172.21.0.2:41233
[36malgo-1    |[0m 21/11/14 16:54:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[36malgo-1    |[0m 21/11/14 16:54:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.21.0.2, 41233, None)
[36malgo-1    |[0m 21/11/14 16:54:32 INFO BlockManagerMasterEndpoint: Registering block manager 172.21.0.2:41233 with 1007.8 MiB RAM, BlockManagerId(driver, 172.21.0.2, 41233, None)
[36malgo-1    |[0m 21/11/14 16:54:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.21.0.2, 41233, None)
[36malgo-1    |[0m 21/11/14 16:54:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.21.0.2, 41233, None)
[36malgo-1    |[0m 21/11/14 16:54:32 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1.spark-network, PROXY_URI_BASES -> http://algo-1.spark-network:8088/proxy/application_1636908833443_0001), /proxy/application_1636908833443_0001
[36malgo-1    |[0m 21/11/14 16:54:32 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[36malgo-1    |[0m 21/11/14 16:54:33 INFO SharedState: loading hive config file: file:/etc/spark/conf.dist/hive-site.xml
[36malgo-1    |[0m 21/11/14 16:54:33 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/lib/spark/spark-warehouse').
[36malgo-1    |[0m 21/11/14 16:54:33 INFO SharedState: Warehouse path is 'file:/usr/lib/spark/spark-warehouse'.
[36malgo-1    |[0m 21/11/14 16:54:33 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 16:54:33 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 16:54:33 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 16:54:33 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 16:54:33 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 16:54:33 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
[36malgo-1    |[0m Created spark context
[36malgo-1    |[0m Created sql context
[36malgo-1    |[0m 21/11/14 16:54:33 INFO InMemoryFileIndex: It took 29 ms to list leaf files for 1 paths.
[36malgo-1    |[0m 21/11/14 16:54:34 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr] Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr] 21/11/14 16:54:31 INFO SignalUtils: Registered signal handler for TERM
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr] 21/11/14 16:54:31 INFO SignalUtils: Registered signal handler for HUP
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr] 21/11/14 16:54:31 INFO SignalUtils: Registered signal handler for INT
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr] 21/11/14 16:54:31 INFO SecurityManager: Changing view acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr] 21/11/14 16:54:31 INFO SecurityManager: Changing modify acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr] 21/11/14 16:54:31 INFO SecurityManager: Changing view acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr] 21/11/14 16:54:31 INFO SecurityManager: Changing modify acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr] 21/11/14 16:54:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr] 21/11/14 16:54:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr] 21/11/14 16:54:32 INFO ApplicationMaster: ApplicationAttemptId: appattempt_1636908833443_0001_000001
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr] 21/11/14 16:54:32 INFO RMProxy: Connecting to ResourceManager at /172.21.0.2:8030
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr] 21/11/14 16:54:32 INFO YarnRMClient: Registering the ApplicationMaster
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr] 21/11/14 16:54:32 INFO TransportClientFactory: Successfully created connection to /172.21.0.2:38805 after 87 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr] 21/11/14 16:54:32 INFO ApplicationMaster: Preparing Local resources
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr] 21/11/14 16:54:33 INFO ApplicationMaster: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr] ===============================================================================
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr] Default YARN executor launch context:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]   env:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]     CLASSPATH -> /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]     SPARK_YARN_STAGING_DIR -> hdfs://172.21.0.2/user/root/.sparkStaging/application_1636908833443_0001
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]     SPARK_USER -> root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]     PYTHONPATH -> {{PWD}}/__pyfiles__<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9-src.zip
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]   command:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]     LD_LIBRARY_PATH=\"/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:$LD_LIBRARY_PATH\" \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       {{JAVA_HOME}}/bin/java \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       -server \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       -Xmx2048m \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       '-verbose:gc' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       '-XX:OnOutOfMemoryError=kill -9 %p' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       '-XX:+PrintGCDetails' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       '-XX:+PrintGCDateStamps' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       '-XX:+UseParallelGC' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       '-XX:InitiatingHeapOccupancyPercent=70' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       '-XX:ConcGCThreads=1' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       '-XX:ParallelGCThreads=3' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       -Djava.io.tmpdir={{PWD}}/tmp \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       '-Dspark.rpc.askTimeout=300s' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       '-Dspark.driver.port=38805' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       -Dspark.yarn.app.container.log.dir=<LOG_DIR> \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       org.apache.spark.executor.YarnCoarseGrainedExecutorBackend \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       --driver-url \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       spark://CoarseGrainedScheduler@172.21.0.2:38805 \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       --executor-id \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       <executorId> \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       --hostname \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       <hostname> \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       --cores \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       1 \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       --app-id \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       application_1636908833443_0001 \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       --resourceProfileId \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       0 \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       --user-class-path \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       file:$PWD/__app__.jar \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       1><LOG_DIR>/stdout \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]       2><LOG_DIR>/stderr
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]   resources:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]     __spark_conf__ -> resource { scheme: "hdfs" host: "172.21.0.2" port: -1 file: "/user/root/.sparkStaging/application_1636908833443_0001/__spark_conf__.zip" } size: 252572 timestamp: 1636908865360 type: ARCHIVE visibility: PRIVATE
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]     pyspark.zip -> resource { scheme: "hdfs" host: "172.21.0.2" port: -1 file: "/user/root/.sparkStaging/application_1636908833443_0001/pyspark.zip" } size: 732492 timestamp: 1636908865190 type: FILE visibility: PRIVATE
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]     py4j-0.10.9-src.zip -> resource { scheme: "hdfs" host: "172.21.0.2" port: -1 file: "/user/root/.sparkStaging/application_1636908833443_0001/py4j-0.10.9-src.zip" } size: 41587 timestamp: 1636908865213 type: FILE visibility: PRIVATE
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]     __pyfiles__/hello_py_spark_udfs.py -> resource { scheme: "hdfs" host: "172.21.0.2" port: -1 file: "/user/root/.sparkStaging/application_1636908833443_0001/hello_py_spark_udfs.py" } size: 6228 timestamp: 1636908865236 type: FILE visibility: PRIVATE
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000001/stderr]     __spark_libs__ -> resource { scheme: "hdfs" host: "172.21.0.2" port: -1 file: "/user/root/.sparkStaging/application_1636908833443_0001/__spark_libs__4952292821467923690.zip" } size: 397064094 timestamp: 1636908865080 type: ARCHIVE visibility: PRIVATE
[36malgo-1    |[0m 2021-11-14 16:54:34,499 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1636908833443_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=
[36malgo-1    |[0m 2021-11-14 16:54:34,499 INFO rmcontainer.RMContainerImpl: container_1636908833443_0001_01_000002 Container Transitioned from NEW to ALLOCATED
[36malgo-1    |[0m 2021-11-14 16:54:34,499 INFO fica.FiCaSchedulerNode: Assigned container container_1636908833443_0001_01_000002 of capacity <memory:3287, vCores:1> on host algo-1:40223, which has 1 containers, <memory:3287, vCores:1> used and <memory:12605, vCores:3> available after allocation
[36malgo-1    |[0m 2021-11-14 16:54:34,499 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636908833443_0001	CONTAINERID=container_1636908833443_0001_01_000002	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 16:54:34,500 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.1316071 absoluteUsedCapacity=0.1316071 used=<memory:4183, vCores:2> cluster=<memory:31784, vCores:8>
[36malgo-1    |[0m 2021-11-14 16:54:34,500 INFO capacity.CapacityScheduler: Allocation proposal accepted
[36malgo-1    |[0m 2021-11-14 16:54:34,515 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1636908833443_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=
[36malgo-1    |[0m 2021-11-14 16:54:34,515 INFO rmcontainer.RMContainerImpl: container_1636908833443_0001_01_000003 Container Transitioned from NEW to ALLOCATED
[36malgo-1    |[0m 2021-11-14 16:54:34,515 INFO fica.FiCaSchedulerNode: Assigned container container_1636908833443_0001_01_000003 of capacity <memory:3287, vCores:1> on host algo-2:44783, which has 2 containers, <memory:4183, vCores:2> used and <memory:11709, vCores:2> available after allocation
[36malgo-1    |[0m 2021-11-14 16:54:34,515 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636908833443_0001	CONTAINERID=container_1636908833443_0001_01_000003	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 16:54:34,516 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.23502392 absoluteUsedCapacity=0.23502392 used=<memory:7470, vCores:3> cluster=<memory:31784, vCores:8>
[36malgo-1    |[0m 2021-11-14 16:54:34,516 INFO capacity.CapacityScheduler: Allocation proposal accepted
[36malgo-1    |[0m 2021-11-14 16:54:34,657 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:40223 for container : container_1636908833443_0001_01_000002
[36malgo-1    |[0m 2021-11-14 16:54:34,658 INFO rmcontainer.RMContainerImpl: container_1636908833443_0001_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
[36malgo-1    |[0m 2021-11-14 16:54:34,659 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-2:44783 for container : container_1636908833443_0001_01_000003
[36malgo-1    |[0m 2021-11-14 16:54:34,660 INFO rmcontainer.RMContainerImpl: container_1636908833443_0001_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
[33malgo-2    |[0m 2021-11-14 16:54:34,777 INFO ipc.Server: Auth successful for appattempt_1636908833443_0001_000001 (auth:SIMPLE)
[33malgo-2    |[0m 2021-11-14 16:54:34,781 INFO containermanager.ContainerManagerImpl: Start request for container_1636908833443_0001_01_000003 by user root
[33malgo-2    |[0m 2021-11-14 16:54:34,783 INFO nodemanager.NMAuditLogger: USER=root	IP=172.21.0.3	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1636908833443_0001	CONTAINERID=container_1636908833443_0001_01_000003
[33malgo-2    |[0m 2021-11-14 16:54:34,783 INFO application.ApplicationImpl: Adding container_1636908833443_0001_01_000003 to application application_1636908833443_0001
[33malgo-2    |[0m 2021-11-14 16:54:34,784 INFO container.ContainerImpl: Container container_1636908833443_0001_01_000003 transitioned from NEW to LOCALIZING
[33malgo-2    |[0m 2021-11-14 16:54:34,784 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1636908833443_0001
[33malgo-2    |[0m 2021-11-14 16:54:34,785 INFO container.ContainerImpl: Container container_1636908833443_0001_01_000003 transitioned from LOCALIZING to SCHEDULED
[33malgo-2    |[0m 2021-11-14 16:54:34,785 INFO scheduler.ContainerScheduler: Starting container [container_1636908833443_0001_01_000003]
[33malgo-2    |[0m 2021-11-14 16:54:34,799 INFO container.ContainerImpl: Container container_1636908833443_0001_01_000003 transitioned from SCHEDULED to RUNNING
[33malgo-2    |[0m 2021-11-14 16:54:34,799 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1636908833443_0001_01_000003
[33malgo-2    |[0m 2021-11-14 16:54:34,802 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/default_container_executor.sh]
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/prelaunch.out
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/prelaunch.err
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/launch_container.sh
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stdout
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr
[36malgo-1    |[0m 2021-11-14 16:54:34,851 INFO ipc.Server: Auth successful for appattempt_1636908833443_0001_000001 (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 16:54:34,928 INFO containermanager.ContainerManagerImpl: Start request for container_1636908833443_0001_01_000002 by user root
[36malgo-1    |[0m 2021-11-14 16:54:34,975 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1636908833443_0001
[36malgo-1    |[0m 2021-11-14 16:54:34,983 INFO application.ApplicationImpl: Application application_1636908833443_0001 transitioned from NEW to INITING
[36malgo-1    |[0m 2021-11-14 16:54:34,983 INFO nodemanager.NMAuditLogger: USER=root	IP=172.21.0.3	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1636908833443_0001	CONTAINERID=container_1636908833443_0001_01_000002
[36malgo-1    |[0m 2021-11-14 16:54:34,983 INFO application.ApplicationImpl: Adding container_1636908833443_0001_01_000002 to application application_1636908833443_0001
[36malgo-1    |[0m 2021-11-14 16:54:34,987 INFO application.ApplicationImpl: Application application_1636908833443_0001 transitioned from INITING to RUNNING
[36malgo-1    |[0m 2021-11-14 16:54:34,990 INFO container.ContainerImpl: Container container_1636908833443_0001_01_000002 transitioned from NEW to LOCALIZING
[36malgo-1    |[0m 2021-11-14 16:54:34,990 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1636908833443_0001
[36malgo-1    |[0m 2021-11-14 16:54:34,999 INFO localizer.ResourceLocalizationService: Created localizer for container_1636908833443_0001_01_000002
[36malgo-1    |[0m 2021-11-14 16:54:35,069 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1636908833443_0001_01_000002.tokens
[36malgo-1    |[0m 2021-11-14 16:54:35,080 INFO nodemanager.DefaultContainerExecutor: Initializing user root
[36malgo-1    |[0m 2021-11-14 16:54:35,085 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1636908833443_0001_01_000002.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002.tokens
[36malgo-1    |[0m 2021-11-14 16:54:35,085 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001
[36malgo-1    |[0m 2021-11-14 16:54:35,516 INFO rmcontainer.RMContainerImpl: container_1636908833443_0001_01_000003 Container Transitioned from ACQUIRED to RUNNING
[36malgo-1    |[0m 2021-11-14 16:54:35,517 INFO rmcontainer.RMContainerImpl: container_1636908833443_0001_01_000002 Container Transitioned from ACQUIRED to RUNNING
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1551186 1364 -r-x------   1 root     root      1393617 Nov 14 16:54 ./__spark_libs__/hadoop-yarn-server-common-3.2.1-amzn-1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553103 1192 -r-x------   1 root     root      1220227 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-servicecatalog-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553104 1068 -r-x------   1 root     root      1091650 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-opsworks-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553105  180 -r-x------   1 root     root       182585 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-autoscalingplans-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553106 1000 -r-x------   1 root     root      1023417 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-macie2-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1551187 2252 -r-x------   1 root     root      2305169 Nov 14 16:54 ./__spark_libs__/netlib-native_ref-win-x86_64-1.1-natives.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1551188  544 -r-x------   1 root     root       553993 Nov 14 16:54 ./__spark_libs__/netlib-native_system-osx-x86_64-1.1-natives.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1551189   12 -r-x------   1 root     root         9498 Nov 14 16:54 ./__spark_libs__/spark-tags_2.12-3.0.0-amzn-0-tests.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1551190   28 -r-x------   1 root     root        27006 Nov 14 16:54 ./__spark_libs__/aopalliance-repackaged-2.6.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552848  116 -r-x------   1 root     root       116120 Nov 14 16:54 ./__spark_libs__/kerb-crypto-1.0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553107  208 -r-x------   1 root     root       208995 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-iot1clickprojects-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553108 2484 -r-x------   1 root     root      2541217 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-ssm-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552849   64 -r-x------   1 root     root        65464 Nov 14 16:54 ./__spark_libs__/kerb-common-1.0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552850  200 -r-x------   1 root     root       204650 Nov 14 16:54 ./__spark_libs__/kerby-pkix-1.0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553109  716 -r-x------   1 root     root       729294 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-organizations-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552851  504 -r-x------   1 root     root       512742 Nov 14 16:54 ./__spark_libs__/woodstox-core-5.0.3.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552852 2344 -r-x------   1 root     root      2397719 Nov 14 16:54 ./__spark_libs__/spark-network-common_2.12-3.0.0-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552853  360 -r-x------   1 root     root       365552 Nov 14 16:54 ./__spark_libs__/commons-compress-1.8.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553110  804 -r-x------   1 root     root       819646 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-directory-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552854  120 -r-x------   1 root     root       120527 Nov 14 16:54 ./__spark_libs__/hive-shims-common-2.3.7-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553111  344 -r-x------   1 root     root       348636 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-transcribe-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552855 10424 -r-x------   1 root     root     10672015 Nov 14 16:54 ./__spark_libs__/scala-compiler-2.12.10.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552856  656 -r-x------   1 root     root       668235 Nov 14 16:54 ./__spark_libs__/guice-4.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553112 1100 -r-x------   1 root     root      1122498 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-securityhub-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552857 1468 -r-x------   1 root     root      1502280 Nov 14 16:54 ./__spark_libs__/htrace-core4-4.1.0-incubating.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553113  176 -r-x------   1 root     root       177649 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-iot1clickdevices-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552858 3152 -r-x------   1 root     root      3226851 Nov 14 16:54 ./__spark_libs__/cats-kernel_2.12-2.0.0-M4.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552859 3596 -r-x------   1 root     root      3678534 Nov 14 16:54 ./__spark_libs__/scala-reflect-2.12.10.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552860   32 -r-x------   1 root     root        30674 Nov 14 16:54 ./__spark_libs__/kerby-config-1.0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552861  300 -r-x------   1 root     root       305001 Nov 14 16:54 ./__spark_libs__/commons-httpclient-3.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553114  132 -r-x------   1 root     root       131735 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-ioteventsdata-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552862   56 -r-x------   1 root     root        55236 Nov 14 16:54 ./__spark_libs__/geronimo-jcache_1.0_spec-1.0-alpha-1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552863  224 -r-x------   1 root     root       226672 Nov 14 16:54 ./__spark_libs__/kerb-core-1.0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553115  140 -r-x------   1 root     root       139799 Nov 14 16:54 ./__spark_libs__/aws-glue-datacatalog-hive3-client-3.0.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553116   48 -r-x------   1 root     root        47606 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-cloudwatchmetrics-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553117  404 -r-x------   1 root     root       411539 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-servicediscovery-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553118 1876 -r-x------   1 root     root      1918555 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-medialive-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552864 1712 -r-x------   1 root     root      1749371 Nov 14 16:54 ./__spark_libs__/netlib-native_ref-linux-x86_64-1.1-natives.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553119  252 -r-x------   1 root     root       254750 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-cognitosync-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552865 13504 -r-x------   1 root     root     13826799 Nov 14 16:54 ./__spark_libs__/breeze_2.12-1.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552866   96 -r-x------   1 root     root        96221 Nov 14 16:54 ./__spark_libs__/commons-pool-1.5.4.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552867 1688 -r-x------   1 root     root      1726527 Nov 14 16:54 ./__spark_libs__/ehcache-3.3.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553120  392 -r-x------   1 root     root       398329 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-elastictranscoder-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552868   92 -r-x------   1 root     root        91930 Nov 14 16:54 ./__spark_libs__/jakarta.validation-api-2.0.2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552869   28 -r-x------   1 root     root        28355 Nov 14 16:54 ./__spark_libs__/spark-ganglia-lgpl_2.12-3.0.0-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553121  684 -r-x------   1 root     root       699987 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-personalize-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552870   44 -r-x------   1 root     root        44513 Nov 14 16:54 ./__spark_libs__/gmetric4j-1.0.10.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552871   20 -r-x------   1 root     root        19479 Nov 14 16:54 ./__spark_libs__/osgi-resource-locator-1.0.3.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552872  900 -r-x------   1 root     root       918300 Nov 14 16:54 ./__spark_libs__/hive-serde-2.3.7-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553122  192 -r-x------   1 root     root       194406 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-cloudhsmv2-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553123 1040 -r-x------   1 root     root      1061132 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-s3-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552873   16 -r-x------   1 root     root        14395 Nov 14 16:54 ./__spark_libs__/generex-1.0.2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553124  728 -r-x------   1 root     root       743779 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-sesv2-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552874  908 -r-x------   1 root     root       926574 Nov 14 16:54 ./__spark_libs__/janino-3.0.16.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553125  136 -r-x------   1 root     root       138924 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-marketplacecatalog-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553126  488 -r-x------   1 root     root       496063 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-ecr-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553127  248 -r-x------   1 root     root       252611 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-computeoptimizer-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552875   76 -r-x------   1 root     root        76983 Nov 14 16:54 ./__spark_libs__/guice-servlet-4.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553128  616 -r-x------   1 root     root       629711 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-kendra-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552876   12 -r-x------   1 root     root        12211 Nov 14 16:54 ./__spark_libs__/slf4j-log4j12-1.7.30.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553129  764 -r-x------   1 root     root       779663 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-codepipeline-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552877  232 -r-x------   1 root     root       234942 Nov 14 16:54 ./__spark_libs__/hive-storage-api-2.7.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553130  948 -r-x------   1 root     root       966965 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-apigatewayv2-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552878  432 -r-x------   1 root     root       438843 Nov 14 16:54 ./__spark_libs__/hive-common-2.3.7-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552879  200 -r-x------   1 root     root       201965 Nov 14 16:54 ./__spark_libs__/curator-framework-2.13.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553131  368 -r-x------   1 root     root       374969 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-codeguruprofiler-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553132  296 -r-x------   1 root     root       300742 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-mediapackagevod-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552880   56 -r-x------   1 root     root        54509 Nov 14 16:54 ./__spark_libs__/native_system-java-1.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552881  244 -r-x------   1 root     root       249790 Nov 14 16:54 ./__spark_libs__/javax.jdo-3.2.0-m3.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553133  384 -r-x------   1 root     root       393130 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-networkmanager-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553134  104 -r-x------   1 root     root       103083 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-outposts-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552882  576 -r-x------   1 root     root       588337 Nov 14 16:54 ./__spark_libs__/commons-collections-3.2.2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552883   28 -r-x------   1 root     root        25475 Nov 14 16:54 ./__spark_libs__/json-1.8.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553135  620 -r-x------   1 root     root       632485 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-lexmodelbuilding-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553136  228 -r-x------   1 root     root       229423 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-applicationautoscaling-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553137 1092 -r-x------   1 root     root      1117335 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-devicefarm-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553138  240 -r-x------   1 root     root       244715 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-lakeformation-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552884   24 -r-x------   1 root     root        24239 Nov 14 16:54 ./__spark_libs__/commons-daemon-1.0.13.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553139 2800 -r-x------   1 root     root      2864744 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-sagemaker-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552885   72 -r-x------   1 root     root        73349 Nov 14 16:54 ./__spark_libs__/jersey-container-servlet-core-2.30.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553140  552 -r-x------   1 root     root       563962 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-logs-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552886 1988 -r-x------   1 root     root      2035066 Nov 14 16:54 ./__spark_libs__/commons-math3-3.4.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553141  464 -r-x------   1 root     root       473688 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-kinesisvideo-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552887   68 -r-x------   1 root     root        69409 Nov 14 16:54 ./__spark_libs__/activation-1.1.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552888  416 -r-x------   1 root     root       423175 Nov 14 16:54 ./__spark_libs__/okhttp-3.12.6.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553142  776 -r-x------   1 root     root       791754 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-ses-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552889  220 -r-x------   1 root     root       222980 Nov 14 16:54 ./__spark_libs__/scala-parser-combinators_2.12-1.1.2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553143   84 -r-x------   1 root     root        82717 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-pi-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552890  788 -r-x------   1 root     root       805848 Nov 14 16:54 ./__spark_libs__/hadoop-mapreduce-client-common-3.2.1-amzn-1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553144  288 -r-x------   1 root     root       293007 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-licensemanager-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552891   56 -r-x------   1 root     root        54391 Nov 14 16:54 ./__spark_libs__/objenesis-2.5.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553145   56 -r-x------   1 root     root        53254 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-apigatewaymanagementapi-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553146  628 -r-x------   1 root     root       641941 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-stepfunctions-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553147 1808 -r-x------   1 root     root      1848502 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-waf-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553148 1824 -r-x------   1 root     root      1866513 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-lightsail-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553149  396 -r-x------   1 root     root       403163 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-discovery-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553150   96 -r-x------   1 root     root        97822 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-augmentedairuntime-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553151  544 -r-x------   1 root     root       554655 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-elasticloadbalancingv2-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552892   28 -r-x------   1 root     root        27749 Nov 14 16:54 ./__spark_libs__/orc-shims-1.5.10.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552893  320 -r-x------   1 root     root       326874 Nov 14 16:54 ./__spark_libs__/httpcore-4.4.11.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552894  244 -r-x------   1 root     root       246918 Nov 14 16:54 ./__spark_libs__/commons-beanutils-1.9.4.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552895   36 -r-x------   1 root     root        35518 Nov 14 16:54 ./__spark_libs__/zjsonpatch-0.3.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552896  448 -r-x------   1 root     root       458605 Nov 14 16:54 ./__spark_libs__/netlib-native_system-linux-x86_64-1.1-natives.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553152  308 -r-x------   1 root     root       315095 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-s3control-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552897   84 -r-x------   1 root     root        85815 Nov 14 16:54 ./__spark_libs__/jersey-media-jaxb-2.30.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552898  604 -r-x------   1 root     root       616888 Nov 14 16:54 ./__spark_libs__/commons-configuration2-2.1.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553153 1436 -r-x------   1 root     root      1469407 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-chime-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552899   20 -r-x------   1 root     root        20046 Nov 14 16:54 ./__spark_libs__/kerb-identity-1.0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552900   76 -r-x------   1 root     root        76733 Nov 14 16:54 ./__spark_libs__/jersey-hk2-2.30.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553154 40140 -r-x------   1 root     root     41099632 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-models-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552901  100 -r-x------   1 root     root       100636 Nov 14 16:54 ./__spark_libs__/jsp-api-2.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553155 1028 -r-x------   1 root     root      1051487 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-codedeploy-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552902   36 -r-x------   1 root     root        34654 Nov 14 16:54 ./__spark_libs__/paranamer-2.8.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553156  404 -r-x------   1 root     root       413024 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-managedblockchain-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553157  492 -r-x------   1 root     root       501832 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-amplify-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553158   44 -r-x------   1 root     root        43408 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-marketplaceentitlement-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552903   68 -r-x------   1 root     root        67897 Nov 14 16:54 ./__spark_libs__/jackson-annotations-2.10.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552904 1168 -r-x------   1 root     root      1194003 Nov 14 16:54 ./__spark_libs__/arpack_combined_all-0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552905 5752 -r-x------   1 root     root      5886128 Nov 14 16:54 ./__spark_libs__/spark-mllib_2.12-3.0.0-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552906    8 -r-x------   1 root     root         4467 Nov 14 16:54 ./__spark_libs__/aopalliance-1.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552907  624 -r-x------   1 root     root       637428 Nov 14 16:54 ./__spark_libs__/netlib-native_system-win-x86_64-1.1-natives.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552908   28 -r-x------   1 root     root        26514 Nov 14 16:54 ./__spark_libs__/stax-api-1.0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552909   80 -r-x------   1 root     root        79588 Nov 14 16:54 ./__spark_libs__/spire-macros_2.12-0.17.0-M1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552910 4064 -r-x------   1 root     root      4158923 Nov 14 16:54 ./__spark_libs__/hadoop-common-3.2.1-amzn-1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552911  332 -r-x------   1 root     root       336803 Nov 14 16:54 ./__spark_libs__/antlr4-runtime-4.7.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553159  896 -r-x------   1 root     root       913853 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-simpleworkflow-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552912  132 -r-x------   1 root     root       134696 Nov 14 16:54 ./__spark_libs__/breeze-macros_2.12-1.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553160  276 -r-x------   1 root     root       281444 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-codestar-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553161 1464 -r-x------   1 root     root      1498425 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-codecommit-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553162 1644 -r-x------   1 root     root      1683061 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-quicksight-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552913   28 -r-x------   1 root     root        27156 Nov 14 16:54 ./__spark_libs__/istack-commons-runtime-3.0.8.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552914 1112 -r-x------   1 root     root      1137921 Nov 14 16:54 ./__spark_libs__/spark-streaming_2.12-3.0.0-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553163  284 -r-x------   1 root     root       290178 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-datapipeline-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553164  696 -r-x------   1 root     root       711839 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-elasticbeanstalk-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552915  268 -r-x------   1 root     root       273370 Nov 14 16:54 ./__spark_libs__/commons-net-3.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553165  804 -r-x------   1 root     root       822890 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-kinesisanalyticsv2-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552916  228 -r-x------   1 root     root       232248 Nov 14 16:54 ./__spark_libs__/jackson-core-asl-1.9.13.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552917   56 -r-x------   1 root     root        56273 Nov 14 16:54 ./__spark_libs__/hive-shims-0.23-2.3.7-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552918  524 -r-x------   1 root     root       533455 Nov 14 16:54 ./__spark_libs__/protobuf-java-2.5.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552919   92 -r-x------   1 root     root        93210 Nov 14 16:54 ./__spark_libs__/super-csv-2.2.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553166 1008 -r-x------   1 root     root      1029561 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-dms-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552920   84 -r-x------   1 root     root        83632 Nov 14 16:54 ./__spark_libs__/json4s-ast_2.12-3.6.6.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552921 1864 -r-x------   1 root     root      1908681 Nov 14 16:54 ./__spark_libs__/datanucleus-rdbms-4.1.19.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552922  188 -r-x------   1 root     root       190432 Nov 14 16:54 ./__spark_libs__/gson-2.2.4.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553167  408 -r-x------   1 root     root       416480 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-cloudwatch-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552923  552 -r-x------   1 root     root       561459 Nov 14 16:54 ./__spark_libs__/netlib-native_system-win-i686-1.1-natives.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552924 1140 -r-x------   1 root     root      1166647 Nov 14 16:54 ./__spark_libs__/jersey-common-2.30.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553168  400 -r-x------   1 root     root       408589 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-eks-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552925  308 -r-x------   1 root     root       313702 Nov 14 16:54 ./__spark_libs__/libfb303-0.9.3.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553169  360 -r-x------   1 root     root       366549 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-dax-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553170  364 -r-x------   1 root     root       369379 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-worklink-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552926  320 -r-x------   1 root     root       325335 Nov 14 16:54 ./__spark_libs__/RoaringBitmap-0.7.45.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553171 1796 -r-x------   1 root     root      1835621 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-dynamodb-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552927  708 -r-x------   1 root     root       723203 Nov 14 16:54 ./__spark_libs__/parquet-format-2.4.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553172  264 -r-x------   1 root     root       268887 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-opsworkscm-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552928  184 -r-x------   1 root     root       186708 Nov 14 16:54 ./__spark_libs__/avro-mapred-1.8.2-hadoop2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553173   72 -r-x------   1 root     root        70061 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-pricing-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553174  512 -r-x------   1 root     root       520978 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-glacier-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552929   84 -r-x------   1 root     root        85902 Nov 14 16:54 ./__spark_libs__/hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552930  336 -r-x------   1 root     root       341862 Nov 14 16:54 ./__spark_libs__/jackson-module-scala_2.12-2.10.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553175   44 -r-x------   1 root     root        44094 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-marketplacecommerceanalytics-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552931  280 -r-x------   1 root     root       283653 Nov 14 16:54 ./__spark_libs__/curator-recipes-2.13.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553176  888 -r-x------   1 root     root       905885 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-emr-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552932   16 -r-x------   1 root     root        15169 Nov 14 16:54 ./__spark_libs__/spark-tags_2.12-3.0.0-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552933  176 -r-x------   1 root     root       176285 Nov 14 16:54 ./__spark_libs__/automaton-1.11-8.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552934   40 -r-x------   1 root     root        38370 Nov 14 16:54 ./__spark_libs__/hive-vector-code-gen-2.3.7-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552935   36 -r-x------   1 root     root        36175 Nov 14 16:54 ./__spark_libs__/json4s-jackson_2.12-3.6.6.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552936 8008 -r-x------   1 root     root      8198272 Nov 14 16:54 ./__spark_libs__/hive-metastore-2.3.7-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553177  216 -r-x------   1 root     root       218500 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-codegurureviewer-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552937 9292 -r-x------   1 root     root      9511792 Nov 14 16:54 ./__spark_libs__/spark-catalyst_2.12-3.0.0-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552938   60 -r-x------   1 root     root        59890 Nov 14 16:54 ./__spark_libs__/spark-kvstore_2.12-3.0.0-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552939 1144 -r-x------   1 root     root      1168113 Nov 14 16:54 ./__spark_libs__/algebra_2.12-2.0.0-M2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552940  680 -r-x------   1 root     root       693271 Nov 14 16:54 ./__spark_libs__/spark-hive_2.12-3.0.0-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553178  240 -r-x------   1 root     root       245684 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-servicequotas-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553179  112 -r-x------   1 root     root       113976 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-elasticinference-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552941  132 -r-x------   1 root     root       131590 Nov 14 16:54 ./__spark_libs__/hk2-utils-2.6.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552942  164 -r-x------   1 root     root       167761 Nov 14 16:54 ./__spark_libs__/antlr-runtime-3.5.2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552943   72 -r-x------   1 root     root        71626 Nov 14 16:54 ./__spark_libs__/commons-compiler-3.0.16.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552944 1076 -r-x------   1 root     root      1098934 Nov 14 16:54 ./__spark_libs__/parquet-column-1.10.1-spark-amzn-2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553180  700 -r-x------   1 root     root       714238 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-robomaker-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552945  280 -r-x------   1 root     root       286277 Nov 14 16:54 ./__spark_libs__/parquet-hadoop-1.10.1-spark-amzn-2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552946 2140 -r-x------   1 root     root      2189117 Nov 14 16:54 ./__spark_libs__/guava-14.0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552947   36 -r-x------   1 root     root        33786 Nov 14 16:54 ./__spark_libs__/machinist_2.12-0.6.8.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553181  392 -r-x------   1 root     root       398971 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-fms-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552948  112 -r-x------   1 root     root       112235 Nov 14 16:54 ./__spark_libs__/scala-collection-compat_2.12-2.1.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552949  304 -r-x------   1 root     root       310891 Nov 14 16:54 ./__spark_libs__/netlib-native_system-linux-armhf-1.1-natives.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553182  148 -r-x------   1 root     root       149482 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-codestarconnections-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552950  120 -r-x------   1 root     root       120316 Nov 14 16:54 ./__spark_libs__/json-smart-2.3.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552951   20 -r-x------   1 root     root        16537 Nov 14 16:54 ./__spark_libs__/jcl-over-slf4j-1.7.30.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553183  468 -r-x------   1 root     root       477556 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-machinelearning-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552952  908 -r-x------   1 root     root       927721 Nov 14 16:54 ./__spark_libs__/jersey-server-2.30.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552953  100 -r-x------   1 root     root       102174 Nov 14 16:54 ./__spark_libs__/kerby-asn1-1.0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553184  728 -r-x------   1 root     root       741552 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-elasticsearch-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553185  624 -r-x------   1 root     root       637712 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-pinpointemail-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552954  344 -r-x------   1 root     root       348635 Nov 14 16:54 ./__spark_libs__/jackson-core-2.10.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552955 1404 -r-x------   1 root     root      1437215 Nov 14 16:54 ./__spark_libs__/arrow-vector-0.15.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553186  388 -r-x------   1 root     root       396097 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-elasticloadbalancing-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552956   16 -r-x------   1 root     root        15071 Nov 14 16:54 ./__spark_libs__/transaction-api-1.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553187  364 -r-x------   1 root     root       370023 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-ram-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553188  356 -r-x------   1 root     root       364262 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-globalaccelerator-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552957   48 -r-x------   1 root     root        46646 Nov 14 16:54 ./__spark_libs__/jackson-dataformat-yaml-2.10.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1552958  264 -r-x------   1 root     root       268780 Nov 14 16:54 ./__spark_libs__/jline-2.14.6.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553189  204 -r-x------   1 root     root       205056 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-codestarnotifications-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553191    4 drwx------   3 root     root         4096 Nov 14 16:54 ./__spark_conf__
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553220    4 -r-x------   1 root     root          919 Nov 14 16:54 ./__spark_conf__/__spark_dist_cache__.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553193    4 -r-x------   1 root     root           10 Nov 14 16:54 ./__spark_conf__/metrics.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553218  128 -r-x------   1 root     root       130388 Nov 14 16:54 ./__spark_conf__/__spark_hadoop_conf__.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553192    4 -r-x------   1 root     root           10 Nov 14 16:54 ./__spark_conf__/log4j.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553194    4 drwx------   2 root     root         4096 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553206    4 -r-x------   1 root     root         2979 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/hdfs-site.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553202    4 -r-x------   1 root     root         1159 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/core-site.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553211    4 -r-x------   1 root     root         2316 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/ssl-client.xml.example
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553204    4 -r-x------   1 root     root         3593 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/container-log4j.properties.default
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553217    8 -r-x------   1 root     root         4113 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/mapred-queues.xml.template
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553210    4 -r-x------   1 root     root          188 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/hive-site.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553199    4 -r-x------   1 root     root         1973 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/yarn-site.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553198    4 -r-x------   1 root     root         3321 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/hadoop-metrics2.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553207    8 -r-x------   1 root     root         6174 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/yarn-env.sh
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553215    4 -r-x------   1 root     root         2697 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/ssl-server.xml.example
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553212   16 -r-x------   1 root     root        14890 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/log4j.properties.default
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553200   12 -r-x------   1 root     root         8260 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/capacity-scheduler.xml.default
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553197   20 -r-x------   1 root     root        16403 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/hadoop-env.sh
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553209    4 -r-x------   1 root     root         1764 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/mapred-env.sh
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553196    4 -r-x------   1 root     root          758 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/mapred-site.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553195   16 -r-x------   1 root     root        14900 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/log4j.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553203   12 -r-x------   1 root     root         8260 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/capacity-scheduler.xml
[36malgo-1    |[0m 21/11/14 16:54:36 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 16:54:36 INFO FileSourceStrategy: Pushed Filters: 
[36malgo-1    |[0m 21/11/14 16:54:36 INFO FileSourceStrategy: Post-Scan Filters: 
[36malgo-1    |[0m 21/11/14 16:54:36 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[36malgo-1    |[0m 21/11/14 16:54:36 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 318.0 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:54:36 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:54:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.21.0.2:41233 (size: 30.3 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:54:36 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 16:54:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 16:54:36 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 16:54:36 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 16:54:36 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[36malgo-1    |[0m 21/11/14 16:54:36 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[36malgo-1    |[0m 21/11/14 16:54:36 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:54:36 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:54:36 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:54:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.7 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:54:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:54:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.21.0.2:41233 (size: 7.4 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:54:37 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:54:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 16:54:37 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 16:54:37 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 1239, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/directory.info] 1553208    4 -r-x------   1 root     root         1940 Nov 14 16:54 ./__spark_conf__/__hadoo2021-11-14 16:54:37,482 INFO monitor.ContainersMonitorImpl: container_1636908833443_0001_01_000003's ip = 172.21.0.3, and hostname = algo-2
[33malgo-2    |[0m 2021-11-14 16:54:37,487 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1636908833443_0001_01_000003 since CPU usage is not yet available.
[36malgo-1    |[0m 2021-11-14 16:54:37,683 INFO scheduler.AppSchedulingInfo: checking for deactivate of application :application_1636908833443_0001
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:36 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 534@algo-2
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:36 INFO SignalUtils: Registered signal handler for TERM
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:36 INFO SignalUtils: Registered signal handler for HUP
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:36 INFO SignalUtils: Registered signal handler for INT
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:36 INFO SecurityManager: Changing view acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:36 INFO SecurityManager: Changing modify acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:36 INFO SecurityManager: Changing view acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:36 INFO SecurityManager: Changing modify acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:37 INFO TransportClientFactory: Successfully created connection to /172.21.0.2:38805 after 97 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:37 INFO SecurityManager: Changing view acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:37 INFO SecurityManager: Changing modify acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:37 INFO SecurityManager: Changing view acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:37 INFO SecurityManager: Changing modify acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:37 INFO TransportClientFactory: Successfully created connection to /172.21.0.2:38805 after 1 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:37 INFO DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/blockmgr-1499cd29-e18e-475b-bee5-6c2ab7138645
[36malgo-1    |[0m 21/11/14 16:54:37 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.21.0.3:47558) with ID 2
[36malgo-1    |[0m 21/11/14 16:54:38 INFO BlockManagerMasterEndpoint: Registering block manager algo-2:36353 with 912.3 MiB RAM, BlockManagerId(2, algo-2, 36353, None)
[36malgo-1    |[0m 21/11/14 16:54:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, algo-2, executor 2, partition 0, PROCESS_LOCAL, 7755 bytes)
[36malgo-1    |[0m 21/11/14 16:54:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-2:36353 (size: 7.4 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 2021-11-14 16:54:38,686 INFO container.ContainerImpl: Container container_1636908833443_0001_01_000002 transitioned from LOCALIZING to SCHEDULED
[36malgo-1    |[0m 2021-11-14 16:54:38,686 INFO scheduler.ContainerScheduler: Starting container [container_1636908833443_0001_01_000002]
[36malgo-1    |[0m 2021-11-14 16:54:38,715 INFO container.ContainerImpl: Container container_1636908833443_0001_01_000002 transitioned from SCHEDULED to RUNNING
[36malgo-1    |[0m 2021-11-14 16:54:38,715 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1636908833443_0001_01_000002
[36malgo-1    |[0m 2021-11-14 16:54:38,719 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/default_container_executor.sh]
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/prelaunch.out
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/prelaunch.err
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/launch_container.sh
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stdout
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:37 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:37 INFO YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@172.21.0.2:38805
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:37 INFO ResourceUtils: ==============================================================
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:37 INFO ResourceUtils: Resources for spark.executor:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:37 INFO ResourceUtils: ==============================================================
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:37 INFO YarnCoarseGrainedExecutorBackend: Successfully registered with driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:37 INFO Executor: Starting executor ID 2 on host algo-2
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:38 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36353.
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:38 INFO NettyBlockTransferService: Server created on algo-2:36353
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:38 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(2, algo-2, 36353, None)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:38 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(2, algo-2, 36353, None)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:38 INFO BlockManager: Initialized BlockManager: BlockManagerId(2, algo-2, 36353, None)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:38 INFO MetricsConfig: Loaded properties from hadoop-metrics2.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:38 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:38 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:38 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:38 INFO MetricsSystemImpl: s3a-file-system metrics system started
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:38 INFO TorrentBroadcast: Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:38 INFO TransportClientFactory: Successfully created connection to /172.21.0.2:41233 after 1 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m 2021-11-14 16:54:39,495 INFO monitor.ContainersMonitorImpl: container_1636908833443_0001_01_000002's ip = 172.21.0.2, and hostname = algo-1
[36malgo-1    |[0m 2021-11-14 16:54:39,500 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1636908833443_0001_01_000002 since CPU usage is not yet available.
[36malgo-1    |[0m 21/11/14 16:54:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-2:36353 (size: 30.3 KiB, free: 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553550  604 -r-x------   1 root     root       616888 Nov 14 16:54 ./__spark_libs__/commons-configuration2-2.1.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553806 1436 -r-x------   1 root     root      1469407 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-chime-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553551   20 -r-x------   1 root     root        20046 Nov 14 16:54 ./__spark_libs__/kerb-identity-1.0.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553552   76 -r-x------   1 root     root        76733 Nov 14 16:54 ./__spark_libs__/jersey-hk2-2.30.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553807 40140 -r-x------   1 root     root     41099632 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-models-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553553  100 -r-x------   1 root     root       100636 Nov 14 16:54 ./__spark_libs__/jsp-api-2.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553811 1028 -r-x------   1 root     root      1051487 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-codedeploy-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553554   36 -r-x------   1 root     root        34654 Nov 14 16:54 ./__spark_libs__/paranamer-2.8.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553812  404 -r-x------   1 root     root       413024 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-managedblockchain-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553813  492 -r-x------   1 root     root       501832 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-amplify-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553814   44 -r-x------   1 root     root        43408 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-marketplaceentitlement-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553555   68 -r-x------   1 root     root        67897 Nov 14 16:54 ./__spark_libs__/jackson-annotations-2.10.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553556 1168 -r-x------   1 root     root      1194003 Nov 14 16:54 ./__spark_libs__/arpack_combined_all-0.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553557 5752 -r-x------   1 root     root      5886128 Nov 14 16:54 ./__spark_libs__/spark-mllib_2.12-3.0.0-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553558    8 -r-x------   1 root     root         4467 Nov 14 16:54 ./__spark_libs__/aopalliance-1.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553559  624 -r-x------   1 root     root       637428 Nov 14 16:54 ./__spark_libs__/netlib-native_system-win-x86_64-1.1-natives.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553560   28 -r-x------   1 root     root        26514 Nov 14 16:54 ./__spark_libs__/stax-api-1.0.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553561   80 -r-x------   1 root     root        79588 Nov 14 16:54 ./__spark_libs__/spire-macros_2.12-0.17.0-M1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553562 4064 -r-x------   1 root     root      4158923 Nov 14 16:54 ./__spark_libs__/hadoop-common-3.2.1-amzn-1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553563  332 -r-x------   1 root     root       336803 Nov 14 16:54 ./__spark_libs__/antlr4-runtime-4.7.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553815  896 -r-x------   1 root     root       913853 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-simpleworkflow-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553564  132 -r-x------   1 root     root       134696 Nov 14 16:54 ./__spark_libs__/breeze-macros_2.12-1.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553816  276 -r-x------   1 root     root       281444 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-codestar-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553817 1464 -r-x------   1 root     root      1498425 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-codecommit-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553818 1644 -r-x------   1 root     root      1683061 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-quicksight-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553565   28 -r-x------   1 root     root        27156 Nov 14 16:54 ./__spark_libs__/istack-commons-runtime-3.0.8.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553566 1112 -r-x------   1 root     root      1137921 Nov 14 16:54 ./__spark_libs__/spark-streaming_2.12-3.0.0-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553819  284 -r-x------   1 root     root       290178 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-datapipeline-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553820  696 -r-x------   1 root     root       711839 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-elasticbeanstalk-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553567  268 -r-x------   1 root     root       273370 Nov 14 16:54 ./__spark_libs__/commons-net-3.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553821  804 -r-x------   1 root     root       822890 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-kinesisanalyticsv2-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553568  228 -r-x------   1 root     root       232248 Nov 14 16:54 ./__spark_libs__/jackson-core-asl-1.9.13.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553569   56 -r-x------   1 root     root        56273 Nov 14 16:54 ./__spark_libs__/hive-shims-0.23-2.3.7-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553570  524 -r-x------   1 root     root       533455 Nov 14 16:54 ./__spark_libs__/protobuf-java-2.5.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553571   92 -r-x------   1 root     root        93210 Nov 14 16:54 ./__spark_libs__/super-csv-2.2.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553822 1008 -r-x------   1 root     root      1029561 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-dms-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553572   84 -r-x------   1 root     root        83632 Nov 14 16:54 ./__spark_libs__/json4s-ast_2.12-3.6.6.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553573 1864 -r-x------   1 root     root      1908681 Nov 14 16:54 ./__spark_libs__/datanucleus-rdbms-4.1.19.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553574  188 -r-x------   1 root     root       190432 Nov 14 16:54 ./__spark_libs__/gson-2.2.4.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553823  408 -r-x------   1 root     root       416480 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-cloudwatch-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553575  552 -r-x------   1 root     root       561459 Nov 14 16:54 ./__spark_libs__/netlib-native_system-win-i686-1.1-natives.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553576 1140 -r-x------   1 root     root      1166647 Nov 14 16:54 ./__spark_libs__/jersey-common-2.30.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553824  400 -r-x------   1 root     root       408589 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-eks-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553577  308 -r-x------   1 root     root       313702 Nov 14 16:54 ./__spark_libs__/libfb303-0.9.3.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553825  360 -r-x------   1 root     root       366549 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-dax-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553826  364 -r-x------   1 root     root       369379 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-worklink-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553578  320 -r-x------   1 root     root       325335 Nov 14 16:54 ./__spark_libs__/RoaringBitmap-0.7.45.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553827 1796 -r-x------   1 root     root      1835621 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-dynamodb-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553579  708 -r-x------   1 root     root       723203 Nov 14 16:54 ./__spark_libs__/parquet-format-2.4.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553829  264 -r-x------   1 root     root       268887 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-opsworkscm-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553580  184 -r-x------   1 root     root       186708 Nov 14 16:54 ./__spark_libs__/avro-mapred-1.8.2-hadoop2.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553830   72 -r-x------   1 root     root        70061 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-pricing-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553831  512 -r-x------   1 root     root       520978 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-glacier-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553581   84 -r-x------   1 root     root        85902 Nov 14 16:54 ./__spark_libs__/hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553582  336 -r-x------   1 root     root       341862 Nov 14 16:54 ./__spark_libs__/jackson-module-scala_2.12-2.10.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553832   44 -r-x------   1 root     root        44094 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-marketplacecommerceanalytics-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553583  280 -r-x------   1 root     root       283653 Nov 14 16:54 ./__spark_libs__/curator-recipes-2.13.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553833  888 -r-x------   1 root     root       905885 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-emr-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553584   16 -r-x------   1 root     root        15169 Nov 14 16:54 ./__spark_libs__/spark-tags_2.12-3.0.0-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553585  176 -r-x------   1 root     root       176285 Nov 14 16:54 ./__spark_libs__/automaton-1.11-8.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553586   40 -r-x------   1 root     root        38370 Nov 14 16:54 ./__spark_libs__/hive-vector-code-gen-2.3.7-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553587   36 -r-x------   1 root     root        36175 Nov 14 16:54 ./__spark_libs__/json4s-jackson_2.12-3.6.6.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553588 8008 -r-x------   1 root     root      8198272 Nov 14 16:54 ./__spark_libs__/hive-metastore-2.3.7-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553834  216 -r-x------   1 root     root       218500 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-codegurureviewer-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553589 9292 -r-x------   1 root     root      9511792 Nov 14 16:54 ./__spark_libs__/spark-catalyst_2.12-3.0.0-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553590   60 -r-x------   1 root     root        59890 Nov 14 16:54 ./__spark_libs__/spark-kvstore_2.12-3.0.0-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553591 1144 -r-x------   1 root     root      1168113 Nov 14 16:54 ./__spark_libs__/algebra_2.12-2.0.0-M2.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553592  680 -r-x------   1 root     root       693271 Nov 14 16:54 ./__spark_libs__/spark-hive_2.12-3.0.0-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553835  240 -r-x------   1 root     root       245684 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-servicequotas-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553836  112 -r-x------   1 root     root       113976 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-elasticinference-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553593  132 -r-x------   1 root     root       131590 Nov 14 16:54 ./__spark_libs__/hk2-utils-2.6.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553594  164 -r-x------   1 root     root       167761 Nov 14 16:54 ./__spark_libs__/antlr-runtime-3.5.2.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553595   72 -r-x------   1 root     root        71626 Nov 14 16:54 ./__spark_libs__/commons-compiler-3.0.16.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553596 1076 -r-x------   1 root     root      1098934 Nov 14 16:54 ./__spark_libs__/parquet-column-1.10.1-spark-amzn-2.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553837  700 -r-x------   1 root     root       714238 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-robomaker-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553597  280 -r-x------   1 root     root       286277 Nov 14 16:54 ./__spark_libs__/parquet-hadoop-1.10.1-spark-amzn-2.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553598 2140 -r-x------   1 root     root      2189117 Nov 14 16:54 ./__spark_libs__/guava-14.0.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553600   36 -r-x------   1 root     root        33786 Nov 14 16:54 ./__spark_libs__/machinist_2.12-0.6.8.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553838  392 -r-x------   1 root     root       398971 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-fms-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553601  112 -r-x------   1 root     root       112235 Nov 14 16:54 ./__spark_libs__/scala-collection-compat_2.12-2.1.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553602  304 -r-x------   1 root     root       310891 Nov 14 16:54 ./__spark_libs__/netlib-native_system-linux-armhf-1.1-natives.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553839  148 -r-x------   1 root     root       149482 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-codestarconnections-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553603  120 -r-x------   1 root     root       120316 Nov 14 16:54 ./__spark_libs__/json-smart-2.3.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553604   20 -r-x------   1 root     root        16537 Nov 14 16:54 ./__spark_libs__/jcl-over-slf4j-1.7.30.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553840  468 -r-x------   1 root     root       477556 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-machinelearning-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553605  908 -r-x------   1 root     root       927721 Nov 14 16:54 ./__spark_libs__/jersey-server-2.30.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553606  100 -r-x------   1 root     root       102174 Nov 14 16:54 ./__spark_libs__/kerby-asn1-1.0.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553841  728 -r-x------   1 root     root       741552 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-elasticsearch-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553842  624 -r-x------   1 root     root       637712 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-pinpointemail-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553607  344 -r-x------   1 root     root       348635 Nov 14 16:54 ./__spark_libs__/jackson-core-2.10.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553608 1404 -r-x------   1 root     root      1437215 Nov 14 16:54 ./__spark_libs__/arrow-vector-0.15.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553843  388 -r-x------   1 root     root       396097 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-elasticloadbalancing-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553609   16 -r-x------   1 root     root        15071 Nov 14 16:54 ./__spark_libs__/transaction-api-1.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553844  364 -r-x------   1 root     root       370023 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-ram-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553845  356 -r-x------   1 root     root       364262 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-globalaccelerator-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553610   48 -r-x------   1 root     root        46646 Nov 14 16:54 ./__spark_libs__/jackson-dataformat-yaml-2.10.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553611  264 -r-x------   1 root     root       268780 Nov 14 16:54 ./__spark_libs__/jline-2.14.6.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553846  204 -r-x------   1 root     root       205056 Nov 14 16:54 ./__spark_libs__/aws-java-sdk-codestarnotifications-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553300    4 drwx------   3 root     root         4096 Nov 14 16:54 ./__spark_conf__
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553329    4 -r-x------   1 root     root          919 Nov 14 16:54 ./__spark_conf__/__spark_dist_cache__.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553302    4 -r-x------   1 root     root           10 Nov 14 16:54 ./__spark_conf__/metrics.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553327  128 -r-x------   1 root     root       130388 Nov 14 16:54 ./__spark_conf__/__spark_hadoop_conf__.xml
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553301    4 -r-x------   1 root     root           10 Nov 14 16:54 ./__spark_conf__/log4j.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553303    4 drwx------   2 root     root         4096 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553315    4 -r-x------   1 root     root         2979 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/hdfs-site.xml
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553311    4 -r-x------   1 root     root         1159 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/core-site.xml
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553320    4 -r-x------   1 root     root         2316 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/ssl-client.xml.example
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553313    4 -r-x------   1 root     root         3593 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/container-log4j.properties.default
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553326    8 -r-x------   1 root     root         4113 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/mapred-queues.xml.template
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553319    4 -r-x------   1 root     root          188 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/hive-site.xml
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553308    4 -r-x------   1 root     root         1973 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/yarn-site.xml
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553307    4 -r-x------   1 root     root         3321 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/hadoop-metrics2.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553316    8 -r-x------   1 root     root         6174 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/yarn-env.sh
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553324    4 -r-x------   1 root     root         2697 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/ssl-server.xml.example
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553321   16 -r-x------   1 root     root        14890 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/log4j.properties.default
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553309   12 -r-x------   1 root     root         8260 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/capacity-scheduler.xml.default
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553306   20 -r-x------   1 root     root        16403 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/hadoop-env.sh
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553318    4 -r-x------   1 root     root         1764 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/mapred-env.sh
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553305    4 -r-x------   1 root     root          758 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/mapred-site.xml
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553304   16 -r-x------   1 root     root        14900 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/log4j.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553312   12 -r-x------   1 root     root         8260 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/capacity-scheduler.xml
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/directory.info] 1553317    4 -r-x------   1 root     root         1940 Nov 14 16:54 ./__spark_conf__/__hadoop_conf__/container-executor.cfg
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/direct21/11/14 16:54:39 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1808 ms on algo-2 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 16:54:39 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:54:39 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 2.942 s
[36malgo-1    |[0m 21/11/14 16:54:39 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:54:39 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished
[36malgo-1    |[0m 21/11/14 16:54:39 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 2.993883 s
[36malgo-1    |[0m Loaded data
[36malgo-1    |[0m root
[36malgo-1    |[0m  |-- date: string (nullable = true)
[36malgo-1    |[0m  |-- sale: long (nullable = true)
[36malgo-1    |[0m 
[36malgo-1    |[0m 21/11/14 16:54:39 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.21.0.2:41233 in memory (size: 7.4 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:54:39 INFO BlockManagerInfo: Removed broadcast_1_piece0 on algo-2:36353 in memory (size: 7.4 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:54:40 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.21.0.2:41233 in memory (size: 30.3 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:54:40 INFO BlockManagerInfo: Removed broadcast_0_piece0 on algo-2:36353 in memory (size: 30.3 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:54:40 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 16:54:40 INFO FileSourceStrategy: Pushed Filters: IsNotNull(sale),GreaterThan(sale,750)
[36malgo-1    |[0m 21/11/14 16:54:40 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(sale#8L),(sale#8L > 750)
[36malgo-1    |[0m 21/11/14 16:54:40 INFO FileSourceStrategy: Output Data Schema: struct<date: string, sale: bigint>
[36malgo-1    |[0m 21/11/14 16:54:40 INFO CodeGenerator: Code generated in 271.97803 ms
[36malgo-1    |[0m 21/11/14 16:54:40 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 318.0 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:54:40 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:54:40 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.21.0.2:41233 (size: 30.3 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:54:40 INFO SparkContext: Created broadcast 2 from showString at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 16:54:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 16:54:40 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 16:54:40 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 16:54:40 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[36malgo-1    |[0m 21/11/14 16:54:40 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
[36malgo-1    |[0m 21/11/14 16:54:40 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:54:40 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:54:40 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:54:40 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.1 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:54:40 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:54:40 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.21.0.2:41233 (size: 6.1 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:54:40 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:54:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 16:54:40 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 16:54:40 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, algo-2, executor 2, partition 0, PROCESS_LOCAL, 7755 bytes)
[36malgo-1    |[0m 21/11/14 16:54:40 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-2:36353 (size: 6.1 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:54:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-2:36353 (size: 30.3 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:54:41 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 150 ms on algo-2 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 16:54:41 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:54:41 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.160 s
[36malgo-1    |[0m 21/11/14 16:54:41 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:54:41 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
[36malgo-1    |[0m 21/11/14 16:54:41 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.165399 s
[36malgo-1    |[0m 21/11/14 16:54:41 INFO CodeGenerator: Code generated in 19.290345 ms
[36malgo-1    |[0m +----------+----+
[36malgo-1    |[0m |      date|sale|
[36malgo-1    |[0m +----------+----+
[36malgo-1    |[0m |2020-01-05| 820|
[36malgo-1    |[0m |2020-01-06| 857|
[36malgo-1    |[0m |2020-01-02| 850|
[36malgo-1    |[0m |2020-01-07| 919|
[36malgo-1    |[0m |2020-01-07| 833|
[36malgo-1    |[0m |2020-01-07| 784|
[36malgo-1    |[0m |2020-01-07| 985|
[36malgo-1    |[0m |2020-01-04| 917|
[36malgo-1    |[0m |2020-01-05| 990|
[36malgo-1    |[0m |2020-01-05| 788|
[36malgo-1    |[0m |2020-01-01| 865|
[36malgo-1    |[0m |2020-01-04| 851|
[36malgo-1    |[0m |2020-01-02| 777|
[36malgo-1    |[0m |2020-01-04| 930|
[36malgo-1    |[0m |2020-01-02| 821|
[36malgo-1    |[0m |2020-01-02| 886|
[36malgo-1    |[0m |2020-01-07| 993|
[36malgo-1    |[0m |2020-01-06| 825|
[36malgo-1    |[0m |2020-01-02| 991|
[36malgo-1    |[0m |2020-01-03| 989|
[36malgo-1    |[0m +----------+----+
[36malgo-1    |[0m only showing top 20 rows
[36malgo-1    |[0m 
[36malgo-1    |[0m 21/11/14 16:54:41 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.21.0.2:53186) with ID 1
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:39 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 1275@algo-1
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:39 INFO SignalUtils: Registered signal handler for TERM
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:39 INFO SignalUtils: Registered signal handler for HUP
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:39 INFO SignalUtils: Registered signal handler for INT
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:40 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:40 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:40 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:40 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:40 INFO TransportClientFactory: Successfully created connection to /172.21.0.2:38805 after 99 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:41 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:41 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:41 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:41 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:41 INFO TransportClientFactory: Successfully created connection to /172.21.0.2:38805 after 1 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:41 INFO DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/blockmgr-f0d7b6ce-3ba5-4fca-a3cf-6881a085d065
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:38 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:38 INFO TorrentBroadcast: Reading broadcast variable 1 took 140 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:38 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.7 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:39 INFO FileScanRDD: TID: 0 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:39 INFO CodeGenerator: Code generated in 271.657711 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:39 INFO TorrentBroadcast: Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:39 INFO TorrentBroadcast: Reading broadcast variable 0 took 11 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 445.6 KiB, free 911.8 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:39 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2013 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:40 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 1
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:40 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:40 INFO TorrentBroadcast: Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:40 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:40 INFO TorrentBroadcast: Reading broadcast variable 3 took 11 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:40 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.1 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:40 INFO CodeGenerator: Code generated in 28.127102 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:40 INFO FileScanRDD: TID: 1 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:41 INFO MemoryStore: MemoryStore started with capacity 912.3 M21/11/14 16:54:41 INFO BlockManagerMasterEndpoint: Registering block manager algo-1:33651 with 912.3 MiB RAM, BlockManagerId(1, algo-1, 33651, None)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:40 INFO CodeGenerator: Code generated in 11.113747 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:41 INFO TorrentBroadcast: Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:41 INFO TorrentBroadcast: Reading broadcast variable 2 took 9 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 445.6 KiB, free 911.8 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:41 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1790 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:54:44 WARN YarnCoarseGrainedExecutorBackend: eagerFSInit: Unable to eagerly init filesystem s3://does/not/exist
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] java.nio.file.AccessDeniedException: does: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@2341072d: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@737fc187: Failed to connect to service endpoint: ]
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:187)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:111)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:380)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:314)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3358)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:123)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3407)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3375)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:486)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.executor.CoarseGrainedExecutorBackend.$anonfun$eagerFSInit$1(CoarseGrainedExecutorBackend.scala:274)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at scala.collection.parallel.mutable.ParArray$ParArrayIterator.flatmap2combiner(ParArray.scala:419)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at scala.collection.parallel.ParIterableLike$FlatMap.leaf(ParIterableLike.scala:1074)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at scala.collection.parallel.ParIterableLike$FlatMap.tryLeaf(ParIterableLike.scala:1070)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at scala.collection.parallel.FutureTasks.$anonfun$exec$5(Tasks.scala:499)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at scala.util.Success.$anonfun$map$1(Try.scala:255)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at scala.util.Success.map(Try.scala:213)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at java.lang.Thread.run(Thread.java:748)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] Caused by: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@2341072d: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@737fc187: Failed to connect to service endpoint: ]
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:159)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1257)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:833)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:783)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5062)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:5850)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:5823)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5046)
[36malgo-1    |[0m iB
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:41 INFO YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@172.21.0.2:38805
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:41 INFO ResourceUtils: ==============================================================
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:41 INFO ResourceUtils: Resources for spark.executor:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:41 INFO ResourceUtils: ==============================================================
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:41 INFO YarnCoarseGrainedExecutorBackend: Successfully registered with driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:41 INFO Executor: Starting executor ID 1 on host algo-1
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:41 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33651.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:41 INFO NettyBlockTransferService: Server created on algo-1:33651
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:41 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(1, algo-1, 33651, None)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(1, algo-1, 33651, None)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:41 INFO BlockManager: Initialized BlockManager: BlockManagerId(1, algo-1, 33651, None)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:41 INFO MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:41 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:41 INFO MetricsSystemImpl: s3a-file-system metrics system started
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:54:48 WARN YarnCoarseGrainedExecutorBackend: eagerFSInit: Unable to eagerly init filesystem s3://does/not/exist
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] java.nio.file.AccessDeniedException: does: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@17ac830f: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@dd53960: Failed to connect to service endpoint: ]
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:187)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:111)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:380)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:314)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3358)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:123)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3407)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3375)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:486)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.executor.CoarseGrainedExecutorBackend.$anonfun$eagerFSInit$1(CoarseGrainedExecutorBackend.scala:274)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at scala.collection.parallel.mutable.ParArray$ParArrayIterator.flatmap2combiner(ParArray.scala:419)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at scala.collection.parallel.ParIterableLike$FlatMap.leaf(ParIterableLike.scala:1074)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at scala.collection.parallel.ParIterableLike$FlatMap.tryLeaf(ParIterableLike.scala:1070)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at scala.collection.parallel.FutureTasks.$anonfun$exec$5(Tasks.scala:499)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at scala.util.Success.$anonfun$map$1(Try.scala:255)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at scala.util.Success.map(Try.scala:213)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at java.lang.Thread.run(Thread.java:748)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] Caused by: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@17ac830f: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@dd53960: Failed to connect to service endpoint: ]
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:159)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1257)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:833)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:783)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5062)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:5850)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:5823)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5046)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5008)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1416)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1352)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$verifyBucketExists$1(S3AFileSystem.java:381)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	... 32 more
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] Caused by: com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@17ac830f: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@dd53960: Failed to connect to service endpoint: ]
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at com.amazonaws.auth.AWSCre21/11/14 16:55:41 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 16:55:41 INFO FileSourceStrategy: Pushed Filters: 
[36malgo-1    |[0m 21/11/14 16:55:41 INFO FileSourceStrategy: Post-Scan Filters: 
[36malgo-1    |[0m 21/11/14 16:55:41 INFO FileSourceStrategy: Output Data Schema: struct<date: string, sale: bigint>
[36malgo-1    |[0m 21/11/14 16:55:41 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.21.0.2:41233 in memory (size: 30.3 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:41 INFO BlockManagerInfo: Removed broadcast_2_piece0 on algo-2:36353 in memory (size: 30.3 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:55:41 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.21.0.2:41233 in memory (size: 6.1 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:41 INFO BlockManagerInfo: Removed broadcast_3_piece0 on algo-2:36353 in memory (size: 6.1 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:55:41 INFO CodeGenerator: Code generated in 65.037129 ms
[36malgo-1    |[0m 21/11/14 16:55:41 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 318.0 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:55:41 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:55:41 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.21.0.2:41233 (size: 30.3 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:41 INFO SparkContext: Created broadcast 4 from collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82
[36malgo-1    |[0m 21/11/14 16:55:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 16:55:41 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 16:55:41 INFO DAGScheduler: Registering RDD 11 (collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82) as input to shuffle 0
[36malgo-1    |[0m 21/11/14 16:55:41 INFO DAGScheduler: Got map stage job 2 (collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82) with 1 output partitions
[36malgo-1    |[0m 21/11/14 16:55:41 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82)
[36malgo-1    |[0m 21/11/14 16:55:41 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:55:41 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:55:41 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:55:41 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 29.7 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:55:41 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.9 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:55:41 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.21.0.2:41233 (size: 13.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:41 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:55:41 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 16:55:41 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 16:55:41 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7744 bytes)
[36malgo-1    |[0m 21/11/14 16:55:41 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:33651 (size: 13.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:33651 (size: 30.3 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1780 ms on algo-1 (executor 1) (1/1)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:55:43 INFO DAGScheduler: ShuffleMapStage 2 (collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82) finished in 1.798 s
[36malgo-1    |[0m 21/11/14 16:55:43 INFO DAGScheduler: looking for newly runnable stages
[36malgo-1    |[0m 21/11/14 16:55:43 INFO DAGScheduler: running: Set()
[36malgo-1    |[0m 21/11/14 16:55:43 INFO DAGScheduler: waiting: Set()
[36malgo-1    |[0m 21/11/14 16:55:43 INFO DAGScheduler: failed: Set()
[36malgo-1    |[0m 21/11/14 16:55:43 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 16.
[36malgo-1    |[0m 21/11/14 16:55:43 INFO CodeGenerator: Code generated in 22.703026 ms
[36malgo-1    |[0m 21/11/14 16:55:43 INFO SparkContext: Starting job: collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82
[36malgo-1    |[0m 21/11/14 16:55:43 INFO DAGScheduler: Got job 3 (collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82) with 15 output partitions
[36malgo-1    |[0m 21/11/14 16:55:43 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:55:43 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[14] at collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:55:43 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 29.1 KiB, free 1007.4 MiB)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.0 KiB, free 1007.4 MiB)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.21.0.2:41233 (size: 14.0 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:55:43 INFO DAGScheduler: Submitting 15 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[36malgo-1    |[0m 21/11/14 16:55:43 INFO YarnScheduler: Adding task set 4.0 with 15 tasks
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3, algo-2, executor 2, partition 0, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 4, algo-1, executor 1, partition 1, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-2:36353 (size: 14.0 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:33651 (size: 14.0 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.21.0.3:47558
[36malgo-1    |[0m 21/11/14 16:55:43 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.21.0.2:53186
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 5, algo-1, executor 1, partition 2, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 4) in 257 ms on algo-1 (executor 1) (1/15)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 6, algo-1, executor 1, partition 3, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 5) in 27 ms on algo-1 (executor 1) (2/15)
[36malgo-1    |[0m dentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:136)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:137)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	... 50 more
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:41 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 2
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:41 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:41 INFO TorrentBroadcast: Started reading broadcast variable 5 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:41 INFO TransportClientFactory: Successfully created connection to /172.21.0.2:41233 after 2 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:41 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.9 KiB, free 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:41 INFO TorrentBroadcast: Reading broadcast variable 5 took 93 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:41 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 29.7 KiB, free 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:42 INFO CodeGenerator: Code generated in 331.025592 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO CodeGenerator: Code generated in 17.467755 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO CodeGenerator: Code generated in 8.562764 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO CodeGenerator: Code generated in 9.699431 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO CodeGenerator: Code generated in 7.374894 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO FileScanRDD: TID: 2 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO CodeGenerator: Code generated in 8.53523 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO TorrentBroadcast: Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 879.9 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO TorrentBroadcast: Reading broadcast variable 4 took 9 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 445.6 KiB, free 879.5 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 4574 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 4
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO Executor: Running task 1.0 in stage 4.0 (TID 4)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO TorrentBroadcast: Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.0 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO TorrentBroadcast: Reading broadcast variable 6 took 10 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 29.1 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@172.21.0.2:38805)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO MapOutputTrackerWorker: Got the output locations
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO CodeGenerator: Code generated in 23.826156 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO Executor: Finished task 1.0 in stage 4.0 (TID 4). 3705 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 5
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO Executor: Running task 2.0 in stage 4.0 (TID 5)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/lo21/11/14 16:55:43 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 7, algo-1, executor 1, partition 4, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 6) in 28 ms on algo-1 (executor 1) (3/15)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 8, algo-2, executor 2, partition 5, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 319 ms on algo-2 (executor 2) (4/15)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 9, algo-1, executor 1, partition 6, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 7) in 28 ms on algo-1 (executor 1) (5/15)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 10, algo-1, executor 1, partition 7, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 9) in 25 ms on algo-1 (executor 1) (6/15)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 11, algo-1, executor 1, partition 8, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 10) in 26 ms on algo-1 (executor 1) (7/15)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 12, algo-2, executor 2, partition 9, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 8) in 70 ms on algo-2 (executor 2) (8/15)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5008)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1416)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1352)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$verifyBucketExists$1(S3AFileSystem.java:381)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	... 32 more
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] Caused by: com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@2341072d: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@737fc187: Failed to connect to service endpoint: ]
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:136)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:137)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	... 50 more
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 3
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO TorrentBroadcast: Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.0 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO TorrentBroadcast: Reading broadcast variable 6 took 8 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 29.1 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 13, algo-1, executor 1, partition 10, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 11) in 19 ms on algo-1 (executor 1) (9/15)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 14, algo-2, executor 2, partition 11, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 12) in 27 ms on algo-2 (executor 2) (10/15)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 15, algo-1, executor 1, partition 12, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 13) in 21 ms on algo-1 (executor 1) (11/15)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 16, algo-2, executor 2, partition 13, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 14) in 21 ms on algo-2 (executor 2) (12/15)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 17, algo-1, executor 1, partition 14, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 15) in 18 ms on algo-1 (executor 1) (13/15)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 16) in 26 ms on algo-2 (executor 2) (14/15)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 17) in 17 ms on algo-1 (executor 1) (15/15)
[36malgo-1    |[0m 21/11/14 16:55:43 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:55:43 INFO DAGScheduler: ResultStage 4 (collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82) finished in 0.467 s
[36malgo-1    |[0m 21/11/14 16:55:43 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:55:43 INFO YarnScheduler: Killing all running tasks in stage 4: Stage finished
[36malgo-1    |[0m 21/11/14 16:55:43 INFO DAGScheduler: Job 3 finished: collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82, took 0.472706 s
[36malgo-1    |[0m Calculated data
[36malgo-1    |[0m 21/11/14 16:55:44 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 16:55:44 INFO FileSourceStrategy: Pushed Filters: 
[36malgo-1    |[0m 21/11/14 16:55:44 INFO FileSourceStrategy: Post-Scan Filters: 
[36malgo-1    |[0m 21/11/14 16:55:44 INFO FileSourceStrategy: Output Data Schema: struct<date: string, sale: bigint>
[36malgo-1    |[0m 21/11/14 16:55:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
[36malgo-1    |[0m 21/11/14 16:55:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[36malgo-1    |[0m 21/11/14 16:55:44 INFO DirectFileOutputCommitter: Direct Write: DISABLED
[36malgo-1    |[0m 21/11/14 16:55:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter
[36malgo-1    |[0m 21/11/14 16:55:44 INFO CodeGenerator: Code generated in 8.410148 ms
[36malgo-1    |[0m 21/11/14 16:55:44 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 318.0 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 16:55:44 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 16:55:44 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.21.0.2:41233 (size: 30.3 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:44 INFO SparkContext: Created broadcast 7 from json at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 16:55:44 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 16:55:44 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 16:55:44 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.21.0.2:41233 in memory (size: 14.0 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:44 INFO BlockManagerInfo: Removed broadcast_6_piece0 on algo-2:36353 in memory (size: 14.0 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:55:44 INFO BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:33651 in memory (size: 14.0 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:55:44 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 16:55:44 INFO DAGScheduler: Got job 4 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[36malgo-1    |[0m 21/11/14 16:55:44 INFO DAGScheduler: Final stage: ResultStage 5 (json at NativeMethodAccessorImpl.java:0)
[36malgo-1    |[0m 21/11/14 16:55:44 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:55:44 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:55:44 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[19] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:55:44 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.21.0.2:41233 in memory (size: 13.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:44 INFO BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:33651 in memory (size: 13.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:55:44 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 188.6 KiB, free 1007.0 MiB)
[36malgo-1    |[0m 21/11/14 16:55:44 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 68.8 KiB, free 1006.9 MiB)
[36malgo-1    |[0m 21/11/14 16:55:44 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.21.0.2:41233 (size: 68.8 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:55:44 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:55:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[19] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 16:55:44 INFO YarnScheduler: Adding task set 5.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 16:55:44 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 18, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7755 bytes)
[36malgo-1    |[0m 21/11/14 16:55:44 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:33651 (size: 68.8 KiB, free: 912.2 MiB)
[36malgo-1    |[0m g/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO Executor: Finished task 2.0 in stage 4.0 (TID 5). 3667 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 6
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO Executor: Running task 3.0 in stage 4.0 (TID 6)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO Executor: Finished task 3.0 in stage 4.0 (TID 6). 3709 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 7
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO Executor: Running task 4.0 in stage 4.0 (TID 7)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO Executor: Finished task 4.0 in stage 4.0 (TID 7). 3667 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 9
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO Executor: Running task 6.0 in stage 4.0 (TID 9)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO Executor: Finished task 6.0 in stage 4.0 (TID 9). 3667 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 10
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO Executor: Running task 7.0 in stage 4.0 (TID 10)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO Executor: Finished task 7.0 in stage 4.0 (TID 10). 3709 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 11
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO Executor: Running task 8.0 in stage 4.0 (TID 11)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO Executor: Finished task 8.0 in stage 4.0 (TID 11). 3667 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 13
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO Executor: Running task 10.0 in stage 4.0 (TID 13)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO Executor: Finished task 10.0 in stage 4.0 (TID 13). 3667 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 15
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO Executor: Running task 12.0 in stage 4.0 (TID 15)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO Executor: Finished task 12.0 in stage 4.0 (TID 15). 3667 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 17
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO Executor: Running task 14.0 in stage 4.0 (TID 17)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local21/11/14 16:55:44 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:33651 (size: 30.3 KiB, free: 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@172.21.0.2:38805)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO MapOutputTrackerWorker: Got the output locations
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO CodeGenerator: Code generated in 42.2404 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO CodeGenerator: Code generated in 10.035381 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO CodeGenerator: Code generated in 11.84454 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 3667 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 8
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO Executor: Running task 5.0 in stage 4.0 (TID 8)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO TransportClientFactory: Successfully created connection to algo-1/172.21.0.2:33651 after 2 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 16 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO Executor: Finished task 5.0 in stage 4.0 (TID 8). 3709 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 12
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO Executor: Running task 9.0 in stage 4.0 (TID 12)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO Executor: Finished task 9.0 in stage 4.0 (TID 12). 3709 bytes result sent to driver
[36malgo-1    |[0m 21/11/14 16:55:45 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 18) in 1301 ms on algo-1 (executor 1) (1/1)
[36malgo-1    |[0m 21/11/14 16:55:45 INFO YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:55:45 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 46181
[36malgo-1    |[0m 21/11/14 16:55:45 INFO DAGScheduler: ResultStage 5 (json at NativeMethodAccessorImpl.java:0) finished in 1.329 s
[36malgo-1    |[0m 21/11/14 16:55:45 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:55:45 INFO YarnScheduler: Killing all running tasks in stage 5: Stage finished
[36malgo-1    |[0m 21/11/14 16:55:45 INFO DAGScheduler: Job 4 finished: json at NativeMethodAccessorImpl.java:0, took 1.333782 s
[36malgo-1    |[0m 21/11/14 16:55:45 INFO FileFormatWriter: Write Job a71efde7-0a1a-47b2-9141-5e8bb59e2be8 committed.
[36malgo-1    |[0m 21/11/14 16:55:45 INFO FileFormatWriter: Finished processing stats for write job a71efde7-0a1a-47b2-9141-5e8bb59e2be8.
[36malgo-1    |[0m Saved data
[36malgo-1    |[0m 3.6.5
[36malgo-1    |[0m 21/11/14 16:55:45 INFO CodeGenerator: Code generated in 11.397326 ms
[36malgo-1    |[0m 21/11/14 16:55:45 INFO CodeGenerator: Code generated in 8.765257 ms
[36malgo-1    |[0m 21/11/14 16:55:45 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 16:55:45 INFO DAGScheduler: Got job 5 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[36malgo-1    |[0m 21/11/14 16:55:45 INFO DAGScheduler: Final stage: ResultStage 6 (showString at NativeMethodAccessorImpl.java:0)
[36malgo-1    |[0m 21/11/14 16:55:45 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:55:45 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:55:45 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[31] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:55:45 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 17.3 KiB, free 1006.9 MiB)
[36malgo-1    |[0m 21/11/14 16:55:45 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 1006.9 MiB)
[36malgo-1    |[0m 21/11/14 16:55:45 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.21.0.2:41233 (size: 8.0 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:55:45 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:55:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[31] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 16:55:45 INFO YarnScheduler: Adding task set 6.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 16:55:45 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 19, algo-2, executor 2, partition 0, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:45 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-2:36353 (size: 8.0 KiB, free: 912.3 MiB)
[36malgo-1    |[0m  and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:43 INFO Executor: Finished task 14.0 in stage 4.0 (TID 17). 3667 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:44 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 18
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:44 INFO Executor: Running task 0.0 in stage 5.0 (TID 18)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:44 INFO TorrentBroadcast: Started reading broadcast variable 8 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:44 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 68.8 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:44 INFO TorrentBroadcast: Reading broadcast variable 8 took 8 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:44 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 188.6 KiB, free 911.6 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:44 INFO CodeGenerator: Code generated in 7.259844 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:44 INFO FileScanRDD: TID: 18 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:44 INFO TorrentBroadcast: Started reading broadcast variable 7 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:44 INFO CodeGenerator: Code generated in 10.919722 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:44 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 911.6 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:44 INFO TorrentBroadcast: Reading broadcast variable 7 took 8 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:44 INFO CodeGenerator: Code generated in 10.971503 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:44 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 445.6 KiB, free 911.1 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:44 INFO DirectFileOutputCommitter: Direct Write: DISABLED
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 14
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO Executor: Running task 11.0 in stage 4.0 (TID 14)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO Executor: Finished task 11.0 in stage 4.0 (TID 14). 3708 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 16
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO Executor: Running task 13.0 in stage 4.0 (TID 16)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 2 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:43 INFO Executor: Finished task 13.0 in stage 4.0 (TID 16). 3708 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:45 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 19
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:45 INFO Executor: Running task 0.0 in stage 6.0 (TID 19)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:45 INFO TorrentBroadcast: Started reading broadcast variable 9 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:45 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:45 INFO TorrentBroadcast: Reading broadcast variable 9 took 6 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:45 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 17.3 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:46 INFO CodeGenerator: Code generated in 11.029209 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:46 INFO CodeGenerator: Code generated in 8.109461 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:46 INFO PythonRunner: Times: total = 412, boot = 399, init = 13, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:46 INFO CodeGenerator: Code generated in 9.882886 ms
[36malgo-1    |[0m [/var/log/yarn/export/use21/11/14 16:55:46 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 19) in 1241 ms on algo-2 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 16:55:46 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:55:46 INFO DAGScheduler: ResultStage 6 (showString at NativeMethodAccessorImpl.java:0) finished in 1.248 s
[36malgo-1    |[0m 21/11/14 16:55:46 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:55:46 INFO YarnScheduler: Killing all running tasks in stage 6: Stage finished
[36malgo-1    |[0m 21/11/14 16:55:46 INFO DAGScheduler: Job 5 finished: showString at NativeMethodAccessorImpl.java:0, took 1.252544 s
[36malgo-1    |[0m 21/11/14 16:55:46 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 16:55:46 INFO DAGScheduler: Got job 6 (showString at NativeMethodAccessorImpl.java:0) with 4 output partitions
[36malgo-1    |[0m 21/11/14 16:55:46 INFO DAGScheduler: Final stage: ResultStage 7 (showString at NativeMethodAccessorImpl.java:0)
[36malgo-1    |[0m 21/11/14 16:55:46 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:55:46 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:55:46 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[31] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:55:46 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 17.3 KiB, free 1006.9 MiB)
[36malgo-1    |[0m 21/11/14 16:55:46 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 1006.9 MiB)
[36malgo-1    |[0m 21/11/14 16:55:46 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.21.0.2:41233 (size: 8.0 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:55:46 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:55:46 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 7 (MapPartitionsRDD[31] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1, 2, 3, 4))
[36malgo-1    |[0m 21/11/14 16:55:46 INFO YarnScheduler: Adding task set 7.0 with 4 tasks
[36malgo-1    |[0m 21/11/14 16:55:46 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 20, algo-2, executor 2, partition 1, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:46 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 21, algo-1, executor 1, partition 2, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-2:36353 (size: 8.0 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:33651 (size: 8.0 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 22, algo-2, executor 2, partition 3, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 20) in 28 ms on algo-2 (executor 2) (1/4)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 23, algo-2, executor 2, partition 4, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 22) in 28 ms on algo-2 (executor 2) (2/4)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 23) in 52 ms on algo-2 (executor 2) (3/4)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 21) in 738 ms on algo-1 (executor 1) (4/4)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:55:47 INFO DAGScheduler: ResultStage 7 (showString at NativeMethodAccessorImpl.java:0) finished in 0.744 s
[36malgo-1    |[0m 21/11/14 16:55:47 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:55:47 INFO YarnScheduler: Killing all running tasks in stage 7: Stage finished
[36malgo-1    |[0m 21/11/14 16:55:47 INFO DAGScheduler: Job 6 finished: showString at NativeMethodAccessorImpl.java:0, took 0.748266 s
[36malgo-1    |[0m 21/11/14 16:55:47 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 16:55:47 INFO DAGScheduler: Got job 7 (showString at NativeMethodAccessorImpl.java:0) with 11 output partitions
[36malgo-1    |[0m 21/11/14 16:55:47 INFO DAGScheduler: Final stage: ResultStage 8 (showString at NativeMethodAccessorImpl.java:0)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:55:47 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:55:47 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[31] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:55:47 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 17.3 KiB, free 1006.9 MiB)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.21.0.2:41233 (size: 8.0 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:55:47 INFO DAGScheduler: Submitting 11 missing tasks from ResultStage 8 (MapPartitionsRDD[31] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15))
[36malgo-1    |[0m 21/11/14 16:55:47 INFO YarnScheduler: Adding task set 8.0 with 11 tasks
[36malgo-1    |[0m 21/11/14 16:55:47 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 24, algo-2, executor 2, partition 5, PROCESS_LOCAL, 7398 bytes)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 25, algo-1, executor 1, partition 6, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-2:36353 (size: 8.0 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:33651 (size: 8.0 KiB, free: 912.2 MiB)
[36malgo-1    |[0m rlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:45 INFO PythonUDFRunner: Times: total = 1114, boot = 401, init = 711, finish = 2
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:45 INFO FileOutputCommitter: Saved output of task 'attempt_20211114165544_0005_m_000000_18' to file:/opt/ml/processing/output/data
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:45 INFO SparkHadoopMapRedUtil: attempt_20211114165544_0005_m_000000_18: Committed
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:45 INFO Executor: Finished task 0.0 in stage 5.0 (TID 18). 3249 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:46 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 21
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:46 INFO Executor: Running task 1.0 in stage 7.0 (TID 21)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO TorrentBroadcast: Started reading broadcast variable 10 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 911.1 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO TorrentBroadcast: Reading broadcast variable 10 took 7 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 17.3 KiB, free 911.1 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO CodeGenerator: Code generated in 10.917538 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO CodeGenerator: Code generated in 8.959551 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO PythonRunner: Times: total = 2, boot = -1593, init = 1595, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO CodeGenerator: Code generated in 9.017387 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO CodeGenerator: Code generated in 10.637911 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO PythonUDFRunner: Times: total = 603, boot = 3, init = 600, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO Executor: Finished task 1.0 in stage 7.0 (TID 21). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 25
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO Executor: Running task 1.0 in stage 8.0 (TID 25)
[36malgo-1    |[0m [/var/log/yarn/expo21/11/14 16:55:47 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 26, algo-1, executor 1, partition 7, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 25) in 27 ms on algo-1 (executor 1) (1/11)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO TaskSetManager: Starting task 3.0 in stage 8.0 (TID 27, algo-1, executor 1, partition 8, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 26) in 25 ms on algo-1 (executor 1) (2/11)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO TaskSetManager: Starting task 4.0 in stage 8.0 (TID 28, algo-2, executor 2, partition 9, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 24) in 71 ms on algo-2 (executor 2) (3/11)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO TaskSetManager: Starting task 5.0 in stage 8.0 (TID 29, algo-2, executor 2, partition 10, PROCESS_LOCAL, 7398 bytes)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO TaskSetManager: Finished task 4.0 in stage 8.0 (TID 28) in 24 ms on algo-2 (executor 2) (4/11)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:46 INFO CodeGenerator: Code generated in 12.634464 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:46 INFO PythonUDFRunner: Times: total = 663, boot = 4, init = 659, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:46 INFO Executor: Finished task 0.0 in stage 6.0 (TID 19). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:46 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 20
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:46 INFO Executor: Running task 0.0 in stage 7.0 (TID 20)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:46 INFO TorrentBroadcast: Started reading broadcast variable 10 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO TorrentBroadcast: Reading broadcast variable 10 took 7 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 17.3 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO PythonRunner: Times: total = 2, boot = -734, init = 736, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO PythonUDFRunner: Times: total = 2, boot = -18, init = 20, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO Executor: Finished task 0.0 in stage 7.0 (TID 20). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 22
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO Executor: Running task 2.0 in stage 7.0 (TID 22)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO PythonRunner: Times: total = 2, boot = -9, init = 11, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO PythonUDFRunner: Times: total = 13, boot = 13, init = 0, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO Executor: Finished task 2.0 in stage 7.0 (TID 22). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 23
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO Executor: Running task 3.0 in stage 7.0 (TID 23)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO PythonRunner: Times: total = 42, boot = -17, init = 59, finish = 0
[36malgo-1    |[0m 21/11/14 16:55:47 INFO TaskSetManager: Starting task 6.0 in stage 8.0 (TID 30, algo-1, executor 1, partition 11, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO TaskSetManager: Finished task 3.0 in stage 8.0 (TID 27) in 57 ms on algo-1 (executor 1) (5/11)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO TaskSetManager: Starting task 7.0 in stage 8.0 (TID 31, algo-2, executor 2, partition 12, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO TaskSetManager: Finished task 5.0 in stage 8.0 (TID 29) in 55 ms on algo-2 (executor 2) (6/11)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO TaskSetManager: Starting task 8.0 in stage 8.0 (TID 32, algo-1, executor 1, partition 13, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO TaskSetManager: Finished task 6.0 in stage 8.0 (TID 30) in 55 ms on algo-1 (executor 1) (7/11)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO TaskSetManager: Starting task 9.0 in stage 8.0 (TID 33, algo-2, executor 2, partition 14, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO TaskSetManager: Finished task 7.0 in stage 8.0 (TID 31) in 58 ms on algo-2 (executor 2) (8/11)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO TaskSetManager: Starting task 10.0 in stage 8.0 (TID 34, algo-1, executor 1, partition 15, PROCESS_LOCAL, 7398 bytes)
[36malgo-1    |[0m 21/11/14 16:55:47 INFO TaskSetManager: Finished task 8.0 in stage 8.0 (TID 32) in 57 ms on algo-1 (executor 1) (9/11)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Finished task 9.0 in stage 8.0 (TID 33) in 61 ms on algo-2 (executor 2) (10/11)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Finished task 10.0 in stage 8.0 (TID 34) in 64 ms on algo-1 (executor 1) (11/11)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: ResultStage 8 (showString at NativeMethodAccessorImpl.java:0) finished in 0.288 s
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:55:48 INFO YarnScheduler: Killing all running tasks in stage 8: Stage finished
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Job 7 finished: showString at NativeMethodAccessorImpl.java:0, took 0.293668 s
[36malgo-1    |[0m 21/11/14 16:55:48 INFO CodeGenerator: Code generated in 12.500616 ms
[36malgo-1    |[0m +--------+-----------+--------------------+--------------------+
[36malgo-1    |[0m |batch_id|primary_key|                text|               words|
[36malgo-1    |[0m +--------+-----------+--------------------+--------------------+
[36malgo-1    |[0m |       1|          1|This is a great book|This is a great book|
[36malgo-1    |[0m |       1|          1|This is a great book|This is a great book|
[36malgo-1    |[0m |       1|          1|This is a great book|This is a great book|
[36malgo-1    |[0m +--------+-----------+--------------------+--------------------+
[36malgo-1    |[0m 
[36malgo-1    |[0m 21/11/14 16:55:48 INFO CodeGenerator: Code generated in 11.760965 ms
[36malgo-1    |[0m 21/11/14 16:55:48 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Got job 8 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Final stage: ResultStage 9 (showString at NativeMethodAccessorImpl.java:0)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[36] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:55:48 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 18.2 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.21.0.2:41233 (size: 8.2 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[36] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 16:55:48 INFO YarnScheduler: Adding task set 9.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 35, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:33651 (size: 8.2 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 35) in 72 ms on algo-1 (executor 1) (1/1)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: ResultStage 9 (showString at NativeMethodAccessorImpl.java:0) finished in 0.079 s
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:55:48 INFO YarnScheduler: Killing all running tasks in stage 9: Stage finished
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Job 8 finished: showString at NativeMethodAccessorImpl.java:0, took 0.082712 s
[36malgo-1    |[0m 21/11/14 16:55:48 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Got job 9 (showString at NativeMethodAccessorImpl.java:0) with 4 output partitions
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Final stage: ResultStage 10 (showString at NativeMethodAccessorImpl.java:0)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[36] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:55:48 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 18.2 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.21.0.2:41233 (size: 8.2 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 10 (MapPartitionsRDD[36] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1, 2, 3, 4))
[36malgo-1    |[0m 21/11/14 16:55:48 INFO YarnScheduler: Adding task set 10.0 with 4 tasks
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 36, algo-2, executor 2, partition 1, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 37, algo-1, executor 1, partition 2, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-2:36353 (size: 8.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:33651 (size: 8.2 KiB, free: 912.1 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 38, algo-2, executor 2, partition 3, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 36) in 65 ms on algo-2 (executor 2) (1/4)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 39, algo-1, executor 1, partition 4, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 37) in 69 ms on algo-1 (executor 1) (2/4)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 38) in 56 ms on algo-2 (executor 2) (3/4)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 39) in 60 ms on algo-1 (executor 1) (4/4)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: ResultStage 10 (showString at NativeMethodAccessorImpl.java:0) finished in 0.135 s
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:55:48 INFO YarnScheduler: Killing all running tasks in stage 10: Stage finished
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Job 9 finished: showString at NativeMethodAccessorImpl.java:0, took 0.138620 s
[36malgo-1    |[0m 21/11/14 16:55:48 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Got job 10 (showString at NativeMethodAccessorImpl.java:0) with 11 output partitions
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Final stage: ResultStage 11 (showString at NativeMethodAccessorImpl.java:0)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[36] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:55:48 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 18.2 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.21.0.2:41233 (size: 8.2 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Submitting 11 missing tasks from ResultStage 11 (MapPartitionsRDD[36] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15))
[36malgo-1    |[0m 21/11/14 16:55:48 INFO YarnScheduler: Adding task set 11.0 with 11 tasks
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 40, algo-2, executor 2, partition 5, PROCESS_LOCAL, 7398 bytes)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 41, algo-1, executor 1, partition 6, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:33651 (size: 8.2 KiB, free: 912.1 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-2:36353 (size: 8.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Starting task 2.0 in stage 11.0 (TID 42, algo-2, executor 2, partition 7, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 40) in 37 ms on algo-2 (executor 2) (1/11)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Starting task 3.0 in stage 11.0 (TID 43, algo-1, executor 1, partition 8, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 41) in 66 ms on algo-1 (executor 1) (2/11)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Starting task 4.0 in stage 11.0 (TID 44, algo-2, executor 2, partition 9, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Finished task 2.0 in stage 11.0 (TID 42) in 58 ms on algo-2 (executor 2) (3/11)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Starting task 5.0 in stage 11.0 (TID 45, algo-1, executor 1, partition 10, PROCESS_LOCAL, 7398 bytes)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Finished task 3.0 in stage 11.0 (TID 43) in 56 ms on algo-1 (executor 1) (4/11)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Starting task 6.0 in stage 11.0 (TID 46, algo-2, executor 2, partition 11, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Finished task 4.0 in stage 11.0 (TID 44) in 56 ms on algo-2 (executor 2) (5/11)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Starting task 7.0 in stage 11.0 (TID 47, algo-1, executor 1, partition 12, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Finished task 5.0 in stage 11.0 (TID 45) in 81 ms on algo-1 (executor 1) (6/11)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Starting task 8.0 in stage 11.0 (TID 48, algo-2, executor 2, partition 13, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Finished task 6.0 in stage 11.0 (TID 46) in 62 ms on algo-2 (executor 2) (7/11)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.21.0.2:41233 in memory (size: 8.0 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:33651 in memory (size: 8.0 KiB, free: 912.1 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Removed broadcast_10_piece0 on algo-2:36353 in memory (size: 8.0 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.21.0.2:41233 in memory (size: 30.3 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:33651 in memory (size: 30.3 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.21.0.2:41233 in memory (size: 8.2 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:33651 in memory (size: 8.2 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.21.0.2:41233 in memory (size: 30.3 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:33651 in memory (size: 30.3 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.21.0.2:41233 in memory (size: 8.0 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:33651 in memory (size: 8.0 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Removed broadcast_11_piece0 on algo-2:36353 in memory (size: 8.0 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.21.0.2:41233 in memory (size: 68.8 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Starting task 9.0 in stage 11.0 (TID 49, algo-1, executor 1, partition 14, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Finished task 7.0 in stage 11.0 (TID 47) in 68 ms on algo-1 (executor 1) (8/11)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Starting task 10.0 in stage 11.0 (TID 50, algo-2, executor 2, partition 15, PROCESS_LOCAL, 7398 bytes)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Finished task 8.0 in stage 11.0 (TID 48) in 61 ms on algo-2 (executor 2) (9/11)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:33651 in memory (size: 68.8 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.21.0.2:41233 in memory (size: 8.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Removed broadcast_13_piece0 on algo-2:36353 in memory (size: 8.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:33651 in memory (size: 8.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.21.0.2:41233 in memory (size: 8.0 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Removed broadcast_9_piece0 on algo-2:36353 in memory (size: 8.0 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Finished task 10.0 in stage 11.0 (TID 50) in 71 ms on algo-2 (executor 2) (10/11)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Finished task 9.0 in stage 11.0 (TID 49) in 98 ms on algo-1 (executor 1) (11/11)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: ResultStage 11 (showString at NativeMethodAccessorImpl.java:0) finished in 0.364 s
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:55:48 INFO YarnScheduler: Killing all running tasks in stage 11: Stage finished
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Job 10 finished: showString at NativeMethodAccessorImpl.java:0, took 0.367867 s
[36malgo-1    |[0m rt/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO TorrentBroadcast: Started reading broadcast variable 11 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 911.1 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO TorrentBroadcast: Reading broadcast variable 11 took 7 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 17.3 KiB, free 911.1 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO PythonRunner: Times: total = 2, boot = -633, init = 635, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO PythonUDFRunner: Times: total = 2, boot = -17, init = 19, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO Executor: Finished task 1.0 in stage 8.0 (TID 25). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 26
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO Executor: Running task 2.0 in stage 8.0 (TID 26)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO PythonRunner: Times: total = 14, boot = 14, init = 0, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO PythonUDFRunner: Times: total = 12, boot = 11, init = 1, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO Executor: Finished task 2.0 in stage 8.0 (TID 26). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 27
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO Executor: Running task 3.0 in stage 8.0 (TID 27)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO PythonRunner: Times: total = 45, boot = 18, init = 27, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO PythonUDFRunner: Times: total = 43, boot = 14, init = 29, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO Executor: Finished task 3.0 in stage 8.0 (TID 27). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 30
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO Executor: Running task 6.0 in stage 8.0 (TID 30)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO PythonRunner: Times: total = 17, boot = 17, init = 0, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO PythonUDFRunner: Times: total = 43, boot = 19, init = 24, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO Executor: Finished task 6.0 in stage 8.0 (TID 30). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 32
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO Executor: Running task 8.0 in stage 8.0 (TID 32)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO PythonRunner: Times: total = 2, boot = -20, init = 22, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO PythonUDFRunner: Times: total = 44, boot = 20, init = 24, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO Executor: Finished task 8.0 in stage 8.0 (TID 32). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 34
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:47 INFO Executor: Running task 10.0 in stage 8.0 (TID 34)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO PythonRunner: Times: total = 43, boot = -31, init = 74, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO PythonUDFRunner: Times: total = 51, boot = 17, init = 31, finish = 3
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO Executor: Finished task 10.0 in stage 8.0 (TID 34). 2094 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 35
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO Executor: Running task 0.0 in stage 9.0 (TID 35)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO TorrentBroadcast: Started reading broadcast variable 12 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 911.1 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO TorrentBroadcast: Reading broadcast variable 12 took 6 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 18.2 KiB, free 911.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO PythonRunner: Times: total = 2, boot = -132, init = 134, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO CodeGenerator: Code generated in 9.869195 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO CodeGenerator: Code generated in 12.093093 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO PythonUDFRunner: Times: total = 48, boot = -123, init = 171, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO Executor: Finished task 0.0 in stage 9.0 (TID 35). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 37
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO Executor: Running task 1.0 in stage 10.0 (TID 37)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO TorrentBroadcast: Started reading broadcast variable 13 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 911.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO TorrentBroadcast: Reading broadcast variable 13 took 6 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 18.2 KiB, free 911.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO PythonRunner: Times: total = 2, boot = -59, init = 61, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO PythonUDFRunner: Times: total = 47, boot = -7, init = 54, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO Executor: Finished task 1.0 in stage 10.0 (TID 37). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 39
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO Executor: Running task 3.0 in stage 10.0 (TID 39)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO PythonRunner: Times: total = 42, boot = -29, init = 71, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO PythonUDFRunner: Times: total = 46, boot = 20, init = 26, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO Executor: Finished task 3.0 in stage 10.0 (TID 39). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 41
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO Executor: Running task 1.0 in stage 11.0 (TID 41)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO TorrentBroadcast: Started reading broadcast variable 14 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 911.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO TorrentBroadcast: Reading broadcast variable 14 took 6 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 18.2 KiB, free 911.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO PythonRunner: Times: total = 2, boot = -10, init = 12, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO PythonUDFRunner: Times: total = 45, boot = 3, init = 42, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO Executor: Finished task 1.0 in stage 11.0 (TID 41). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 43
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO Executor: Running task 3.0 in stage 11.0 (TID 43)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO PythonRunner: Times: total = 3, boot = -25, init = 28, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO PythonUDFRunner: Times: total = 42, boot = 14, init = 28, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO Executor: Finished task 3.0 in stage 11.0 (TID 43). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 45
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO Executor: Running task 5.0 in stage 11.0 (TID 45)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO PythonRunner: Times: total = 43, boot = -26, init = 69, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] Performing Sentiment Analysis
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO PythonUDFRunner: Times: total = 57, boot = 11, init = 32, finish = 14
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO Executor: Finished task 5.0 in stage 11.0 (TID 45). 2109 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 47
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO Executor: Running task 7.0 in stage 11.0 (TID 47)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO PythonRunner: Times: total = 2, boot = -11, init = 13, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO PythonUDFRunner: Times: total = 43, boot = 10, init = 33, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 21/11/14 16:55:48 INFO CodeGenerator: Code generated in 11.717171 ms
[36malgo-1    |[0m +--------+-----------+--------------------+--------------------+---------------+
[36malgo-1    |[0m |batch_id|primary_key|                text|               words|sentiment_score|
[36malgo-1    |[0m +--------+-----------+--------------------+--------------------+---------------+
[36malgo-1    |[0m |       1|          1|This is a great book|This is a great book|         0.6249|
[36malgo-1    |[0m |       1|          1|This is a great book|This is a great book|         0.6249|
[36malgo-1    |[0m |       1|          1|This is a great book|This is a great book|         0.6249|
[36malgo-1    |[0m +--------+-----------+--------------------+--------------------+---------------+
[36malgo-1    |[0m 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO PythonUDFRunner: Times: total = 39, boot = 12, init = 27, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO Executor: Finished task 3.0 in stage 7.0 (TID 23). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 24
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO Executor: Running task 0.0 in stage 8.0 (TID 24)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO TorrentBroadcast: Started reading broadcast variable 11 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO TorrentBroadcast: Reading broadcast variable 11 took 6 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 17.3 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO PythonRunner: Times: total = 2, boot = -661, init = 663, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO PythonUDFRunner: Times: total = 45, boot = -644, init = 687, finish = 2
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO Executor: Finished task 0.0 in stage 8.0 (TID 24). 2094 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 28
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO Executor: Running task 4.0 in stage 8.0 (TID 28)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO PythonRunner: Times: total = 2, boot = -47, init = 49, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO PythonUDFRunner: Times: total = 12, boot = 12, init = 0, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO Executor: Finished task 4.0 in stage 8.0 (TID 28). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 29
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO Executor: Running task 5.0 in stage 8.0 (TID 29)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO PythonRunner: Times: total = 44, boot = -15, init = 59, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO PythonUDFRunner: Times: total = 43, boot = 17, init = 26, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO Executor: Finished task 5.0 in stage 8.0 (TID 29). 2094 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 31
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO Executor: Running task 7.0 in stage 8.0 (TID 31)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO PythonRunner: Times: total = 43, boot = -3, init = 46, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO PythonUDFRunner: Times: total = 44, boot = 20, init = 24, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO Executor: Finished task 7.0 in stage 8.0 (TID 31). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 33
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:47 INFO Executor: Running task 9.0 in stage 8.0 (TID 33)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO PythonRunner: Times: total = 43, boot = -8, init = 51, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO PythonUDFRunner: Times: total = 48, boot = 20, init = 28, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO Executor: Finished task 9.0 in stage 8.0 (TID 33). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 36
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO Executor: Running task 0.0 in stage 10.0 (TID 36)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO TorrentBroadcast: Started reading broadcast variable 13 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO TorrentBroadcast: Reading broadcast variable 13 took 6 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 18.2 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO CodeGenerator: Code generated in 12.047314 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO CodeGenerator: Code generated in 13.707199 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO PythonRunner: Times: total = 44, boot = -255, init = 299, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO PythonUDFRunner: Times: total = 40, boot = -225, init = 265, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO Executor: Finished task 0.0 in stage 10.0 (TID 36). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 38
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO Executor: Running task 2.0 in stage 10.0 (TID 38)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO PythonRunner: Times: total = 1, boot = -4, init = 5, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO PythonUDFRunner: Times: total = 42, boot = 22, init = 20, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO Executor: Finished task 2.0 in stage 10.0 (TID 38). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 40
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO Executor: Running task 0.0 in stage 11.0 (TID 40)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO TorrentBroadcast: Started reading broadcast variable 14 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO TorrentBroadcast: Reading broadcast variable 14 took 6 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 18.2 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO PythonRunner: Times: total = 2, boot = -77, init = 79, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] Performing Sentiment Analysis
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO PythonUDFRunner: Times: total = 15, boot = -8, init = 11, finish = 12
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO Executor: Finished task 0.0 in stage 11.0 (TID 40). 2109 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 42
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO Executor: Running task 2.0 in stage 11.0 (TID 42)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO PythonRunner: Times: total = 48, boot = -19, init = 67, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO PythonUDFRunner: Times: total = 46, boot = 13, init = 33, finish = 0
[36malgo-1    |[0m 21/11/14 16:55:48 INFO CodeGenerator: Code generated in 17.412979 ms
[36malgo-1    |[0m 21/11/14 16:55:48 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Got job 11 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Final stage: ResultStage 12 (showString at NativeMethodAccessorImpl.java:0)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[41] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:55:48 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 20.9 KiB, free 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.21.0.2:41233 (size: 8.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:55:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[41] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 16:55:48 INFO YarnScheduler: Adding task set 12.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 16:55:48 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 51, algo-2, executor 2, partition 0, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:48 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-2:36353 (size: 8.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 51) in 75 ms on algo-2 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO YarnScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:55:49 INFO DAGScheduler: ResultStage 12 (showString at NativeMethodAccessorImpl.java:0) finished in 0.082 s
[36malgo-1    |[0m 21/11/14 16:55:49 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:55:49 INFO YarnScheduler: Killing all running tasks in stage 12: Stage finished
[36malgo-1    |[0m 21/11/14 16:55:49 INFO DAGScheduler: Job 11 finished: showString at NativeMethodAccessorImpl.java:0, took 0.085973 s
[36malgo-1    |[0m 21/11/14 16:55:49 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 16:55:49 INFO DAGScheduler: Got job 12 (showString at NativeMethodAccessorImpl.java:0) with 4 output partitions
[36malgo-1    |[0m 21/11/14 16:55:49 INFO DAGScheduler: Final stage: ResultStage 13 (showString at NativeMethodAccessorImpl.java:0)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:55:49 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:55:49 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[41] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:55:49 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 20.9 KiB, free 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.21.0.2:41233 (size: 8.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:55:49 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 13 (MapPartitionsRDD[41] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1, 2, 3, 4))
[36malgo-1    |[0m 21/11/14 16:55:49 INFO YarnScheduler: Adding task set 13.0 with 4 tasks
[36malgo-1    |[0m 21/11/14 16:55:49 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 52, algo-1, executor 1, partition 1, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 53, algo-2, executor 2, partition 2, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:33651 (size: 8.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-2:36353 (size: 8.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO TaskSetManager: Starting task 2.0 in stage 13.0 (TID 54, algo-1, executor 1, partition 3, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 52) in 63 ms on algo-1 (executor 1) (1/4)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO TaskSetManager: Starting task 3.0 in stage 13.0 (TID 55, algo-2, executor 2, partition 4, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 53) in 67 ms on algo-2 (executor 2) (2/4)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO TaskSetManager: Finished task 2.0 in stage 13.0 (TID 54) in 59 ms on algo-1 (executor 1) (3/4)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO TaskSetManager: Finished task 3.0 in stage 13.0 (TID 55) in 58 ms on algo-2 (executor 2) (4/4)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:55:49 INFO DAGScheduler: ResultStage 13 (showString at NativeMethodAccessorImpl.java:0) finished in 0.130 s
[36malgo-1    |[0m 21/11/14 16:55:49 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:55:49 INFO YarnScheduler: Killing all running tasks in stage 13: Stage finished
[36malgo-1    |[0m 21/11/14 16:55:49 INFO DAGScheduler: Job 12 finished: showString at NativeMethodAccessorImpl.java:0, took 0.132220 s
[36malgo-1    |[0m 21/11/14 16:55:49 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 16:55:49 INFO DAGScheduler: Got job 13 (showString at NativeMethodAccessorImpl.java:0) with 11 output partitions
[36malgo-1    |[0m 21/11/14 16:55:49 INFO DAGScheduler: Final stage: ResultStage 14 (showString at NativeMethodAccessorImpl.java:0)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:55:49 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:55:49 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[41] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:55:49 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 20.9 KiB, free 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.21.0.2:41233 (size: 8.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:55:49 INFO DAGScheduler: Submitting 11 missing tasks from ResultStage 14 (MapPartitionsRDD[41] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15))
[36malgo-1    |[0m 21/11/14 16:55:49 INFO YarnScheduler: Adding task set 14.0 with 11 tasks
[36malgo-1    |[0m 21/11/14 16:55:49 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 56, algo-1, executor 1, partition 5, PROCESS_LOCAL, 7398 bytes)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 57, algo-2, executor 2, partition 6, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:33651 (size: 8.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-2:36353 (size: 8.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO TaskSetManager: Starting task 2.0 in stage 14.0 (TID 58, algo-1, executor 1, partition 7, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:49 WARN TaskSetManager: Lost task 0.0 in stage 14.0 (TID 56, algo-1, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 35, in decorated
[36malgo-1    |[0m     return func(*args, **kwargs)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 57, in tokenize
[36malgo-1    |[0m     return nltk.tokenize.sent_tokenize(text)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/tokenize/__init__.py", line 106, in sent_tokenize
[36malgo-1    |[0m     tokenizer = load(f"tokenizers/punkt/{language}.pickle")
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 750, in load
[36malgo-1    |[0m     opened_resource = _open(resource_url)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 876, in _open
[36malgo-1    |[0m     return find(path_, path + [""]).open()
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 583, in find
[36malgo-1    |[0m     raise LookupError(resource_not_found)
[36malgo-1    |[0m LookupError: 
[36malgo-1    |[0m **********************************************************************
[36malgo-1    |[0m   Resource [93mpunkt[0m not found.
[36malgo-1    |[0m   Please use the NLTK Downloader to obtain the resource:
[36malgo-1    |[0m 
[36malgo-1    |[0m   [31m>>> import nltk
[36malgo-1    |[0m   >>> nltk.download('punkt')
[36malgo-1    |[0m   [0m
[36malgo-1    |[0m   For more information see: https://www.nltk.org/data.html
[36malgo-1    |[0m 
[36malgo-1    |[0m   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[36malgo-1    |[0m 
[36malgo-1    |[0m   Searched in:
[36malgo-1    |[0m     - '/home/nltk_data'
[36malgo-1    |[0m     - '/usr/nltk_data'
[36malgo-1    |[0m     - '/usr/share/nltk_data'
[36malgo-1    |[0m     - '/usr/lib/nltk_data'
[36malgo-1    |[0m     - '/usr/share/nltk_data'
[36malgo-1    |[0m     - '/usr/local/share/nltk_data'
[36malgo-1    |[0m     - '/usr/lib/nltk_data'
[36malgo-1    |[0m     - '/usr/local/lib/nltk_data'
[36malgo-1    |[0m     - ''
[36malgo-1    |[0m **********************************************************************
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m During handling of the above exception, another exception occurred:
[36malgo-1    |[0m 
[36malgo-1    |[0m Traceback (most recent call last):
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/worker.py", line 605, in main
[36malgo-1    |[0m     process()
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/worker.py", line 597, in process
[36malgo-1    |[0m     serializer.dump_stream(out_iter, outfile)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/serializers.py", line 223, in dump_stream
[36malgo-1    |[0m     self.serializer.dump_stream(self._batched(iterator), stream)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/serializers.py", line 141, in dump_stream
[36malgo-1    |[0m     for obj in iterator:
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/serializers.py", line 212, in _batched
[36malgo-1    |[0m     for item in iterator:
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/worker.py", line 450, in mapper
[36malgo-1    |[0m     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/worker.py", line 450, in <genexpr>
[36malgo-1    |[0m     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/worker.py", line 90, in <lambda>
[36malgo-1    |[0m     return lambda *a: f(*a)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/util.py", line 121, in wrapper
[36malgo-1    |[0m     return f(*args, **kwargs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/worker.py", line 82, in <lambda>
[36malgo-1    |[0m     return lambda *a: g(f(*a))
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/__pyfiles__/hello_py_spark_udfs.py", line 60, in f_emo
[36malgo-1    |[0m     out = emotion_fit(lower_s)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/__pyfiles__/hello_py_spark_udfs.py", line 158, in emotion_fit
[36malgo-1    |[0m     nrclex_out = NRCLex(str(clean_cell))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nrclex.py", line 2873, in __init__
[36malgo-1    |[0m     self.words = list(blob.words)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 24, in __get__
[36malgo-1    |[0m     value = obj.__dict__[self.func.__name__] = self.func(obj)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/blob.py", line 678, in words
[36malgo-1    |[0m     return WordList(word_tokenize(self.raw, include_punc=False))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 73, in word_tokenize
[36malgo-1    |[0m     for sentence in sent_tokenize(text))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/base.py", line 64, in itokenize
[36malgo-1    |[0m     return (t for t in self.tokenize(text, *args, **kwargs))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 38, in decorated
[36malgo-1    |[0m     raise MissingCorpusError()
[36malgo-1    |[0m textblob.exceptions.MissingCorpusError: 
[36malgo-1    |[0m Looks like you are missing some required data for this feature.
[36malgo-1    |[0m 
[36malgo-1    |[0m To download the necessary data, simply run
[36malgo-1    |[0m 
[36malgo-1    |[0m     python -m textblob.download_corpora
[36malgo-1    |[0m 
[36malgo-1    |[0m or use the NLTK downloader to download the missing data: http://nltk.org/data.html
[36malgo-1    |[0m If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)
[36malgo-1    |[0m 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
[36malgo-1    |[0m 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[36malgo-1    |[0m 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
[36malgo-1    |[0m 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[36malgo-1    |[0m 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[36malgo-1    |[0m 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
[36malgo-1    |[0m 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
[36malgo-1    |[0m 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
[36malgo-1    |[0m 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[36malgo-1    |[0m 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
[36malgo-1    |[0m 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
[36malgo-1    |[0m 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[36malgo-1    |[0m 	at org.apache.spark.scheduler.Task.run(Task.scala:127)
[36malgo-1    |[0m 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
[36malgo-1    |[0m 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
[36malgo-1    |[0m 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
[36malgo-1    |[0m 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[36malgo-1    |[0m 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[36malgo-1    |[0m 	at java.lang.Thread.run(Thread.java:748)
[36malgo-1    |[0m 
[36malgo-1    |[0m 21/11/14 16:55:49 INFO TaskSetManager: Starting task 0.1 in stage 14.0 (TID 59, algo-2, executor 2, partition 5, PROCESS_LOCAL, 7398 bytes)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 57) in 65 ms on algo-2 (executor 2) (1/11)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO TaskSetManager: Starting task 3.0 in stage 14.0 (TID 60, algo-2, executor 2, partition 8, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:49 WARN TaskSetManager: Lost task 0.1 in stage 14.0 (TID 59, algo-2, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 35, in decorated
[36malgo-1    |[0m     return func(*args, **kwargs)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 57, in tokenize
[36malgo-1    |[0m     return nltk.tokenize.sent_tokenize(text)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/tokenize/__init__.py", line 106, in sent_tokenize
[36malgo-1    |[0m     tokenizer = load(f"tokenizers/punkt/{language}.pickle")
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 750, in load
[36malgo-1    |[0m     opened_resource = _open(resource_url)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 876, in _open
[36malgo-1    |[0m     return find(path_, path + [""]).open()
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 583, in find
[36malgo-1    |[0m     raise LookupError(resource_not_found)
[36malgo-1    |[0m LookupError: 
[36malgo-1    |[0m **********************************************************************
[36malgo-1    |[0m   Resource [93mpunkt[0m not found.
[36malgo-1    |[0m   Please use the NLTK Downloader to obtain the resource:
[36malgo-1    |[0m 
[36malgo-1    |[0m   [31m>>> import nltk
[36malgo-1    |[0m   >>> nltk.download('punkt')
[36malgo-1    |[0m   [0m
[36malgo-1    |[0m   For more information see: https://www.nltk.org/data.html
[36malgo-1    |[0m 
[36malgo-1    |[0m   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[36malgo-1    |[0m 
[36malgo-1    |[0m   Searched in:
[36malgo-1    |[0m     - '/home/nltk_data'
[36malgo-1    |[0m     - '/usr/nltk_data'
[36malgo-1    |[0m     - '/usr/share/nltk_data'
[36malgo-1    |[0m     - '/usr/lib/nltk_data'
[36malgo-1    |[0m     - '/usr/share/nltk_data'
[36malgo-1    |[0m     - '/usr/local/share/nltk_data'
[36malgo-1    |[0m     - '/usr/lib/nltk_data'
[36malgo-1    |[0m     - '/usr/local/lib/nltk_data'
[36malgo-1    |[0m     - ''
[36malgo-1    |[0m **********************************************************************
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m During handling of the above exception, another exception occurred:
[36malgo-1    |[0m 
[36malgo-1    |[0m Traceback (most recent call last):
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 605, in main
[36malgo-1    |[0m     process()
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 597, in process
[36malgo-1    |[0m     serializer.dump_stream(out_iter, outfile)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 223, in dump_stream
[36malgo-1    |[0m     self.serializer.dump_stream(self._batched(iterator), stream)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 141, in dump_stream
[36malgo-1    |[0m     for obj in iterator:
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 212, in _batched
[36malgo-1    |[0m     for item in iterator:
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 450, in mapper
[36malgo-1    |[0m     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 450, in <genexpr>
[36malgo-1    |[0m     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 90, in <lambda>
[36malgo-1    |[0m     return lambda *a: f(*a)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/util.py", line 121, in wrapper
[36malgo-1    |[0m     return f(*args, **kwargs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 82, in <lambda>
[36malgo-1    |[0m     return lambda *a: g(f(*a))
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/__pyfiles__/hello_py_spark_udfs.py", line 60, in f_emo
[36malgo-1    |[0m     out = emotion_fit(lower_s)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/__pyfiles__/hello_py_spark_udfs.py", line 158, in emotion_fit
[36malgo-1    |[0m     nrclex_out = NRCLex(str(clean_cell))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nrclex.py", line 2873, in __init__
[36malgo-1    |[0m     self.words = list(blob.words)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 24, in __get__
[36malgo-1    |[0m     value = obj.__dict__[self.func.__name__] = self.func(obj)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/blob.py", line 678, in words
[36malgo-1    |[0m     return WordList(word_tokenize(self.raw, include_punc=False))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 73, in word_tokenize
[36malgo-1    |[0m     for sentence in sent_tokenize(text))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/base.py", line 64, in itokenize
[36malgo-1    |[0m     return (t for t in self.tokenize(text, *args, **kwargs))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 38, in decorated
[36malgo-1    |[0m     raise MissingCorpusError()
[36malgo-1    |[0m textblob.exceptions.MissingCorpusError: 
[36malgo-1    |[0m Looks like you are missing some required data for this feature.
[36malgo-1    |[0m 
[36malgo-1    |[0m To download the necessary data, simply run
[36malgo-1    |[0m 
[36malgo-1    |[0m     python -m textblob.download_corpora
[36malgo-1    |[0m 
[36malgo-1    |[0m or use the NLTK downloader to download the missing data: http://nltk.org/data.html
[36malgo-1    |[0m If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)
[36malgo-1    |[0m 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
[36malgo-1    |[0m 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[36malgo-1    |[0m 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
[36malgo-1    |[0m 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[36malgo-1    |[0m 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[36malgo-1    |[0m 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
[36malgo-1    |[0m 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
[36malgo-1    |[0m 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
[36malgo-1    |[0m 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[36malgo-1    |[0m 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
[36malgo-1    |[0m 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
[36malgo-1    |[0m 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[36malgo-1    |[0m 	at org.apache.spark.scheduler.Task.run(Task.scala:127)
[36malgo-1    |[0m 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
[36malgo-1    |[0m 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
[36malgo-1    |[0m 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
[36malgo-1    |[0m 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[36malgo-1    |[0m 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[36malgo-1    |[0m 	at java.lang.Thread.run(Thread.java:748)
[36malgo-1    |[0m 
[36malgo-1    |[0m 16:55:48 INFO Executor: Finished task 7.0 in stage 11.0 (TID 47). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 49
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO Executor: Running task 9.0 in stage 11.0 (TID 49)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO PythonRunner: Times: total = 2, boot = -66, init = 68, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO PythonUDFRunner: Times: total = 41, boot = -15, init = 56, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:48 INFO Executor: Finished task 9.0 in stage 11.0 (TID 49). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 52
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO Executor: Running task 0.0 in stage 13.0 (TID 52)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO TorrentBroadcast: Started reading broadcast variable 16 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO TorrentBroadcast: Reading broadcast variable 16 took 6 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 20.9 KiB, free 912.2 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO CodeGenerator: Code generated in 12.131062 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO CodeGenerator: Code generated in 14.303307 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO PythonRunner: Times: total = 43, boot = -316, init = 359, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO PythonUDFRunner: Times: total = 41, boot = -268, init = 309, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO Executor: Finished task 0.0 in stage 13.0 (TID 52). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 54
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO Executor: Running task 2.0 in stage 13.0 (TID 54)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO PythonRunner: Times: total = 29, boot = 29, init = 0, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO PythonUDFRunner: Times: total = 45, boot = 23, init = 22, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO Executor: Finished task 2.0 in stage 13.0 (TID 54). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 56
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO Executor: Running task 0.0 in stage 14.0 (TID 56)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO TorrentBroadcast: Started reading broadcast variable 17 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 912.2 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO TorrentBroadcast: Reading broadcast variable 17 took 5 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 20.9 KiB, free 912.2 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO PythonRunner: Times: total = 2, boot = -32, init = 34, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 1st Check
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] **********************************************************************
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   Resource [93mpunkt[0m not found.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   Please use the NLTK Downloader to obtain the resource:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   [31m>>> import nltk
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   >>> nltk.download('punkt')
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   [0m
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   For more information see: https://www.nltk.org/data.html
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   Searched in:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/home/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/share/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/lib/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/share/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/local/share/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/lib/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/local/lib/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - ''
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] **********************************************************************
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 ERROR Executor: Exception in task 0.0 in stage 14.0 (TID 56)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 35, in decorated
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     return func(*args, **kwargs)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 57, in tokenize
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     return nltk.tokenize.sent_tokenize(text)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/tokenize/__init__.py", line 106, in sent_tokenize
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     tokenizer = load(f"tokenizers/punkt/{language}.pickle")
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 750, in load
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     opened_resource = _open(resource_url)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 876, in _open
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     return find(path_, path + [""]).open()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 583, in find
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     raise LookupError(resource_not_found)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] LookupError: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] **********************************************************************
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   Resource [93mpunkt[0m not found.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   Please use the NLTK Downloader to obtain the resource:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   [31m>>> import nltk
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   >>> nltk.download('punkt')
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   [0m
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   For more information see: https://www.nltk.org/data.html
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   Searched in:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/home/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/share/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/lib/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/share/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/local/share/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/lib/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/local/lib/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - ''
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] **********************************************************************
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] During handling of the above exception, another exception occurred:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] Traceback (most recent call last):
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/worker.py", line 605, in main
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     process()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/worker.py", line 597, in process
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     serializer.dump_stream(out_iter, outfile)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/serializers.py", line 223, in dump_stream
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     self.serializer.dump_stream(self._batched(iterator), stream)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/serializers.py", line 141, in dump_stream
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     for obj in iterator:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/serializers.py", line 212, in _batched
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     for item in iterator:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/worker.py", line 450, in mapper
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/worker.py", line 450, in <genexpr>
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/worker.py", line 90, in <lambda>
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     return lambda *a: f(*a)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/util.py", line 121, in wrapper
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     return f(*args, **kwargs)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/worker.py", line 82, in <lambda>
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     return lambda *a: g(f(*a))
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/__pyfiles__/hello_py_spark_udfs.py", line 60, in f_emo
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     out = emotion_fit(lower_s)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/__pyfiles__/hello_py_spark_udfs.py", line 158, in emotion_fit
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     nrclex_out = NRCLex(str(clean_cell))
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/nrclex.py", line 2873, in __init__
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     self.words = list(blob.words)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 24, in __get__
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     value = obj.__dict__[self.func.__name__] = self.func(obj)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/blob.py", line 678, in words
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     return WordList(word_tokenize(self.raw, include_punc=False))
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 73, in word_tokenize
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     for sentence in sent_tokenize(text))
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/base.py", line 64, in itokenize
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     return (t for t in self.tokenize(text, *args, **kwargs))
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 38, in decorated
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     raise MissingCorpusError()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] textblob.exceptions.MissingCorpusError: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] Looks like you are missing some required data for this feature.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] To download the necessary data, simply run
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     python -m textblob.download_corpora
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] or use the NLTK downloader to download the missing data: http://nltk.org/data.html
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.scheduler.Task.run(Task.scala:127)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO Executor: Finished task 2.0 in stage 11.0 (TID 42). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 44
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO Executor: Running task 4.0 in stage 11.0 (TID 44)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO PythonRunner: Times: total = 42, boot = -2, init = 44, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO PythonUDFRunner: Times: total = 43, boot = 20, init = 23, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO Executor: Finished task 4.0 in stage 11.0 (TID 44). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 46
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO Executor: Running task 6.0 in stage 11.0 (TID 46)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO PythonRunner: Times: total = 43, boot = -8, init = 51, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO PythonUDFRunner: Times: total = 48, boot = 20, init = 28, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO Executor: Finished task 6.0 in stage 11.0 (TID 46). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 48
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO Executor: Running task 8.0 in stage 11.0 (TID 48)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO PythonRunner: Times: total = 43, boot = -14, init = 57, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO PythonUDFRunner: Times: total = 43, boot = 16, init = 27, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO Executor: Finished task 8.0 in stage 11.0 (TID 48). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 50
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO Executor: Running task 10.0 in stage 11.0 (TID 50)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO PythonRunner: Times: total = 3, boot = -6, init = 9, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] Performing Sentiment Analysis
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO PythonUDFRunner: Times: total = 53, boot = 19, init = 23, finish = 11
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO Executor: Finished task 10.0 in stage 11.0 (TID 50). 2109 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 51
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO Executor: Running task 0.0 in stage 12.0 (TID 51)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO TorrentBroadcast: Started reading broadcast variable 15 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO TorrentBroadcast: Reading broadcast variable 15 took 6 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 20.9 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO PythonRunner: Times: total = 2, boot = -278, init = 280, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:48 INFO CodeGenerator: Code generated in 16.005473 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO CodeGenerator: Code generated in 19.406637 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO PythonUDFRunner: Times: total = 2, boot = -205, init = 207, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO Executor: Finished task 0.0 in stage 12.0 (TID 51). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 53
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO Executor: Running task 1.0 in stage 13.0 (TID 53)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO TorrentBroadcast: Started reading broadcast variable 16 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO TorrentBroadcast: Reading broadcast variable 16 took 6 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 20.9 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO PythonRunner: Times: total = 47, boot = -76, init = 123, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO PythonUDFRunner: Times: total = 44, boot = -52, init = 96, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO Executor: Finished task 1.0 in stage 13.0 (TID 53). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 55
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO Executor: Running task 3.0 in stage 13.0 (TID 55)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO PythonRunner: Times: total = 2, boot = -1, init = 3, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO PythonUDFRunner: Times: total = 44, boot = 23, init = 21, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO Executor: Finished task 3.0 in stage 13.0 (TID 55). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 57
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO Executor: Running task 1.0 in stage 14.0 (TID 57)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO TorrentBroadcast: Started reading broadcast variable 17 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO TorrentBroadcast: Reading broadcast variable 17 took 6 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 20.9 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO PythonRunner: Times: total = 2, boot = -67, init = 69, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO PythonUDFRunner: Times: total = 44, boot = 6, init = 38, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO Executor: Finished task 1.0 in stage 14.0 (TID 57). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 59
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO Executor: Running task 0.1 in stage 14.0 (TID 59)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO PythonRunner: Times: total = 45, boot = -49, init = 94, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 1st Check
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] **********************************************************************
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   Resource [93mpunkt[0m not found.
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   Please use the NLTK Downloader to obtain the resource:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   [31m>>> import nltk
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   >>> nltk.download('punkt')
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   [0m
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   For more information see: https://www.nltk.org/data.html
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   Searched in:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/home/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/share/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/lib/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/share/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/local/share/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/lib/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/local/lib/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - ''
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] **********************************************************************
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 ERROR Executor: Exception in task 0.1 in stage 14.0 (TID 59)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 35, in decorated
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     return func(*args, **kwargs)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 57, in tokenize
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     return nltk.tokenize.sent_tokenize(text)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/tokenize/__init__.py", line 106, in sent_tokenize
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     tokenizer = load(f"tokenizers/punkt/{language}.pickle")
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 750, in load
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     opened_resource = _open(resource_url)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 876, in _open
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     return find(path_, path + [""]).open()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 583, in find
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     raise LookupError(resource_not_found)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] LookupError: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] **********************************************************************
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   Resource [93mpunkt[0m not found.
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   Please use the NLTK Downloader to obtain the resource:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   [31m>>> import nltk
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   >>> nltk.download('punkt')
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   [0m
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   For more information see: https://www.nltk.org/data.html
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   Searched in:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/home/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/share/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/lib/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/share/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/local/share/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/lib/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/local/lib/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - ''
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] **********************************************************************
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] During handling of the above exception, another exception occurred:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] Traceback (most recent call last):
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 605, in main
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     process()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 597, in process
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     serializer.dump_stream(out_iter, outfile)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 223, in dump_stream
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     self.serializer.dump_stream(self._batched(iterator), stream)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 141, in dump_stream
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     for obj in iterator:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 212, in _batched
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     for item in iterator:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 450, in mapper
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 450, in <genexpr>
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 90, in <lambda>
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     return lambda *a: f(*a)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/util.py", line 121, in wrapper
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     return f(*args, **kwargs)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 82, in <lambda>
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     return lambda *a: g(f(*a))
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/__pyfiles__/hello_py_spark_udfs.py", line 60, in f_emo
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     out = emotion_fit(lower_s)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/__pyfiles__/hello_py_spark_udfs.py", line 158, in emotion_fit
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     nrclex_out = NRCLex(str(clean_cell))
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/nrclex.py", line 2873, in __init__
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     self.words = list(blob.words)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 24, in __get__
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     value = obj.__dict__[self.func.__name__] = self.func(obj)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/blob.py", line 678, in words
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     return WordList(word_tokenize(self.raw, include_punc=False))
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 73, in word_tokenize
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     for sentence in sent_tokenize(text))
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/base.py", line 64, in itokenize
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     return (t for t in self.tokenize(text, *args, **kwargs))
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 38, in decorated
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     raise MissingCorpusError()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] textblob.exceptions.MissingCorpusError: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] Looks like you are missing some required data for this feature.
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] To download the necessary data, simply run
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     python -m textblob.download_corpora
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] or use the NLTK downloader to download the missing data: http://nltk.org/data.html
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/21/11/14 16:55:49 INFO TaskSetManager: Starting task 0.2 in stage 14.0 (TID 61, algo-1, executor 1, partition 5, PROCESS_LOCAL, 7398 bytes)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO TaskSetManager: Finished task 2.0 in stage 14.0 (TID 58) in 685 ms on algo-1 (executor 1) (2/11)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO TaskSetManager: Starting task 4.0 in stage 14.0 (TID 62, algo-1, executor 1, partition 9, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO TaskSetManager: Lost task 0.2 in stage 14.0 (TID 61) on algo-1, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 35, in decorated
[36malgo-1    |[0m     return func(*args, **kwargs)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 57, in tokenize
[36malgo-1    |[0m     return nltk.tokenize.sent_tokenize(text)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/tokenize/__init__.py", line 106, in sent_tokenize
[36malgo-1    |[0m     tokenizer = load(f"tokenizers/punkt/{language}.pickle")
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 750, in load
[36malgo-1    |[0m     opened_resource = _open(resource_url)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 876, in _open
[36malgo-1    |[0m     return find(path_, path + [""]).open()
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 583, in find
[36malgo-1    |[0m     raise LookupError(resource_not_found)
[36malgo-1    |[0m LookupError: 
[36malgo-1    |[0m **********************************************************************
[36malgo-1    |[0m   Resource [93mpunkt[0m not found.
[36malgo-1    |[0m   Please use the NLTK Downloader to obtain the resource:
[36malgo-1    |[0m 
[36malgo-1    |[0m   [31m>>> import nltk
[36malgo-1    |[0m   >>> nltk.download('punkt')
[36malgo-1    |[0m   [0m
[36malgo-1    |[0m   For more information see: https://www.nltk.org/data.html
[36malgo-1    |[0m 
[36malgo-1    |[0m   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[36malgo-1    |[0m 
[36malgo-1    |[0m   Searched in:
[36malgo-1    |[0m     - '/home/nltk_data'
[36malgo-1    |[0m     - '/usr/nltk_data'
[36malgo-1    |[0m     - '/usr/share/nltk_data'
[36malgo-1    |[0m     - '/usr/lib/nltk_data'
[36malgo-1    |[0m     - '/usr/share/nltk_data'
[36malgo-1    |[0m     - '/usr/local/share/nltk_data'
[36malgo-1    |[0m     - '/usr/lib/nltk_data'
[36malgo-1    |[0m     - '/usr/local/lib/nltk_data'
[36malgo-1    |[0m     - ''
[36malgo-1    |[0m **********************************************************************
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m During handling of the above exception, another exception occurred:
[36malgo-1    |[0m 
[36malgo-1    |[0m Traceback (most recent call last):
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/worker.py", line 605, in main
[36malgo-1    |[0m     process()
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/worker.py", line 597, in process
[36malgo-1    |[0m     serializer.dump_stream(out_iter, outfile)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/serializers.py", line 223, in dump_stream
[36malgo-1    |[0m     self.serializer.dump_stream(self._batched(iterator), stream)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/serializers.py", line 141, in dump_stream
[36malgo-1    |[0m     for obj in iterator:
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/serializers.py", line 212, in _batched
[36malgo-1    |[0m     for item in iterator:
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/worker.py", line 450, in mapper
[36malgo-1    |[0m     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/worker.py", line 450, in <genexpr>
[36malgo-1    |[0m     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/worker.py", line 90, in <lambda>
[36malgo-1    |[0m     return lambda *a: f(*a)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/util.py", line 121, in wrapper
[36malgo-1    |[0m     return f(*args, **kwargs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/worker.py", line 82, in <lambda>
[36malgo-1    |[0m     return lambda *a: g(f(*a))
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/__pyfiles__/hello_py_spark_udfs.py", line 60, in f_emo
[36malgo-1    |[0m     out = emotion_fit(lower_s)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/__pyfiles__/hello_py_spark_udfs.py", line 158, in emotion_fit
[36malgo-1    |[0m     nrclex_out = NRCLex(str(clean_cell))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nrclex.py", line 2873, in __init__
[36malgo-1    |[0m     self.words = list(blob.words)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 24, in __get__
[36malgo-1    |[0m     value = obj.__dict__[self.func.__name__] = self.func(obj)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/blob.py", line 678, in words
[36malgo-1    |[0m     return WordList(word_tokenize(self.raw, include_punc=False))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 73, in word_tokenize
[36malgo-1    |[0m     for sentence in sent_tokenize(text))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/base.py", line 64, in itokenize
[36malgo-1    |[0m     return (t for t in self.tokenize(text, *args, **kwargs))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 38, in decorated
[36malgo-1    |[0m     raise MissingCorpusError()
[36malgo-1    |[0m textblob.exceptions.MissingCorpusError: 
[36malgo-1    |[0m Looks like you are missing some required data for this feature.
[36malgo-1    |[0m 
[36malgo-1    |[0m To download the necessary data, simply run
[36malgo-1    |[0m 
[36malgo-1    |[0m     python -m textblob.download_corpora
[36malgo-1    |[0m 
[36malgo-1    |[0m or use the NLTK downloader to download the missing data: http://nltk.org/data.html
[36malgo-1    |[0m If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.
[36malgo-1    |[0m 
[36malgo-1    |[0m ) [duplicate 1]
[36malgo-1    |[0m 21/11/14 16:55:49 INFO TaskSetManager: Starting task 0.3 in stage 14.0 (TID 63, algo-2, executor 2, partition 5, PROCESS_LOCAL, 7398 bytes)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO TaskSetManager: Finished task 3.0 in stage 14.0 (TID 60) in 639 ms on algo-2 (executor 2) (3/11)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO TaskSetManager: Starting task 5.0 in stage 14.0 (TID 64, algo-2, executor 2, partition 10, PROCESS_LOCAL, 7398 bytes)
[36malgo-1    |[0m 21/11/14 16:55:49 INFO TaskSetManager: Lost task 0.3 in stage 14.0 (TID 63) on algo-2, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 35, in decorated
[36malgo-1    |[0m     return func(*args, **kwargs)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 57, in tokenize
[36malgo-1    |[0m     return nltk.tokenize.sent_tokenize(text)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/tokenize/__init__.py", line 106, in sent_tokenize
[36malgo-1    |[0m     tokenizer = load(f"tokenizers/punkt/{language}.pickle")
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 750, in load
[36malgo-1    |[0m     opened_resource = _open(resource_url)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 876, in _open
[36malgo-1    |[0m     return find(path_, path + [""]).open()
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 583, in find
[36malgo-1    |[0m     raise LookupError(resource_not_found)
[36malgo-1    |[0m LookupError: 
[36malgo-1    |[0m **********************************************************************
[36malgo-1    |[0m   Resource [93mpunkt[0m not found.
[36malgo-1    |[0m   Please use the NLTK Downloader to obtain the resource:
[36malgo-1    |[0m 
[36malgo-1    |[0m   [31m>>> import nltk
[36malgo-1    |[0m   >>> nltk.download('punkt')
[36malgo-1    |[0m   [0m
[36malgo-1    |[0m   For more information see: https://www.nltk.org/data.html
[36malgo-1    |[0m 
[36malgo-1    |[0m   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[36malgo-1    |[0m 
[36malgo-1    |[0m   Searched in:
[36malgo-1    |[0m     - '/home/nltk_data'
[36malgo-1    |[0m     - '/usr/nltk_data'
[36malgo-1    |[0m     - '/usr/share/nltk_data'
[36malgo-1    |[0m     - '/usr/lib/nltk_data'
[36malgo-1    |[0m     - '/usr/share/nltk_data'
[36malgo-1    |[0m     - '/usr/local/share/nltk_data'
[36malgo-1    |[0m     - '/usr/lib/nltk_data'
[36malgo-1    |[0m     - '/usr/local/lib/nltk_data'
[36malgo-1    |[0m     - ''
[36malgo-1    |[0m **********************************************************************
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m During handling of the above exception, another exception occurred:
[36malgo-1    |[0m 
[36malgo-1    |[0m Traceback (most recent call last):
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 605, in main
[36malgo-1    |[0m     process()
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 597, in process
[36malgo-1    |[0m     serializer.dump_stream(out_iter, outfile)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 223, in dump_stream
[36malgo-1    |[0m     self.serializer.dump_stream(self._batched(iterator), stream)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 141, in dump_stream
[36malgo-1    |[0m     for obj in iterator:
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 212, in _batched
[36malgo-1    |[0m     for item in iterator:
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 450, in mapper
[36malgo-1    |[0m     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 450, in <genexpr>
[36malgo-1    |[0m     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 90, in <lambda>
[36malgo-1    |[0m     return lambda *a: f(*a)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/util.py", line 121, in wrapper
[36malgo-1    |[0m     return f(*args, **kwargs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 82, in <lambda>
[36malgo-1    |[0m     return lambda *a: g(f(*a))
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/__pyfiles__/hello_py_spark_udfs.py", line 60, in f_emo
[36malgo-1    |[0m     out = emotion_fit(lower_s)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/__pyfiles__/hello_py_spark_udfs.py", line 158, in emotion_fit
[36malgo-1    |[0m     nrclex_out = NRCLex(str(clean_cell))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nrclex.py", line 2873, in __init__
[36malgo-1    |[0m     self.words = list(blob.words)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 24, in __get__
[36malgo-1    |[0m     value = obj.__dict__[self.func.__name__] = self.func(obj)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/blob.py", line 678, in words
[36malgo-1    |[0m     return WordList(word_tokenize(self.raw, include_punc=False))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 73, in word_tokenize
[36malgo-1    |[0m     for sentence in sent_tokenize(text))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/base.py", line 64, in itokenize
[36malgo-1    |[0m     return (t for t in self.tokenize(text, *args, **kwargs))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 38, in decorated
[36malgo-1    |[0m     raise MissingCorpusError()
[36malgo-1    |[0m textblob.exceptions.MissingCorpusError: 
[36malgo-1    |[0m Looks like you are missing some required data for this feature.
[36malgo-1    |[0m 
[36malgo-1    |[0m To download the necessary data, simply run
[36malgo-1    |[0m 
[36malgo-1    |[0m     python -m textblob.download_corpora
[36malgo-1    |[0m 
[36malgo-1    |[0m or use the NLTK downloader to download the missing data: http://nltk.org/data.html
[36malgo-1    |[0m If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.
[36malgo-1    |[0m 
[36malgo-1    |[0m ) [duplicate 1]
[36malgo-1    |[0m 21/11/14 16:55:49 ERROR TaskSetManager: Task 0 in stage 14.0 failed 4 times; aborting job
[36malgo-1    |[0m 21/11/14 16:55:49 INFO YarnScheduler: Cancelling stage 14
[36malgo-1    |[0m 21/11/14 16:55:49 INFO YarnScheduler: Killing all running tasks in stage 14: Stage cancelled
[36malgo-1    |[0m 21/11/14 16:55:49 INFO YarnScheduler: Stage 14 was cancelled
[36malgo-1    |[0m 21/11/14 16:55:49 INFO DAGScheduler: ResultStage 14 (showString at NativeMethodAccessorImpl.java:0) failed in 0.833 s due to Job aborted due to stage failure: Task 0 in stage 14.0 failed 4 times, most recent failure: Lost task 0.3 in stage 14.0 (TID 63, algo-2, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 35, in decorated
[36malgo-1    |[0m     return func(*args, **kwargs)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 57, in tokenize
[36malgo-1    |[0m     return nltk.tokenize.sent_tokenize(text)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/tokenize/__init__.py", line 106, in sent_tokenize
[36malgo-1    |[0m     tokenizer = load(f"tokenizers/punkt/{language}.pickle")
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 750, in load
[36malgo-1    |[0m     opened_resource = _open(resource_url)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 876, in _open
[36malgo-1    |[0m     return find(path_, path + [""]).open()
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 583, in find
[36malgo-1    |[0m     raise LookupError(resource_not_found)
[36malgo-1    |[0m LookupError: 
[36malgo-1    |[0m **********************************************************************
[36malgo-1    |[0m   Resource [93mpunkt[0m not found.
[36malgo-1    |[0m   Please use the NLTK Downloader to obtain the resource:
[36malgo-1    |[0m 
[36malgo-1    |[0m   [31m>>> import nltk
[36malgo-1    |[0m   >>> nltk.download('punkt')
[36malgo-1    |[0m   [0m
[36malgo-1    |[0m   For more information see: https://www.nltk.org/data.html
[36malgo-1    |[0m 
[36malgo-1    |[0m   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[36malgo-1    |[0m 
[36malgo-1    |[0m   Searched in:
[36malgo-1    |[0m     - '/home/nltk_data'
[36malgo-1    |[0m     - '/usr/nltk_data'
[36malgo-1    |[0m     - '/usr/share/nltk_data'
[36malgo-1    |[0m     - '/usr/lib/nltk_data'
[36malgo-1    |[0m     - '/usr/share/nltk_data'
[36malgo-1    |[0m     - '/usr/local/share/nltk_data'
[36malgo-1    |[0m     - '/usr/lib/nltk_data'
[36malgo-1    |[0m     - '/usr/local/lib/nltk_data'
[36malgo-1    |[0m     - ''
[36malgo-1    |[0m **********************************************************************
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m During handling of the above exception, another exception occurred:
[36malgo-1    |[0m 
[36malgo-1    |[0m Traceback (most recent call last):
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 605, in main
[36malgo-1    |[0m     process()
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 597, in process
[36malgo-1    |[0m     serializer.dump_stream(out_iter, outfile)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 223, in dump_stream
[36malgo-1    |[0m     self.serializer.dump_stream(self._batched(iterator), stream)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 141, in dump_stream
[36malgo-1    |[0m     for obj in iterator:
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 212, in _batched
[36malgo-1    |[0m     for item in iterator:
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 450, in mapper
[36malgo-1    |[0m     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 450, in <genexpr>
[36malgo-1    |[0m     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 90, in <lambda>
[36malgo-1    |[0m     return lambda *a: f(*a)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/util.py", line 121, in wrapper
[36malgo-1    |[0m     return f(*args, **kwargs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 82, in <lambda>
[36malgo-1    |[0m     return lambda *a: g(f(*a))
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/__pyfiles__/hello_py_spark_udfs.py", line 60, in f_emo
[36malgo-1    |[0m     out = emotion_fit(lower_s)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/__pyfiles__/hello_py_spark_udfs.py", line 158, in emotion_fit
[36malgo-1    |[0m     nrclex_out = NRCLex(str(clean_cell))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nrclex.py", line 2873, in __init__
[36malgo-1    |[0m     self.words = list(blob.words)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 24, in __get__
[36malgo-1    |[0m     value = obj.__dict__[self.func.__name__] = self.func(obj)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/blob.py", line 678, in words
[36malgo-1    |[0m     return WordList(word_tokenize(self.raw, include_punc=False))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 73, in word_tokenize
[36malgo-1    |[0m     for sentence in sent_tokenize(text))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/base.py", line 64, in itokenize
[36malgo-1    |[0m     return (t for t in self.tokenize(text, *args, **kwargs))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 38, in decorated
[36malgo-1    |[0m     raise MissingCorpusError()
[36malgo-1    |[0m textblob.exceptions.MissingCorpusError: 
[36malgo-1    |[0m Looks like you are missing some required data for this feature.
[36malgo-1    |[0m 
[36malgo-1    |[0m To download the necessary data, simply run
[36malgo-1    |[0m 
[36malgo-1    |[0m     python -m textblob.download_corpora
[36malgo-1    |[0m 
[36malgo-1    |[0m or use the NLTK downloader to download the missing data: http://nltk.org/data.html
[36malgo-1    |[0m If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)
[36malgo-1    |[0m 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
[36malgo-1    |[0m 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[36malgo-1    |[0m 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
[36malgo-1    |[0m 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[36malgo-1    |[0m 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[36malgo-1    |[0m 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
[36malgo-1    |[0m 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
[36malgo-1    |[0m 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
[36malgo-1    |[0m 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[36malgo-1    |[0m 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
[36malgo-1    |[0m 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
[36malgo-1    |[0m 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[36malgo-1    |[0m 	at org.apache.spark.scheduler.Task.run(Task.scala:127)
[36malgo-1    |[0m 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
[36malgo-1    |[0m 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
[36malgo-1    |[0m 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
[36malgo-1    |[0m 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[36malgo-1    |[0m 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[36malgo-1    |[0m 	at java.lang.Thread.run(Thread.java:748)
[36malgo-1    |[0m 
[36malgo-1    |[0m Driver stacktrace:
[36malgo-1    |[0m 21/11/14 16:55:49 INFO DAGScheduler: Job 13 failed: showString at NativeMethodAccessorImpl.java:0, took 0.837032 s
[36malgo-1    |[0m 21/11/14 16:55:50 WARN TaskSetManager: Lost task 5.0 in stage 14.0 (TID 64, algo-2, executor 2): TaskKilled (Stage cancelled)
[36malgo-1    |[0m Traceback (most recent call last):
[36malgo-1    |[0m   File "/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py", line 123, in <module>
[36malgo-1    |[0m     output.show()
[36malgo-1    |[0m   File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 441, in show
[36malgo-1    |[0m   File "/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__
[36malgo-1    |[0m   File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 137, in deco
[36malgo-1    |[0m   File "<string>", line 3, in raise_from
[36malgo-1    |[0m pyspark.sql.utils.PythonException: 
[36malgo-1    |[0m   An exception was thrown from Python worker in the executor. The below is the Python worker stacktrace.
[36malgo-1    |[0m Traceback (most recent call last):
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 35, in decorated
[36malgo-1    |[0m     return func(*args, **kwargs)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 57, in tokenize
[36malgo-1    |[0m     return nltk.tokenize.sent_tokenize(text)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/tokenize/__init__.py", line 106, in sent_tokenize
[36malgo-1    |[0m     tokenizer = load(f"tokenizers/punkt/{language}.pickle")
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 750, in load
[36malgo-1    |[0m     opened_resource = _open(resource_url)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 876, in _open
[36malgo-1    |[0m     return find(path_, path + [""]).open()
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 583, in find
[36malgo-1    |[0m     raise LookupError(resource_not_found)
[36malgo-1    |[0m LookupError: 
[36malgo-1    |[0m **********************************************************************
[36malgo-1    |[0m   Resource [93mpunkt[0m not found.
[36malgo-1    |[0m   Please use the NLTK Downloader to obtain the resource:
[36malgo-1    |[0m 
[36malgo-1    |[0m   [31m>>> import nltk
[36malgo-1    |[0m   >>> nltk.download('punkt')
[36malgo-1    |[0m   [0m
[36malgo-1    |[0m   For more information see: https://www.nltk.org/data.html
[36malgo-1    |[0m 
[36malgo-1    |[0m   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[36malgo-1    |[0m 
[36malgo-1    |[0m   Searched in:
[36malgo-1    |[0m     - '/home/nltk_data'
[36malgo-1    |[0m     - '/usr/nltk_data'
[36malgo-1    |[0m     - '/usr/share/nltk_data'
[36malgo-1    |[0m     - '/usr/lib/nltk_data'
[36malgo-1    |[0m     - '/usr/share/nltk_data'
[36malgo-1    |[0m     - '/usr/local/share/nltk_data'
[36malgo-1    |[0m     - '/usr/lib/nltk_data'
[36malgo-1    |[0m     - '/usr/local/lib/nltk_data'
[36malgo-1    |[0m     - ''
[36malgo-1    |[0m **********************************************************************
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m During handling of the above exception, another exception occurred:
[36malgo-1    |[0m 
[36malgo-1    |[0m Traceback (most recent call last):
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 605, in main
[36malgo-1    |[0m     process()
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 597, in process
[36malgo-1    |[0m     serializer.dump_stream(out_iter, outfile)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 223, in dump_stream
[36malgo-1    |[0m     self.serializer.dump_stream(self._batched(iterator), stream)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 141, in dump_stream
[36malgo-1    |[0m     for obj in iterator:
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 212, in _batched
[36malgo-1    |[0m     for item in iterator:
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 450, in mapper
[36malgo-1    |[0m     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 450, in <genexpr>
[36malgo-1    |[0m     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 90, in <lambda>
[36malgo-1    |[0m     return lambda *a: f(*a)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/util.py", line 121, in wrapper
[36malgo-1    |[0m     return f(*args, **kwargs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 82, in <lambda>
[36malgo-1    |[0m     return lambda *a: g(f(*a))
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/__pyfiles__/hello_py_spark_udfs.py", line 60, in f_emo
[36malgo-1    |[0m     out = emotion_fit(lower_s)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/__pyfiles__/hello_py_spark_udfs.py", line 158, in emotion_fit
[36malgo-1    |[0m     nrclex_out = NRCLex(str(clean_cell))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nrclex.py", line 2873, in __init__
[36malgo-1    |[0m     self.words = list(blob.words)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 24, in __get__
[36malgo-1    |[0m     value = obj.__dict__[self.func.__name__] = self.func(obj)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/blob.py", line 678, in words
[36malgo-1    |[0m     return WordList(word_tokenize(self.raw, include_punc=False))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 73, in word_tokenize
[36malgo-1    |[0m     for sentence in sent_tokenize(text))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/base.py", line 64, in itokenize
[36malgo-1    |[0m     return (t for t in self.tokenize(text, *args, **kwargs))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 38, in decorated
[36malgo-1    |[0m     raise MissingCorpusError()
[36malgo-1    |[0m textblob.exceptions.MissingCorpusError: 
[36malgo-1    |[0m Looks like you are missing some required data for this feature.
[36malgo-1    |[0m 
[36malgo-1    |[0m To download the necessary data, simply run
[36malgo-1    |[0m 
[36malgo-1    |[0m     python -m textblob.download_corpora
[36malgo-1    |[0m 
[36malgo-1    |[0m or use the NLTK downloader to download the missing data: http://nltk.org/data.html
[36malgo-1    |[0m If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m 21/11/14 16:55:50 WARN TaskSetManager: Lost task 4.0 in stage 14.0 (TID 62, algo-1, executor 1): TaskKilled (Stage cancelled)
[36malgo-1    |[0m 21/11/14 16:55:50 INFO YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:55:50 INFO SparkContext: Invoking stop() from shutdown hook
[36malgo-1    |[0m 21/11/14 16:55:50 INFO SparkUI: Stopped Spark web UI at http://172.21.0.2:4040
[36malgo-1    |[0m 21/11/14 16:55:50 INFO YarnClientSchedulerBackend: Interrupting monitor thread
[36malgo-1    |[0m 21/11/14 16:55:50 INFO YarnClientSchedulerBackend: Shutting down all executors
[36malgo-1    |[0m 21/11/14 16:55:50 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
[36malgo-1    |[0m 21/11/14 16:55:50 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
[36malgo-1    |[0m 21/11/14 16:55:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[36malgo-1    |[0m 21/11/14 16:55:50 INFO MemoryStore: MemoryStore cleared
[36malgo-1    |[0m 21/11/14 16:55:50 INFO BlockManager: BlockManager stopped
[36malgo-1    |[0m 21/11/14 16:55:50 INFO BlockManagerMaster: BlockManagerMaster stopped
[36malgo-1    |[0m 21/11/14 16:55:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[36malgo-1    |[0m 21/11/14 16:55:50 INFO SparkContext: Successfully stopped SparkContext
[36malgo-1    |[0m 21/11/14 16:55:50 INFO ShutdownHookManager: Shutdown hook called
[36malgo-1    |[0m 21/11/14 16:55:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-cc65bcd2-9cf6-433a-ae81-58b52cb16609/pyspark-4002dde9-61ff-431b-a57c-405c5ca47f6e
[36malgo-1    |[0m 21/11/14 16:55:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-3e1b556f-a775-4773-ba9a-b61fc2dbc13c
[36malgo-1    |[0m 21/11/14 16:55:50 INFO ShutdownHookManager: Deleting directory /tmp/localPyFiles-257a66a9-c8c7-4e22-b7d4-dc5d7eb95038
[36malgo-1    |[0m 21/11/14 16:55:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-cc65bcd2-9cf6-433a-ae81-58b52cb16609
[36malgo-1    |[0m 2021-11-14 16:55:50,711 INFO attempt.RMAppAttemptImpl: Updating application attempt appattempt_1636908833443_0001_000001 with final state: FINISHING, and exit status: -1000
[36malgo-1    |[0m 2021-11-14 16:55:50,712 INFO attempt.RMAppAttemptImpl: appattempt_1636908833443_0001_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
[36malgo-1    |[0m 2021-11-14 16:55:50,712 INFO rmapp.RMAppImpl: Updating application application_1636908833443_0001 with final state: FINISHING
[36malgo-1    |[0m 2021-11-14 16:55:50,712 INFO recovery.RMStateStore: Updating info for app: application_1636908833443_0001
[36malgo-1    |[0m 2021-11-14 16:55:50,712 INFO rmapp.RMAppImpl: application_1636908833443_0001 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
[36malgo-1    |[0m 2021-11-14 16:55:50,712 INFO attempt.RMAppAttemptImpl: appattempt_1636908833443_0001_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
[36malgo-1    |[0m 2021-11-14 16:55:50,712 INFO rmapp.RMAppImpl: application_1636908833443_0001 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]2021-11-14 16:55:50,732 INFO launcher.ContainerLaunch: Container container_1636908833443_0001_01_000003 succeeded 
[36malgo-1    |[0m 2021-11-14 16:55:50,738 INFO launcher.ContainerLaunch: Container container_1636908833443_0001_01_000002 succeeded 
[33malgo-2    |[0m 2021-11-14 16:55:50,740 INFO container.ContainerImpl: Container container_1636908833443_0001_01_000003 transitioned from RUNNING to EXITED_WITH_SUCCESS
[33malgo-2    |[0m 2021-11-14 16:55:50,741 INFO launcher.ContainerCleanup: Cleaning up container container_1636908833443_0001_01_000003
[33malgo-2    |[0m 2021-11-14 16:55:50,742 INFO nodemanager.NMAuditLogger: USER=root	OPERATION=Container Finished - Succeeded	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1636908833443_0001	CONTAINERID=container_1636908833443_0001_01_000003
[33malgo-2    |[0m 2021-11-14 16:55:50,743 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003
[33malgo-2    |[0m 2021-11-14 16:55:50,744 INFO container.ContainerImpl: Container container_1636908833443_0001_01_000003 transitioned from EXITED_WITH_SUCCESS to DONE
[33malgo-2    |[0m 2021-11-14 16:55:50,744 INFO application.ApplicationImpl: Removing container_1636908833443_0001_01_000003 from application application_1636908833443_0001
[33malgo-2    |[0m 2021-11-14 16:55:50,745 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1636908833443_0001_01_000003
[33malgo-2    |[0m 2021-11-14 16:55:50,745 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1636908833443_0001
[36malgo-1    |[0m 2021-11-14 16:55:50,746 INFO container.ContainerImpl: Container container_1636908833443_0001_01_000002 transitioned from RUNNING to EXITED_WITH_SUCCESS
[36malgo-1    |[0m 2021-11-14 16:55:50,747 INFO rmcontainer.RMContainerImpl: container_1636908833443_0001_01_000003 Container Transitioned from RUNNING to COMPLETED
[36malgo-1    |[0m 2021-11-14 16:55:50,747 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636908833443_0001	CONTAINERID=container_1636908833443_0001_01_000003	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 16:55:50,748 INFO launcher.ContainerCleanup: Cleaning up container container_1636908833443_0001_01_000002
[36malgo-1    |[0m 2021-11-14 16:55:50,751 INFO nodemanager.NMAuditLogger: USER=root	OPERATION=Container Finished - Succeeded	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1636908833443_0001	CONTAINERID=container_1636908833443_0001_01_000002
[36malgo-1    |[0m 2021-11-14 16:55:50,752 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002
[36malgo-1    |[0m 2021-11-14 16:55:50,753 INFO container.ContainerImpl: Container container_1636908833443_0001_01_000002 transitioned from EXITED_WITH_SUCCESS to DONE
[36malgo-1    |[0m 2021-11-14 16:55:50,753 INFO application.ApplicationImpl: Removing container_1636908833443_0001_01_000002 from application application_1636908833443_0001
[36malgo-1    |[0m 2021-11-14 16:55:50,753 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1636908833443_0001_01_000002
[36malgo-1    |[0m 2021-11-14 16:55:50,753 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1636908833443_0001
[36malgo-1    |[0m 2021-11-14 16:55:50,755 INFO rmcontainer.RMContainerImpl: container_1636908833443_0001_01_000002 Container Transitioned from RUNNING to COMPLETED
[36malgo-1    |[0m 2021-11-14 16:55:50,755 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636908833443_0001	CONTAINERID=container_1636908833443_0001_01_000002	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[33malgo-2    |[0m 2021-11-14 16:55:50,758 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/launch_container.sh
[33malgo-2    |[0m 2021-11-14 16:55:50,758 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/launch_container.sh]
[33malgo-2    |[0m 2021-11-14 16:55:50,758 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/container_tokens
[33malgo-2    |[0m 2021-11-14 16:55:50,759 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/container_tokens]
[33malgo-2    |[0m 2021-11-14 16:55:50,759 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/sysfs
[33malgo-2    |[0m 2021-11-14 16:55:50,759 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/sysfs]
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stdout] 2021-11-14T16:54:39.960+0000: [GC (Allocation Failure) [PSYoungGen: 120320K->8960K(140288K)] 120320K->8976K(461312K), 0.0075177 secs] [Times: user=0.01 sys=0.01, real=0.00 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stdout] 2021-11-14T16:54:40.446+0000: [GC (Allocation Failure) [PSYoungGen: 129280K->7707K(140288K)] 129296K->7731K(461312K), 0.0058898 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stdout] 2021-11-14T16:54:40.769+0000: [GC (Metadata GC Threshold) [PSYoungGen: 108583K->8673K(140288K)] 108607K->8705K(461312K), 0.0049221 secs] [Times: user=0.02 sys=0.00, real=0.00 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stdout] 2021-11-14T16:54:40.774+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 8673K->0K(140288K)] [ParOldGen: 32K->8401K(189952K)] 8705K->8401K(330240K), [Metaspace: 20381K->20381K(1067008K)], 0.0227093 secs] [Times: user=0.04 sys=0.00, real=0.03 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stdout] 2021-11-14T16:54:41.266+0000: [GC (Allocation Failure) [PSYoungGen: 120320K->4524K(182784K)] 128721K->12933K(372736K), 0.0042210 secs] [Times: user=0.01 sys=0.01, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stdout] 2021-11-14T16:54:41.676+0000: [GC (Allocation Failure) [PSYoungGen: 182700K->7423K(245248K)] 191109K->15832K(435200K), 0.0063629 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stdout] 2021-11-14T16:54:41.782+0000: [GC (Metadata GC Threshold) [PSYoungGen: 55241K->4035K(279040K)] 63650K->12453K(468992K), 0.0041987 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stdout] 2021-11-14T16:54:41.787+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 4035K->0K(279040K)] [ParOldGen: 8417K->10157K(280064K)] 12453K->10157K(559104K), [Metaspace: 33977K->33974K(1079296K)], 0.0240318 secs] [Times: user=0.05 sys=0.00, real=0.03 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stdout] 2021-11-14T16:55:41.900+0000: [GC (Allocation Failure) [PSYoungGen: 268800K->10224K(279040K)] 278957K->21783K(559104K), 0.0102987 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stdout] 2021-11-14T16:55:42.842+0000: [GC (Metadata GC Threshold) [PSYoungGen: 279024K->12286K(396288K)] 290583K->27843K(676352K), 0.0120026 secs] [Times: user=0.04 sys=0.01, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stdout] 2021-11-14T16:55:42.854+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 12286K->0K(396288K)] [ParOldGen: 15556K->24493K(374272K)] 27843K->24493K(770560K), [Metaspace: 54764K->54756K(1099776K)], 0.0798940 secs] [Times: user=0.18 sys=0.00, real=0.08 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stdout] 2021-11-14T16:55:48.648+0000: [GC (Allocation Failure) [PSYoungGen: 384000K->12120K(399872K)] 408493K->134926K(774144K), 0.0376659 secs] [Times: user=0.07 sys=0.06, real=0.03 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stdout] Heap
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stdout]  PSYoungGen      total 399872K, used 69136K [0x00000000d5580000, 0x00000000f5b80000, 0x0000000100000000)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/cocontainer_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at java.lang.Thread.run(Thread.java:748)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 58
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO Executor: Running task 2.0 in stage 14.0 (TID 58)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO PythonRunner: Times: total = 48, boot = -19, init = 67, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO PythonUDFRunner: Times: total = 673, boot = 3, init = 670, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO Executor: Finished task 2.0 in stage 14.0 (TID 58). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 61
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO Executor: Running task 0.2 in stage 14.0 (TID 61)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 INFO PythonRunner: Times: total = 2, boot = -609, init = 611, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 1st Check
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] **********************************************************************
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   Resource [93mpunkt[0m not found.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   Please use the NLTK Downloader to obtain the resource:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   [31m>>> import nltk
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   >>> nltk.download('punkt')
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   [0m
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   For more information see: https://www.nltk.org/data.html
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   Searched in:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/home/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/share/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/lib/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/share/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/local/share/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/lib/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/local/lib/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - ''
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] **********************************************************************
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 21/11/14 16:55:49 ERROR Executor: Exception in task 0.2 in stage 14.0 (TID 61)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 35, in decorated
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     return func(*args, **kwargs)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 57, in tokenize
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     return nltk.tokenize.sent_tokenize(text)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/tokenize/__init__.py", line 106, in sent_tokenize
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     tokenizer = load(f"tokenizers/punkt/{language}.pickle")
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 750, in load
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     opened_resource = _open(resource_url)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 876, in _open
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     return find(path_, path + [""]).open()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 583, in find
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     raise LookupError(resource_not_found)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] LookupError: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] **********************************************************************
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   Resource [93mpunkt[0m not found.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   Please use the NLTK Downloader to obtain the resource:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   [31m>>> import nltk
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   >>> nltk.download('punkt')
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   [0m
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   For more information see: https://www.nltk.org/data.html
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   Searched in:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/home/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/share/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/lib/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/share/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/local/share/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/lib/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - '/usr/local/lib/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     - ''
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] **********************************************************************
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] During handling of the above exception, another exception occurred:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] Traceback (most recent call last):
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/worker.py", line 605, in main
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     process()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/worker.py", line 597, in process
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     serializer.dump_stream(out_iter, outfile)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/serializers.py", line 223, in dump_stream
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     self.serializer.dump_stream(self._batched(iterator), stream)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/serializers.py", line 141, in dump_stream
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     for obj in iterator:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/serializers.py", line 212, in _batched
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     for item in iterator:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/worker.py", line 450, in mapper
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/worker.py", line 450, in <genexpr>
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/worker.py", line 90, in <lambda>
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     return lambda *a: f(*a)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/util.py", line 121, in wrapper
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     return f(*args, **kwargs)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/pyspark.zip/pyspark/worker.py", line 82, in <lambda>
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     return lambda *a: g(f(*a))
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/__pyfiles__/hello_py_spark_udfs.py", line 60, in f_emo
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     out = emotion_fit(lower_s)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/__pyfiles__/hello_py_spark_udfs.py", line 158, in emotion_fit
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     nrclex_out = NRCLex(str(clean_cell))
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/nrclex.py", line 2873, in __init__
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     self.words = list(blob.words)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 24, in __get__
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     value = obj.__dict__[self.func.__name__] = self.func(obj)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/blob.py", line 678, in words
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     return WordList(word_tokenize(self.raw, include_punc=False))
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 73, in word_tokenize
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     for sentence in sent_tokenize(text))
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/base.py", line 64, in itokenize
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     return (t for t in self.tokenize(text, *args, **kwargs))
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 38, in decorated
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     raise MissingCorpusError()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] textblob.exceptions.MissingCorpusError: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] Looks like you are missing some required data for this feature.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] To download the necessary data, simply run
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr]     python -m textblob.download_corpora
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] or use the NLTK downloader to download the missing data: http://nltk.org/data.html
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000002/stderr] 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_00012021-11-14 16:55:50,765 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/launch_container.sh
[36malgo-1    |[0m 2021-11-14 16:55:50,766 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/launch_container.sh]
[36malgo-1    |[0m 2021-11-14 16:55:50,766 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/container_tokens
[36malgo-1    |[0m 2021-11-14 16:55:50,766 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/container_tokens]
[36malgo-1    |[0m 2021-11-14 16:55:50,766 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/sysfs
[36malgo-1    |[0m 2021-11-14 16:55:50,766 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000002/sysfs]
[36malgo-1    |[0m 2021-11-14 16:55:50,812 INFO resourcemanager.ApplicationMasterService: application_1636908833443_0001 unregistered successfully. 
[36malgo-1    |[0m 2021-11-14 16:55:50,825 INFO namenode.FSEditLog: Number of transactions: 48 Total time for transactions(ms): 15 Number of transactions batched in Syncs: 9 Number of syncs: 39 SyncTimes(ms): 33 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stdout] 2021-11-14T16:54:36.080+0000: [GC (Allocation Failure) [PSYoungGen: 120320K->9030K(140288K)] 120320K->9046K(461312K), 0.0078153 secs] [Times: user=0.02 sys=0.00, real=0.00 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stdout] 2021-11-14T16:54:36.547+0000: [GC (Allocation Failure) [PSYoungGen: 129350K->7708K(140288K)] 129366K->7732K(461312K), 0.0065534 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stdout] 2021-11-14T16:54:36.887+0000: [GC (Metadata GC Threshold) [PSYoungGen: 109333K->8661K(140288K)] 109357K->8693K(461312K), 0.0052881 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stdout] 2021-11-14T16:54:36.892+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 8661K->0K(140288K)] [ParOldGen: 32K->8408K(195584K)] 8693K->8408K(335872K), [Metaspace: 20380K->20380K(1067008K)], 0.0295549 secs] [Times: user=0.04 sys=0.02, real=0.03 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stdout] 2021-11-14T16:54:37.432+0000: [GC (Allocation Failure) [PSYoungGen: 120320K->4626K(177152K)] 128728K->13043K(372736K), 0.0042916 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stdout] 2021-11-14T16:54:37.845+0000: [GC (Allocation Failure) [PSYoungGen: 176658K->7315K(245760K)] 185075K->15731K(441344K), 0.0063270 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stdout] 2021-11-14T16:54:37.967+0000: [GC (Metadata GC Threshold) [PSYoungGen: 56624K->4087K(260096K)] 65041K->12512K(455680K), 0.0034929 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stdout] 2021-11-14T16:54:37.971+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 4087K->0K(260096K)] [ParOldGen: 8424K->10210K(297984K)] 12512K->10210K(558080K), [Metaspace: 33956K->33953K(1079296K)], 0.0242543 secs] [Times: user=0.04 sys=0.00, real=0.02 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stdout] 2021-11-14T16:54:38.618+0000: [GC (Allocation Failure) [PSYoungGen: 249856K->10731K(260608K)] 260066K->21389K(558592K), 0.0100837 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stdout] 2021-11-14T16:54:39.439+0000: [GC (Allocation Failure) [PSYoungGen: 260587K->12263K(374784K)] 271245K->24932K(672768K), 0.0126325 secs] [Times: user=0.01 sys=0.02, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stdout] 2021-11-14T16:54:39.568+0000: [GC (Metadata GC Threshold) [PSYoungGen: 42214K->10606K(377344K)] 54883K->23283K(675328K), 0.0090817 secs] [Times: user=0.03 sys=0.00, real=0.02 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stdout] 2021-11-14T16:54:39.577+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 10606K->0K(377344K)] [ParOldGen: 12677K->19163K(394752K)] 23283K->19163K(772096K), [Metaspace: 54361K->54361K(1099776K)], 0.0850130 secs] [Times: user=0.20 sys=0.00, real=0.08 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stdout] Heap
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stdout]  PSYoungGen      total 377344K, used 344835K [0x00000000d5580000, 0x00000000f5780000, 0x0000000100000000)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/cont 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.scheduler.Task.run(Task.scala:127)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at java.lang.Thread.run(Thread.java:748)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 60
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO Executor: Running task 3.0 in stage 14.0 (TID 60)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO PythonRunner: Times: total = 2, boot = -32, init = 34, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO PythonUDFRunner: Times: total = 625, boot = 3, init = 622, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO Executor: Finished task 3.0 in stage 14.0 (TID 60). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 63
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO Executor: Running task 0.3 in stage 14.0 (TID 63)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO PythonRunner: Times: total = 2, boot = -629, init = 631, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 1st Check
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] **********************************************************************
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   Resource [93mpunkt[0m not found.
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   Please use the NLTK Downloader to obtain the resource:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   [31m>>> import nltk
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   >>> nltk.download('punkt')
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   [0m
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   For more information see: https://www.nltk.org/data.html
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   Searched in:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/home/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/share/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/lib/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/share/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/local/share/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/lib/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/local/lib/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - ''
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] **********************************************************************
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 ERROR Executor: Exception in task 0.3 in stage 14.0 (TID 63)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 35, in decorated
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     return func(*args, **kwargs)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 57, in tokenize
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     return nltk.tokenize.sent_tokenize(text)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/tokenize/__init__.py", line 106, in sent_tokenize
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     tokenizer = load(f"tokenizers/punkt/{language}.pickle")
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 750, in load
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     opened_resource = _open(resource_url)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 876, in _open
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     return find(path_, path + [""]).open()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 583, in find
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     raise LookupError(resource_not_found)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] LookupError: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] **********************************************************************
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   Resource [93mpunkt[0m not found.
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   Please use the NLTK Downloader to obtain the resource:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   [31m>>> import nltk
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   >>> nltk.download('punkt')
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   [0m
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   For more information see: https://www.nltk.org/data.html
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   Searched in:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/home/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/share/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/lib/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/share/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/local/share/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/lib/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - '/usr/local/lib/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     - ''
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] **********************************************************************
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] During handling of the above exception, another exception occurred:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] Traceback (most recent call last):
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 605, in main
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     process()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 597, in process
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     serializer.dump_stream(out_iter, outfile)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 223, in dump_stream
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     self.serializer.dump_stream(self._batched(iterator), stream)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 141, in dump_stream
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     for obj in iterator:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 212, in _batched
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     for item in iterator:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 450, in mapper
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 450, in <genexpr>
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 90, in <lambda>
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     return lambda *a: f(*a)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/util.py", line 121, in wrapper
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     return f(*args, **kwargs)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/pyspark.zip/pyspark/worker.py", line 82, in <lambda>
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     return lambda *a: g(f(*a))
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/__pyfiles__/hello_py_spark_udfs.py", line 60, in f_emo
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     out = emotion_fit(lower_s)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000003/__pyfiles__/hello_py_spark_udfs.py", line 158, in emotion_fit
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     nrclex_out = NRCLex(str(clean_cell))
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/nrclex.py", line 2873, in __init__
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     self.words = list(blob.words)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 24, in __get__
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     value = obj.__dict__[self.func.__name__] = self.func(obj)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/blob.py", line 678, in words
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     return WordList(word_tokenize(self.raw, include_punc=False))
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 73, in word_tokenize
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     for sentence in sent_tokenize(text))
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/base.py", line 64, in itokenize
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     return (t for t in self.tokenize(text, *args, **kwargs))
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 38, in decorated
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     raise MissingCorpusError()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] textblob.exceptions.MissingCorpusError: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] Looks like you are missing some required data for this feature.
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] To download the necessary data, simply run
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr]     python -m textblob.download_corpora
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] or use the NLTK downloader to download the missing data: http://nltk.org/data.html
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.scheduler.Task.run(Task.scala:127)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 	at java.lang.Thread.run(Thread.java:748)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 64
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO Executor: Running task 5.0 in stage 14.0 (TID 64)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:49 INFO Executor: Executor is trying to kill task 5.0 in stage 14.0 (TID 64), reason: Stage cancelled
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:50 INFO Executor: Executor killed task 5.0 in stage 14.0 (TID 64), reason: Stage cancelled
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/14 16:55:50 INFO YarnCoarseGrainedExecutorBackend: Driver commanded a shutdown
[36malgo-1    |[0m 11-14 16:55 smspark-submit ERROR    spark-submit command failed with exit code 1: Command 'spark-submit --master yarn --deploy-mode client --py-files /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py --verbose /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data' returned non-zero exit status 1.
[36malgo-1    |[0m Traceback (most recent call last):
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/smspark/job.py", line 149, in run
[36malgo-1    |[0m     subprocess.run(spark_submit_cmd, check=True, shell=True)
[36malgo-1    |[0m   File "/usr/lib64/python3.7/subprocess.py", line 512, in run
[36malgo-1    |[0m     output=stdout, stderr=stderr)
[36malgo-1    |[0m subprocess.CalledProcessError: Command 'spark-submit --master yarn --deploy-mode client --py-files /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py --verbose /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data' returned non-zero exit status 1.
[36malgo-1    |[0m Command 'spark-submit --master yarn --deploy-mode client --py-files /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py --verbose /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data' returned non-zero exit status 1.
[36malgo-1    |[0m Traceback (most recent call last):
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/smspark/job.py", line 149, in run
[36malgo-1    |[0m     subprocess.run(spark_submit_cmd, check=True, shell=True)
[36malgo-1    |[0m   File "/usr/lib64/python3.7/subprocess.py", line 512, in run
[36malgo-1    |[0m     output=stdout, stderr=stderr)
[36malgo-1    |[0m subprocess.CalledProcessError: Command 'spark-submit --master yarn --deploy-mode client --py-files /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py --verbose /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data' returned non-zero exit status 1.
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:55 smspark-submit INFO     exiting with code 1: Algorithm Error: (caused by CalledProcessError): spark failed with a non-zero exit code: Command 'spark-submit --master yarn --deploy-mode client --py-files /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py --verbose /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data' returned non-zero exit status 1.
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908833443_0001/container_1636908833443_0001_01_000003/stderr] 21/11/142021-11-14 16:55:51,172 INFO launcher.ContainerLaunch: Container container_1636908833443_0001_01_000001 succeeded 
[33malgo-2    |[0m 2021-11-14 16:55:51,172 INFO container.ContainerImpl: Container container_1636908833443_0001_01_000001 transitioned from RUNNING to EXITED_WITH_SUCCESS
[33malgo-2    |[0m 2021-11-14 16:55:51,172 INFO launcher.ContainerCleanup: Cleaning up container container_1636908833443_0001_01_000001
[33malgo-2    |[0m 2021-11-14 16:55:51,172 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000001
[33malgo-2    |[0m 2021-11-14 16:55:51,173 INFO nodemanager.NMAuditLogger: USER=root	OPERATION=Container Finished - Succeeded	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1636908833443_0001	CONTAINERID=container_1636908833443_0001_01_000001
[33malgo-2    |[0m 2021-11-14 16:55:51,173 INFO container.ContainerImpl: Container container_1636908833443_0001_01_000001 transitioned from EXITED_WITH_SUCCESS to DONE
[33malgo-2    |[0m 2021-11-14 16:55:51,173 INFO application.ApplicationImpl: Removing container_1636908833443_0001_01_000001 from application application_1636908833443_0001
[33malgo-2    |[0m 2021-11-14 16:55:51,173 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1636908833443_0001_01_000001
[33malgo-2    |[0m 2021-11-14 16:55:51,173 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1636908833443_0001
[33malgo-2    |[0m 2021-11-14 16:55:51,177 INFO retry.RetryInvocationHandler: java.io.EOFException: End of File Exception between local host is: "algo-2/172.21.0.3"; destination host is: "algo-1.spark-network":8031; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException, while invoking ResourceTrackerPBClientImpl.nodeHeartbeat over null. Retrying after sleeping for 30000ms.
[33malgo-2    |[0m 2021-11-14 16:55:51,185 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000001/launch_container.sh
[33malgo-2    |[0m 2021-11-14 16:55:51,185 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000001/launch_container.sh]
[33malgo-2    |[0m 2021-11-14 16:55:51,185 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000001/container_tokens
[33malgo-2    |[0m 2021-11-14 16:55:51,185 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000001/container_tokens]
[33malgo-2    |[0m 2021-11-14 16:55:51,185 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000001/sysfs
[33malgo-2    |[0m 2021-11-14 16:55:51,185 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908833443_0001/container_1636908833443_0001_01_000001/sysfs]
[36malgo-1 exited with code 1
[0m[33malgo-2    |[0m 11-14 16:55 urllib3.connectionpool WARNING  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f651906c190>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m 11-14 16:55 urllib3.connectionpool WARNING  Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f651906c110>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m 2021-11-14 16:55:53,620 WARN datanode.DataNode: IOException in offerService
[33malgo-2    |[0m java.io.EOFException: End of File Exception between local host is: "algo-2/172.21.0.3"; destination host is: "algo-1.spark-network":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
[33malgo-2    |[0m 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[33malgo-2    |[0m 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[33malgo-2    |[0m 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[33malgo-2    |[0m 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
[33malgo-2    |[0m 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
[33malgo-2    |[0m 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:791)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
[33malgo-2    |[0m 	at com.sun.proxy.$Proxy16.sendHeartbeat(Unknown Source)
[33malgo-2    |[0m 	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:168)
[33malgo-2    |[0m 	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:517)
[33malgo-2    |[0m 	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
[33malgo-2    |[0m 	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
[33malgo-2    |[0m 	at java.lang.Thread.run(Thread.java:748)
[33malgo-2    |[0m Caused by: java.io.EOFException
[33malgo-2    |[0m 	at java.io.DataInputStream.readInt(DataInputStream.java:392)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)
[33malgo-2    |[0m 11-14 16:55 urllib3.connectionpool WARNING  Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f651902b1d0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m 11-14 16:55 urllib3.connectionpool WARNING  Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f651902b5d0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m 11-14 16:56 urllib3.connectionpool WARNING  Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f651902b9d0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m 11-14 16:56 smspark-submit INFO     primary is down, worker now exiting
[33malgo-2 exited with code 0
[0m
FAILED
test/integration/local/test_multinode_container.py::test_scala_spark_multinode JARS_MOUNT=./test/resources/code/scala/hello-scala-spark/lib_managed/jars/org.json4s/json4s-native_2.12:/opt/ml/processing/input/jars CMD='--jars /opt/ml/processing/input/jars --class com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp --verbose /opt/ml/processing/input/code/scala/hello-scala-spark/target/scala-2.12/hello-scala-spark_2.12-1.0.jar --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data' docker-compose up --force-recreate
Recreating algo-2 ... 
Recreating algo-1 ... 
[2A[2K
Recreating algo-2 ... [32mdone[0m
[2B[1A[2K
Recreating algo-1 ... [32mdone[0m
[1BAttaching to algo-2, algo-1
[33malgo-2    |[0m 11-14 16:56 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--jars', '/opt/ml/processing/input/jars', '--class', 'com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp', '--verbose', '/opt/ml/processing/input/code/scala/hello-scala-spark/target/scala-2.12/hello-scala-spark_2.12-1.0.jar', '--input', 'file:///opt/ml/processing/input/data/data.jsonl', '--output', 'file:///opt/ml/processing/output/data']
[33malgo-2    |[0m 11-14 16:56 smspark.cli  INFO     Raw spark options before processing: {'jars': '/opt/ml/processing/input/jars', 'class_': 'com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp', 'verbose': True, 'py_files': None, 'files': None}
[33malgo-2    |[0m 11-14 16:56 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/scala/hello-scala-spark/target/scala-2.12/hello-scala-spark_2.12-1.0.jar', '--input', 'file:///opt/ml/processing/input/data/data.jsonl', '--output', 'file:///opt/ml/processing/output/data']
[33malgo-2    |[0m 11-14 16:56 smspark.cli  INFO     Rendered spark options: {'jars': '/opt/ml/processing/input/jars/json4s-native_2.12-3.6.9.jar', 'class_': 'com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp', 'verbose': True, 'py_files': None, 'files': None}
[33malgo-2    |[0m 11-14 16:56 smspark.cli  INFO     Initializing processing job.
[33malgo-2    |[0m 11-14 16:56 smspark-submit INFO     {'current_host': 'algo-2', 'hosts': ['algo-1', 'algo-2']}
[33malgo-2    |[0m 11-14 16:56 smspark-submit INFO     {'ProcessingJobArn': 'processing_job_arn', 'ProcessingJobName': 'processing_job_name', 'AppSpecification': {'ImageUri': 'image_uri'}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30}}, 'RoleArn': 'iam_role'}
[33malgo-2    |[0m 11-14 16:56 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client --jars /opt/ml/processing/input/jars/json4s-native_2.12-3.6.9.jar --class com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp --verbose /opt/ml/processing/input/code/scala/hello-scala-spark/target/scala-2.12/hello-scala-spark_2.12-1.0.jar --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data
[33malgo-2    |[0m 11-14 16:56 smspark-submit INFO     waiting for hosts
[33malgo-2    |[0m 11-14 16:56 smspark-submit INFO     starting status server
[33malgo-2    |[0m 11-14 16:56 smspark-submit INFO     Status server listening on algo-2:5555
[33malgo-2    |[0m 11-14 16:56 smspark-submit INFO     bootstrapping cluster
[33malgo-2    |[0m 11-14 16:56 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING
[33malgo-2    |[0m 11-14 16:56 smspark-submit INFO     copying aws jars
[33malgo-2    |[0m Serving on http://algo-2:5555
[36malgo-1    |[0m 11-14 16:56 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--jars', '/opt/ml/processing/input/jars', '--class', 'com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp', '--verbose', '/opt/ml/processing/input/code/scala/hello-scala-spark/target/scala-2.12/hello-scala-spark_2.12-1.0.jar', '--input', 'file:///opt/ml/processing/input/data/data.jsonl', '--output', 'file:///opt/ml/processing/output/data']
[36malgo-1    |[0m 11-14 16:56 smspark.cli  INFO     Raw spark options before processing: {'jars': '/opt/ml/processing/input/jars', 'class_': 'com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp', 'verbose': True, 'py_files': None, 'files': None}
[36malgo-1    |[0m 11-14 16:56 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/scala/hello-scala-spark/target/scala-2.12/hello-scala-spark_2.12-1.0.jar', '--input', 'file:///opt/ml/processing/input/data/data.jsonl', '--output', 'file:///opt/ml/processing/output/data']
[36malgo-1    |[0m 11-14 16:56 smspark.cli  INFO     Rendered spark options: {'jars': '/opt/ml/processing/input/jars/json4s-native_2.12-3.6.9.jar', 'class_': 'com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp', 'verbose': True, 'py_files': None, 'files': None}
[36malgo-1    |[0m 11-14 16:56 smspark.cli  INFO     Initializing processing job.
[36malgo-1    |[0m 11-14 16:56 smspark-submit INFO     {'current_host': 'algo-1', 'hosts': ['algo-1', 'algo-2']}
[36malgo-1    |[0m 11-14 16:56 smspark-submit INFO     {'ProcessingJobArn': 'processing_job_arn', 'ProcessingJobName': 'processing_job_name', 'AppSpecification': {'ImageUri': 'image_uri'}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30}}, 'RoleArn': 'iam_role'}
[36malgo-1    |[0m 11-14 16:56 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client --jars /opt/ml/processing/input/jars/json4s-native_2.12-3.6.9.jar --class com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp --verbose /opt/ml/processing/input/code/scala/hello-scala-spark/target/scala-2.12/hello-scala-spark_2.12-1.0.jar --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data
[36malgo-1    |[0m 11-14 16:56 smspark-submit INFO     waiting for hosts
[36malgo-1    |[0m 11-14 16:56 smspark-submit INFO     starting status server
[36malgo-1    |[0m 11-14 16:56 smspark-submit INFO     Status server listening on algo-1:5555
[36malgo-1    |[0m 11-14 16:56 smspark-submit INFO     bootstrapping cluster
[36malgo-1    |[0m 11-14 16:56 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING
[36malgo-1    |[0m 11-14 16:56 smspark-submit INFO     copying aws jars
[36malgo-1    |[0m Serving on http://algo-1:5555
[33malgo-2    |[0m 11-14 16:56 smspark-submit INFO     Found hadoop jar hadoop-aws.jar
[33malgo-2    |[0m 11-14 16:56 smspark-submit INFO     Optional jar jets3t-0.9.0.jar in /usr/lib/hadoop/lib does not exist
[33malgo-2    |[0m 11-14 16:56 smspark-submit INFO     copying cluster config
[33malgo-2    |[0m 11-14 16:56 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml
[33malgo-2    |[0m 11-14 16:56 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml
[33malgo-2    |[0m 11-14 16:56 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[33malgo-2    |[0m 11-14 16:56 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 16:56 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh
[33malgo-2    |[0m 11-14 16:56 root         INFO     Detected instance type: m5.xlarge with total memory: 16384M and total cores: 4
[33malgo-2    |[0m 11-14 16:56 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[33malgo-2    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0"?>
[33malgo-2    |[0m <!-- Site specific YARN configuration properties -->
[33malgo-2    |[0m  <configuration>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.resourcemanager.hostname</name>
[33malgo-2    |[0m          <value>172.21.0.2</value>
[33malgo-2    |[0m          <description>The hostname of the RM.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.hostname</name>
[33malgo-2    |[0m          <value>algo-2</value>
[33malgo-2    |[0m          <description>The hostname of the NM.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.webapp.address</name>
[33malgo-2    |[0m          <value>algo-2:8042</value>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.vmem-pmem-ratio</name>
[33malgo-2    |[0m          <value>5</value>
[33malgo-2    |[0m          <description>Ratio between virtual memory to physical memory.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.resourcemanager.am.max-attempts</name>
[33malgo-2    |[0m          <value>1</value>
[33malgo-2    |[0m          <description>The maximum number of application attempts.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.env-whitelist</name>
[33malgo-2    |[0m          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>
[33malgo-2    |[0m          <description>Environment variable whitelist</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m  
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.minimum-allocation-mb</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.maximum-allocation-mb</name>
[33malgo-2    |[0m     <value>15892</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.minimum-allocation-vcores</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.maximum-allocation-vcores</name>
[33malgo-2    |[0m     <value>4</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.nodemanager.resource.memory-mb</name>
[33malgo-2    |[0m     <value>15892</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.nodemanager.resource.cpu-vcores</name>
[33malgo-2    |[0m     <value>4</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:56 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[33malgo-2    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.driver.host=172.21.0.2
[33malgo-2    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[33malgo-2    |[0m 
[33malgo-2    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[33malgo-2    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[33malgo-2    |[0m spark.rpc.askTimeout=300s
[33malgo-2    |[0m spark.driver.memory 2048m
[33malgo-2    |[0m spark.driver.memoryOverhead 204m
[33malgo-2    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[33malgo-2    |[0m spark.executor.memory 12399m
[33malgo-2    |[0m spark.executor.memoryOverhead 1239m
[33malgo-2    |[0m spark.executor.cores 4
[33malgo-2    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[33malgo-2    |[0m spark.executor.instances 2
[33malgo-2    |[0m spark.default.parallelism 16
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:56 root         INFO     Finished Yarn configuration files setup.
[33malgo-2    |[0m 11-14 16:56 root         INFO     reading user configuration from /opt/ml/processing/input/conf/configuration.json
[33malgo-2    |[0m 11-14 16:56 root         INFO     User configuration list or dict: [{'Classification': 'spark-defaults', 'Properties': {'spark.executor.memory': '2g', 'spark.executor.cores': '1'}}, {'Classification': 'hadoop-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'core-site', 'Properties': {'spark.executor.memory': '2g', 'spark.executor.cores': '1'}}, {'Classification': 'hadoop-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'hive-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-exec-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-site', 'Properties': {'key': 'value'}}, {'Classification': 'spark-defaults', 'Properties': {'key': 'value'}}, {'Classification': 'spark-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'spark-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'spark-hive-site', 'Properties': {'key': 'value'}}, {'Classification': 'spark-metrics', 'Properties': {'key': 'value'}}, {'Classification': 'yarn-site', 'Properties': {'key': 'value'}}, {'Classification': 'yarn-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}] , type <class 'list'>
[33malgo-2    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[33malgo-2    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.driver.host=172.21.0.2
[33malgo-2    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[33malgo-2    |[0m 
[33malgo-2    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[33malgo-2    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[33malgo-2    |[0m spark.rpc.askTimeout=300s
[33malgo-2    |[0m spark.driver.memory 2048m
[33malgo-2    |[0m spark.driver.memoryOverhead 204m
[33malgo-2    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[33malgo-2    |[0m spark.executor.memory 12399m
[33malgo-2    |[0m spark.executor.memoryOverhead 1239m
[33malgo-2    |[0m spark.executor.cores 4
[33malgo-2    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[33malgo-2    |[0m spark.executor.instances 2
[33malgo-2    |[0m spark.default.parallelism 16
[33malgo-2    |[0m spark.executor.memory 2g
[33malgo-2    |[0m spark.executor.cores 1
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/hadoop-env.sh
[33malgo-2    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/hadoop-env.sh is: 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[33malgo-2    |[0m # or more contributor license agreements.  See the NOTICE file
[33malgo-2    |[0m # distributed with this work for additional information
[33malgo-2    |[0m # regarding copyright ownership.  The ASF licenses this file
[33malgo-2    |[0m # to you under the Apache License, Version 2.0 (the
[33malgo-2    |[0m # "License"); you may not use this file except in compliance
[33malgo-2    |[0m # with the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m # Set Hadoop-specific environment variables here.
[33malgo-2    |[0m 
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## THIS FILE ACTS AS THE MASTER FILE FOR ALL HADOOP PROJECTS.
[33malgo-2    |[0m ## SETTINGS HERE WILL BE READ BY ALL HADOOP COMMANDS.  THEREFORE,
[33malgo-2    |[0m ## ONE CAN USE THIS FILE TO SET YARN, HDFS, AND MAPREDUCE
[33malgo-2    |[0m ## CONFIGURATION OPTIONS INSTEAD OF xxx-env.sh.
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## Precedence rules:
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## {yarn-env.sh|hdfs-env.sh} > hadoop-env.sh > hard-coded defaults
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## {YARN_xyz|HDFS_xyz} > HADOOP_xyz > hard-coded defaults
[33malgo-2    |[0m ##
[33malgo-2    |[0m 
[33malgo-2    |[0m # Many of the options here are built from the perspective that users
[33malgo-2    |[0m # may want to provide OVERWRITING values on the command line.
[33malgo-2    |[0m # For example:
[33malgo-2    |[0m #
[33malgo-2    |[0m #  JAVA_HOME=/usr/java/testing hdfs dfs -ls
[33malgo-2    |[0m #
[33malgo-2    |[0m # Therefore, the vast majority (BUT NOT ALL!) of these defaults
[33malgo-2    |[0m # are configured for substitution and not append.  If append
[33malgo-2    |[0m # is preferable, modify this file accordingly.
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Generic settings for HADOOP
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Technically, the only required environment variable is JAVA_HOME.
[33malgo-2    |[0m # All others are optional.  However, the defaults are probably not
[33malgo-2    |[0m # preferred.  Many sites configure these options outside of Hadoop,
[33malgo-2    |[0m # such as in /etc/profile.d
[33malgo-2    |[0m 
[33malgo-2    |[0m # The java implementation to use. By default, this environment
[33malgo-2    |[0m # variable is REQUIRED on ALL platforms except OS X!
[33malgo-2    |[0m # export JAVA_HOME=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Location of Hadoop.  By default, Hadoop will attempt to determine
[33malgo-2    |[0m # this location based upon its execution path.
[33malgo-2    |[0m # export HADOOP_HOME=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Location of Hadoop's configuration information.  i.e., where this
[33malgo-2    |[0m # file is living. If this is not defined, Hadoop will attempt to
[33malgo-2    |[0m # locate it based upon its execution path.
[33malgo-2    |[0m #
[33malgo-2    |[0m # NOTE: It is recommend that this variable not be set here but in
[33malgo-2    |[0m # /etc/profile.d or equivalent.  Some options (such as
[33malgo-2    |[0m # --config) may react strangely otherwise.
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
[33malgo-2    |[0m 
[33malgo-2    |[0m # The maximum amount of heap to use (Java -Xmx).  If no unit
[33malgo-2    |[0m # is provided, it will be converted to MB.  Daemons will
[33malgo-2    |[0m # prefer any Xmx setting in their respective _OPT variable.
[33malgo-2    |[0m # There is no default; the JVM will autoscale based upon machine
[33malgo-2    |[0m # memory size.
[33malgo-2    |[0m # export HADOOP_HEAPSIZE_MAX=
[33malgo-2    |[0m 
[33malgo-2    |[0m # The minimum amount of heap to use (Java -Xms).  If no unit
[33malgo-2    |[0m # is provided, it will be converted to MB.  Daemons will
[33malgo-2    |[0m # prefer any Xms setting in their respective _OPT variable.
[33malgo-2    |[0m # There is no default; the JVM will autoscale based upon machine
[33malgo-2    |[0m # memory size.
[33malgo-2    |[0m # export HADOOP_HEAPSIZE_MIN=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Enable extra debugging of Hadoop's JAAS binding, used to set up
[33malgo-2    |[0m # Kerberos security.
[33malgo-2    |[0m # export HADOOP_JAAS_DEBUG=true
[33malgo-2    |[0m 
[33malgo-2    |[0m # Extra Java runtime options for all Hadoop commands. We don't support
[33malgo-2    |[0m # IPv6 yet/still, so by default the preference is set to IPv4.
[33malgo-2    |[0m # export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true"
[33malgo-2    |[0m # For Kerberos debugging, an extended option set logs more information
[33malgo-2    |[0m # export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug"
[33malgo-2    |[0m 
[33malgo-2    |[0m # Some parts of the shell code may do special things dependent upon
[33malgo-2    |[0m # the operating system.  We have to set this here. See the next
[33malgo-2    |[0m # section as to why....
[33malgo-2    |[0m #export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}
[33malgo-2    |[0m 
[33malgo-2    |[0m # Extra Java runtime options for some Hadoop commands
[33malgo-2    |[0m # and clients (i.e., hdfs dfs -blah).  These get appended to HADOOP_OPTS for
[33malgo-2    |[0m # such commands.  In most cases, # this should be left empty and
[33malgo-2    |[0m # let users supply it on the command line.
[33malgo-2    |[0m # export HADOOP_CLIENT_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # A note about classpaths.
[33malgo-2    |[0m #
[33malgo-2    |[0m # By default, Apache Hadoop overrides Java's CLASSPATH
[33malgo-2    |[0m # environment variable.  It is configured such
[33malgo-2    |[0m # that it starts out blank with new entries added after passing
[33malgo-2    |[0m # a series of checks (file/dir exists, not already listed aka
[33malgo-2    |[0m # de-deduplication).  During de-deduplication, wildcards and/or
[33malgo-2    |[0m # directories are *NOT* expanded to keep it simple. Therefore,
[33malgo-2    |[0m # if the computed classpath has two specific mentions of
[33malgo-2    |[0m # awesome-methods-1.0.jar, only the first one added will be seen.
[33malgo-2    |[0m # If two directories are in the classpath that both contain
[33malgo-2    |[0m # awesome-methods-1.0.jar, then Java will pick up both versions.
[33malgo-2    |[0m 
[33malgo-2    |[0m # An additional, custom CLASSPATH. Site-wide configs should be
[33malgo-2    |[0m # handled via the shellprofile functionality, utilizing the
[33malgo-2    |[0m # hadoop_add_classpath function for greater control and much
[33malgo-2    |[0m # harder for apps/end-users to accidentally override.
[33malgo-2    |[0m # Similarly, end users should utilize ${HOME}/.hadooprc .
[33malgo-2    |[0m # This variable should ideally only be used as a short-cut,
[33malgo-2    |[0m # interactive way for temporary additions on the command line.
[33malgo-2    |[0m # export HADOOP_CLASSPATH="/some/cool/path/on/your/machine"
[33malgo-2    |[0m 
[33malgo-2    |[0m # Should HADOOP_CLASSPATH be first in the official CLASSPATH?
[33malgo-2    |[0m # export HADOOP_USER_CLASSPATH_FIRST="yes"
[33malgo-2    |[0m 
[33malgo-2    |[0m # If HADOOP_USE_CLIENT_CLASSLOADER is set, the classpath along
[33malgo-2    |[0m # with the main jar are handled by a separate isolated
[33malgo-2    |[0m # client classloader when 'hadoop jar', 'yarn jar', or 'mapred job'
[33malgo-2    |[0m # is utilized. If it is set, HADOOP_CLASSPATH and
[33malgo-2    |[0m # HADOOP_USER_CLASSPATH_FIRST are ignored.
[33malgo-2    |[0m # export HADOOP_USE_CLIENT_CLASSLOADER=true
[33malgo-2    |[0m 
[33malgo-2    |[0m # HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES overrides the default definition of
[33malgo-2    |[0m # system classes for the client classloader when HADOOP_USE_CLIENT_CLASSLOADER
[33malgo-2    |[0m # is enabled. Names ending in '.' (period) are treated as package names, and
[33malgo-2    |[0m # names starting with a '-' are treated as negative matches. For example,
[33malgo-2    |[0m # export HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES="-org.apache.hadoop.UserClass,java.,javax.,org.apache.hadoop."
[33malgo-2    |[0m 
[33malgo-2    |[0m # Enable optional, bundled Hadoop features
[33malgo-2    |[0m # This is a comma delimited list.  It may NOT be overridden via .hadooprc
[33malgo-2    |[0m # Entries may be added/removed as needed.
[33malgo-2    |[0m # export HADOOP_OPTIONAL_TOOLS="hadoop-azure,hadoop-azure-datalake,hadoop-aws,hadoop-openstack,hadoop-aliyun,hadoop-kafka"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Options for remote shell connectivity
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # There are some optional components of hadoop that allow for
[33malgo-2    |[0m # command and control of remote hosts.  For example,
[33malgo-2    |[0m # start-dfs.sh will attempt to bring up all NNs, DNS, etc.
[33malgo-2    |[0m 
[33malgo-2    |[0m # Options to pass to SSH when one of the "log into a host and
[33malgo-2    |[0m # start/stop daemons" scripts is executed
[33malgo-2    |[0m # export HADOOP_SSH_OPTS="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10s"
[33malgo-2    |[0m 
[33malgo-2    |[0m # The built-in ssh handler will limit itself to 10 simultaneous connections.
[33malgo-2    |[0m # For pdsh users, this sets the fanout size ( -f )
[33malgo-2    |[0m # Change this to increase/decrease as necessary.
[33malgo-2    |[0m # export HADOOP_SSH_PARALLEL=10
[33malgo-2    |[0m 
[33malgo-2    |[0m # Filename which contains all of the hosts for any remote execution
[33malgo-2    |[0m # helper scripts # such as workers.sh, start-dfs.sh, etc.
[33malgo-2    |[0m # export HADOOP_WORKERS="${HADOOP_CONF_DIR}/workers"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Options for all daemons
[33malgo-2    |[0m ###
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Many options may also be specified as Java properties.  It is
[33malgo-2    |[0m # very common, and in many cases, desirable, to hard-set these
[33malgo-2    |[0m # in daemon _OPTS variables.  Where applicable, the appropriate
[33malgo-2    |[0m # Java property is also identified.  Note that many are re-used
[33malgo-2    |[0m # or set differently in certain contexts (e.g., secure vs
[33malgo-2    |[0m # non-secure)
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m # Where (primarily) daemon log files are stored.
[33malgo-2    |[0m # ${HADOOP_HOME}/logs by default.
[33malgo-2    |[0m # Java property: hadoop.log.dir
[33malgo-2    |[0m # export HADOOP_LOG_DIR=${HADOOP_HOME}/logs
[33malgo-2    |[0m 
[33malgo-2    |[0m # A string representing this instance of hadoop. $USER by default.
[33malgo-2    |[0m # This is used in writing log and pid files, so keep that in mind!
[33malgo-2    |[0m # Java property: hadoop.id.str
[33malgo-2    |[0m # export HADOOP_IDENT_STRING=$USER
[33malgo-2    |[0m 
[33malgo-2    |[0m # How many seconds to pause after stopping a daemon
[33malgo-2    |[0m # export HADOOP_STOP_TIMEOUT=5
[33malgo-2    |[0m 
[33malgo-2    |[0m # Where pid files are stored.  /tmp by default.
[33malgo-2    |[0m # export HADOOP_PID_DIR=/tmp
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default log4j setting for interactive commands
[33malgo-2    |[0m # Java property: hadoop.root.logger
[33malgo-2    |[0m # export HADOOP_ROOT_LOGGER=INFO,console
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default log4j setting for daemons spawned explicitly by
[33malgo-2    |[0m # --daemon option of hadoop, hdfs, mapred and yarn command.
[33malgo-2    |[0m # Java property: hadoop.root.logger
[33malgo-2    |[0m # export HADOOP_DAEMON_ROOT_LOGGER=INFO,RFA
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default log level and output location for security-related messages.
[33malgo-2    |[0m # You will almost certainly want to change this on a per-daemon basis via
[33malgo-2    |[0m # the Java property (i.e., -Dhadoop.security.logger=foo). (Note that the
[33malgo-2    |[0m # defaults for the NN and 2NN override this by default.)
[33malgo-2    |[0m # Java property: hadoop.security.logger
[33malgo-2    |[0m # export HADOOP_SECURITY_LOGGER=INFO,NullAppender
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default process priority level
[33malgo-2    |[0m # Note that sub-processes will also run at this level!
[33malgo-2    |[0m # export HADOOP_NICENESS=0
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default name for the service level authorization file
[33malgo-2    |[0m # Java property: hadoop.policy.file
[33malgo-2    |[0m # export HADOOP_POLICYFILE="hadoop-policy.xml"
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # NOTE: this is not used by default!  <-----
[33malgo-2    |[0m # You can define variables right here and then re-use them later on.
[33malgo-2    |[0m # For example, it is common to use the same garbage collection settings
[33malgo-2    |[0m # for all the daemons.  So one could define:
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HADOOP_GC_SETTINGS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps"
[33malgo-2    |[0m #
[33malgo-2    |[0m # .. and then use it as per the b option under the namenode.
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Secure/privileged execution
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Out of the box, Hadoop uses jsvc from Apache Commons to launch daemons
[33malgo-2    |[0m # on privileged ports.  This functionality can be replaced by providing
[33malgo-2    |[0m # custom functions.  See hadoop-functions.sh for more information.
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m # The jsvc implementation to use. Jsvc is required to run secure datanodes
[33malgo-2    |[0m # that bind to privileged ports to provide authentication of data transfer
[33malgo-2    |[0m # protocol.  Jsvc is not required if SASL is configured for authentication of
[33malgo-2    |[0m # data transfer protocol using non-privileged ports.
[33malgo-2    |[0m # export JSVC_HOME=/usr/bin
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # This directory contains pids for secure and privileged processes.
[33malgo-2    |[0m #export HADOOP_SECURE_PID_DIR=${HADOOP_PID_DIR}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # This directory contains the logs for secure and privileged processes.
[33malgo-2    |[0m # Java property: hadoop.log.dir
[33malgo-2    |[0m # export HADOOP_SECURE_LOG=${HADOOP_LOG_DIR}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # When running a secure daemon, the default value of HADOOP_IDENT_STRING
[33malgo-2    |[0m # ends up being a bit bogus.  Therefore, by default, the code will
[33malgo-2    |[0m # replace HADOOP_IDENT_STRING with HADOOP_xx_SECURE_USER.  If one wants
[33malgo-2    |[0m # to keep HADOOP_IDENT_STRING untouched, then uncomment this line.
[33malgo-2    |[0m # export HADOOP_SECURE_IDENT_PRESERVE="true"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # NameNode specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default log level and output location for file system related change
[33malgo-2    |[0m # messages. For non-namenode daemons, the Java property must be set in
[33malgo-2    |[0m # the appropriate _OPTS if one wants something other than INFO,NullAppender
[33malgo-2    |[0m # Java property: hdfs.audit.logger
[33malgo-2    |[0m # export HDFS_AUDIT_LOGGER=INFO,NullAppender
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the NameNode.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # a) Set JMX options
[33malgo-2    |[0m # export HDFS_NAMENODE_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
[33malgo-2    |[0m #
[33malgo-2    |[0m # b) Set garbage collection logs
[33malgo-2    |[0m # export HDFS_NAMENODE_OPTS="${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[33malgo-2    |[0m #
[33malgo-2    |[0m # c) ... or set them directly
[33malgo-2    |[0m # export HDFS_NAMENODE_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[33malgo-2    |[0m 
[33malgo-2    |[0m # this is the default:
[33malgo-2    |[0m # export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # SecondaryNameNode specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the SecondaryNameNode.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # This is the default:
[33malgo-2    |[0m # export HDFS_SECONDARYNAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # DataNode specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the DataNode.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # This is the default:
[33malgo-2    |[0m # export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS"
[33malgo-2    |[0m 
[33malgo-2    |[0m # On secure datanodes, user to run the datanode as after dropping privileges.
[33malgo-2    |[0m # This **MUST** be uncommented to enable secure HDFS if using privileged ports
[33malgo-2    |[0m # to provide authentication of data transfer protocol.  This **MUST NOT** be
[33malgo-2    |[0m # defined if SASL is configured for authentication of data transfer protocol
[33malgo-2    |[0m # using non-privileged ports.
[33malgo-2    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[33malgo-2    |[0m # export HDFS_DATANODE_SECURE_USER=hdfs
[33malgo-2    |[0m 
[33malgo-2    |[0m # Supplemental options for secure datanodes
[33malgo-2    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[33malgo-2    |[0m # server jvm.
[33malgo-2    |[0m # export HDFS_DATANODE_SECURE_EXTRA_OPTS="-jvm server"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # NFS3 Gateway specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the NFS3 Gateway.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_NFS3_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the Hadoop portmapper.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_PORTMAP_OPTS="-Xmx512m"
[33malgo-2    |[0m 
[33malgo-2    |[0m # Supplemental options for priviliged gateways
[33malgo-2    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[33malgo-2    |[0m # server jvm.
[33malgo-2    |[0m # export HDFS_NFS3_SECURE_EXTRA_OPTS="-jvm server"
[33malgo-2    |[0m 
[33malgo-2    |[0m # On privileged gateways, user to run the gateway as after dropping privileges
[33malgo-2    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[33malgo-2    |[0m # export HDFS_NFS3_SECURE_USER=nfsserver
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # ZKFailoverController specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the ZKFailoverController.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_ZKFC_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # QuorumJournalNode specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the QuorumJournalNode.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_JOURNALNODE_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # HDFS Balancer specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the HDFS Balancer.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_BALANCER_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # HDFS Mover specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the HDFS Mover.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_MOVER_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Router-based HDFS Federation specific parameters
[33malgo-2    |[0m # Specify the JVM options to be used when starting the RBF Routers.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_DFSROUTER_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # HDFS StorageContainerManager specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the HDFS Storage Container Manager.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_STORAGECONTAINERMANAGER_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Advanced Users Only!
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # When building Hadoop, one can add the class paths to the commands
[33malgo-2    |[0m # via this special env var:
[33malgo-2    |[0m # export HADOOP_ENABLE_BUILD_PATHS="true"
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # To prevent accidents, shell commands be (superficially) locked
[33malgo-2    |[0m # to only allow certain users to execute certain subcommands.
[33malgo-2    |[0m # It uses the format of (command)_(subcommand)_USER.
[33malgo-2    |[0m #
[33malgo-2    |[0m # For example, to limit who can execute the namenode command,
[33malgo-2    |[0m # export HDFS_NAMENODE_USER=hdfs
[33malgo-2    |[0m export SPARK_MASTER_HOST=172.21.0.2
[33malgo-2    |[0m export AWS_CONTAINER_CREDENTIALS_RELATIVE_URI=
[33malgo-2    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[33malgo-2    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/core-site.xml
[33malgo-2    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/core-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0" encoding="UTF-8"?>
[33malgo-2    |[0m  <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[33malgo-2    |[0m  <!-- Put site-specific property overrides in this file. -->
[33malgo-2    |[0m 
[33malgo-2    |[0m  <configuration>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>fs.defaultFS</name>
[33malgo-2    |[0m          <value>hdfs://172.21.0.2/</value>
[33malgo-2    |[0m          <description>NameNode URI</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>fs.s3a.aws.credentials.provider</name>
[33malgo-2    |[0m          <value>com.amazonaws.auth.DefaultAWSCredentialsProviderChain</value>
[33malgo-2    |[0m          <description>AWS S3 credential provider</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>fs.s3.impl</name>
[33malgo-2    |[0m          <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
[33malgo-2    |[0m          <description>s3a filesystem implementation</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>fs.AbstractFileSystem.s3a.imp</name>
[33malgo-2    |[0m          <value>org.apache.hadoop.fs.s3a.S3A</value>
[33malgo-2    |[0m          <description>s3a filesystem implementation</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m  
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>spark.executor.memory</name>
[33malgo-2    |[0m     <value>2g</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>spark.executor.cores</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/log4j.properties
[33malgo-2    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/log4j.properties is: 
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[33malgo-2    |[0m # or more contributor license agreements.  See the NOTICE file
[33malgo-2    |[0m # distributed with this work for additional information
[33malgo-2    |[0m # regarding copyright ownership.  The ASF licenses this file
[33malgo-2    |[0m # to you under the Apache License, Version 2.0 (the
[33malgo-2    |[0m # "License"); you may not use this file except in compliance
[33malgo-2    |[0m # with the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m # Define some default values that can be overridden by system properties
[33malgo-2    |[0m hadoop.root.logger=INFO,console
[33malgo-2    |[0m hadoop.log.dir=.
[33malgo-2    |[0m hadoop.log.file=hadoop.log
[33malgo-2    |[0m 
[33malgo-2    |[0m # Define the root logger to the system property "hadoop.root.logger".
[33malgo-2    |[0m log4j.rootLogger=${hadoop.root.logger}, EventCounter
[33malgo-2    |[0m 
[33malgo-2    |[0m # Logging Threshold
[33malgo-2    |[0m log4j.threshold=ALL
[33malgo-2    |[0m 
[33malgo-2    |[0m # Null Appender
[33malgo-2    |[0m log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Rolling File Appender - cap space usage at 5gb.
[33malgo-2    |[0m #
[33malgo-2    |[0m hadoop.log.maxfilesize=256MB
[33malgo-2    |[0m hadoop.log.maxbackupindex=20
[33malgo-2    |[0m log4j.appender.RFA=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.RFA.MaxFileSize=${hadoop.log.maxfilesize}
[33malgo-2    |[0m log4j.appender.RFA.MaxBackupIndex=${hadoop.log.maxbackupindex}
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.RFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m 
[33malgo-2    |[0m # Pattern format: Date LogLevel LoggerName LogMessage
[33malgo-2    |[0m log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[33malgo-2    |[0m # Debugging Pattern format
[33malgo-2    |[0m #log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Daily Rolling File Appender
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.DRFA.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m 
[33malgo-2    |[0m # Rollver at midnight
[33malgo-2    |[0m #log4j.appender.DRFA.DatePattern=.yyyy-MM-dd
[33malgo-2    |[0m # Rollver at every hour
[33malgo-2    |[0m log4j.appender.DRFA.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m 
[33malgo-2    |[0m # Pattern format: Date LogLevel LoggerName LogMessage
[33malgo-2    |[0m log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m # Debugging Pattern format
[33malgo-2    |[0m #log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # console
[33malgo-2    |[0m # Add "console" to rootlogger above if you want to use this
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.console=org.apache.log4j.ConsoleAppender
[33malgo-2    |[0m log4j.appender.console.target=System.err
[33malgo-2    |[0m log4j.appender.console.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # TaskLog Appender
[33malgo-2    |[0m #
[33malgo-2    |[0m log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.TLA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # HDFS block state change log from block manager
[33malgo-2    |[0m #
[33malgo-2    |[0m # Uncomment the following to log normal block state change
[33malgo-2    |[0m # messages from BlockManager in NameNode.
[33malgo-2    |[0m #log4j.logger.BlockStateChange=DEBUG
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m #Security appender
[33malgo-2    |[0m #
[33malgo-2    |[0m hadoop.security.logger=INFO,NullAppender
[33malgo-2    |[0m hadoop.security.log.maxfilesize=256MB
[33malgo-2    |[0m hadoop.security.log.maxbackupindex=20
[33malgo-2    |[0m log4j.category.SecurityLogger=${hadoop.security.logger}
[33malgo-2    |[0m hadoop.security.log.file=SecurityAuth-${user.name}.audit
[33malgo-2    |[0m log4j.appender.RFAS=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.RFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
[33malgo-2    |[0m log4j.appender.RFAS.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[33malgo-2    |[0m log4j.appender.RFAS.MaxFileSize=${hadoop.security.log.maxfilesize}
[33malgo-2    |[0m log4j.appender.RFAS.MaxBackupIndex=${hadoop.security.log.maxbackupindex}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Daily Rolling Security appender
[33malgo-2    |[0m #
[33malgo-2    |[0m log4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.DRFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
[33malgo-2    |[0m log4j.appender.DRFAS.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.DRFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[33malgo-2    |[0m log4j.appender.DRFAS.DatePattern=.yyyy-MM-dd
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # hadoop configuration logging
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m # Uncomment the following line to turn off configuration deprecation warnings.
[33malgo-2    |[0m # log4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # hdfs audit logging
[33malgo-2    |[0m #
[33malgo-2    |[0m hdfs.audit.logger=INFO,NullAppender
[33malgo-2    |[0m hdfs.audit.log.maxfilesize=256MB
[33malgo-2    |[0m hdfs.audit.log.maxbackupindex=20
[33malgo-2    |[0m log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}
[33malgo-2    |[0m log4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false
[33malgo-2    |[0m log4j.appender.RFAAUDIT=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.RFAAUDIT.File=${hadoop.log.dir}/hdfs-audit.log
[33malgo-2    |[0m log4j.appender.RFAAUDIT.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.RFAAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[33malgo-2    |[0m log4j.appender.RFAAUDIT.MaxFileSize=${hdfs.audit.log.maxfilesize}
[33malgo-2    |[0m log4j.appender.RFAAUDIT.MaxBackupIndex=${hdfs.audit.log.maxbackupindex}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # NameNode metrics logging.
[33malgo-2    |[0m # The default is to retain two namenode-metrics.log files up to 64MB each.
[33malgo-2    |[0m #
[33malgo-2    |[0m namenode.metrics.logger=INFO,NullAppender
[33malgo-2    |[0m log4j.logger.NameNodeMetricsLog=${namenode.metrics.logger}
[33malgo-2    |[0m log4j.additivity.NameNodeMetricsLog=false
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.File=${hadoop.log.dir}/namenode-metrics.log
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.MaxBackupIndex=1
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.MaxFileSize=64MB
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # DataNode metrics logging.
[33malgo-2    |[0m # The default is to retain two datanode-metrics.log files up to 64MB each.
[33malgo-2    |[0m #
[33malgo-2    |[0m datanode.metrics.logger=INFO,NullAppender
[33malgo-2    |[0m log4j.logger.DataNodeMetricsLog=${datanode.metrics.logger}
[33malgo-2    |[0m log4j.additivity.DataNodeMetricsLog=false
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.File=${hadoop.log.dir}/datanode-metrics.log
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.MaxBackupIndex=1
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.MaxFileSize=64MB
[33malgo-2    |[0m 
[33malgo-2    |[0m # Custom Logging levels
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=DEBUG
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m # AWS SDK & S3A FileSystem
[33malgo-2    |[0m #log4j.logger.com.amazonaws=ERROR
[33malgo-2    |[0m log4j.logger.com.amazonaws.http.AmazonHttpClient=ERROR
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.fs.s3a.S3AFileSystem=WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Event Counter Appender
[33malgo-2    |[0m # Sends counts of logging messages at different severity levels to Hadoop Metrics.
[33malgo-2    |[0m #
[33malgo-2    |[0m log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Job Summary Appender
[33malgo-2    |[0m #
[33malgo-2    |[0m # Use following logger to send summary to separate file defined by
[33malgo-2    |[0m # hadoop.mapreduce.jobsummary.log.file :
[33malgo-2    |[0m # hadoop.mapreduce.jobsummary.logger=INFO,JSA
[33malgo-2    |[0m # 
[33malgo-2    |[0m hadoop.mapreduce.jobsummary.logger=${hadoop.root.logger}
[33malgo-2    |[0m hadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log
[33malgo-2    |[0m hadoop.mapreduce.jobsummary.log.maxfilesize=256MB
[33malgo-2    |[0m hadoop.mapreduce.jobsummary.log.maxbackupindex=20
[33malgo-2    |[0m log4j.appender.JSA=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.JSA.File=${hadoop.log.dir}/${hadoop.mapreduce.jobsummary.log.file}
[33malgo-2    |[0m log4j.appender.JSA.MaxFileSize=${hadoop.mapreduce.jobsummary.log.maxfilesize}
[33malgo-2    |[0m log4j.appender.JSA.MaxBackupIndex=${hadoop.mapreduce.jobsummary.log.maxbackupindex}
[33malgo-2    |[0m log4j.appender.JSA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.JSA.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
[33malgo-2    |[0m log4j.appender.JSA.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.logger.org.apache.hadoop.mapred.JobInProgress$JobSummary=${hadoop.mapreduce.jobsummary.logger}
[33malgo-2    |[0m log4j.additivity.org.apache.hadoop.mapred.JobInProgress$JobSummary=false
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # shuffle connection log from shuffleHandler
[33malgo-2    |[0m # Uncomment the following line to enable logging of shuffle connections
[33malgo-2    |[0m # log4j.logger.org.apache.hadoop.mapred.ShuffleHandler.audit=DEBUG
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Yarn ResourceManager Application Summary Log
[33malgo-2    |[0m #
[33malgo-2    |[0m # Set the ResourceManager summary log filename
[33malgo-2    |[0m yarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log
[33malgo-2    |[0m # Set the ResourceManager summary log level and appender
[33malgo-2    |[0m yarn.server.resourcemanager.appsummary.logger=${hadoop.root.logger}
[33malgo-2    |[0m #yarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY
[33malgo-2    |[0m 
[33malgo-2    |[0m # To enable AppSummaryLogging for the RM,
[33malgo-2    |[0m # set yarn.server.resourcemanager.appsummary.logger to
[33malgo-2    |[0m # <LEVEL>,RMSUMMARY in hadoop-env.sh
[33malgo-2    |[0m 
[33malgo-2    |[0m # Appender for ResourceManager Application Summary Log
[33malgo-2    |[0m # Requires the following properties to be set
[33malgo-2    |[0m #    - hadoop.log.dir (Hadoop Log directory)
[33malgo-2    |[0m #    - yarn.server.resourcemanager.appsummary.log.file (resource manager app summary log filename)
[33malgo-2    |[0m #    - yarn.server.resourcemanager.appsummary.logger (resource manager app summary log level and appender)
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=${yarn.server.resourcemanager.appsummary.logger}
[33malgo-2    |[0m log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=false
[33malgo-2    |[0m log4j.appender.RMSUMMARY=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.RMSUMMARY.File=${hadoop.log.dir}/${yarn.server.resourcemanager.appsummary.log.file}
[33malgo-2    |[0m log4j.appender.RMSUMMARY.MaxFileSize=256MB
[33malgo-2    |[0m log4j.appender.RMSUMMARY.MaxBackupIndex=20
[33malgo-2    |[0m log4j.appender.RMSUMMARY.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.RMSUMMARY.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m # HS audit log configs
[33malgo-2    |[0m #mapreduce.hs.audit.logger=INFO,HSAUDIT
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=${mapreduce.hs.audit.logger}
[33malgo-2    |[0m #log4j.additivity.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=false
[33malgo-2    |[0m #log4j.appender.HSAUDIT=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m #log4j.appender.HSAUDIT.File=${hadoop.log.dir}/hs-audit.log
[33malgo-2    |[0m #log4j.appender.HSAUDIT.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m #log4j.appender.HSAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[33malgo-2    |[0m #log4j.appender.HSAUDIT.DatePattern=.yyyy-MM-dd
[33malgo-2    |[0m 
[33malgo-2    |[0m # Http Server Request Logs
[33malgo-2    |[0m #log4j.logger.http.requests.namenode=INFO,namenoderequestlog
[33malgo-2    |[0m #log4j.appender.namenoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.namenoderequestlog.Filename=${hadoop.log.dir}/jetty-namenode-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.namenoderequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.http.requests.datanode=INFO,datanoderequestlog
[33malgo-2    |[0m #log4j.appender.datanoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.datanoderequestlog.Filename=${hadoop.log.dir}/jetty-datanode-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.datanoderequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.http.requests.resourcemanager=INFO,resourcemanagerrequestlog
[33malgo-2    |[0m #log4j.appender.resourcemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.resourcemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-resourcemanager-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.resourcemanagerrequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.http.requests.jobhistory=INFO,jobhistoryrequestlog
[33malgo-2    |[0m #log4j.appender.jobhistoryrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.jobhistoryrequestlog.Filename=${hadoop.log.dir}/jetty-jobhistory-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.jobhistoryrequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.http.requests.nodemanager=INFO,nodemanagerrequestlog
[33malgo-2    |[0m #log4j.appender.nodemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.nodemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-nodemanager-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.nodemanagerrequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m # WebHdfs request log on datanodes
[33malgo-2    |[0m # Specify -Ddatanode.webhdfs.logger=INFO,HTTPDRFA on datanode startup to
[33malgo-2    |[0m # direct the log to a separate file.
[33malgo-2    |[0m #datanode.webhdfs.logger=INFO,console
[33malgo-2    |[0m #log4j.logger.datanode.webhdfs=${datanode.webhdfs.logger}
[33malgo-2    |[0m #log4j.appender.HTTPDRFA=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m #log4j.appender.HTTPDRFA.File=${hadoop.log.dir}/hadoop-datanode-webhdfs.log
[33malgo-2    |[0m #log4j.appender.HTTPDRFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m #log4j.appender.HTTPDRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[33malgo-2    |[0m #log4j.appender.HTTPDRFA.DatePattern=.yyyy-MM-dd
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m # Appender for viewing information for errors and warnings
[33malgo-2    |[0m yarn.ewma.cleanupInterval=300
[33malgo-2    |[0m yarn.ewma.messageAgeLimitSeconds=86400
[33malgo-2    |[0m yarn.ewma.maxUniqueMessages=250
[33malgo-2    |[0m log4j.appender.EWMA=org.apache.hadoop.yarn.util.Log4jWarningErrorMetricsAppender
[33malgo-2    |[0m log4j.appender.EWMA.cleanupInterval=${yarn.ewma.cleanupInterval}
[33malgo-2    |[0m log4j.appender.EWMA.messageAgeLimitSeconds=${yarn.ewma.messageAgeLimitSeconds}
[33malgo-2    |[0m log4j.appender.EWMA.maxUniqueMessages=${yarn.ewma.maxUniqueMessages}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Fair scheduler state dump
[33malgo-2    |[0m #
[33malgo-2    |[0m # Use following logger to dump the state to a separate file
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=DEBUG,FSSTATEDUMP
[33malgo-2    |[0m #log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=false
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.File=${hadoop.log.dir}/fairscheduler-statedump.log
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.MaxFileSize=${hadoop.log.maxfilesize}
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.MaxBackupIndex=${hadoop.log.maxbackupindex}
[33malgo-2    |[0m 
[33malgo-2    |[0m # Log levels of third-party libraries
[33malgo-2    |[0m log4j.logger.org.apache.commons.beanutils=WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m #AWS SDK Logging
[33malgo-2    |[0m log4j.logger.com.amazonaws=WARN
[33malgo-2    |[0m log4j.logger.org.apache.zookeeper=ERROR
[33malgo-2    |[0m log4j.logger.org.apache.hadoop.hbase=WARN
[33malgo-2    |[0m log4j.logger.amazon.emr.metrics=WARN
[33malgo-2    |[0m log4j.logger.emr=INFO
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.HADOOP=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.HADOOP.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m log4j.appender.HADOOP.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.appender.HADOOP.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.HADOOP.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.JHS=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.JHS.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m log4j.appender.JHS.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.appender.JHS.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.JHS.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.MAPRED=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.MAPRED.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m log4j.appender.MAPRED.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.appender.MAPRED.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.MAPRED.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.YARN=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.YARN.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m log4j.appender.YARN.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.appender.YARN.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.YARN.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/hive/conf/hive-env.sh
[33malgo-2    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/hive/conf/hive-env.sh is: 
[33malgo-2    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[33malgo-2    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/hive/conf/hive-log4j2.properties
[33malgo-2    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/hive/conf/hive-log4j2.properties is: 
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[33malgo-2    |[0m # or more contributor license agreements.  See the NOTICE file
[33malgo-2    |[0m # distributed with this work for additional information
[33malgo-2    |[0m # regarding copyright ownership.  The ASF licenses this file
[33malgo-2    |[0m # to you under the Apache License, Version 2.0 (the
[33malgo-2    |[0m # "License"); you may not use this file except in compliance
[33malgo-2    |[0m # with the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m status = INFO
[33malgo-2    |[0m name = HiveLog4j2
[33malgo-2    |[0m packages = org.apache.hadoop.hive.ql.log
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of properties
[33malgo-2    |[0m property.hive.log.level = INFO
[33malgo-2    |[0m property.hive.root.logger = DRFA
[33malgo-2    |[0m property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
[33malgo-2    |[0m property.hive.log.file = hive.log
[33malgo-2    |[0m property.hive.perflogger.log.level = INFO
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of all appenders
[33malgo-2    |[0m appenders = console, DRFA
[33malgo-2    |[0m 
[33malgo-2    |[0m # console appender
[33malgo-2    |[0m appender.console.type = Console
[33malgo-2    |[0m appender.console.name = console
[33malgo-2    |[0m appender.console.target = SYSTEM_ERR
[33malgo-2    |[0m appender.console.layout.type = PatternLayout
[33malgo-2    |[0m appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m # daily rolling file appender
[33malgo-2    |[0m appender.DRFA.type = RollingRandomAccessFile
[33malgo-2    |[0m appender.DRFA.name = DRFA
[33malgo-2    |[0m appender.DRFA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
[33malgo-2    |[0m # Use %pid in the filePattern to append <process-id>@<host-name> to the filename if you want separate log files for different CLI session
[33malgo-2    |[0m appender.DRFA.filePattern = ${sys:hive.log.dir}/${sys:hive.log.file}.%d{yyyy-MM-dd}
[33malgo-2    |[0m appender.DRFA.layout.type = PatternLayout
[33malgo-2    |[0m appender.DRFA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[33malgo-2    |[0m appender.DRFA.policies.type = Policies
[33malgo-2    |[0m appender.DRFA.policies.time.type = TimeBasedTriggeringPolicy
[33malgo-2    |[0m appender.DRFA.policies.time.interval = 1
[33malgo-2    |[0m appender.DRFA.policies.time.modulate = true
[33malgo-2    |[0m appender.DRFA.strategy.type = DefaultRolloverStrategy
[33malgo-2    |[0m appender.DRFA.strategy.max = 30
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of all loggers
[33malgo-2    |[0m loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, PerfLogger, AmazonAws, ApacheHttp
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
[33malgo-2    |[0m logger.NIOServerCnxn.level = WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
[33malgo-2    |[0m logger.ClientCnxnSocketNIO.level = WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.DataNucleus.name = DataNucleus
[33malgo-2    |[0m logger.DataNucleus.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.Datastore.name = Datastore
[33malgo-2    |[0m logger.Datastore.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.JPOX.name = JPOX
[33malgo-2    |[0m logger.JPOX.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.AmazonAws.name=com.amazonaws
[33malgo-2    |[0m logger.AmazonAws.level = INFO
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.ApacheHttp.name=org.apache.http
[33malgo-2    |[0m logger.ApacheHttp.level = INFO
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.PerfLogger.name = org.apache.hadoop.hive.ql.log.PerfLogger
[33malgo-2    |[0m logger.PerfLogger.level = ${sys:hive.perflogger.log.level}
[33malgo-2    |[0m 
[33malgo-2    |[0m # root logger
[33malgo-2    |[0m rootLogger.level = ${sys:hive.log.level}
[33malgo-2    |[0m rootLogger.appenderRefs = root
[33malgo-2    |[0m rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/hive/conf/hive-exec-log4j2.properties
[33malgo-2    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/hive/conf/hive-exec-log4j2.properties is: 
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[33malgo-2    |[0m # or more contributor license agreements.  See the NOTICE file
[33malgo-2    |[0m # distributed with this work for additional information
[33malgo-2    |[0m # regarding copyright ownership.  The ASF licenses this file
[33malgo-2    |[0m # to you under the Apache License, Version 2.0 (the
[33malgo-2    |[0m # "License"); you may not use this file except in compliance
[33malgo-2    |[0m # with the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m status = INFO
[33malgo-2    |[0m name = HiveExecLog4j2
[33malgo-2    |[0m packages = org.apache.hadoop.hive.ql.log
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of properties
[33malgo-2    |[0m property.hive.log.level = INFO
[33malgo-2    |[0m property.hive.root.logger = FA
[33malgo-2    |[0m property.hive.query.id = hadoop
[33malgo-2    |[0m property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
[33malgo-2    |[0m property.hive.log.file = ${sys:hive.query.id}.log
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of all appenders
[33malgo-2    |[0m appenders = console, FA
[33malgo-2    |[0m 
[33malgo-2    |[0m # console appender
[33malgo-2    |[0m appender.console.type = Console
[33malgo-2    |[0m appender.console.name = console
[33malgo-2    |[0m appender.console.target = SYSTEM_ERR
[33malgo-2    |[0m appender.console.layout.type = PatternLayout
[33malgo-2    |[0m appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m # simple file appender
[33malgo-2    |[0m appender.FA.type = RandomAccessFile
[33malgo-2    |[0m appender.FA.name = FA
[33malgo-2    |[0m appender.FA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
[33malgo-2    |[0m appender.FA.layout.type = PatternLayout
[33malgo-2    |[0m appender.FA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of all loggers
[33malgo-2    |[0m loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
[33malgo-2    |[0m logger.NIOServerCnxn.level = WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
[33malgo-2    |[0m logger.ClientCnxnSocketNIO.level = WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.DataNucleus.name = DataNucleus
[33malgo-2    |[0m logger.DataNucleus.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.Datastore.name = Datastore
[33malgo-2    |[0m logger.Datastore.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.JPOX.name = JPOX
[33malgo-2    |[0m logger.JPOX.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m # root logger
[33malgo-2    |[0m rootLogger.level = ${sys:hive.log.level}
[33malgo-2    |[0m rootLogger.appenderRefs = root
[33malgo-2    |[0m rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/hive/conf/hive-site.xml
[33malgo-2    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/hive/conf/hive-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0"?>
[33malgo-2    |[0m <!--
[33malgo-2    |[0m   Licensed to the Apache Software Foundation (ASF) under one or more
[33malgo-2    |[0m   contributor license agreements.  See the NOTICE file distributed with
[33malgo-2    |[0m   this work for additional information regarding copyright ownership.
[33malgo-2    |[0m   The ASF licenses this file to You under the Apache License, Version 2.0
[33malgo-2    |[0m   (the "License"); you may not use this file except in compliance with
[33malgo-2    |[0m   the License.  You may obtain a copy of the License at
[33malgo-2    |[0m 
[33malgo-2    |[0m       http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m 
[33malgo-2    |[0m   Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m   distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m   See the License for the specific language governing permissions and
[33malgo-2    |[0m   limitations under the License.
[33malgo-2    |[0m -->
[33malgo-2    |[0m <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[33malgo-2    |[0m 
[33malgo-2    |[0m <configuration>
[33malgo-2    |[0m 
[33malgo-2    |[0m <!-- Hive Configuration can either be stored in this file or in the hadoop configuration files  -->
[33malgo-2    |[0m <!-- that are implied by Hadoop setup variables.                                                -->
[33malgo-2    |[0m <!-- Aside from Hadoop setup variables - this file is provided as a convenience so that Hive    -->
[33malgo-2    |[0m <!-- users do not have to edit hadoop configuration files (that may be managed as a centralized -->
[33malgo-2    |[0m <!-- resource).                                                                                 -->
[33malgo-2    |[0m 
[33malgo-2    |[0m <!-- Hive Execution Parameters -->
[33malgo-2    |[0m 
[33malgo-2    |[0m <property>
[33malgo-2    |[0m   <name>javax.jdo.option.ConnectionURL</name>
[33malgo-2    |[0m   <value>jdbc:derby:;databaseName=/var/lib/hive/metastore/metastore_db;create=true</value>
[33malgo-2    |[0m   <description>JDBC connect string for a JDBC metastore</description>
[33malgo-2    |[0m </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m <property>
[33malgo-2    |[0m   <name>javax.jdo.option.ConnectionDriverName</name>
[33malgo-2    |[0m   <value>org.apache.derby.jdbc.EmbeddedDriver</value>
[33malgo-2    |[0m   <description>Driver class name for a JDBC metastore</description>
[33malgo-2    |[0m </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>key</name>
[33malgo-2    |[0m     <value>value</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[33malgo-2    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.driver.host=172.21.0.2
[33malgo-2    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[33malgo-2    |[0m 
[33malgo-2    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[33malgo-2    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[33malgo-2    |[0m spark.rpc.askTimeout=300s
[33malgo-2    |[0m spark.driver.memory 2048m
[33malgo-2    |[0m spark.driver.memoryOverhead 204m
[33malgo-2    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[33malgo-2    |[0m spark.executor.memory 12399m
[33malgo-2    |[0m spark.executor.memoryOverhead 1239m
[33malgo-2    |[0m spark.executor.cores 4
[33malgo-2    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[33malgo-2    |[0m spark.executor.instances 2
[33malgo-2    |[0m spark.default.parallelism 16
[33malgo-2    |[0m spark.executor.memory 2g
[33malgo-2    |[0m spark.executor.cores 1
[33malgo-2    |[0m key value
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/spark/conf/spark-env.sh
[33malgo-2    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/spark/conf/spark-env.sh is: 
[33malgo-2    |[0m #EMPTY FILE AVOID OVERRIDDING ENV VARS
[33malgo-2    |[0m # Specifically, without copying the empty file, SPARK_HISTORY_OPTS will be overriden, 
[33malgo-2    |[0m # spark.history.ui.port defaults to 18082, and spark.eventLog.dir defaults to local fs
[33malgo-2    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[33malgo-2    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/spark/conf/log4j.properties
[33malgo-2    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/spark/conf/log4j.properties is: 
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/spark/conf/hive-site.xml
[33malgo-2    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/spark/conf/hive-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0"?>
[33malgo-2    |[0m <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[33malgo-2    |[0m <configuration>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>key</name>
[33malgo-2    |[0m     <value>value</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/spark/conf/metrics.properties
[33malgo-2    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/spark/conf/metrics.properties is: 
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[33malgo-2    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0"?>
[33malgo-2    |[0m <!-- Site specific YARN configuration properties -->
[33malgo-2    |[0m  <configuration>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.resourcemanager.hostname</name>
[33malgo-2    |[0m          <value>172.21.0.2</value>
[33malgo-2    |[0m          <description>The hostname of the RM.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.hostname</name>
[33malgo-2    |[0m          <value>algo-2</value>
[33malgo-2    |[0m          <description>The hostname of the NM.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.webapp.address</name>
[33malgo-2    |[0m          <value>algo-2:8042</value>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.vmem-pmem-ratio</name>
[33malgo-2    |[0m          <value>5</value>
[33malgo-2    |[0m          <description>Ratio between virtual memory to physical memory.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.resourcemanager.am.max-attempts</name>
[33malgo-2    |[0m          <value>1</value>
[33malgo-2    |[0m          <description>The maximum number of application attempts.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.env-whitelist</name>
[33malgo-2    |[0m          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>
[33malgo-2    |[0m          <description>Environment variable whitelist</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m  
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.minimum-allocation-mb</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.maximum-allocation-mb</name>
[33malgo-2    |[0m     <value>15892</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.minimum-allocation-vcores</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.maximum-allocation-vcores</name>
[33malgo-2    |[0m     <value>4</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.nodemanager.resource.memory-mb</name>
[33malgo-2    |[0m     <value>15892</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.nodemanager.resource.cpu-vcores</name>
[33malgo-2    |[0m     <value>4</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>key</name>
[33malgo-2    |[0m     <value>value</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/yarn-env.sh
[33malgo-2    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-env.sh is: 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one or more
[33malgo-2    |[0m # contributor license agreements.  See the NOTICE file distributed with
[33malgo-2    |[0m # this work for additional information regarding copyright ownership.
[33malgo-2    |[0m # The ASF licenses this file to You under the Apache License, Version 2.0
[33malgo-2    |[0m # (the "License"); you may not use this file except in compliance with
[33malgo-2    |[0m # the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## THIS FILE ACTS AS AN OVERRIDE FOR hadoop-env.sh FOR ALL
[33malgo-2    |[0m ## WORK DONE BY THE yarn AND RELATED COMMANDS.
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## Precedence rules:
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## yarn-env.sh > hadoop-env.sh > hard-coded defaults
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## YARN_xyz > HADOOP_xyz > hard-coded defaults
[33malgo-2    |[0m ##
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Resource Manager specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the max heapsize for the ResourceManager.  If no units are
[33malgo-2    |[0m # given, it will be assumed to be in MB.
[33malgo-2    |[0m # This value will be overridden by an Xmx setting specified in either
[33malgo-2    |[0m # HADOOP_OPTS and/or YARN_RESOURCEMANAGER_OPTS.
[33malgo-2    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX
[33malgo-2    |[0m #export YARN_RESOURCEMANAGER_HEAPSIZE=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the ResourceManager.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # Examples for a Sun/Oracle JDK:
[33malgo-2    |[0m # a) override the appsummary log file:
[33malgo-2    |[0m # export YARN_RESOURCEMANAGER_OPTS="-Dyarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log -Dyarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY"
[33malgo-2    |[0m #
[33malgo-2    |[0m # b) Set JMX options
[33malgo-2    |[0m # export YARN_RESOURCEMANAGER_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
[33malgo-2    |[0m #
[33malgo-2    |[0m # c) Set garbage collection logs from hadoop-env.sh
[33malgo-2    |[0m # export YARN_RESOURCE_MANAGER_OPTS="${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[33malgo-2    |[0m #
[33malgo-2    |[0m # d) ... or set them directly
[33malgo-2    |[0m # export YARN_RESOURCEMANAGER_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[33malgo-2    |[0m #
[33malgo-2    |[0m #
[33malgo-2    |[0m # export YARN_RESOURCEMANAGER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Node Manager specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the max heapsize for the NodeManager.  If no units are
[33malgo-2    |[0m # given, it will be assumed to be in MB.
[33malgo-2    |[0m # This value will be overridden by an Xmx setting specified in either
[33malgo-2    |[0m # HADOOP_OPTS and/or YARN_NODEMANAGER_OPTS.
[33malgo-2    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[33malgo-2    |[0m #export YARN_NODEMANAGER_HEAPSIZE=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the NodeManager.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_NODEMANAGER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # TimeLineServer specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the max heapsize for the timelineserver.  If no units are
[33malgo-2    |[0m # given, it will be assumed to be in MB.
[33malgo-2    |[0m # This value will be overridden by an Xmx setting specified in either
[33malgo-2    |[0m # HADOOP_OPTS and/or YARN_TIMELINESERVER_OPTS.
[33malgo-2    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[33malgo-2    |[0m #export YARN_TIMELINE_HEAPSIZE=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the TimeLineServer.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_TIMELINESERVER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # TimeLineReader specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the TimeLineReader.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_TIMELINEREADER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Web App Proxy Server specifc parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the max heapsize for the web app proxy server.  If no units are
[33malgo-2    |[0m # given, it will be assumed to be in MB.
[33malgo-2    |[0m # This value will be overridden by an Xmx setting specified in either
[33malgo-2    |[0m # HADOOP_OPTS and/or YARN_PROXYSERVER_OPTS.
[33malgo-2    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[33malgo-2    |[0m #export YARN_PROXYSERVER_HEAPSIZE=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the proxy server.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_PROXYSERVER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Shared Cache Manager specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the
[33malgo-2    |[0m # shared cache manager server.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_SHAREDCACHEMANAGER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Router specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the Router.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_ROUTER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Registry DNS specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # For privileged registry DNS, user to run as after dropping privileges
[33malgo-2    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[33malgo-2    |[0m # export YARN_REGISTRYDNS_SECURE_USER=yarn
[33malgo-2    |[0m 
[33malgo-2    |[0m # Supplemental options for privileged registry DNS
[33malgo-2    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[33malgo-2    |[0m # server jvm.
[33malgo-2    |[0m # export YARN_REGISTRYDNS_SECURE_EXTRA_OPTS="-jvm server"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # YARN Services parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Directory containing service examples
[33malgo-2    |[0m # export YARN_SERVICE_EXAMPLES_DIR = $HADOOP_YARN_HOME/share/hadoop/yarn/yarn-service-examples
[33malgo-2    |[0m # export YARN_CONTAINER_RUNTIME_DOCKER_RUN_OVERRIDE_DISABLE=true
[33malgo-2    |[0m export YARN_LOG_DIR=/var/log/yarn/export HADOOP_DATANODE_HEAPSIZE=2048
[33malgo-2    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:56 smspark-submit INFO     waiting for cluster to be up
[36malgo-1    |[0m 11-14 16:56 smspark-submit INFO     Found hadoop jar hadoop-aws.jar
[36malgo-1    |[0m 11-14 16:56 smspark-submit INFO     Optional jar jets3t-0.9.0.jar in /usr/lib/hadoop/lib does not exist
[36malgo-1    |[0m 11-14 16:56 smspark-submit INFO     copying cluster config
[36malgo-1    |[0m 11-14 16:56 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml
[36malgo-1    |[0m 11-14 16:56 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml
[36malgo-1    |[0m 11-14 16:56 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[36malgo-1    |[0m 11-14 16:56 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m 11-14 16:56 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh
[36malgo-1    |[0m 11-14 16:56 root         INFO     Detected instance type: m5.xlarge with total memory: 16384M and total cores: 4
[36malgo-1    |[0m 11-14 16:56 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[36malgo-1    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0"?>
[36malgo-1    |[0m <!-- Site specific YARN configuration properties -->
[36malgo-1    |[0m  <configuration>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.resourcemanager.hostname</name>
[36malgo-1    |[0m          <value>172.21.0.2</value>
[36malgo-1    |[0m          <description>The hostname of the RM.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.hostname</name>
[36malgo-1    |[0m          <value>algo-1</value>
[36malgo-1    |[0m          <description>The hostname of the NM.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.webapp.address</name>
[36malgo-1    |[0m          <value>algo-1:8042</value>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.vmem-pmem-ratio</name>
[36malgo-1    |[0m          <value>5</value>
[36malgo-1    |[0m          <description>Ratio between virtual memory to physical memory.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.resourcemanager.am.max-attempts</name>
[36malgo-1    |[0m          <value>1</value>
[36malgo-1    |[0m          <description>The maximum number of application attempts.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.env-whitelist</name>
[36malgo-1    |[0m          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>
[36malgo-1    |[0m          <description>Environment variable whitelist</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m  
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.minimum-allocation-mb</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.maximum-allocation-mb</name>
[36malgo-1    |[0m     <value>15892</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.minimum-allocation-vcores</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.maximum-allocation-vcores</name>
[36malgo-1    |[0m     <value>4</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.nodemanager.resource.memory-mb</name>
[36malgo-1    |[0m     <value>15892</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.nodemanager.resource.cpu-vcores</name>
[36malgo-1    |[0m     <value>4</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:56 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[36malgo-1    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.driver.host=172.21.0.2
[36malgo-1    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[36malgo-1    |[0m 
[36malgo-1    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[36malgo-1    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[36malgo-1    |[0m spark.rpc.askTimeout=300s
[36malgo-1    |[0m spark.driver.memory 2048m
[36malgo-1    |[0m spark.driver.memoryOverhead 204m
[36malgo-1    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[36malgo-1    |[0m spark.executor.memory 12399m
[36malgo-1    |[0m spark.executor.memoryOverhead 1239m
[36malgo-1    |[0m spark.executor.cores 4
[36malgo-1    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[36malgo-1    |[0m spark.executor.instances 2
[36malgo-1    |[0m spark.default.parallelism 16
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:56 root         INFO     Finished Yarn configuration files setup.
[36malgo-1    |[0m 11-14 16:56 root         INFO     reading user configuration from /opt/ml/processing/input/conf/configuration.json
[36malgo-1    |[0m 11-14 16:56 root         INFO     User configuration list or dict: [{'Classification': 'spark-defaults', 'Properties': {'spark.executor.memory': '2g', 'spark.executor.cores': '1'}}, {'Classification': 'hadoop-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'core-site', 'Properties': {'spark.executor.memory': '2g', 'spark.executor.cores': '1'}}, {'Classification': 'hadoop-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'hive-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-exec-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-site', 'Properties': {'key': 'value'}}, {'Classification': 'spark-defaults', 'Properties': {'key': 'value'}}, {'Classification': 'spark-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'spark-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'spark-hive-site', 'Properties': {'key': 'value'}}, {'Classification': 'spark-metrics', 'Properties': {'key': 'value'}}, {'Classification': 'yarn-site', 'Properties': {'key': 'value'}}, {'Classification': 'yarn-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}] , type <class 'list'>
[36malgo-1    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[36malgo-1    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.driver.host=172.21.0.2
[36malgo-1    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[36malgo-1    |[0m 
[36malgo-1    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[36malgo-1    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[36malgo-1    |[0m spark.rpc.askTimeout=300s
[36malgo-1    |[0m spark.driver.memory 2048m
[36malgo-1    |[0m spark.driver.memoryOverhead 204m
[36malgo-1    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[36malgo-1    |[0m spark.executor.memory 12399m
[36malgo-1    |[0m spark.executor.memoryOverhead 1239m
[36malgo-1    |[0m spark.executor.cores 4
[36malgo-1    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[36malgo-1    |[0m spark.executor.instances 2
[36malgo-1    |[0m spark.default.parallelism 16
[36malgo-1    |[0m spark.executor.memory 2g
[36malgo-1    |[0m spark.executor.cores 1
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/hadoop-env.sh
[36malgo-1    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/hadoop-env.sh is: 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[36malgo-1    |[0m # or more contributor license agreements.  See the NOTICE file
[36malgo-1    |[0m # distributed with this work for additional information
[36malgo-1    |[0m # regarding copyright ownership.  The ASF licenses this file
[36malgo-1    |[0m # to you under the Apache License, Version 2.0 (the
[36malgo-1    |[0m # "License"); you may not use this file except in compliance
[36malgo-1    |[0m # with the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m # Set Hadoop-specific environment variables here.
[36malgo-1    |[0m 
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## THIS FILE ACTS AS THE MASTER FILE FOR ALL HADOOP PROJECTS.
[36malgo-1    |[0m ## SETTINGS HERE WILL BE READ BY ALL HADOOP COMMANDS.  THEREFORE,
[36malgo-1    |[0m ## ONE CAN USE THIS FILE TO SET YARN, HDFS, AND MAPREDUCE
[36malgo-1    |[0m ## CONFIGURATION OPTIONS INSTEAD OF xxx-env.sh.
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## Precedence rules:
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## {yarn-env.sh|hdfs-env.sh} > hadoop-env.sh > hard-coded defaults
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## {YARN_xyz|HDFS_xyz} > HADOOP_xyz > hard-coded defaults
[36malgo-1    |[0m ##
[36malgo-1    |[0m 
[36malgo-1    |[0m # Many of the options here are built from the perspective that users
[36malgo-1    |[0m # may want to provide OVERWRITING values on the command line.
[36malgo-1    |[0m # For example:
[36malgo-1    |[0m #
[36malgo-1    |[0m #  JAVA_HOME=/usr/java/testing hdfs dfs -ls
[36malgo-1    |[0m #
[36malgo-1    |[0m # Therefore, the vast majority (BUT NOT ALL!) of these defaults
[36malgo-1    |[0m # are configured for substitution and not append.  If append
[36malgo-1    |[0m # is preferable, modify this file accordingly.
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Generic settings for HADOOP
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Technically, the only required environment variable is JAVA_HOME.
[36malgo-1    |[0m # All others are optional.  However, the defaults are probably not
[36malgo-1    |[0m # preferred.  Many sites configure these options outside of Hadoop,
[36malgo-1    |[0m # such as in /etc/profile.d
[36malgo-1    |[0m 
[36malgo-1    |[0m # The java implementation to use. By default, this environment
[36malgo-1    |[0m # variable is REQUIRED on ALL platforms except OS X!
[36malgo-1    |[0m # export JAVA_HOME=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Location of Hadoop.  By default, Hadoop will attempt to determine
[36malgo-1    |[0m # this location based upon its execution path.
[36malgo-1    |[0m # export HADOOP_HOME=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Location of Hadoop's configuration information.  i.e., where this
[36malgo-1    |[0m # file is living. If this is not defined, Hadoop will attempt to
[36malgo-1    |[0m # locate it based upon its execution path.
[36malgo-1    |[0m #
[36malgo-1    |[0m # NOTE: It is recommend that this variable not be set here but in
[36malgo-1    |[0m # /etc/profile.d or equivalent.  Some options (such as
[36malgo-1    |[0m # --config) may react strangely otherwise.
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
[36malgo-1    |[0m 
[36malgo-1    |[0m # The maximum amount of heap to use (Java -Xmx).  If no unit
[36malgo-1    |[0m # is provided, it will be converted to MB.  Daemons will
[36malgo-1    |[0m # prefer any Xmx setting in their respective _OPT variable.
[36malgo-1    |[0m # There is no default; the JVM will autoscale based upon machine
[36malgo-1    |[0m # memory size.
[36malgo-1    |[0m # export HADOOP_HEAPSIZE_MAX=
[36malgo-1    |[0m 
[36malgo-1    |[0m # The minimum amount of heap to use (Java -Xms).  If no unit
[36malgo-1    |[0m # is provided, it will be converted to MB.  Daemons will
[36malgo-1    |[0m # prefer any Xms setting in their respective _OPT variable.
[36malgo-1    |[0m # There is no default; the JVM will autoscale based upon machine
[36malgo-1    |[0m # memory size.
[36malgo-1    |[0m # export HADOOP_HEAPSIZE_MIN=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Enable extra debugging of Hadoop's JAAS binding, used to set up
[36malgo-1    |[0m # Kerberos security.
[36malgo-1    |[0m # export HADOOP_JAAS_DEBUG=true
[36malgo-1    |[0m 
[36malgo-1    |[0m # Extra Java runtime options for all Hadoop commands. We don't support
[36malgo-1    |[0m # IPv6 yet/still, so by default the preference is set to IPv4.
[36malgo-1    |[0m # export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true"
[36malgo-1    |[0m # For Kerberos debugging, an extended option set logs more information
[36malgo-1    |[0m # export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug"
[36malgo-1    |[0m 
[36malgo-1    |[0m # Some parts of the shell code may do special things dependent upon
[36malgo-1    |[0m # the operating system.  We have to set this here. See the next
[36malgo-1    |[0m # section as to why....
[36malgo-1    |[0m #export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}
[36malgo-1    |[0m 
[36malgo-1    |[0m # Extra Java runtime options for some Hadoop commands
[36malgo-1    |[0m # and clients (i.e., hdfs dfs -blah).  These get appended to HADOOP_OPTS for
[36malgo-1    |[0m # such commands.  In most cases, # this should be left empty and
[36malgo-1    |[0m # let users supply it on the command line.
[36malgo-1    |[0m # export HADOOP_CLIENT_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # A note about classpaths.
[36malgo-1    |[0m #
[36malgo-1    |[0m # By default, Apache Hadoop overrides Java's CLASSPATH
[36malgo-1    |[0m # environment variable.  It is configured such
[36malgo-1    |[0m # that it starts out blank with new entries added after passing
[36malgo-1    |[0m # a series of checks (file/dir exists, not already listed aka
[36malgo-1    |[0m # de-deduplication).  During de-deduplication, wildcards and/or
[36malgo-1    |[0m # directories are *NOT* expanded to keep it simple. Therefore,
[36malgo-1    |[0m # if the computed classpath has two specific mentions of
[36malgo-1    |[0m # awesome-methods-1.0.jar, only the first one added will be seen.
[36malgo-1    |[0m # If two directories are in the classpath that both contain
[36malgo-1    |[0m # awesome-methods-1.0.jar, then Java will pick up both versions.
[36malgo-1    |[0m 
[36malgo-1    |[0m # An additional, custom CLASSPATH. Site-wide configs should be
[36malgo-1    |[0m # handled via the shellprofile functionality, utilizing the
[36malgo-1    |[0m # hadoop_add_classpath function for greater control and much
[36malgo-1    |[0m # harder for apps/end-users to accidentally override.
[36malgo-1    |[0m # Similarly, end users should utilize ${HOME}/.hadooprc .
[36malgo-1    |[0m # This variable should ideally only be used as a short-cut,
[36malgo-1    |[0m # interactive way for temporary additions on the command line.
[36malgo-1    |[0m # export HADOOP_CLASSPATH="/some/cool/path/on/your/machine"
[36malgo-1    |[0m 
[36malgo-1    |[0m # Should HADOOP_CLASSPATH be first in the official CLASSPATH?
[36malgo-1    |[0m # export HADOOP_USER_CLASSPATH_FIRST="yes"
[36malgo-1    |[0m 
[36malgo-1    |[0m # If HADOOP_USE_CLIENT_CLASSLOADER is set, the classpath along
[36malgo-1    |[0m # with the main jar are handled by a separate isolated
[36malgo-1    |[0m # client classloader when 'hadoop jar', 'yarn jar', or 'mapred job'
[36malgo-1    |[0m # is utilized. If it is set, HADOOP_CLASSPATH and
[36malgo-1    |[0m # HADOOP_USER_CLASSPATH_FIRST are ignored.
[36malgo-1    |[0m # export HADOOP_USE_CLIENT_CLASSLOADER=true
[36malgo-1    |[0m 
[36malgo-1    |[0m # HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES overrides the default definition of
[36malgo-1    |[0m # system classes for the client classloader when HADOOP_USE_CLIENT_CLASSLOADER
[36malgo-1    |[0m # is enabled. Names ending in '.' (period) are treated as package names, and
[36malgo-1    |[0m # names starting with a '-' are treated as negative matches. For example,
[36malgo-1    |[0m # export HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES="-org.apache.hadoop.UserClass,java.,javax.,org.apache.hadoop."
[36malgo-1    |[0m 
[36malgo-1    |[0m # Enable optional, bundled Hadoop features
[36malgo-1    |[0m # This is a comma delimited list.  It may NOT be overridden via .hadooprc
[36malgo-1    |[0m # Entries may be added/removed as needed.
[36malgo-1    |[0m # export HADOOP_OPTIONAL_TOOLS="hadoop-azure,hadoop-azure-datalake,hadoop-aws,hadoop-openstack,hadoop-aliyun,hadoop-kafka"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Options for remote shell connectivity
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # There are some optional components of hadoop that allow for
[36malgo-1    |[0m # command and control of remote hosts.  For example,
[36malgo-1    |[0m # start-dfs.sh will attempt to bring up all NNs, DNS, etc.
[36malgo-1    |[0m 
[36malgo-1    |[0m # Options to pass to SSH when one of the "log into a host and
[36malgo-1    |[0m # start/stop daemons" scripts is executed
[36malgo-1    |[0m # export HADOOP_SSH_OPTS="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10s"
[36malgo-1    |[0m 
[36malgo-1    |[0m # The built-in ssh handler will limit itself to 10 simultaneous connections.
[36malgo-1    |[0m # For pdsh users, this sets the fanout size ( -f )
[36malgo-1    |[0m # Change this to increase/decrease as necessary.
[36malgo-1    |[0m # export HADOOP_SSH_PARALLEL=10
[36malgo-1    |[0m 
[36malgo-1    |[0m # Filename which contains all of the hosts for any remote execution
[36malgo-1    |[0m # helper scripts # such as workers.sh, start-dfs.sh, etc.
[36malgo-1    |[0m # export HADOOP_WORKERS="${HADOOP_CONF_DIR}/workers"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Options for all daemons
[36malgo-1    |[0m ###
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Many options may also be specified as Java properties.  It is
[36malgo-1    |[0m # very common, and in many cases, desirable, to hard-set these
[36malgo-1    |[0m # in daemon _OPTS variables.  Where applicable, the appropriate
[36malgo-1    |[0m # Java property is also identified.  Note that many are re-used
[36malgo-1    |[0m # or set differently in certain contexts (e.g., secure vs
[36malgo-1    |[0m # non-secure)
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m # Where (primarily) daemon log files are stored.
[36malgo-1    |[0m # ${HADOOP_HOME}/logs by default.
[36malgo-1    |[0m # Java property: hadoop.log.dir
[36malgo-1    |[0m # export HADOOP_LOG_DIR=${HADOOP_HOME}/logs
[36malgo-1    |[0m 
[36malgo-1    |[0m # A string representing this instance of hadoop. $USER by default.
[36malgo-1    |[0m # This is used in writing log and pid files, so keep that in mind!
[36malgo-1    |[0m # Java property: hadoop.id.str
[36malgo-1    |[0m # export HADOOP_IDENT_STRING=$USER
[36malgo-1    |[0m 
[36malgo-1    |[0m # How many seconds to pause after stopping a daemon
[36malgo-1    |[0m # export HADOOP_STOP_TIMEOUT=5
[36malgo-1    |[0m 
[36malgo-1    |[0m # Where pid files are stored.  /tmp by default.
[36malgo-1    |[0m # export HADOOP_PID_DIR=/tmp
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default log4j setting for interactive commands
[36malgo-1    |[0m # Java property: hadoop.root.logger
[36malgo-1    |[0m # export HADOOP_ROOT_LOGGER=INFO,console
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default log4j setting for daemons spawned explicitly by
[36malgo-1    |[0m # --daemon option of hadoop, hdfs, mapred and yarn command.
[36malgo-1    |[0m # Java property: hadoop.root.logger
[36malgo-1    |[0m # export HADOOP_DAEMON_ROOT_LOGGER=INFO,RFA
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default log level and output location for security-related messages.
[36malgo-1    |[0m # You will almost certainly want to change this on a per-daemon basis via
[36malgo-1    |[0m # the Java property (i.e., -Dhadoop.security.logger=foo). (Note that the
[36malgo-1    |[0m # defaults for the NN and 2NN override this by default.)
[36malgo-1    |[0m # Java property: hadoop.security.logger
[36malgo-1    |[0m # export HADOOP_SECURITY_LOGGER=INFO,NullAppender
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default process priority level
[36malgo-1    |[0m # Note that sub-processes will also run at this level!
[36malgo-1    |[0m # export HADOOP_NICENESS=0
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default name for the service level authorization file
[36malgo-1    |[0m # Java property: hadoop.policy.file
[36malgo-1    |[0m # export HADOOP_POLICYFILE="hadoop-policy.xml"
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # NOTE: this is not used by default!  <-----
[36malgo-1    |[0m # You can define variables right here and then re-use them later on.
[36malgo-1    |[0m # For example, it is common to use the same garbage collection settings
[36malgo-1    |[0m # for all the daemons.  So one could define:
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HADOOP_GC_SETTINGS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps"
[36malgo-1    |[0m #
[36malgo-1    |[0m # .. and then use it as per the b option under the namenode.
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Secure/privileged execution
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Out of the box, Hadoop uses jsvc from Apache Commons to launch daemons
[36malgo-1    |[0m # on privileged ports.  This functionality can be replaced by providing
[36malgo-1    |[0m # custom functions.  See hadoop-functions.sh for more information.
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m # The jsvc implementation to use. Jsvc is required to run secure datanodes
[36malgo-1    |[0m # that bind to privileged ports to provide authentication of data transfer
[36malgo-1    |[0m # protocol.  Jsvc is not required if SASL is configured for authentication of
[36malgo-1    |[0m # data transfer protocol using non-privileged ports.
[36malgo-1    |[0m # export JSVC_HOME=/usr/bin
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # This directory contains pids for secure and privileged processes.
[36malgo-1    |[0m #export HADOOP_SECURE_PID_DIR=${HADOOP_PID_DIR}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # This directory contains the logs for secure and privileged processes.
[36malgo-1    |[0m # Java property: hadoop.log.dir
[36malgo-1    |[0m # export HADOOP_SECURE_LOG=${HADOOP_LOG_DIR}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # When running a secure daemon, the default value of HADOOP_IDENT_STRING
[36malgo-1    |[0m # ends up being a bit bogus.  Therefore, by default, the code will
[36malgo-1    |[0m # replace HADOOP_IDENT_STRING with HADOOP_xx_SECURE_USER.  If one wants
[36malgo-1    |[0m # to keep HADOOP_IDENT_STRING untouched, then uncomment this line.
[36malgo-1    |[0m # export HADOOP_SECURE_IDENT_PRESERVE="true"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # NameNode specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default log level and output location for file system related change
[36malgo-1    |[0m # messages. For non-namenode daemons, the Java property must be set in
[36malgo-1    |[0m # the appropriate _OPTS if one wants something other than INFO,NullAppender
[36malgo-1    |[0m # Java property: hdfs.audit.logger
[36malgo-1    |[0m # export HDFS_AUDIT_LOGGER=INFO,NullAppender
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the NameNode.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # a) Set JMX options
[36malgo-1    |[0m # export HDFS_NAMENODE_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
[36malgo-1    |[0m #
[36malgo-1    |[0m # b) Set garbage collection logs
[36malgo-1    |[0m # export HDFS_NAMENODE_OPTS="${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[36malgo-1    |[0m #
[36malgo-1    |[0m # c) ... or set them directly
[36malgo-1    |[0m # export HDFS_NAMENODE_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[36malgo-1    |[0m 
[36malgo-1    |[0m # this is the default:
[36malgo-1    |[0m # export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # SecondaryNameNode specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the SecondaryNameNode.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # This is the default:
[36malgo-1    |[0m # export HDFS_SECONDARYNAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # DataNode specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the DataNode.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # This is the default:
[36malgo-1    |[0m # export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS"
[36malgo-1    |[0m 
[36malgo-1    |[0m # On secure datanodes, user to run the datanode as after dropping privileges.
[36malgo-1    |[0m # This **MUST** be uncommented to enable secure HDFS if using privileged ports
[36malgo-1    |[0m # to provide authentication of data transfer protocol.  This **MUST NOT** be
[36malgo-1    |[0m # defined if SASL is configured for authentication of data transfer protocol
[36malgo-1    |[0m # using non-privileged ports.
[36malgo-1    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[36malgo-1    |[0m # export HDFS_DATANODE_SECURE_USER=hdfs
[36malgo-1    |[0m 
[36malgo-1    |[0m # Supplemental options for secure datanodes
[36malgo-1    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[36malgo-1    |[0m # server jvm.
[36malgo-1    |[0m # export HDFS_DATANODE_SECURE_EXTRA_OPTS="-jvm server"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # NFS3 Gateway specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the NFS3 Gateway.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_NFS3_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the Hadoop portmapper.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_PORTMAP_OPTS="-Xmx512m"
[36malgo-1    |[0m 
[36malgo-1    |[0m # Supplemental options for priviliged gateways
[36malgo-1    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[36malgo-1    |[0m # server jvm.
[36malgo-1    |[0m # export HDFS_NFS3_SECURE_EXTRA_OPTS="-jvm server"
[36malgo-1    |[0m 
[36malgo-1    |[0m # On privileged gateways, user to run the gateway as after dropping privileges
[36malgo-1    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[36malgo-1    |[0m # export HDFS_NFS3_SECURE_USER=nfsserver
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # ZKFailoverController specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the ZKFailoverController.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_ZKFC_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # QuorumJournalNode specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the QuorumJournalNode.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_JOURNALNODE_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # HDFS Balancer specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the HDFS Balancer.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_BALANCER_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # HDFS Mover specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the HDFS Mover.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_MOVER_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Router-based HDFS Federation specific parameters
[36malgo-1    |[0m # Specify the JVM options to be used when starting the RBF Routers.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_DFSROUTER_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # HDFS StorageContainerManager specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the HDFS Storage Container Manager.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_STORAGECONTAINERMANAGER_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Advanced Users Only!
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # When building Hadoop, one can add the class paths to the commands
[36malgo-1    |[0m # via this special env var:
[36malgo-1    |[0m # export HADOOP_ENABLE_BUILD_PATHS="true"
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # To prevent accidents, shell commands be (superficially) locked
[36malgo-1    |[0m # to only allow certain users to execute certain subcommands.
[36malgo-1    |[0m # It uses the format of (command)_(subcommand)_USER.
[36malgo-1    |[0m #
[36malgo-1    |[0m # For example, to limit who can execute the namenode command,
[36malgo-1    |[0m # export HDFS_NAMENODE_USER=hdfs
[36malgo-1    |[0m export SPARK_MASTER_HOST=172.21.0.2
[36malgo-1    |[0m export AWS_CONTAINER_CREDENTIALS_RELATIVE_URI=
[36malgo-1    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[36malgo-1    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/core-site.xml
[33malgo-2    |[0m WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.
[36malgo-1    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/core-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0" encoding="UTF-8"?>
[36malgo-1    |[0m  <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[36malgo-1    |[0m  <!-- Put site-specific property overrides in this file. -->
[36malgo-1    |[0m 
[36malgo-1    |[0m  <configuration>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>fs.defaultFS</name>
[36malgo-1    |[0m          <value>hdfs://172.21.0.2/</value>
[36malgo-1    |[0m          <description>NameNode URI</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>fs.s3a.aws.credentials.provider</name>
[36malgo-1    |[0m          <value>com.amazonaws.auth.DefaultAWSCredentialsProviderChain</value>
[36malgo-1    |[0m          <description>AWS S3 credential provider</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>fs.s3.impl</name>
[36malgo-1    |[0m          <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
[36malgo-1    |[0m          <description>s3a filesystem implementation</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>fs.AbstractFileSystem.s3a.imp</name>
[36malgo-1    |[0m          <value>org.apache.hadoop.fs.s3a.S3A</value>
[36malgo-1    |[0m          <description>s3a filesystem implementation</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m  
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>spark.executor.memory</name>
[36malgo-1    |[0m     <value>2g</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>spark.executor.cores</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/log4j.properties
[36malgo-1    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/log4j.properties is: 
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[36malgo-1    |[0m # or more contributor license agreements.  See the NOTICE file
[36malgo-1    |[0m # distributed with this work for additional information
[36malgo-1    |[0m # regarding copyright ownership.  The ASF licenses this file
[36malgo-1    |[0m # to you under the Apache License, Version 2.0 (the
[36malgo-1    |[0m # "License"); you may not use this file except in compliance
[36malgo-1    |[0m # with the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m # Define some default values that can be overridden by system properties
[36malgo-1    |[0m hadoop.root.logger=INFO,console
[36malgo-1    |[0m hadoop.log.dir=.
[36malgo-1    |[0m hadoop.log.file=hadoop.log
[36malgo-1    |[0m 
[36malgo-1    |[0m # Define the root logger to the system property "hadoop.root.logger".
[36malgo-1    |[0m log4j.rootLogger=${hadoop.root.logger}, EventCounter
[36malgo-1    |[0m 
[36malgo-1    |[0m # Logging Threshold
[36malgo-1    |[0m log4j.threshold=ALL
[36malgo-1    |[0m 
[36malgo-1    |[0m # Null Appender
[36malgo-1    |[0m log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Rolling File Appender - cap space usage at 5gb.
[36malgo-1    |[0m #
[36malgo-1    |[0m hadoop.log.maxfilesize=256MB
[36malgo-1    |[0m hadoop.log.maxbackupindex=20
[36malgo-1    |[0m log4j.appender.RFA=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.RFA.MaxFileSize=${hadoop.log.maxfilesize}
[36malgo-1    |[0m log4j.appender.RFA.MaxBackupIndex=${hadoop.log.maxbackupindex}
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.RFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m 
[36malgo-1    |[0m # Pattern format: Date LogLevel LoggerName LogMessage
[36malgo-1    |[0m log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[36malgo-1    |[0m # Debugging Pattern format
[36malgo-1    |[0m #log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Daily Rolling File Appender
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.DRFA.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m 
[36malgo-1    |[0m # Rollver at midnight
[36malgo-1    |[0m #log4j.appender.DRFA.DatePattern=.yyyy-MM-dd
[36malgo-1    |[0m # Rollver at every hour
[36malgo-1    |[0m log4j.appender.DRFA.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m 
[36malgo-1    |[0m # Pattern format: Date LogLevel LoggerName LogMessage
[36malgo-1    |[0m log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m # Debugging Pattern format
[36malgo-1    |[0m #log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # console
[36malgo-1    |[0m # Add "console" to rootlogger above if you want to use this
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.console=org.apache.log4j.ConsoleAppender
[36malgo-1    |[0m log4j.appender.console.target=System.err
[36malgo-1    |[0m log4j.appender.console.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # TaskLog Appender
[36malgo-1    |[0m #
[36malgo-1    |[0m log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.TLA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # HDFS block state change log from block manager
[36malgo-1    |[0m #
[36malgo-1    |[0m # Uncomment the following to log normal block state change
[36malgo-1    |[0m # messages from BlockManager in NameNode.
[36malgo-1    |[0m #log4j.logger.BlockStateChange=DEBUG
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m #Security appender
[36malgo-1    |[0m #
[36malgo-1    |[0m hadoop.security.logger=INFO,NullAppender
[36malgo-1    |[0m hadoop.security.log.maxfilesize=256MB
[36malgo-1    |[0m hadoop.security.log.maxbackupindex=20
[36malgo-1    |[0m log4j.category.SecurityLogger=${hadoop.security.logger}
[36malgo-1    |[0m hadoop.security.log.file=SecurityAuth-${user.name}.audit
[36malgo-1    |[0m log4j.appender.RFAS=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.RFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
[36malgo-1    |[0m log4j.appender.RFAS.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[36malgo-1    |[0m log4j.appender.RFAS.MaxFileSize=${hadoop.security.log.maxfilesize}
[36malgo-1    |[0m log4j.appender.RFAS.MaxBackupIndex=${hadoop.security.log.maxbackupindex}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Daily Rolling Security appender
[36malgo-1    |[0m #
[36malgo-1    |[0m log4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.DRFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
[36malgo-1    |[0m log4j.appender.DRFAS.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.DRFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[36malgo-1    |[0m log4j.appender.DRFAS.DatePattern=.yyyy-MM-dd
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # hadoop configuration logging
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m # Uncomment the following line to turn off configuration deprecation warnings.
[36malgo-1    |[0m # log4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # hdfs audit logging
[36malgo-1    |[0m #
[36malgo-1    |[0m hdfs.audit.logger=INFO,NullAppender
[36malgo-1    |[0m hdfs.audit.log.maxfilesize=256MB
[36malgo-1    |[0m hdfs.audit.log.maxbackupindex=20
[36malgo-1    |[0m log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}
[36malgo-1    |[0m log4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false
[36malgo-1    |[0m log4j.appender.RFAAUDIT=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.RFAAUDIT.File=${hadoop.log.dir}/hdfs-audit.log
[36malgo-1    |[0m log4j.appender.RFAAUDIT.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.RFAAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[36malgo-1    |[0m log4j.appender.RFAAUDIT.MaxFileSize=${hdfs.audit.log.maxfilesize}
[36malgo-1    |[0m log4j.appender.RFAAUDIT.MaxBackupIndex=${hdfs.audit.log.maxbackupindex}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # NameNode metrics logging.
[36malgo-1    |[0m # The default is to retain two namenode-metrics.log files up to 64MB each.
[36malgo-1    |[0m #
[36malgo-1    |[0m namenode.metrics.logger=INFO,NullAppender
[36malgo-1    |[0m log4j.logger.NameNodeMetricsLog=${namenode.metrics.logger}
[36malgo-1    |[0m log4j.additivity.NameNodeMetricsLog=false
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.File=${hadoop.log.dir}/namenode-metrics.log
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.MaxBackupIndex=1
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.MaxFileSize=64MB
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # DataNode metrics logging.
[36malgo-1    |[0m # The default is to retain two datanode-metrics.log files up to 64MB each.
[36malgo-1    |[0m #
[36malgo-1    |[0m datanode.metrics.logger=INFO,NullAppender
[36malgo-1    |[0m log4j.logger.DataNodeMetricsLog=${datanode.metrics.logger}
[36malgo-1    |[0m log4j.additivity.DataNodeMetricsLog=false
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.File=${hadoop.log.dir}/datanode-metrics.log
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.MaxBackupIndex=1
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.MaxFileSize=64MB
[36malgo-1    |[0m 
[36malgo-1    |[0m # Custom Logging levels
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=DEBUG
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m # AWS SDK & S3A FileSystem
[36malgo-1    |[0m #log4j.logger.com.amazonaws=ERROR
[36malgo-1    |[0m log4j.logger.com.amazonaws.http.AmazonHttpClient=ERROR
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.fs.s3a.S3AFileSystem=WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Event Counter Appender
[36malgo-1    |[0m # Sends counts of logging messages at different severity levels to Hadoop Metrics.
[36malgo-1    |[0m #
[36malgo-1    |[0m log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Job Summary Appender
[36malgo-1    |[0m #
[36malgo-1    |[0m # Use following logger to send summary to separate file defined by
[36malgo-1    |[0m # hadoop.mapreduce.jobsummary.log.file :
[36malgo-1    |[0m # hadoop.mapreduce.jobsummary.logger=INFO,JSA
[36malgo-1    |[0m # 
[36malgo-1    |[0m hadoop.mapreduce.jobsummary.logger=${hadoop.root.logger}
[36malgo-1    |[0m hadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log
[36malgo-1    |[0m hadoop.mapreduce.jobsummary.log.maxfilesize=256MB
[36malgo-1    |[0m hadoop.mapreduce.jobsummary.log.maxbackupindex=20
[36malgo-1    |[0m log4j.appender.JSA=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.JSA.File=${hadoop.log.dir}/${hadoop.mapreduce.jobsummary.log.file}
[36malgo-1    |[0m log4j.appender.JSA.MaxFileSize=${hadoop.mapreduce.jobsummary.log.maxfilesize}
[36malgo-1    |[0m log4j.appender.JSA.MaxBackupIndex=${hadoop.mapreduce.jobsummary.log.maxbackupindex}
[36malgo-1    |[0m log4j.appender.JSA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.JSA.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
[36malgo-1    |[0m log4j.appender.JSA.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.logger.org.apache.hadoop.mapred.JobInProgress$JobSummary=${hadoop.mapreduce.jobsummary.logger}
[36malgo-1    |[0m log4j.additivity.org.apache.hadoop.mapred.JobInProgress$JobSummary=false
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # shuffle connection log from shuffleHandler
[36malgo-1    |[0m # Uncomment the following line to enable logging of shuffle connections
[36malgo-1    |[0m # log4j.logger.org.apache.hadoop.mapred.ShuffleHandler.audit=DEBUG
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Yarn ResourceManager Application Summary Log
[36malgo-1    |[0m #
[36malgo-1    |[0m # Set the ResourceManager summary log filename
[36malgo-1    |[0m yarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log
[36malgo-1    |[0m # Set the ResourceManager summary log level and appender
[36malgo-1    |[0m yarn.server.resourcemanager.appsummary.logger=${hadoop.root.logger}
[36malgo-1    |[0m #yarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY
[36malgo-1    |[0m 
[36malgo-1    |[0m # To enable AppSummaryLogging for the RM,
[36malgo-1    |[0m # set yarn.server.resourcemanager.appsummary.logger to
[36malgo-1    |[0m # <LEVEL>,RMSUMMARY in hadoop-env.sh
[36malgo-1    |[0m 
[36malgo-1    |[0m # Appender for ResourceManager Application Summary Log
[36malgo-1    |[0m # Requires the following properties to be set
[36malgo-1    |[0m #    - hadoop.log.dir (Hadoop Log directory)
[36malgo-1    |[0m #    - yarn.server.resourcemanager.appsummary.log.file (resource manager app summary log filename)
[36malgo-1    |[0m #    - yarn.server.resourcemanager.appsummary.logger (resource manager app summary log level and appender)
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=${yarn.server.resourcemanager.appsummary.logger}
[36malgo-1    |[0m log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=false
[36malgo-1    |[0m log4j.appender.RMSUMMARY=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.RMSUMMARY.File=${hadoop.log.dir}/${yarn.server.resourcemanager.appsummary.log.file}
[36malgo-1    |[0m log4j.appender.RMSUMMARY.MaxFileSize=256MB
[36malgo-1    |[0m log4j.appender.RMSUMMARY.MaxBackupIndex=20
[36malgo-1    |[0m log4j.appender.RMSUMMARY.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.RMSUMMARY.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m # HS audit log configs
[36malgo-1    |[0m #mapreduce.hs.audit.logger=INFO,HSAUDIT
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=${mapreduce.hs.audit.logger}
[36malgo-1    |[0m #log4j.additivity.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=false
[36malgo-1    |[0m #log4j.appender.HSAUDIT=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m #log4j.appender.HSAUDIT.File=${hadoop.log.dir}/hs-audit.log
[36malgo-1    |[0m #log4j.appender.HSAUDIT.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m #log4j.appender.HSAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[36malgo-1    |[0m #log4j.appender.HSAUDIT.DatePattern=.yyyy-MM-dd
[36malgo-1    |[0m 
[36malgo-1    |[0m # Http Server Request Logs
[36malgo-1    |[0m #log4j.logger.http.requests.namenode=INFO,namenoderequestlog
[36malgo-1    |[0m #log4j.appender.namenoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.namenoderequestlog.Filename=${hadoop.log.dir}/jetty-namenode-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.namenoderequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.http.requests.datanode=INFO,datanoderequestlog
[36malgo-1    |[0m #log4j.appender.datanoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.datanoderequestlog.Filename=${hadoop.log.dir}/jetty-datanode-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.datanoderequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.http.requests.resourcemanager=INFO,resourcemanagerrequestlog
[36malgo-1    |[0m #log4j.appender.resourcemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.resourcemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-resourcemanager-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.resourcemanagerrequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.http.requests.jobhistory=INFO,jobhistoryrequestlog
[36malgo-1    |[0m #log4j.appender.jobhistoryrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.jobhistoryrequestlog.Filename=${hadoop.log.dir}/jetty-jobhistory-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.jobhistoryrequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.http.requests.nodemanager=INFO,nodemanagerrequestlog
[36malgo-1    |[0m #log4j.appender.nodemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.nodemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-nodemanager-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.nodemanagerrequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m # WebHdfs request log on datanodes
[36malgo-1    |[0m # Specify -Ddatanode.webhdfs.logger=INFO,HTTPDRFA on datanode startup to
[36malgo-1    |[0m # direct the log to a separate file.
[36malgo-1    |[0m #datanode.webhdfs.logger=INFO,console
[36malgo-1    |[0m #log4j.logger.datanode.webhdfs=${datanode.webhdfs.logger}
[36malgo-1    |[0m #log4j.appender.HTTPDRFA=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m #log4j.appender.HTTPDRFA.File=${hadoop.log.dir}/hadoop-datanode-webhdfs.log
[36malgo-1    |[0m #log4j.appender.HTTPDRFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m #log4j.appender.HTTPDRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[36malgo-1    |[0m #log4j.appender.HTTPDRFA.DatePattern=.yyyy-MM-dd
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m # Appender for viewing information for errors and warnings
[36malgo-1    |[0m yarn.ewma.cleanupInterval=300
[36malgo-1    |[0m yarn.ewma.messageAgeLimitSeconds=86400
[36malgo-1    |[0m yarn.ewma.maxUniqueMessages=250
[36malgo-1    |[0m log4j.appender.EWMA=org.apache.hadoop.yarn.util.Log4jWarningErrorMetricsAppender
[36malgo-1    |[0m log4j.appender.EWMA.cleanupInterval=${yarn.ewma.cleanupInterval}
[36malgo-1    |[0m log4j.appender.EWMA.messageAgeLimitSeconds=${yarn.ewma.messageAgeLimitSeconds}
[36malgo-1    |[0m log4j.appender.EWMA.maxUniqueMessages=${yarn.ewma.maxUniqueMessages}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Fair scheduler state dump
[36malgo-1    |[0m #
[36malgo-1    |[0m # Use following logger to dump the state to a separate file
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=DEBUG,FSSTATEDUMP
[36malgo-1    |[0m #log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=false
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.File=${hadoop.log.dir}/fairscheduler-statedump.log
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.MaxFileSize=${hadoop.log.maxfilesize}
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.MaxBackupIndex=${hadoop.log.maxbackupindex}
[36malgo-1    |[0m 
[36malgo-1    |[0m # Log levels of third-party libraries
[36malgo-1    |[0m log4j.logger.org.apache.commons.beanutils=WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m #AWS SDK Logging
[36malgo-1    |[0m log4j.logger.com.amazonaws=WARN
[36malgo-1    |[0m log4j.logger.org.apache.zookeeper=ERROR
[36malgo-1    |[0m log4j.logger.org.apache.hadoop.hbase=WARN
[36malgo-1    |[0m log4j.logger.amazon.emr.metrics=WARN
[36malgo-1    |[0m log4j.logger.emr=INFO
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.HADOOP=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.HADOOP.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m log4j.appender.HADOOP.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.appender.HADOOP.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.HADOOP.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.JHS=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.JHS.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m log4j.appender.JHS.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.appender.JHS.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.JHS.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.MAPRED=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.MAPRED.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m log4j.appender.MAPRED.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.appender.MAPRED.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.MAPRED.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.YARN=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.YARN.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m log4j.appender.YARN.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.appender.YARN.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.YARN.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/hive/conf/hive-env.sh
[36malgo-1    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/hive/conf/hive-env.sh is: 
[36malgo-1    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[36malgo-1    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/hive/conf/hive-log4j2.properties
[36malgo-1    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/hive/conf/hive-log4j2.properties is: 
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[36malgo-1    |[0m # or more contributor license agreements.  See the NOTICE file
[36malgo-1    |[0m # distributed with this work for additional information
[36malgo-1    |[0m # regarding copyright ownership.  The ASF licenses this file
[36malgo-1    |[0m # to you under the Apache License, Version 2.0 (the
[36malgo-1    |[0m # "License"); you may not use this file except in compliance
[36malgo-1    |[0m # with the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m status = INFO
[36malgo-1    |[0m name = HiveLog4j2
[36malgo-1    |[0m packages = org.apache.hadoop.hive.ql.log
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of properties
[36malgo-1    |[0m property.hive.log.level = INFO
[36malgo-1    |[0m property.hive.root.logger = DRFA
[36malgo-1    |[0m property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
[36malgo-1    |[0m property.hive.log.file = hive.log
[36malgo-1    |[0m property.hive.perflogger.log.level = INFO
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of all appenders
[36malgo-1    |[0m appenders = console, DRFA
[36malgo-1    |[0m 
[36malgo-1    |[0m # console appender
[36malgo-1    |[0m appender.console.type = Console
[36malgo-1    |[0m appender.console.name = console
[36malgo-1    |[0m appender.console.target = SYSTEM_ERR
[36malgo-1    |[0m appender.console.layout.type = PatternLayout
[36malgo-1    |[0m appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m # daily rolling file appender
[36malgo-1    |[0m appender.DRFA.type = RollingRandomAccessFile
[36malgo-1    |[0m appender.DRFA.name = DRFA
[36malgo-1    |[0m appender.DRFA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
[36malgo-1    |[0m # Use %pid in the filePattern to append <process-id>@<host-name> to the filename if you want separate log files for different CLI session
[36malgo-1    |[0m appender.DRFA.filePattern = ${sys:hive.log.dir}/${sys:hive.log.file}.%d{yyyy-MM-dd}
[36malgo-1    |[0m appender.DRFA.layout.type = PatternLayout
[36malgo-1    |[0m appender.DRFA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[36malgo-1    |[0m appender.DRFA.policies.type = Policies
[36malgo-1    |[0m appender.DRFA.policies.time.type = TimeBasedTriggeringPolicy
[36malgo-1    |[0m appender.DRFA.policies.time.interval = 1
[36malgo-1    |[0m appender.DRFA.policies.time.modulate = true
[36malgo-1    |[0m appender.DRFA.strategy.type = DefaultRolloverStrategy
[36malgo-1    |[0m appender.DRFA.strategy.max = 30
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of all loggers
[36malgo-1    |[0m loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, PerfLogger, AmazonAws, ApacheHttp
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
[36malgo-1    |[0m logger.NIOServerCnxn.level = WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
[36malgo-1    |[0m logger.ClientCnxnSocketNIO.level = WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.DataNucleus.name = DataNucleus
[36malgo-1    |[0m logger.DataNucleus.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.Datastore.name = Datastore
[36malgo-1    |[0m logger.Datastore.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.JPOX.name = JPOX
[36malgo-1    |[0m logger.JPOX.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.AmazonAws.name=com.amazonaws
[36malgo-1    |[0m logger.AmazonAws.level = INFO
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.ApacheHttp.name=org.apache.http
[36malgo-1    |[0m logger.ApacheHttp.level = INFO
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.PerfLogger.name = org.apache.hadoop.hive.ql.log.PerfLogger
[36malgo-1    |[0m logger.PerfLogger.level = ${sys:hive.perflogger.log.level}
[36malgo-1    |[0m 
[36malgo-1    |[0m # root logger
[36malgo-1    |[0m rootLogger.level = ${sys:hive.log.level}
[36malgo-1    |[0m rootLogger.appenderRefs = root
[36malgo-1    |[0m rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/hive/conf/hive-exec-log4j2.properties
[36malgo-1    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/hive/conf/hive-exec-log4j2.properties is: 
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[36malgo-1    |[0m # or more contributor license agreements.  See the NOTICE file
[36malgo-1    |[0m # distributed with this work for additional information
[36malgo-1    |[0m # regarding copyright ownership.  The ASF licenses this file
[36malgo-1    |[0m # to you under the Apache License, Version 2.0 (the
[36malgo-1    |[0m # "License"); you may not use this file except in compliance
[36malgo-1    |[0m # with the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m status = INFO
[36malgo-1    |[0m name = HiveExecLog4j2
[36malgo-1    |[0m packages = org.apache.hadoop.hive.ql.log
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of properties
[36malgo-1    |[0m property.hive.log.level = INFO
[36malgo-1    |[0m property.hive.root.logger = FA
[36malgo-1    |[0m property.hive.query.id = hadoop
[36malgo-1    |[0m property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
[36malgo-1    |[0m property.hive.log.file = ${sys:hive.query.id}.log
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of all appenders
[36malgo-1    |[0m appenders = console, FA
[36malgo-1    |[0m 
[36malgo-1    |[0m # console appender
[36malgo-1    |[0m appender.console.type = Console
[36malgo-1    |[0m appender.console.name = console
[36malgo-1    |[0m appender.console.target = SYSTEM_ERR
[36malgo-1    |[0m appender.console.layout.type = PatternLayout
[36malgo-1    |[0m appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m # simple file appender
[36malgo-1    |[0m appender.FA.type = RandomAccessFile
[36malgo-1    |[0m appender.FA.name = FA
[36malgo-1    |[0m appender.FA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
[36malgo-1    |[0m appender.FA.layout.type = PatternLayout
[36malgo-1    |[0m appender.FA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of all loggers
[36malgo-1    |[0m loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
[36malgo-1    |[0m logger.NIOServerCnxn.level = WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
[36malgo-1    |[0m logger.ClientCnxnSocketNIO.level = WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.DataNucleus.name = DataNucleus
[36malgo-1    |[0m logger.DataNucleus.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.Datastore.name = Datastore
[36malgo-1    |[0m logger.Datastore.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.JPOX.name = JPOX
[36malgo-1    |[0m logger.JPOX.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m # root logger
[36malgo-1    |[0m rootLogger.level = ${sys:hive.log.level}
[36malgo-1    |[0m rootLogger.appenderRefs = root
[36malgo-1    |[0m rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/hive/conf/hive-site.xml
[36malgo-1    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/hive/conf/hive-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0"?>
[36malgo-1    |[0m <!--
[36malgo-1    |[0m   Licensed to the Apache Software Foundation (ASF) under one or more
[36malgo-1    |[0m   contributor license agreements.  See the NOTICE file distributed with
[36malgo-1    |[0m   this work for additional information regarding copyright ownership.
[36malgo-1    |[0m   The ASF licenses this file to You under the Apache License, Version 2.0
[36malgo-1    |[0m   (the "License"); you may not use this file except in compliance with
[36malgo-1    |[0m   the License.  You may obtain a copy of the License at
[36malgo-1    |[0m 
[36malgo-1    |[0m       http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m 
[36malgo-1    |[0m   Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m   distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m   See the License for the specific language governing permissions and
[36malgo-1    |[0m   limitations under the License.
[36malgo-1    |[0m -->
[36malgo-1    |[0m <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[36malgo-1    |[0m 
[36malgo-1    |[0m <configuration>
[36malgo-1    |[0m 
[36malgo-1    |[0m <!-- Hive Configuration can either be stored in this file or in the hadoop configuration files  -->
[36malgo-1    |[0m <!-- that are implied by Hadoop setup variables.                                                -->
[36malgo-1    |[0m <!-- Aside from Hadoop setup variables - this file is provided as a convenience so that Hive    -->
[36malgo-1    |[0m <!-- users do not have to edit hadoop configuration files (that may be managed as a centralized -->
[36malgo-1    |[0m <!-- resource).                                                                                 -->
[36malgo-1    |[0m 
[36malgo-1    |[0m <!-- Hive Execution Parameters -->
[36malgo-1    |[0m 
[36malgo-1    |[0m <property>
[36malgo-1    |[0m   <name>javax.jdo.option.ConnectionURL</name>
[36malgo-1    |[0m   <value>jdbc:derby:;databaseName=/var/lib/hive/metastore/metastore_db;create=true</value>
[36malgo-1    |[0m   <description>JDBC connect string for a JDBC metastore</description>
[36malgo-1    |[0m </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m <property>
[36malgo-1    |[0m   <name>javax.jdo.option.ConnectionDriverName</name>
[36malgo-1    |[0m   <value>org.apache.derby.jdbc.EmbeddedDriver</value>
[36malgo-1    |[0m   <description>Driver class name for a JDBC metastore</description>
[36malgo-1    |[0m </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>key</name>
[36malgo-1    |[0m     <value>value</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[36malgo-1    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.driver.host=172.21.0.2
[36malgo-1    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[36malgo-1    |[0m 
[36malgo-1    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[36malgo-1    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[36malgo-1    |[0m spark.rpc.askTimeout=300s
[36malgo-1    |[0m spark.driver.memory 2048m
[36malgo-1    |[0m spark.driver.memoryOverhead 204m
[36malgo-1    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[36malgo-1    |[0m spark.executor.memory 12399m
[36malgo-1    |[0m spark.executor.memoryOverhead 1239m
[36malgo-1    |[0m spark.executor.cores 4
[36malgo-1    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[36malgo-1    |[0m spark.executor.instances 2
[36malgo-1    |[0m spark.default.parallelism 16
[36malgo-1    |[0m spark.executor.memory 2g
[36malgo-1    |[0m spark.executor.cores 1
[36malgo-1    |[0m key value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/spark/conf/spark-env.sh
[36malgo-1    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/spark/conf/spark-env.sh is: 
[36malgo-1    |[0m #EMPTY FILE AVOID OVERRIDDING ENV VARS
[36malgo-1    |[0m # Specifically, without copying the empty file, SPARK_HISTORY_OPTS will be overriden, 
[36malgo-1    |[0m # spark.history.ui.port defaults to 18082, and spark.eventLog.dir defaults to local fs
[36malgo-1    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[36malgo-1    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/spark/conf/log4j.properties
[36malgo-1    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/spark/conf/log4j.properties is: 
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[33malgo-2    |[0m WARNING: /usr/lib/hadoop/logs does not exist. Creating.
[36malgo-1    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/spark/conf/hive-site.xml
[36malgo-1    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/spark/conf/hive-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0"?>
[36malgo-1    |[0m <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[36malgo-1    |[0m <configuration>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>key</name>
[36malgo-1    |[0m     <value>value</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/spark/conf/metrics.properties
[33malgo-2    |[0m WARNING: /var/log/yarn/export does not exist. Creating.
[36malgo-1    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/spark/conf/metrics.properties is: 
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[36malgo-1    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0"?>
[36malgo-1    |[0m <!-- Site specific YARN configuration properties -->
[36malgo-1    |[0m  <configuration>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.resourcemanager.hostname</name>
[36malgo-1    |[0m          <value>172.21.0.2</value>
[36malgo-1    |[0m          <description>The hostname of the RM.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.hostname</name>
[36malgo-1    |[0m          <value>algo-1</value>
[36malgo-1    |[0m          <description>The hostname of the NM.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.webapp.address</name>
[36malgo-1    |[0m          <value>algo-1:8042</value>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.vmem-pmem-ratio</name>
[36malgo-1    |[0m          <value>5</value>
[36malgo-1    |[0m          <description>Ratio between virtual memory to physical memory.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.resourcemanager.am.max-attempts</name>
[36malgo-1    |[0m          <value>1</value>
[36malgo-1    |[0m          <description>The maximum number of application attempts.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.env-whitelist</name>
[36malgo-1    |[0m          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>
[36malgo-1    |[0m          <description>Environment variable whitelist</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m  
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.minimum-allocation-mb</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.maximum-allocation-mb</name>
[36malgo-1    |[0m     <value>15892</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.minimum-allocation-vcores</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.maximum-allocation-vcores</name>
[36malgo-1    |[0m     <value>4</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.nodemanager.resource.memory-mb</name>
[36malgo-1    |[0m     <value>15892</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.nodemanager.resource.cpu-vcores</name>
[36malgo-1    |[0m     <value>4</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>key</name>
[36malgo-1    |[0m     <value>value</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:56 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/yarn-env.sh
[36malgo-1    |[0m 11-14 16:56 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-env.sh is: 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one or more
[36malgo-1    |[0m # contributor license agreements.  See the NOTICE file distributed with
[36malgo-1    |[0m # this work for additional information regarding copyright ownership.
[36malgo-1    |[0m # The ASF licenses this file to You under the Apache License, Version 2.0
[36malgo-1    |[0m # (the "License"); you may not use this file except in compliance with
[36malgo-1    |[0m # the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## THIS FILE ACTS AS AN OVERRIDE FOR hadoop-env.sh FOR ALL
[36malgo-1    |[0m ## WORK DONE BY THE yarn AND RELATED COMMANDS.
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## Precedence rules:
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## yarn-env.sh > hadoop-env.sh > hard-coded defaults
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## YARN_xyz > HADOOP_xyz > hard-coded defaults
[36malgo-1    |[0m ##
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Resource Manager specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the max heapsize for the ResourceManager.  If no units are
[36malgo-1    |[0m # given, it will be assumed to be in MB.
[36malgo-1    |[0m # This value will be overridden by an Xmx setting specified in either
[36malgo-1    |[0m # HADOOP_OPTS and/or YARN_RESOURCEMANAGER_OPTS.
[36malgo-1    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX
[36malgo-1    |[0m #export YARN_RESOURCEMANAGER_HEAPSIZE=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the ResourceManager.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # Examples for a Sun/Oracle JDK:
[36malgo-1    |[0m # a) override the appsummary log file:
[36malgo-1    |[0m # export YARN_RESOURCEMANAGER_OPTS="-Dyarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log -Dyarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY"
[36malgo-1    |[0m #
[36malgo-1    |[0m # b) Set JMX options
[36malgo-1    |[0m # export YARN_RESOURCEMANAGER_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
[36malgo-1    |[0m #
[36malgo-1    |[0m # c) Set garbage collection logs from hadoop-env.sh
[36malgo-1    |[0m # export YARN_RESOURCE_MANAGER_OPTS="${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[36malgo-1    |[0m #
[36malgo-1    |[0m # d) ... or set them directly
[36malgo-1    |[0m # export YARN_RESOURCEMANAGER_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[36malgo-1    |[0m #
[36malgo-1    |[0m #
[36malgo-1    |[0m # export YARN_RESOURCEMANAGER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Node Manager specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the max heapsize for the NodeManager.  If no units are
[36malgo-1    |[0m # given, it will be assumed to be in MB.
[36malgo-1    |[0m # This value will be overridden by an Xmx setting specified in either
[36malgo-1    |[0m # HADOOP_OPTS and/or YARN_NODEMANAGER_OPTS.
[36malgo-1    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[36malgo-1    |[0m #export YARN_NODEMANAGER_HEAPSIZE=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the NodeManager.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_NODEMANAGER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # TimeLineServer specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the max heapsize for the timelineserver.  If no units are
[36malgo-1    |[0m # given, it will be assumed to be in MB.
[36malgo-1    |[0m # This value will be overridden by an Xmx setting specified in either
[36malgo-1    |[0m # HADOOP_OPTS and/or YARN_TIMELINESERVER_OPTS.
[36malgo-1    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[36malgo-1    |[0m #export YARN_TIMELINE_HEAPSIZE=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the TimeLineServer.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_TIMELINESERVER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # TimeLineReader specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the TimeLineReader.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_TIMELINEREADER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Web App Proxy Server specifc parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the max heapsize for the web app proxy server.  If no units are
[36malgo-1    |[0m # given, it will be assumed to be in MB.
[36malgo-1    |[0m # This value will be overridden by an Xmx setting specified in either
[36malgo-1    |[0m # HADOOP_OPTS and/or YARN_PROXYSERVER_OPTS.
[36malgo-1    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[36malgo-1    |[0m #export YARN_PROXYSERVER_HEAPSIZE=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the proxy server.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_PROXYSERVER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Shared Cache Manager specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the
[36malgo-1    |[0m # shared cache manager server.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_SHAREDCACHEMANAGER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Router specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the Router.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_ROUTER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Registry DNS specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # For privileged registry DNS, user to run as after dropping privileges
[36malgo-1    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[36malgo-1    |[0m # export YARN_REGISTRYDNS_SECURE_USER=yarn
[36malgo-1    |[0m 
[36malgo-1    |[0m # Supplemental options for privileged registry DNS
[36malgo-1    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[36malgo-1    |[0m # server jvm.
[36malgo-1    |[0m # export YARN_REGISTRYDNS_SECURE_EXTRA_OPTS="-jvm server"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # YARN Services parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Directory containing service examples
[36malgo-1    |[0m # export YARN_SERVICE_EXAMPLES_DIR = $HADOOP_YARN_HOME/share/hadoop/yarn/yarn-service-examples
[36malgo-1    |[0m # export YARN_CONTAINER_RUNTIME_DOCKER_RUN_OVERRIDE_DISABLE=true
[36malgo-1    |[0m export YARN_LOG_DIR=/var/log/yarn/export HADOOP_DATANODE_HEAPSIZE=2048
[36malgo-1    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[36malgo-1    |[0m 
[36malgo-1    |[0m WARNING: HADOOP_NAMENODE_OPTS has been replaced by HDFS_NAMENODE_OPTS. Using value of HADOOP_NAMENODE_OPTS.
[36malgo-1    |[0m WARNING: /usr/lib/hadoop/logs does not exist. Creating.
[33malgo-2    |[0m 2021-11-14 16:56:09,014 INFO nodemanager.NodeManager: STARTUP_MSG: 
[33malgo-2    |[0m /************************************************************
[33malgo-2    |[0m STARTUP_MSG: Starting NodeManager
[33malgo-2    |[0m STARTUP_MSG:   host = algo-2/172.21.0.3
[33malgo-2    |[0m STARTUP_MSG:   args = []
[33malgo-2    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[33malgo-2    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar
[33malgo-2    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[33malgo-2    |[0m STARTUP_MSG:   java = 1.8.0_302
[33malgo-2    |[0m ************************************************************/
[33malgo-2    |[0m 2021-11-14 16:56:09,022 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]
[33malgo-2    |[0m 2021-11-14 16:56:09,094 INFO datanode.DataNode: STARTUP_MSG: 
[33malgo-2    |[0m /************************************************************
[33malgo-2    |[0m STARTUP_MSG: Starting DataNode
[33malgo-2    |[0m STARTUP_MSG:   host = algo-2/172.21.0.3
[33malgo-2    |[0m STARTUP_MSG:   args = []
[33malgo-2    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[33malgo-2    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar
[33malgo-2    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[33malgo-2    |[0m STARTUP_MSG:   java = 1.8.0_302
[33malgo-2    |[0m ************************************************************/
[33malgo-2    |[0m 2021-11-14 16:56:09,105 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 16:56:09,161 INFO namenode.NameNode: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting NameNode
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/172.21.0.2
[36malgo-1    |[0m STARTUP_MSG:   args = [-format, -force]
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 16:56:09,176 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 16:56:09,261 INFO namenode.NameNode: createNameNode [-format, -force]
[33malgo-2    |[0m 2021-11-14 16:56:09,433 INFO resourceplugin.ResourcePluginManager: No Resource plugins found from configuration!
[33malgo-2    |[0m 2021-11-14 16:56:09,433 INFO resourceplugin.ResourcePluginManager: Found Resource plugins from configuration: null
[33malgo-2    |[0m 2021-11-14 16:56:09,463 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode
[33malgo-2    |[0m 2021-11-14 16:56:09,502 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.
[33malgo-2    |[0m 2021-11-14 16:56:09,550 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher
[33malgo-2    |[0m 2021-11-14 16:56:09,551 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher
[33malgo-2    |[0m 2021-11-14 16:56:09,553 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper
[33malgo-2    |[0m 2021-11-14 16:56:09,553 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices
[33malgo-2    |[0m 2021-11-14 16:56:09,554 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl
[33malgo-2    |[0m 2021-11-14 16:56:09,554 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher
[33malgo-2    |[0m 2021-11-14 16:56:09,555 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler
[33malgo-2    |[0m 2021-11-14 16:56:09,557 INFO tracker.NMLogAggregationStatusTracker: the rolling interval seconds for the NodeManager Cached Log aggregation status is 600
[33malgo-2    |[0m 2021-11-14 16:56:09,567 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[33malgo-2    |[0m 2021-11-14 16:56:09,575 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl
[33malgo-2    |[0m 2021-11-14 16:56:09,576 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager
[33malgo-2    |[0m 2021-11-14 16:56:09,632 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[33malgo-2    |[0m 2021-11-14 16:56:09,644 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[33malgo-2    |[0m 2021-11-14 16:56:09,644 INFO impl.MetricsSystemImpl: DataNode metrics system started
[36malgo-1    |[0m Formatting using clusterid: CID-c577013d-56ac-45ae-b357-69538cff9a3e
[36malgo-1    |[0m 2021-11-14 16:56:09,684 INFO namenode.FSEditLog: Edit logging is async:true
[36malgo-1    |[0m 2021-11-14 16:56:09,702 INFO namenode.FSNamesystem: KeyProvider: null
[33malgo-2    |[0m 2021-11-14 16:56:09,703 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[33malgo-2    |[0m 2021-11-14 16:56:09,703 INFO impl.MetricsSystemImpl: NodeManager metrics system started
[36malgo-1    |[0m 2021-11-14 16:56:09,704 INFO namenode.FSNamesystem: fsLock is fair: true
[36malgo-1    |[0m 2021-11-14 16:56:09,704 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
[36malgo-1    |[0m 2021-11-14 16:56:09,709 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 16:56:09,709 INFO namenode.FSNamesystem: supergroup          = supergroup
[36malgo-1    |[0m 2021-11-14 16:56:09,709 INFO namenode.FSNamesystem: isPermissionEnabled = true
[36malgo-1    |[0m 2021-11-14 16:56:09,710 INFO namenode.FSNamesystem: HA Enabled: false
[33malgo-2    |[0m 2021-11-14 16:56:09,721 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.
[33malgo-2    |[0m 2021-11-14 16:56:09,732 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.
[36malgo-1    |[0m 2021-11-14 16:56:09,761 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[33malgo-2    |[0m 2021-11-14 16:56:09,773 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@4c60d6e9
[36malgo-1    |[0m 2021-11-14 16:56:09,774 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
[36malgo-1    |[0m 2021-11-14 16:56:09,774 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
[33malgo-2    |[0m 2021-11-14 16:56:09,774 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler
[33malgo-2    |[0m 2021-11-14 16:56:09,776 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService
[33malgo-2    |[0m 2021-11-14 16:56:09,776 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled
[33malgo-2    |[0m 2021-11-14 16:56:09,776 INFO localizer.ResourceLocalizationService: per directory file limit = 8192
[36malgo-1    |[0m 2021-11-14 16:56:09,780 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
[36malgo-1    |[0m 2021-11-14 16:56:09,780 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Nov 14 16:56:09
[36malgo-1    |[0m 2021-11-14 16:56:09,781 INFO util.GSet: Computing capacity for map BlocksMap
[36malgo-1    |[0m 2021-11-14 16:56:09,782 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 16:56:09,783 INFO util.GSet: 2.0% max memory 6.5 GB = 133.6 MB
[36malgo-1    |[0m 2021-11-14 16:56:09,783 INFO util.GSet: capacity      = 2^24 = 16777216 entries
[33malgo-2    |[0m 2021-11-14 16:56:09,794 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker
[33malgo-2    |[0m 2021-11-14 16:56:09,799 INFO resources.ResourceHandlerModule: Using traffic control bandwidth handler
[33malgo-2    |[0m 2021-11-14 16:56:09,802 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@221a3fa4
[33malgo-2    |[0m 2021-11-14 16:56:09,802 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null
[33malgo-2    |[0m 2021-11-14 16:56:09,803 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true
[33malgo-2    |[0m 2021-11-14 16:56:09,803 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true
[33malgo-2    |[0m 2021-11-14 16:56:09,803 INFO monitor.ContainersMonitorImpl: Elastic memory control enabled: false
[33malgo-2    |[0m 2021-11-14 16:56:09,803 INFO monitor.ContainersMonitorImpl: Strict memory control enabled: true
[33malgo-2    |[0m 2021-11-14 16:56:09,803 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true
[33malgo-2    |[0m 2021-11-14 16:56:09,805 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.
[33malgo-2    |[0m 2021-11-14 16:56:09,809 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[33malgo-2    |[0m 2021-11-14 16:56:09,812 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
[33malgo-2    |[0m 2021-11-14 16:56:09,816 INFO datanode.DataNode: Configured hostname is algo-2
[33malgo-2    |[0m 2021-11-14 16:56:09,817 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[33malgo-2    |[0m 2021-11-14 16:56:09,820 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0
[36malgo-1    |[0m 2021-11-14 16:56:09,825 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
[36malgo-1    |[0m 2021-11-14 16:56:09,825 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
[33malgo-2    |[0m 2021-11-14 16:56:09,832 INFO conf.Configuration: resource-types.xml not found
[33malgo-2    |[0m 2021-11-14 16:56:09,832 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
[36malgo-1    |[0m 2021-11-14 16:56:09,833 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
[36malgo-1    |[0m 2021-11-14 16:56:09,833 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
[36malgo-1    |[0m 2021-11-14 16:56:09,833 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
[36malgo-1    |[0m 2021-11-14 16:56:09,833 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
[36malgo-1    |[0m 2021-11-14 16:56:09,834 INFO blockmanagement.BlockManager: defaultReplication         = 3
[36malgo-1    |[0m 2021-11-14 16:56:09,834 INFO blockmanagement.BlockManager: maxReplication             = 512
[36malgo-1    |[0m 2021-11-14 16:56:09,834 INFO blockmanagement.BlockManager: minReplication             = 1
[36malgo-1    |[0m 2021-11-14 16:56:09,834 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
[36malgo-1    |[0m 2021-11-14 16:56:09,834 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
[36malgo-1    |[0m 2021-11-14 16:56:09,834 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
[36malgo-1    |[0m 2021-11-14 16:56:09,834 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
[33malgo-2    |[0m 2021-11-14 16:56:09,838 INFO conf.Configuration: node-resources.xml not found
[33malgo-2    |[0m 2021-11-14 16:56:09,838 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866
[33malgo-2    |[0m 2021-11-14 16:56:09,838 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.
[33malgo-2    |[0m 2021-11-14 16:56:09,839 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:15892, vCores:4>
[33malgo-2    |[0m 2021-11-14 16:56:09,840 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s
[33malgo-2    |[0m 2021-11-14 16:56:09,840 INFO datanode.DataNode: Number threads for balancing is 50
[33malgo-2    |[0m 2021-11-14 16:56:09,843 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=15892 virtual-memory=79460 virtual-cores=4
[36malgo-1    |[0m 2021-11-14 16:56:09,857 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
[36malgo-1    |[0m 2021-11-14 16:56:09,857 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 16:56:09,857 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 16:56:09,857 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 16:56:09,870 INFO util.GSet: Computing capacity for map INodeMap
[36malgo-1    |[0m 2021-11-14 16:56:09,870 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 16:56:09,871 INFO util.GSet: 1.0% max memory 6.5 GB = 66.8 MB
[36malgo-1    |[0m 2021-11-14 16:56:09,871 INFO util.GSet: capacity      = 2^23 = 8388608 entries
[33malgo-2    |[0m 2021-11-14 16:56:09,880 INFO util.log: Logging initialized @1287ms to org.eclipse.jetty.util.log.Slf4jLog
[33malgo-2    |[0m 2021-11-14 16:56:09,881 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 16:56:09,888 INFO namenode.FSDirectory: ACLs enabled? false
[36malgo-1    |[0m 2021-11-14 16:56:09,888 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
[36malgo-1    |[0m 2021-11-14 16:56:09,888 INFO namenode.FSDirectory: XAttrs enabled? true
[36malgo-1    |[0m 2021-11-14 16:56:09,888 INFO namenode.NameNode: Caching file names occurring more than 10 times
[36malgo-1    |[0m 2021-11-14 16:56:09,893 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
[36malgo-1    |[0m 2021-11-14 16:56:09,896 INFO snapshot.SnapshotManager: SkipList is disabled
[33malgo-2    |[0m 2021-11-14 16:56:09,897 INFO ipc.Server: Starting Socket Reader #1 for port 0
[36malgo-1    |[0m 2021-11-14 16:56:09,900 INFO util.GSet: Computing capacity for map cachedBlocks
[36malgo-1    |[0m 2021-11-14 16:56:09,900 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 16:56:09,900 INFO util.GSet: 0.25% max memory 6.5 GB = 16.7 MB
[36malgo-1    |[0m 2021-11-14 16:56:09,900 INFO util.GSet: capacity      = 2^21 = 2097152 entries
[36malgo-1    |[0m 2021-11-14 16:56:09,908 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
[36malgo-1    |[0m 2021-11-14 16:56:09,908 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
[36malgo-1    |[0m 2021-11-14 16:56:09,908 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
[36malgo-1    |[0m 2021-11-14 16:56:09,911 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
[36malgo-1    |[0m 2021-11-14 16:56:09,911 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
[36malgo-1    |[0m 2021-11-14 16:56:09,913 INFO util.GSet: Computing capacity for map NameNodeRetryCache
[36malgo-1    |[0m 2021-11-14 16:56:09,913 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 16:56:09,913 INFO util.GSet: 0.029999999329447746% max memory 6.5 GB = 2.0 MB
[36malgo-1    |[0m 2021-11-14 16:56:09,913 INFO util.GSet: capacity      = 2^18 = 262144 entries
[36malgo-1    |[0m 2021-11-14 16:56:09,940 INFO namenode.FSImage: Allocated new BlockPoolId: BP-686721326-172.21.0.2-1636908969933
[36malgo-1    |[0m 2021-11-14 16:56:09,956 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.
[36malgo-1    |[0m 2021-11-14 16:56:09,982 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression
[33malgo-2    |[0m 2021-11-14 16:56:10,015 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[33malgo-2    |[0m 2021-11-14 16:56:10,021 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined
[33malgo-2    |[0m 2021-11-14 16:56:10,027 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[33malgo-2    |[0m 2021-11-14 16:56:10,029 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
[33malgo-2    |[0m 2021-11-14 16:56:10,029 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[33malgo-2    |[0m 2021-11-14 16:56:10,029 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[33malgo-2    |[0m 2021-11-14 16:56:10,055 INFO http.HttpServer2: Jetty bound to port 34899
[33malgo-2    |[0m 2021-11-14 16:56:10,056 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[36malgo-1    |[0m 2021-11-14 16:56:10,075 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .
[33malgo-2    |[0m 2021-11-14 16:56:10,077 INFO server.session: DefaultSessionIdManager workerName=node0
[33malgo-2    |[0m 2021-11-14 16:56:10,077 INFO server.session: No SessionScavenger set, using defaults
[33malgo-2    |[0m 2021-11-14 16:56:10,079 INFO server.session: node0 Scavenging every 660000ms
[33malgo-2    |[0m 2021-11-14 16:56:10,080 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server
[33malgo-2    |[0m 2021-11-14 16:56:10,080 INFO ipc.Server: IPC Server Responder: starting
[33malgo-2    |[0m 2021-11-14 16:56:10,081 INFO ipc.Server: IPC Server listener on 0: starting
[36malgo-1    |[0m 2021-11-14 16:56:10,083 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
[36malgo-1    |[0m 2021-11-14 16:56:10,087 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
[36malgo-1    |[0m 2021-11-14 16:56:10,088 INFO namenode.NameNode: SHUTDOWN_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m SHUTDOWN_MSG: Shutting down NameNode at algo-1/172.21.0.2
[36malgo-1    |[0m ************************************************************/
[33malgo-2    |[0m 2021-11-14 16:56:10,088 INFO security.NMContainerTokenSecretManager: Updating node address : algo-2:45155
[33malgo-2    |[0m 2021-11-14 16:56:10,088 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a3793c7{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}
[33malgo-2    |[0m 2021-11-14 16:56:10,089 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@bb9e6dc{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}
[33malgo-2    |[0m 2021-11-14 16:56:10,093 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 500, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[33malgo-2    |[0m 2021-11-14 16:56:10,094 INFO ipc.Server: Starting Socket Reader #1 for port 8040
[33malgo-2    |[0m 2021-11-14 16:56:10,097 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server
[33malgo-2    |[0m 2021-11-14 16:56:10,097 INFO ipc.Server: IPC Server Responder: starting
[33malgo-2    |[0m 2021-11-14 16:56:10,097 INFO ipc.Server: IPC Server listener on 8040: starting
[33malgo-2    |[0m 2021-11-14 16:56:10,098 INFO localizer.ResourceLocalizationService: Localizer started on port 8040
[33malgo-2    |[0m 2021-11-14 16:56:10,100 INFO containermanager.ContainerManagerImpl: ContainerManager started at /172.21.0.3:45155
[33malgo-2    |[0m 2021-11-14 16:56:10,100 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-2/172.21.0.3:0
[33malgo-2    |[0m 2021-11-14 16:56:10,100 WARN tracker.NMLogAggregationStatusTracker: Log Aggregation is disabled.So is the LogAggregationStatusTracker.
[33malgo-2    |[0m 2021-11-14 16:56:10,103 INFO webapp.WebServer: Instantiating NMWebApp at algo-2:8042
[33malgo-2    |[0m 2021-11-14 16:56:10,125 INFO util.log: Logging initialized @1527ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m 11-14 16:56 smspark-submit INFO     waiting for cluster to be up
[33malgo-2    |[0m 2021-11-14 16:56:10,142 INFO util.TypeUtil: JVM Runtime does not support Modules
[33malgo-2    |[0m 2021-11-14 16:56:10,153 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@338c99c8{datanode,/,file:///usr/lib/hadoop-hdfs/webapps/datanode/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/datanode}
[33malgo-2    |[0m 2021-11-14 16:56:10,160 INFO server.AbstractConnector: Started ServerConnector@6c372fe6{HTTP/1.1,[http/1.1]}{localhost:34899}
[33malgo-2    |[0m 2021-11-14 16:56:10,160 INFO server.Server: Started @1567ms
[36malgo-1    |[0m WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.
[36malgo-1    |[0m WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.
[36malgo-1    |[0m WARNING: HADOOP_NAMENODE_OPTS has been replaced by HDFS_NAMENODE_OPTS. Using value of HADOOP_NAMENODE_OPTS.
[36malgo-1    |[0m WARNING: /var/log/yarn/export does not exist. Creating.
[33malgo-2    |[0m 2021-11-14 16:56:10,220 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[33malgo-2    |[0m 2021-11-14 16:56:10,224 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined
[33malgo-2    |[0m 2021-11-14 16:56:10,232 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[33malgo-2    |[0m 2021-11-14 16:56:10,233 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node
[33malgo-2    |[0m 2021-11-14 16:56:10,234 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[33malgo-2    |[0m 2021-11-14 16:56:10,234 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[33malgo-2    |[0m 2021-11-14 16:56:10,234 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context node
[33malgo-2    |[0m 2021-11-14 16:56:10,234 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
[33malgo-2    |[0m 2021-11-14 16:56:10,235 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
[33malgo-2    |[0m 2021-11-14 16:56:10,237 INFO http.HttpServer2: adding path spec: /node/*
[33malgo-2    |[0m 2021-11-14 16:56:10,237 INFO http.HttpServer2: adding path spec: /ws/*
[33malgo-2    |[0m 2021-11-14 16:56:10,298 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864
[33malgo-2    |[0m 2021-11-14 16:56:10,303 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[33malgo-2    |[0m 2021-11-14 16:56:10,304 INFO datanode.DataNode: dnUserName = root
[33malgo-2    |[0m 2021-11-14 16:56:10,304 INFO datanode.DataNode: supergroup = supergroup
[33malgo-2    |[0m 2021-11-14 16:56:10,343 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[33malgo-2    |[0m 2021-11-14 16:56:10,357 INFO ipc.Server: Starting Socket Reader #1 for port 9867
[33malgo-2    |[0m 2021-11-14 16:56:10,586 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:9867
[33malgo-2    |[0m 2021-11-14 16:56:10,603 INFO datanode.DataNode: Refresh request received for nameservices: null
[33malgo-2    |[0m 2021-11-14 16:56:10,614 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>
[33malgo-2    |[0m 2021-11-14 16:56:10,623 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1.spark-network/172.21.0.2:8020 starting to offer service
[33malgo-2    |[0m 2021-11-14 16:56:10,629 INFO ipc.Server: IPC Server Responder: starting
[33malgo-2    |[0m 2021-11-14 16:56:10,631 INFO webapp.WebApps: Registered webapp guice modules
[33malgo-2    |[0m 2021-11-14 16:56:10,632 INFO ipc.Server: IPC Server listener on 9867: starting
[33malgo-2    |[0m 2021-11-14 16:56:10,635 INFO http.HttpServer2: Jetty bound to port 8042
[33malgo-2    |[0m 2021-11-14 16:56:10,636 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[33malgo-2    |[0m 2021-11-14 16:56:10,665 INFO server.session: DefaultSessionIdManager workerName=node0
[33malgo-2    |[0m 2021-11-14 16:56:10,665 INFO server.session: No SessionScavenger set, using defaults
[33malgo-2    |[0m 2021-11-14 16:56:10,666 INFO server.session: node0 Scavenging every 600000ms
[33malgo-2    |[0m 2021-11-14 16:56:10,676 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[33malgo-2    |[0m 2021-11-14 16:56:10,679 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4bff64c2{logs,/logs,file:///var/log/yarn/export/,AVAILABLE}
[33malgo-2    |[0m 2021-11-14 16:56:10,679 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3cc41abc{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/static,AVAILABLE}
[33malgo-2    |[0m 2021-11-14 16:56:10,690 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException
[36malgo-1    |[0m 2021-11-14 16:56:10,759 INFO nodemanager.NodeManager: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting NodeManager
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/172.21.0.2
[36malgo-1    |[0m STARTUP_MSG:   args = []
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 16:56:10,766 INFO resourcemanager.ResourceManager: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting ResourceManager
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/172.21.0.2
[36malgo-1    |[0m STARTUP_MSG:   args = []
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 16:56:10,771 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 16:56:10,781 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 16:56:10,797 INFO namenode.NameNode: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting NameNode
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/172.21.0.2
[36malgo-1    |[0m STARTUP_MSG:   args = []
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[33malgo-2    |[0m 2021-11-14 16:56:10,807 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m 2021-11-14 16:56:10,808 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 16:56:10,870 INFO datanode.DataNode: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting DataNode
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/172.21.0.2
[36malgo-1    |[0m STARTUP_MSG:   args = []
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 16:56:10,881 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 16:56:10,908 INFO namenode.NameNode: createNameNode []
[33malgo-2    |[0m Nov 14, 2021 4:56:10 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[33malgo-2    |[0m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class
[33malgo-2    |[0m Nov 14, 2021 4:56:10 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[33malgo-2    |[0m INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
[33malgo-2    |[0m Nov 14, 2021 4:56:10 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[33malgo-2    |[0m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class
[33malgo-2    |[0m Nov 14, 2021 4:56:10 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
[33malgo-2    |[0m INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'
[33malgo-2    |[0m Nov 14, 2021 4:56:11 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[33malgo-2    |[0m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 16:56:11,062 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m 2021-11-14 16:56:11,159 INFO conf.Configuration: found resource core-site.xml at file:/etc/hadoop/conf.empty/core-site.xml
[36malgo-1    |[0m 2021-11-14 16:56:11,196 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m 2021-11-14 16:56:11,196 INFO impl.MetricsSystemImpl: NameNode metrics system started
[33malgo-2    |[0m Nov 14, 2021 4:56:11 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[33malgo-2    |[0m INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 16:56:11,212 INFO conf.Configuration: resource-types.xml not found
[36malgo-1    |[0m 2021-11-14 16:56:11,213 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
[36malgo-1    |[0m 2021-11-14 16:56:11,221 INFO resourceplugin.ResourcePluginManager: No Resource plugins found from configuration!
[36malgo-1    |[0m 2021-11-14 16:56:11,222 INFO resourceplugin.ResourcePluginManager: Found Resource plugins from configuration: null
[36malgo-1    |[0m 2021-11-14 16:56:11,229 INFO namenode.NameNodeUtils: fs.defaultFS is hdfs://172.21.0.2/
[36malgo-1    |[0m 2021-11-14 16:56:11,260 INFO conf.Configuration: found resource yarn-site.xml at file:/etc/hadoop/conf.empty/yarn-site.xml
[36malgo-1    |[0m 2021-11-14 16:56:11,279 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher
[36malgo-1    |[0m 2021-11-14 16:56:11,313 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.
[36malgo-1    |[0m 2021-11-14 16:56:11,325 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms
[36malgo-1    |[0m 2021-11-14 16:56:11,331 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode
[36malgo-1    |[0m 2021-11-14 16:56:11,336 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms
[36malgo-1    |[0m 2021-11-14 16:56:11,356 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms
[36malgo-1    |[0m 2021-11-14 16:56:11,388 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher
[36malgo-1    |[0m 2021-11-14 16:56:11,390 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher
[36malgo-1    |[0m 2021-11-14 16:56:11,392 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper
[36malgo-1    |[0m 2021-11-14 16:56:11,393 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices
[36malgo-1    |[0m 2021-11-14 16:56:11,394 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl
[36malgo-1    |[0m 2021-11-14 16:56:11,395 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher
[36malgo-1    |[0m 2021-11-14 16:56:11,396 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler
[36malgo-1    |[0m 2021-11-14 16:56:11,399 INFO tracker.NMLogAggregationStatusTracker: the rolling interval seconds for the NodeManager Cached Log aggregation status is 600
[36malgo-1    |[0m 2021-11-14 16:56:11,417 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler
[36malgo-1    |[0m 2021-11-14 16:56:11,417 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 16:56:11,423 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager
[36malgo-1    |[0m 2021-11-14 16:56:11,423 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
[36malgo-1    |[0m 2021-11-14 16:56:11,425 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl
[36malgo-1    |[0m 2021-11-14 16:56:11,426 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager
[36malgo-1    |[0m 2021-11-14 16:56:11,452 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:9870
[36malgo-1    |[0m 2021-11-14 16:56:11,462 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m 2021-11-14 16:56:11,470 INFO util.log: Logging initialized @1246ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m 2021-11-14 16:56:11,475 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher
[36malgo-1    |[0m 2021-11-14 16:56:11,476 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher
[36malgo-1    |[0m 2021-11-14 16:56:11,477 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher
[36malgo-1    |[0m 2021-11-14 16:56:11,478 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher
[36malgo-1    |[0m 2021-11-14 16:56:11,500 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m 2021-11-14 16:56:11,555 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[33malgo-2    |[0m Nov 14, 2021 4:56:11 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[33malgo-2    |[0m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 16:56:11,607 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m 2021-11-14 16:56:11,607 INFO impl.MetricsSystemImpl: DataNode metrics system started
[36malgo-1    |[0m 2021-11-14 16:56:11,612 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m 2021-11-14 16:56:11,612 INFO impl.MetricsSystemImpl: NodeManager metrics system started
[33malgo-2    |[0m 2021-11-14 16:56:11,614 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5d5b5fa7{node,/,file:///tmp/jetty-algo-2-8042-_-any-710289731190228079.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/node}
[36malgo-1    |[0m 2021-11-14 16:56:11,616 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[33malgo-2    |[0m 2021-11-14 16:56:11,624 INFO server.AbstractConnector: Started ServerConnector@3f390d63{HTTP/1.1,[http/1.1]}{algo-2:8042}
[33malgo-2    |[0m 2021-11-14 16:56:11,625 INFO server.Server: Started @3027ms
[33malgo-2    |[0m 2021-11-14 16:56:11,625 INFO webapp.WebApps: Web app node started at 8042
[33malgo-2    |[0m 2021-11-14 16:56:11,626 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-2:45155
[33malgo-2    |[0m 2021-11-14 16:56:11,629 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[33malgo-2    |[0m 2021-11-14 16:56:11,633 INFO client.RMProxy: Connecting to ResourceManager at /172.21.0.2:8031
[36malgo-1    |[0m 2021-11-14 16:56:11,638 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.
[36malgo-1    |[0m 2021-11-14 16:56:11,642 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined
[36malgo-1    |[0m 2021-11-14 16:56:11,651 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[36malgo-1    |[0m 2021-11-14 16:56:11,654 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
[36malgo-1    |[0m 2021-11-14 16:56:11,655 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[36malgo-1    |[0m 2021-11-14 16:56:11,655 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.
[36malgo-1    |[0m 2021-11-14 16:56:11,655 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 2021-11-14 16:56:11,665 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m 2021-11-14 16:56:11,665 INFO impl.MetricsSystemImpl: ResourceManager metrics system started
[33malgo-2    |[0m 2021-11-14 16:56:11,681 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []
[33malgo-2    |[0m 2021-11-14 16:56:11,693 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]
[36malgo-1    |[0m 2021-11-14 16:56:11,699 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
[36malgo-1    |[0m 2021-11-14 16:56:11,699 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
[36malgo-1    |[0m 2021-11-14 16:56:11,712 INFO http.HttpServer2: Jetty bound to port 9870
[36malgo-1    |[0m 2021-11-14 16:56:11,713 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[33malgo-2    |[0m 2021-11-14 16:56:11,718 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/172.21.0.2:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 16:56:11,718 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@4c60d6e9
[36malgo-1    |[0m 2021-11-14 16:56:11,720 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler
[36malgo-1    |[0m 2021-11-14 16:56:11,722 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService
[36malgo-1    |[0m 2021-11-14 16:56:11,722 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled
[36malgo-1    |[0m 2021-11-14 16:56:11,722 INFO localizer.ResourceLocalizationService: per directory file limit = 8192
[36malgo-1    |[0m 2021-11-14 16:56:11,723 INFO security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.
[36malgo-1    |[0m 2021-11-14 16:56:11,731 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager
[36malgo-1    |[0m 2021-11-14 16:56:11,740 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher
[36malgo-1    |[0m 2021-11-14 16:56:11,742 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean
[36malgo-1    |[0m 2021-11-14 16:56:11,743 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.
[36malgo-1    |[0m 2021-11-14 16:56:11,745 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker
[36malgo-1    |[0m 2021-11-14 16:56:11,750 INFO placement.MultiNodeSortingManager: Initializing NodeSortingService=MultiNodeSortingManager
[36malgo-1    |[0m 2021-11-14 16:56:11,752 INFO resources.ResourceHandlerModule: Using traffic control bandwidth handler
[36malgo-1    |[0m 2021-11-14 16:56:11,753 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list
[36malgo-1    |[0m 2021-11-14 16:56:11,755 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@221a3fa4
[36malgo-1    |[0m 2021-11-14 16:56:11,756 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null
[36malgo-1    |[0m 2021-11-14 16:56:11,757 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true
[36malgo-1    |[0m 2021-11-14 16:56:11,757 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true
[36malgo-1    |[0m 2021-11-14 16:56:11,758 INFO monitor.ContainersMonitorImpl: Elastic memory control enabled: false
[36malgo-1    |[0m 2021-11-14 16:56:11,758 INFO monitor.ContainersMonitorImpl: Strict memory control enabled: true
[36malgo-1    |[0m 2021-11-14 16:56:11,758 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true
[36malgo-1    |[0m 2021-11-14 16:56:11,760 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.
[36malgo-1    |[0m 2021-11-14 16:56:11,761 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/etc/hadoop/conf.empty/capacity-scheduler.xml
[36malgo-1    |[0m 2021-11-14 16:56:11,767 INFO server.session: DefaultSessionIdManager workerName=node0
[36malgo-1    |[0m 2021-11-14 16:56:11,767 INFO server.session: No SessionScavenger set, using defaults
[36malgo-1    |[0m 2021-11-14 16:56:11,769 INFO server.session: node0 Scavenging every 660000ms
[36malgo-1    |[0m 2021-11-14 16:56:11,777 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1, vCores:1>
[36malgo-1    |[0m 2021-11-14 16:56:11,777 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 16:56:11,781 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6d60fe40{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 16:56:11,782 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@73eb439a{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 16:56:11,815 INFO conf.Configuration: resource-types.xml not found
[36malgo-1    |[0m 2021-11-14 16:56:11,816 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
[36malgo-1    |[0m 2021-11-14 16:56:11,824 INFO conf.Configuration: node-resources.xml not found
[36malgo-1    |[0m 2021-11-14 16:56:11,825 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined
[36malgo-1    |[0m 2021-11-14 16:56:11,826 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined
[36malgo-1    |[0m 2021-11-14 16:56:11,824 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.
[36malgo-1    |[0m 2021-11-14 16:56:11,830 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 16:56:11,836 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=15892 virtual-memory=79460 virtual-cores=4
[36malgo-1    |[0m 2021-11-14 16:56:11,839 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=ADMINISTER_QUEUE:*SUBMIT_APP:*, labels=*,
[36malgo-1    |[0m , reservationsContinueLooking=true, orderingPolicy=utilization, priority=0
[36malgo-1    |[0m 2021-11-14 16:56:11,839 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root
[36malgo-1    |[0m 2021-11-14 16:56:11,853 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined
[36malgo-1    |[0m 2021-11-14 16:56:11,853 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined
[36malgo-1    |[0m 2021-11-14 16:56:11,857 INFO capacity.LeafQueue: Initializing default
[36malgo-1    |[0m capacity = 1.0 [= (float) configuredCapacity / 100 ]
[36malgo-1    |[0m absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]
[36malgo-1    |[0m maxCapacity = 1.0 [= configuredMaxCapacity ]
[36malgo-1    |[0m absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]
[36malgo-1    |[0m effectiveMinResource=<memory:0, vCores:0>
[36malgo-1    |[0m  , effectiveMaxResource=<memory:0, vCores:0>
[36malgo-1    |[0m userLimit = 100 [= configuredUserLimit ]
[36malgo-1    |[0m userLimitFactor = 1.0 [= configuredUserLimitFactor ]
[36malgo-1    |[0m maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]
[36malgo-1    |[0m maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]
[36malgo-1    |[0m usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]
[36malgo-1    |[0m absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]
[36malgo-1    |[0m maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]
[36malgo-1    |[0m minimumAllocationFactor = 0.99993706 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]
[36malgo-1    |[0m maximumAllocation = <memory:15892, vCores:4> [= configuredMaxAllocation ]
[36malgo-1    |[0m numContainers = 0 [= currentNumContainers ]
[36malgo-1    |[0m state = RUNNING [= configuredState ]
[36malgo-1    |[0m acls = ADMINISTER_QUEUE:*SUBMIT_APP:* [= configuredAcls ]
[36malgo-1    |[0m nodeLocalityDelay = 40
[36malgo-1    |[0m rackLocalityAdditionalDelay = -1
[36malgo-1    |[0m labels=*,
[36malgo-1    |[0m reservationsContinueLooking = true
[36malgo-1    |[0m preemptionDisabled = true
[36malgo-1    |[0m defaultAppPriorityPerQueue = 0
[36malgo-1    |[0m priority = 0
[36malgo-1    |[0m maxLifetime = -1 seconds
[36malgo-1    |[0m defaultLifetime = -1 seconds
[36malgo-1    |[0m 2021-11-14 16:56:11,857 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>
[36malgo-1    |[0m 2021-11-14 16:56:11,858 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[36malgo-1    |[0m 2021-11-14 16:56:11,858 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
[36malgo-1    |[0m 2021-11-14 16:56:11,859 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
[36malgo-1    |[0m 2021-11-14 16:56:11,859 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m 2021-11-14 16:56:11,860 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false
[36malgo-1    |[0m 2021-11-14 16:56:11,861 INFO placement.MultiNodeSortingManager: MultiNode scheduling is 'false', and configured policies are 
[36malgo-1    |[0m 2021-11-14 16:56:11,861 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1, vCores:1>>, maximumAllocation=<<memory:15892, vCores:4>>, asynchronousScheduling=false, asyncScheduleInterval=5ms,multiNodePlacementEnabled=false
[36malgo-1    |[0m 2021-11-14 16:56:11,861 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
[36malgo-1    |[0m 2021-11-14 16:56:11,863 INFO conf.Configuration: dynamic-resources.xml not found
[36malgo-1    |[0m 2021-11-14 16:56:11,865 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].
[36malgo-1    |[0m 2021-11-14 16:56:11,865 INFO resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.
[36malgo-1    |[0m 2021-11-14 16:56:11,866 INFO datanode.DataNode: Configured hostname is algo-1
[36malgo-1    |[0m 2021-11-14 16:56:11,867 INFO resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. 
[36malgo-1    |[0m 2021-11-14 16:56:11,867 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[36malgo-1    |[0m 2021-11-14 16:56:11,872 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0
[36malgo-1    |[0m 2021-11-14 16:56:11,873 INFO resourcemanager.ResourceManager: TimelineServicePublisher is not configured
[36malgo-1    |[0m 2021-11-14 16:56:11,877 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4dd6fd0a{hdfs,/,file:///usr/lib/hadoop-hdfs/webapps/hdfs/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/hdfs}
[36malgo-1    |[0m 2021-11-14 16:56:11,879 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 16:56:11,896 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866
[36malgo-1    |[0m 2021-11-14 16:56:11,898 INFO ipc.Server: Starting Socket Reader #1 for port 0
[36malgo-1    |[0m 2021-11-14 16:56:11,898 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s
[36malgo-1    |[0m 2021-11-14 16:56:11,899 INFO datanode.DataNode: Number threads for balancing is 50
[36malgo-1    |[0m 2021-11-14 16:56:11,906 INFO server.AbstractConnector: Started ServerConnector@4d14b6c2{HTTP/1.1,[http/1.1]}{0.0.0.0:9870}
[36malgo-1    |[0m 2021-11-14 16:56:11,906 INFO server.Server: Started @1682ms
[36malgo-1    |[0m 2021-11-14 16:56:11,917 INFO util.log: Logging initialized @1698ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m 2021-11-14 16:56:11,943 INFO util.log: Logging initialized @1721ms to org.eclipse.jetty.util.log.Slf4jLog
[33malgo-2    |[0m 11-14 16:56 smspark-submit INFO     cluster is up
[33malgo-2    |[0m 11-14 16:56 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING
[33malgo-2    |[0m 11-14 16:56 smspark-submit INFO     starting executor logs watcher
[33malgo-2    |[0m 11-14 16:56 smspark-submit INFO     waiting for the primary to come up
[33malgo-2    |[0m Starting executor logs watcher on log_dir: /var/log/yarn
[33malgo-2    |[0m 11-14 16:56 smspark-submit INFO     waiting for the primary to go down
[36malgo-1    |[0m 2021-11-14 16:56:12,037 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 16:56:12,043 INFO http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined
[36malgo-1    |[0m 2021-11-14 16:56:12,051 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[36malgo-1    |[0m 2021-11-14 16:56:12,056 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster
[36malgo-1    |[0m 2021-11-14 16:56:12,056 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs
[36malgo-1    |[0m 2021-11-14 16:56:12,056 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static
[36malgo-1    |[0m 2021-11-14 16:56:12,057 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster
[36malgo-1    |[0m 2021-11-14 16:56:12,057 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[36malgo-1    |[0m 2021-11-14 16:56:12,057 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 2021-11-14 16:56:12,059 INFO http.HttpServer2: adding path spec: /cluster/*
[36malgo-1    |[0m 2021-11-14 16:56:12,059 INFO http.HttpServer2: adding path spec: /ws/*
[36malgo-1    |[0m 2021-11-14 16:56:12,059 INFO http.HttpServer2: adding path spec: /app/*
[36malgo-1    |[0m 2021-11-14 16:56:12,135 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 16:56:12,142 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined
[36malgo-1    |[0m 2021-11-14 16:56:12,152 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[36malgo-1    |[0m 2021-11-14 16:56:12,154 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
[36malgo-1    |[0m 2021-11-14 16:56:12,154 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[36malgo-1    |[0m 2021-11-14 16:56:12,154 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 2021-11-14 16:56:12,164 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 16:56:12,165 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 16:56:12,165 INFO ipc.Server: IPC Server listener on 0: starting
[36malgo-1    |[0m 2021-11-14 16:56:12,173 INFO security.NMContainerTokenSecretManager: Updating node address : algo-1:33635
[36malgo-1    |[0m 2021-11-14 16:56:12,183 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 500, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 16:56:12,183 INFO ipc.Server: Starting Socket Reader #1 for port 8040
[36malgo-1    |[0m 2021-11-14 16:56:12,187 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 16:56:12,187 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 16:56:12,187 INFO ipc.Server: IPC Server listener on 8040: starting
[36malgo-1    |[0m 2021-11-14 16:56:12,188 INFO localizer.ResourceLocalizationService: Localizer started on port 8040
[36malgo-1    |[0m 2021-11-14 16:56:12,190 INFO http.HttpServer2: Jetty bound to port 34969
[36malgo-1    |[0m 2021-11-14 16:56:12,191 INFO containermanager.ContainerManagerImpl: ContainerManager started at /172.21.0.2:33635
[36malgo-1    |[0m 2021-11-14 16:56:12,191 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-1/172.21.0.2:0
[36malgo-1    |[0m 2021-11-14 16:56:12,191 WARN tracker.NMLogAggregationStatusTracker: Log Aggregation is disabled.So is the LogAggregationStatusTracker.
[36malgo-1    |[0m 2021-11-14 16:56:12,191 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[36malgo-1    |[0m 2021-11-14 16:56:12,196 INFO webapp.WebServer: Instantiating NMWebApp at algo-1:8042
[36malgo-1    |[0m 2021-11-14 16:56:12,197 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
[36malgo-1    |[0m 2021-11-14 16:56:12,197 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
[36malgo-1    |[0m 2021-11-14 16:56:12,222 INFO server.session: DefaultSessionIdManager workerName=node0
[36malgo-1    |[0m 2021-11-14 16:56:12,223 INFO server.session: No SessionScavenger set, using defaults
[36malgo-1    |[0m 2021-11-14 16:56:12,224 INFO server.session: node0 Scavenging every 600000ms
[36malgo-1    |[0m 2021-11-14 16:56:12,230 INFO util.log: Logging initialized @2009ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m 2021-11-14 16:56:12,237 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a3793c7{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 16:56:12,238 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@bb9e6dc{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 16:56:12,241 INFO namenode.FSEditLog: Edit logging is async:true
[36malgo-1    |[0m 2021-11-14 16:56:12,251 INFO namenode.FSNamesystem: KeyProvider: null
[36malgo-1    |[0m 2021-11-14 16:56:12,253 INFO namenode.FSNamesystem: fsLock is fair: true
[36malgo-1    |[0m 2021-11-14 16:56:12,253 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
[36malgo-1    |[0m 2021-11-14 16:56:12,258 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 16:56:12,258 INFO namenode.FSNamesystem: supergroup          = supergroup
[36malgo-1    |[0m 2021-11-14 16:56:12,258 INFO namenode.FSNamesystem: isPermissionEnabled = true
[36malgo-1    |[0m 2021-11-14 16:56:12,258 INFO namenode.FSNamesystem: HA Enabled: false
[36malgo-1    |[0m 2021-11-14 16:56:12,290 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[36malgo-1    |[0m 2021-11-14 16:56:12,299 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
[36malgo-1    |[0m 2021-11-14 16:56:12,300 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
[36malgo-1    |[0m 2021-11-14 16:56:12,303 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
[36malgo-1    |[0m 2021-11-14 16:56:12,304 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Nov 14 16:56:12
[36malgo-1    |[0m 2021-11-14 16:56:12,305 INFO util.GSet: Computing capacity for map BlocksMap
[36malgo-1    |[0m 2021-11-14 16:56:12,306 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 16:56:12,307 INFO util.GSet: 2.0% max memory 6.5 GB = 133.6 MB
[36malgo-1    |[0m 2021-11-14 16:56:12,307 INFO util.GSet: capacity      = 2^24 = 16777216 entries
[36malgo-1    |[0m 2021-11-14 16:56:12,312 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m 2021-11-14 16:56:12,322 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@338c99c8{datanode,/,file:///usr/lib/hadoop-hdfs/webapps/datanode/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/datanode}
[36malgo-1    |[0m 2021-11-14 16:56:12,327 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
[36malgo-1    |[0m 2021-11-14 16:56:12,327 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
[36malgo-1    |[0m 2021-11-14 16:56:12,330 INFO server.AbstractConnector: Started ServerConnector@6c372fe6{HTTP/1.1,[http/1.1]}{localhost:34969}
[36malgo-1    |[0m 2021-11-14 16:56:12,330 INFO server.Server: Started @2108ms
[36malgo-1    |[0m 2021-11-14 16:56:12,335 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
[36malgo-1    |[0m 2021-11-14 16:56:12,335 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
[36malgo-1    |[0m 2021-11-14 16:56:12,335 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
[36malgo-1    |[0m 2021-11-14 16:56:12,335 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
[36malgo-1    |[0m 2021-11-14 16:56:12,336 INFO blockmanagement.BlockManager: defaultReplication         = 3
[36malgo-1    |[0m 2021-11-14 16:56:12,336 INFO blockmanagement.BlockManager: maxReplication             = 512
[36malgo-1    |[0m 2021-11-14 16:56:12,336 INFO blockmanagement.BlockManager: minReplication             = 1
[36malgo-1    |[0m 2021-11-14 16:56:12,337 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
[36malgo-1    |[0m 2021-11-14 16:56:12,337 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
[36malgo-1    |[0m 2021-11-14 16:56:12,337 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
[36malgo-1    |[0m 2021-11-14 16:56:12,337 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
[36malgo-1    |[0m 2021-11-14 16:56:12,345 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 16:56:12,348 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined
[36malgo-1    |[0m 2021-11-14 16:56:12,354 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[36malgo-1    |[0m 2021-11-14 16:56:12,355 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node
[36malgo-1    |[0m 2021-11-14 16:56:12,355 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 2021-11-14 16:56:12,355 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[36malgo-1    |[0m 2021-11-14 16:56:12,356 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context node
[36malgo-1    |[0m 2021-11-14 16:56:12,356 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
[36malgo-1    |[0m 2021-11-14 16:56:12,356 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
[36malgo-1    |[0m 2021-11-14 16:56:12,357 INFO http.HttpServer2: adding path spec: /node/*
[36malgo-1    |[0m 2021-11-14 16:56:12,357 INFO http.HttpServer2: adding path spec: /ws/*
[36malgo-1    |[0m 2021-11-14 16:56:12,368 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
[36malgo-1    |[0m 2021-11-14 16:56:12,368 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 16:56:12,368 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 16:56:12,368 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 16:56:12,386 INFO util.GSet: Computing capacity for map INodeMap
[36malgo-1    |[0m 2021-11-14 16:56:12,386 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 16:56:12,386 INFO util.GSet: 1.0% max memory 6.5 GB = 66.8 MB
[36malgo-1    |[0m 2021-11-14 16:56:12,386 INFO util.GSet: capacity      = 2^23 = 8388608 entries
[36malgo-1    |[0m 2021-11-14 16:56:12,501 INFO webapp.WebApps: Registered webapp guice modules
[36malgo-1    |[0m 2021-11-14 16:56:12,509 INFO http.HttpServer2: Jetty bound to port 8088
[36malgo-1    |[0m 2021-11-14 16:56:12,510 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[36malgo-1    |[0m 2021-11-14 16:56:12,519 INFO namenode.FSDirectory: ACLs enabled? false
[36malgo-1    |[0m 2021-11-14 16:56:12,519 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
[36malgo-1    |[0m 2021-11-14 16:56:12,519 INFO namenode.FSDirectory: XAttrs enabled? true
[36malgo-1    |[0m 2021-11-14 16:56:12,519 INFO namenode.NameNode: Caching file names occurring more than 10 times
[36malgo-1    |[0m 2021-11-14 16:56:12,526 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
[36malgo-1    |[0m 2021-11-14 16:56:12,528 INFO snapshot.SnapshotManager: SkipList is disabled
[36malgo-1    |[0m 2021-11-14 16:56:12,533 INFO util.GSet: Computing capacity for map cachedBlocks
[36malgo-1    |[0m 2021-11-14 16:56:12,533 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 16:56:12,534 INFO util.GSet: 0.25% max memory 6.5 GB = 16.7 MB
[36malgo-1    |[0m 2021-11-14 16:56:12,534 INFO util.GSet: capacity      = 2^21 = 2097152 entries
[36malgo-1    |[0m 2021-11-14 16:56:12,540 INFO server.session: DefaultSessionIdManager workerName=node0
[36malgo-1    |[0m 2021-11-14 16:56:12,540 INFO server.session: No SessionScavenger set, using defaults
[36malgo-1    |[0m 2021-11-14 16:56:12,542 INFO server.session: node0 Scavenging every 600000ms
[36malgo-1    |[0m 2021-11-14 16:56:12,543 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864
[36malgo-1    |[0m 2021-11-14 16:56:12,544 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
[36malgo-1    |[0m 2021-11-14 16:56:12,544 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
[36malgo-1    |[0m 2021-11-14 16:56:12,544 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
[36malgo-1    |[0m 2021-11-14 16:56:12,548 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
[36malgo-1    |[0m 2021-11-14 16:56:12,548 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
[36malgo-1    |[0m 2021-11-14 16:56:12,549 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 16:56:12,549 INFO datanode.DataNode: dnUserName = root
[36malgo-1    |[0m 2021-11-14 16:56:12,549 INFO datanode.DataNode: supergroup = supergroup
[36malgo-1    |[0m 2021-11-14 16:56:12,550 INFO util.GSet: Computing capacity for map NameNodeRetryCache
[36malgo-1    |[0m 2021-11-14 16:56:12,550 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 16:56:12,551 INFO util.GSet: 0.029999999329447746% max memory 6.5 GB = 2.0 MB
[36malgo-1    |[0m 2021-11-14 16:56:12,551 INFO util.GSet: capacity      = 2^18 = 262144 entries
[36malgo-1    |[0m 2021-11-14 16:56:12,555 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 16:56:12,563 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
[36malgo-1    |[0m 2021-11-14 16:56:12,565 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
[36malgo-1    |[0m 2021-11-14 16:56:12,565 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
[36malgo-1    |[0m 2021-11-14 16:56:12,574 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/namenode/in_use.lock acquired by nodename 134@algo-1
[36malgo-1    |[0m 2021-11-14 16:56:12,586 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@21129f1f{logs,/logs,file:///var/log/yarn/export/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 16:56:12,586 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@48d61b48{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/static,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 16:56:12,588 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 16:56:12,596 INFO namenode.FileJournalManager: Recovering unfinalized segments in /opt/amazon/hadoop/hdfs/namenode/current
[36malgo-1    |[0m 2021-11-14 16:56:12,596 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException
[36malgo-1    |[0m 2021-11-14 16:56:12,597 INFO namenode.FSImage: No edit log streams selected.
[36malgo-1    |[0m 2021-11-14 16:56:12,597 INFO namenode.FSImage: Planning to load image: FSImageFile(file=/opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
[36malgo-1    |[0m 2021-11-14 16:56:12,600 INFO ipc.Server: Starting Socket Reader #1 for port 9867
[36malgo-1    |[0m 2021-11-14 16:56:12,658 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.
[36malgo-1    |[0m 2021-11-14 16:56:12,693 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
[36malgo-1    |[0m 2021-11-14 16:56:12,693 INFO namenode.FSImage: Loaded image for txid 0 from /opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000
[36malgo-1    |[0m 2021-11-14 16:56:12,698 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m 2021-11-14 16:56:12,699 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
[36malgo-1    |[0m 2021-11-14 16:56:12,700 INFO namenode.FSEditLog: Starting log segment at 1
[36malgo-1    |[0m 2021-11-14 16:56:12,701 INFO webapp.WebApps: Registered webapp guice modules
[36malgo-1    |[0m 2021-11-14 16:56:12,705 INFO http.HttpServer2: Jetty bound to port 8042
[36malgo-1    |[0m 2021-11-14 16:56:12,706 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[33malgo-2    |[0m 2021-11-14 16:56:12,719 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/172.21.0.2:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 16:56:12,738 INFO server.session: DefaultSessionIdManager workerName=node0
[36malgo-1    |[0m 2021-11-14 16:56:12,738 INFO server.session: No SessionScavenger set, using defaults
[36malgo-1    |[0m 2021-11-14 16:56:12,739 INFO server.session: node0 Scavenging every 660000ms
[33malgo-2    |[0m 2021-11-14 16:56:12,745 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/172.21.0.2:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 16:56:12,748 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 16:56:12,750 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4bff64c2{logs,/logs,file:///var/log/yarn/export/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 16:56:12,751 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3cc41abc{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/static,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 16:56:12,763 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException
[36malgo-1    |[0m Nov 14, 2021 4:56:12 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class
[36malgo-1    |[0m Nov 14, 2021 4:56:12 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class
[36malgo-1    |[0m Nov 14, 2021 4:56:12 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
[36malgo-1    |[0m Nov 14, 2021 4:56:12 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
[36malgo-1    |[0m INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'
[36malgo-1    |[0m 2021-11-14 16:56:12,805 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:9867
[36malgo-1    |[0m 2021-11-14 16:56:12,822 INFO datanode.DataNode: Refresh request received for nameservices: null
[36malgo-1    |[0m 2021-11-14 16:56:12,826 INFO namenode.NameCache: initialized with 0 entries 0 lookups
[36malgo-1    |[0m 2021-11-14 16:56:12,826 INFO namenode.FSNamesystem: Finished loading FSImage in 269 msecs
[36malgo-1    |[0m 2021-11-14 16:56:12,830 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>
[36malgo-1    |[0m 2021-11-14 16:56:12,839 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1/172.21.0.2:8020 starting to offer service
[36malgo-1    |[0m 2021-11-14 16:56:12,846 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 16:56:12,846 INFO ipc.Server: IPC Server listener on 9867: starting
[36malgo-1    |[0m Nov 14, 2021 4:56:12 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 16:56:12,853 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m Nov 14, 2021 4:56:12 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class
[36malgo-1    |[0m Nov 14, 2021 4:56:12 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
[36malgo-1    |[0m Nov 14, 2021 4:56:12 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class
[36malgo-1    |[0m Nov 14, 2021 4:56:12 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
[36malgo-1    |[0m INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'
[36malgo-1    |[0m Nov 14, 2021 4:56:13 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 16:56:13,176 INFO namenode.NameNode: RPC server is binding to algo-1:8020
[36malgo-1    |[0m 2021-11-14 16:56:13,208 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m Nov 14, 2021 4:56:13 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 16:56:13,220 INFO ipc.Server: Starting Socket Reader #1 for port 8020
[36malgo-1    |[0m Nov 14, 2021 4:56:13 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 16:56:13,401 INFO namenode.NameNode: Clients are to use algo-1:8020 to access this namenode/service.
[36malgo-1    |[0m 2021-11-14 16:56:13,404 INFO namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
[36malgo-1    |[0m 2021-11-14 16:56:13,416 INFO namenode.LeaseManager: Number of blocks under construction: 0
[36malgo-1    |[0m 2021-11-14 16:56:13,427 INFO blockmanagement.BlockManager: initializing replication queues
[36malgo-1    |[0m 2021-11-14 16:56:13,429 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs
[36malgo-1    |[0m 2021-11-14 16:56:13,429 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
[36malgo-1    |[0m 2021-11-14 16:56:13,429 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
[36malgo-1    |[0m 2021-11-14 16:56:13,450 INFO blockmanagement.BlockManager: Total number of blocks            = 0
[36malgo-1    |[0m 2021-11-14 16:56:13,451 INFO blockmanagement.BlockManager: Number of invalid blocks          = 0
[36malgo-1    |[0m 2021-11-14 16:56:13,451 INFO blockmanagement.BlockManager: Number of under-replicated blocks = 0
[36malgo-1    |[0m 2021-11-14 16:56:13,452 INFO blockmanagement.BlockManager: Number of  over-replicated blocks = 0
[36malgo-1    |[0m 2021-11-14 16:56:13,452 INFO blockmanagement.BlockManager: Number of blocks being written    = 0
[36malgo-1    |[0m 2021-11-14 16:56:13,452 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 24 msec
[36malgo-1    |[0m 2021-11-14 16:56:13,480 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 16:56:13,491 INFO ipc.Server: IPC Server listener on 8020: starting
[36malgo-1    |[0m 2021-11-14 16:56:13,589 INFO namenode.NameNode: NameNode RPC up at: algo-1/172.21.0.2:8020
[36malgo-1    |[0m 2021-11-14 16:56:13,600 INFO namenode.FSNamesystem: Starting services required for active state
[36malgo-1    |[0m 2021-11-14 16:56:13,600 INFO namenode.FSDirectory: Initializing quota with 4 thread(s)
[36malgo-1    |[0m 2021-11-14 16:56:13,653 INFO namenode.FSDirectory: Quota initialization completed in 53 milliseconds
[36malgo-1    |[0m name space=1
[36malgo-1    |[0m storage space=0
[36malgo-1    |[0m storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
[33malgo-2    |[0m 2021-11-14 16:56:13,719 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/172.21.0.2:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 16:56:13,732 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
[33malgo-2    |[0m 2021-11-14 16:56:13,745 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/172.21.0.2:8031. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m Nov 14, 2021 4:56:13 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 16:56:13,797 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@25da615a{cluster,/,file:///tmp/jetty-172_21_0_2-8088-_-any-7370953384254850144.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/cluster}
[36malgo-1    |[0m Nov 14, 2021 4:56:13 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 16:56:13,823 INFO server.AbstractConnector: Started ServerConnector@71e9ebae{HTTP/1.1,[http/1.1]}{172.21.0.2:8088}
[36malgo-1    |[0m 2021-11-14 16:56:13,823 INFO server.Server: Started @3604ms
[36malgo-1    |[0m 2021-11-14 16:56:13,823 INFO webapp.WebApps: Web app cluster started at 8088
[36malgo-1    |[0m 2021-11-14 16:56:13,864 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5d5b5fa7{node,/,file:///tmp/jetty-algo-1-8042-_-any-950026877287362384.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/node}
[36malgo-1    |[0m 2021-11-14 16:56:13,890 INFO server.AbstractConnector: Started ServerConnector@3f390d63{HTTP/1.1,[http/1.1]}{algo-1:8042}
[36malgo-1    |[0m 2021-11-14 16:56:13,890 INFO server.Server: Started @3670ms
[36malgo-1    |[0m 2021-11-14 16:56:13,890 INFO webapp.WebApps: Web app node started at 8042
[36malgo-1    |[0m 2021-11-14 16:56:13,897 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-1:33635
[36malgo-1    |[0m 2021-11-14 16:56:13,899 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 16:56:13,901 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 16:56:13,907 INFO client.RMProxy: Connecting to ResourceManager at /172.21.0.2:8031
[36malgo-1    |[0m 2021-11-14 16:56:13,907 INFO ipc.Client: Retrying connect to server: algo-1/172.21.0.2:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 16:56:13,919 INFO ipc.Server: Starting Socket Reader #1 for port 8033
[36malgo-1    |[0m 2021-11-14 16:56:13,969 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []
[36malgo-1    |[0m 2021-11-14 16:56:13,983 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]
[36malgo-1    |[0m 2021-11-14 16:56:13,989 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1/172.21.0.2:8020
[36malgo-1    |[0m 2021-11-14 16:56:13,994 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
[33malgo-2    |[0m 2021-11-14 16:56:13,997 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1.spark-network/172.21.0.2:8020
[33malgo-2    |[0m 2021-11-14 16:56:13,999 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
[36malgo-1    |[0m 2021-11-14 16:56:14,003 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 135@algo-1
[36malgo-1    |[0m 2021-11-14 16:56:14,005 INFO common.Storage: Storage directory with location [DISK]file:/opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 2146147409. Formatting...
[33malgo-2    |[0m 2021-11-14 16:56:14,006 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 18@algo-2
[36malgo-1    |[0m 2021-11-14 16:56:14,006 INFO common.Storage: Generated new storageID DS-c3d87838-ab4e-4cb9-b23f-6b958f917377 for directory /opt/amazon/hadoop/hdfs/datanode 
[33malgo-2    |[0m 2021-11-14 16:56:14,008 INFO common.Storage: Storage directory with location [DISK]file:/opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 2146147409. Formatting...
[33malgo-2    |[0m 2021-11-14 16:56:14,009 INFO common.Storage: Generated new storageID DS-600e61f5-0030-4d5e-b320-43fd0bd3967b for directory /opt/amazon/hadoop/hdfs/datanode 
[36malgo-1    |[0m 2021-11-14 16:56:14,041 INFO common.Storage: Analyzing storage directories for bpid BP-686721326-172.21.0.2-1636908969933
[36malgo-1    |[0m 2021-11-14 16:56:14,042 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-686721326-172.21.0.2-1636908969933
[36malgo-1    |[0m 2021-11-14 16:56:14,043 INFO common.Storage: Block pool storage directory for location [DISK]file:/opt/amazon/hadoop/hdfs/datanode and block pool id BP-686721326-172.21.0.2-1636908969933 is not formatted. Formatting ...
[36malgo-1    |[0m 2021-11-14 16:56:14,043 INFO common.Storage: Formatting block pool BP-686721326-172.21.0.2-1636908969933 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-686721326-172.21.0.2-1636908969933/current
[33malgo-2    |[0m 2021-11-14 16:56:14,044 INFO common.Storage: Analyzing storage directories for bpid BP-686721326-172.21.0.2-1636908969933
[33malgo-2    |[0m 2021-11-14 16:56:14,045 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-686721326-172.21.0.2-1636908969933
[33malgo-2    |[0m 2021-11-14 16:56:14,045 INFO common.Storage: Block pool storage directory for location [DISK]file:/opt/amazon/hadoop/hdfs/datanode and block pool id BP-686721326-172.21.0.2-1636908969933 is not formatted. Formatting ...
[33malgo-2    |[0m 2021-11-14 16:56:14,045 INFO common.Storage: Formatting block pool BP-686721326-172.21.0.2-1636908969933 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-686721326-172.21.0.2-1636908969933/current
[33malgo-2    |[0m 2021-11-14 16:56:14,051 INFO datanode.DataNode: Setting up storage: nsid=2146147409;bpid=BP-686721326-172.21.0.2-1636908969933;lv=-57;nsInfo=lv=-65;cid=CID-c577013d-56ac-45ae-b357-69538cff9a3e;nsid=2146147409;c=1636908969933;bpid=BP-686721326-172.21.0.2-1636908969933;dnuuid=null
[36malgo-1    |[0m 2021-11-14 16:56:14,054 INFO datanode.DataNode: Setting up storage: nsid=2146147409;bpid=BP-686721326-172.21.0.2-1636908969933;lv=-57;nsInfo=lv=-65;cid=CID-c577013d-56ac-45ae-b357-69538cff9a3e;nsid=2146147409;c=1636908969933;bpid=BP-686721326-172.21.0.2-1636908969933;dnuuid=null
[33malgo-2    |[0m 2021-11-14 16:56:14,054 INFO datanode.DataNode: Generated and persisted new Datanode UUID 47e75933-6556-4116-9d96-6cf0fb8b6171
[36malgo-1    |[0m 2021-11-14 16:56:14,061 INFO datanode.DataNode: Generated and persisted new Datanode UUID e72fe10b-26e1-41a0-b52e-7374580ea20f
[36malgo-1    |[0m 2021-11-14 16:56:14,094 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 16:56:14,095 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 16:56:14,095 INFO ipc.Server: IPC Server listener on 8033: starting
[36malgo-1    |[0m 2021-11-14 16:56:14,096 INFO resourcemanager.ResourceManager: Transitioning to active state
[36malgo-1    |[0m 2021-11-14 16:56:14,145 INFO recovery.RMStateStore: Updating AMRMToken
[36malgo-1    |[0m 2021-11-14 16:56:14,145 INFO security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
[36malgo-1    |[0m 2021-11-14 16:56:14,145 INFO security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
[36malgo-1    |[0m 2021-11-14 16:56:14,145 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
[36malgo-1    |[0m 2021-11-14 16:56:14,145 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 1
[36malgo-1    |[0m 2021-11-14 16:56:14,145 INFO recovery.RMStateStore: Storing RMDTMasterKey.
[36malgo-1    |[0m 2021-11-14 16:56:14,146 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
[36malgo-1    |[0m 2021-11-14 16:56:14,146 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
[36malgo-1    |[0m 2021-11-14 16:56:14,147 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 2
[36malgo-1    |[0m 2021-11-14 16:56:14,147 INFO recovery.RMStateStore: Storing RMDTMasterKey.
[36malgo-1    |[0m 2021-11-14 16:56:14,149 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler
[33malgo-2    |[0m 2021-11-14 16:56:14,164 INFO impl.FsDatasetImpl: Added new volume: DS-600e61f5-0030-4d5e-b320-43fd0bd3967b
[33malgo-2    |[0m 2021-11-14 16:56:14,164 INFO impl.FsDatasetImpl: Added volume - [DISK]file:/opt/amazon/hadoop/hdfs/datanode, StorageType: DISK
[33malgo-2    |[0m 2021-11-14 16:56:14,173 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean
[33malgo-2    |[0m 2021-11-14 16:56:14,179 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode
[33malgo-2    |[0m 2021-11-14 16:56:14,187 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode
[33malgo-2    |[0m 2021-11-14 16:56:14,189 INFO impl.FsDatasetImpl: Adding block pool BP-686721326-172.21.0.2-1636908969933
[33malgo-2    |[0m 2021-11-14 16:56:14,189 INFO impl.FsDatasetImpl: Scanning block pool BP-686721326-172.21.0.2-1636908969933 on volume /opt/amazon/hadoop/hdfs/datanode...
[36malgo-1    |[0m 2021-11-14 16:56:14,217 INFO impl.FsDatasetImpl: Added new volume: DS-c3d87838-ab4e-4cb9-b23f-6b958f917377
[36malgo-1    |[0m 2021-11-14 16:56:14,217 INFO impl.FsDatasetImpl: Added volume - [DISK]file:/opt/amazon/hadoop/hdfs/datanode, StorageType: DISK
[36malgo-1    |[0m 2021-11-14 16:56:14,225 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean
[36malgo-1    |[0m 2021-11-14 16:56:14,234 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode
[33malgo-2    |[0m 2021-11-14 16:56:14,236 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-686721326-172.21.0.2-1636908969933 on /opt/amazon/hadoop/hdfs/datanode: 46ms
[33malgo-2    |[0m 2021-11-14 16:56:14,236 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-686721326-172.21.0.2-1636908969933: 48ms
[33malgo-2    |[0m 2021-11-14 16:56:14,238 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-686721326-172.21.0.2-1636908969933 on volume /opt/amazon/hadoop/hdfs/datanode...
[33malgo-2    |[0m 2021-11-14 16:56:14,238 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-686721326-172.21.0.2-1636908969933/current/replicas doesn't exist 
[33malgo-2    |[0m 2021-11-14 16:56:14,240 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-686721326-172.21.0.2-1636908969933 on volume /opt/amazon/hadoop/hdfs/datanode: 2ms
[33malgo-2    |[0m 2021-11-14 16:56:14,241 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-686721326-172.21.0.2-1636908969933: 3ms
[33malgo-2    |[0m 2021-11-14 16:56:14,243 INFO datanode.VolumeScanner: Now scanning bpid BP-686721326-172.21.0.2-1636908969933 on volume /opt/amazon/hadoop/hdfs/datanode
[36malgo-1    |[0m 2021-11-14 16:56:14,243 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode
[33malgo-2    |[0m 2021-11-14 16:56:14,245 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-600e61f5-0030-4d5e-b320-43fd0bd3967b): finished scanning block pool BP-686721326-172.21.0.2-1636908969933
[36malgo-1    |[0m 2021-11-14 16:56:14,245 INFO impl.FsDatasetImpl: Adding block pool BP-686721326-172.21.0.2-1636908969933
[36malgo-1    |[0m 2021-11-14 16:56:14,246 INFO impl.FsDatasetImpl: Scanning block pool BP-686721326-172.21.0.2-1636908969933 on volume /opt/amazon/hadoop/hdfs/datanode...
[36malgo-1    |[0m 11-14 16:56 smspark-submit INFO     cluster is up
[36malgo-1    |[0m 11-14 16:56 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING
[36malgo-1    |[0m 11-14 16:56 smspark-submit INFO     starting executor logs watcher
[36malgo-1    |[0m 11-14 16:56 smspark-submit INFO     start log event log publisher
[36malgo-1    |[0m Starting executor logs watcher on log_dir: /var/log/yarn
[36malgo-1    |[0m 11-14 16:56 sagemaker-spark-event-logs-publisher INFO     Spark event log not enabled.
[36malgo-1    |[0m 11-14 16:56 smspark-submit INFO     Waiting for hosts to bootstrap: ['algo-1', 'algo-2']
[33malgo-2    |[0m 2021-11-14 16:56:14,258 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 11/14/21 7:06 PM with interval of 21600000ms
[33malgo-2    |[0m 2021-11-14 16:56:14,258 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-600e61f5-0030-4d5e-b320-43fd0bd3967b): no suitable block pools found to scan.  Waiting 1814399984 ms.
[36malgo-1    |[0m 11-14 16:56 smspark-submit INFO     Received host statuses: dict_items([('algo-1', StatusMessage(status='WAITING', timestamp='2021-11-14T16:56:14.256349')), ('algo-2', StatusMessage(status='WAITING', timestamp='2021-11-14T16:56:14.259860'))])
[33malgo-2    |[0m 2021-11-14 16:56:14,265 INFO datanode.DataNode: Block pool BP-686721326-172.21.0.2-1636908969933 (Datanode Uuid 47e75933-6556-4116-9d96-6cf0fb8b6171) service to algo-1.spark-network/172.21.0.2:8020 beginning handshake with NN
[36malgo-1    |[0m 2021-11-14 16:56:14,292 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-686721326-172.21.0.2-1636908969933 on /opt/amazon/hadoop/hdfs/datanode: 45ms
[36malgo-1    |[0m 2021-11-14 16:56:14,292 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-686721326-172.21.0.2-1636908969933: 46ms
[36malgo-1    |[0m 2021-11-14 16:56:14,294 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-686721326-172.21.0.2-1636908969933 on volume /opt/amazon/hadoop/hdfs/datanode...
[36malgo-1    |[0m 2021-11-14 16:56:14,294 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-686721326-172.21.0.2-1636908969933/current/replicas doesn't exist 
[36malgo-1    |[0m 2021-11-14 16:56:14,295 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-686721326-172.21.0.2-1636908969933 on volume /opt/amazon/hadoop/hdfs/datanode: 2ms
[36malgo-1    |[0m 2021-11-14 16:56:14,296 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-686721326-172.21.0.2-1636908969933: 2ms
[36malgo-1    |[0m 2021-11-14 16:56:14,298 INFO datanode.VolumeScanner: Now scanning bpid BP-686721326-172.21.0.2-1636908969933 on volume /opt/amazon/hadoop/hdfs/datanode
[36malgo-1    |[0m 2021-11-14 16:56:14,299 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-c3d87838-ab4e-4cb9-b23f-6b958f917377): finished scanning block pool BP-686721326-172.21.0.2-1636908969933
[36malgo-1    |[0m 2021-11-14 16:56:14,306 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.21.0.3:9866, datanodeUuid=47e75933-6556-4116-9d96-6cf0fb8b6171, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-c577013d-56ac-45ae-b357-69538cff9a3e;nsid=2146147409;c=1636908969933) storage 47e75933-6556-4116-9d96-6cf0fb8b6171
[36malgo-1    |[0m 2021-11-14 16:56:14,308 INFO net.NetworkTopology: Adding a new node: /default-rack/172.21.0.3:9866
[36malgo-1    |[0m 2021-11-14 16:56:14,308 INFO blockmanagement.BlockReportLeaseManager: Registered DN 47e75933-6556-4116-9d96-6cf0fb8b6171 (172.21.0.3:9866).
[36malgo-1    |[0m 2021-11-14 16:56:14,310 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-c3d87838-ab4e-4cb9-b23f-6b958f917377): no suitable block pools found to scan.  Waiting 1814399987 ms.
[36malgo-1    |[0m 2021-11-14 16:56:14,311 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 11/14/21 10:34 PM with interval of 21600000ms
[36malgo-1    |[0m 2021-11-14 16:56:14,317 INFO datanode.DataNode: Block pool BP-686721326-172.21.0.2-1636908969933 (Datanode Uuid e72fe10b-26e1-41a0-b52e-7374580ea20f) service to algo-1/172.21.0.2:8020 beginning handshake with NN
[33malgo-2    |[0m 2021-11-14 16:56:14,321 INFO datanode.DataNode: Block pool Block pool BP-686721326-172.21.0.2-1636908969933 (Datanode Uuid 47e75933-6556-4116-9d96-6cf0fb8b6171) service to algo-1.spark-network/172.21.0.2:8020 successfully registered with NN
[33malgo-2    |[0m 2021-11-14 16:56:14,321 INFO datanode.DataNode: For namenode algo-1.spark-network/172.21.0.2:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
[36malgo-1    |[0m 2021-11-14 16:56:14,339 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.21.0.2:9866, datanodeUuid=e72fe10b-26e1-41a0-b52e-7374580ea20f, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-c577013d-56ac-45ae-b357-69538cff9a3e;nsid=2146147409;c=1636908969933) storage e72fe10b-26e1-41a0-b52e-7374580ea20f
[36malgo-1    |[0m 2021-11-14 16:56:14,339 INFO net.NetworkTopology: Adding a new node: /default-rack/172.21.0.2:9866
[36malgo-1    |[0m 2021-11-14 16:56:14,340 INFO blockmanagement.BlockReportLeaseManager: Registered DN e72fe10b-26e1-41a0-b52e-7374580ea20f (172.21.0.2:9866).
[36malgo-1    |[0m 2021-11-14 16:56:14,343 INFO datanode.DataNode: Block pool Block pool BP-686721326-172.21.0.2-1636908969933 (Datanode Uuid e72fe10b-26e1-41a0-b52e-7374580ea20f) service to algo-1/172.21.0.2:8020 successfully registered with NN
[36malgo-1    |[0m 2021-11-14 16:56:14,343 INFO datanode.DataNode: For namenode algo-1/172.21.0.2:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
[36malgo-1    |[0m 2021-11-14 16:56:14,369 INFO store.AbstractFSNodeStore: Created store directory :file:/tmp/hadoop-yarn-root/node-attribute
[36malgo-1    |[0m 2021-11-14 16:56:14,379 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-600e61f5-0030-4d5e-b320-43fd0bd3967b for DN 172.21.0.3:9866
[36malgo-1    |[0m 2021-11-14 16:56:14,382 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-c3d87838-ab4e-4cb9-b23f-6b958f917377 for DN 172.21.0.2:9866
[36malgo-1    |[0m 2021-11-14 16:56:14,383 INFO store.AbstractFSNodeStore: Finished write mirror at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.mirror
[36malgo-1    |[0m 2021-11-14 16:56:14,383 INFO store.AbstractFSNodeStore: Finished create editlog file at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.editlog
[36malgo-1    |[0m 2021-11-14 16:56:14,392 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl$ForwardingEventHandler
[36malgo-1    |[0m 2021-11-14 16:56:14,393 INFO placement.MultiNodeSortingManager: Starting NodeSortingService=MultiNodeSortingManager
[36malgo-1    |[0m 2021-11-14 16:56:14,403 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 16:56:14,404 INFO ipc.Server: Starting Socket Reader #1 for port 8031
[36malgo-1    |[0m 2021-11-14 16:56:14,406 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server
[36malgo-1    |[0m 2021-11-14 16:56:14,407 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 16:56:14,407 INFO ipc.Server: IPC Server listener on 8031: starting
[36malgo-1    |[0m 2021-11-14 16:56:14,428 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 16:56:14,436 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 16:56:14,444 INFO ipc.Server: Starting Socket Reader #1 for port 8030
[36malgo-1    |[0m 2021-11-14 16:56:14,455 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 16:56:14,470 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 16:56:14,484 INFO ipc.Server: IPC Server listener on 8030: starting
[36malgo-1    |[0m 2021-11-14 16:56:14,526 INFO BlockStateChange: BLOCK* processReport 0x67e1a2c8e477387e: Processing first storage report for DS-600e61f5-0030-4d5e-b320-43fd0bd3967b from datanode 47e75933-6556-4116-9d96-6cf0fb8b6171
[36malgo-1    |[0m 2021-11-14 16:56:14,527 INFO BlockStateChange: BLOCK* processReport 0x67e1a2c8e477387e: from storage DS-600e61f5-0030-4d5e-b320-43fd0bd3967b node DatanodeRegistration(172.21.0.3:9866, datanodeUuid=47e75933-6556-4116-9d96-6cf0fb8b6171, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-c577013d-56ac-45ae-b357-69538cff9a3e;nsid=2146147409;c=1636908969933), blocks: 0, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0
[36malgo-1    |[0m 2021-11-14 16:56:14,528 INFO BlockStateChange: BLOCK* processReport 0x6be63ae8b170d2e5: Processing first storage report for DS-c3d87838-ab4e-4cb9-b23f-6b958f917377 from datanode e72fe10b-26e1-41a0-b52e-7374580ea20f
[36malgo-1    |[0m 2021-11-14 16:56:14,528 INFO BlockStateChange: BLOCK* processReport 0x6be63ae8b170d2e5: from storage DS-c3d87838-ab4e-4cb9-b23f-6b958f917377 node DatanodeRegistration(172.21.0.2:9866, datanodeUuid=e72fe10b-26e1-41a0-b52e-7374580ea20f, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-c577013d-56ac-45ae-b357-69538cff9a3e;nsid=2146147409;c=1636908969933), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
[33malgo-2    |[0m 2021-11-14 16:56:14,560 INFO datanode.DataNode: Successfully sent block report 0x67e1a2c8e477387e,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 5 msec to generate and 155 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
[36malgo-1    |[0m 2021-11-14 16:56:14,560 INFO datanode.DataNode: Successfully sent block report 0x6be63ae8b170d2e5,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 6 msec to generate and 152 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
[36malgo-1    |[0m 2021-11-14 16:56:14,560 INFO datanode.DataNode: Got finalize command for block pool BP-686721326-172.21.0.2-1636908969933
[33malgo-2    |[0m 2021-11-14 16:56:14,560 INFO datanode.DataNode: Got finalize command for block pool BP-686721326-172.21.0.2-1636908969933
[36malgo-1    |[0m 2021-11-14 16:56:14,585 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 16:56:14,585 INFO ipc.Server: Starting Socket Reader #1 for port 8032
[36malgo-1    |[0m 2021-11-14 16:56:14,589 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 16:56:14,589 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 16:56:14,589 INFO ipc.Server: IPC Server listener on 8032: starting
[36malgo-1    |[0m 2021-11-14 16:56:14,599 INFO resourcemanager.ResourceManager: Transitioned to active state
[33malgo-2    |[0m 2021-11-14 16:56:14,746 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/172.21.0.2:8031. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 16:56:14,928 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-2(cmPort: 45155 httpPort: 8042) registered with capability: <memory:15892, vCores:4>, assigned nodeId algo-2:45155
[36malgo-1    |[0m 2021-11-14 16:56:14,931 INFO rmnode.RMNodeImpl: algo-2:45155 Node Transitioned from NEW to RUNNING
[33malgo-2    |[0m 2021-11-14 16:56:14,945 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id -1852810055
[33malgo-2    |[0m 2021-11-14 16:56:14,946 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id -506695806
[33malgo-2    |[0m 2021-11-14 16:56:14,946 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-2:45155 with total resource of <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 16:56:14,949 INFO capacity.CapacityScheduler: Added node algo-2:45155 clusterResource: <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 16:56:15,101 INFO ipc.Client: Retrying connect to server: algo-1/172.21.0.2:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 16:56:15,111 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-1(cmPort: 33635 httpPort: 8042) registered with capability: <memory:15892, vCores:4>, assigned nodeId algo-1:33635
[36malgo-1    |[0m 2021-11-14 16:56:15,111 INFO rmnode.RMNodeImpl: algo-1:33635 Node Transitioned from NEW to RUNNING
[36malgo-1    |[0m 2021-11-14 16:56:15,113 INFO capacity.CapacityScheduler: Added node algo-1:33635 clusterResource: <memory:31784, vCores:8>
[36malgo-1    |[0m 2021-11-14 16:56:15,121 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id -1852810055
[36malgo-1    |[0m 2021-11-14 16:56:15,122 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id -506695806
[36malgo-1    |[0m 2021-11-14 16:56:15,122 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-1:33635 with total resource of <memory:15892, vCores:4>
[36malgo-1    |[0m Using properties file: /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m Adding default property: spark.driver.host=172.21.0.2
[36malgo-1    |[0m Adding default property: spark.executor.memoryOverhead=1239m
[36malgo-1    |[0m Adding default property: spark.driver.extraLibraryPath=/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m Adding default property: spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[36malgo-1    |[0m Adding default property: spark.rpc.askTimeout=300s
[36malgo-1    |[0m Adding default property: spark.driver.memory=2048m
[36malgo-1    |[0m Adding default property: spark.executor.instances=2
[36malgo-1    |[0m Adding default property: spark.driver.memoryOverhead=204m
[36malgo-1    |[0m Adding default property: key=value
[36malgo-1    |[0m Adding default property: spark.default.parallelism=16
[36malgo-1    |[0m Adding default property: spark.executor.defaultJavaOptions=-verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3
[36malgo-1    |[0m Adding default property: spark.driver.defaultJavaOptions=-XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[36malgo-1    |[0m Adding default property: spark.executor.extraLibraryPath=/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m Adding default property: spark.executor.memory=2g
[36malgo-1    |[0m Adding default property: spark.driver.extraClassPath=/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m Adding default property: spark.executor.extraClassPath=/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m Adding default property: spark.executor.cores=1
[36malgo-1    |[0m Warning: Ignoring non-Spark config property: key
[36malgo-1    |[0m Parsed arguments:
[36malgo-1    |[0m   master                  yarn
[36malgo-1    |[0m   deployMode              client
[36malgo-1    |[0m   executorMemory          2g
[36malgo-1    |[0m   executorCores           1
[36malgo-1    |[0m   totalExecutorCores      null
[36malgo-1    |[0m   propertiesFile          /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m   driverMemory            2048m
[36malgo-1    |[0m   driverCores             null
[36malgo-1    |[0m   driverExtraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m   driverExtraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m   driverExtraJavaOptions  null
[36malgo-1    |[0m   supervise               false
[36malgo-1    |[0m   queue                   null
[36malgo-1    |[0m   numExecutors            2
[36malgo-1    |[0m   files                   null
[36malgo-1    |[0m   pyFiles                 null
[36malgo-1    |[0m   archives                null
[36malgo-1    |[0m   mainClass               com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp
[36malgo-1    |[0m   primaryResource         file:/opt/ml/processing/input/code/scala/hello-scala-spark/target/scala-2.12/hello-scala-spark_2.12-1.0.jar
[36malgo-1    |[0m   name                    com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp
[36malgo-1    |[0m   childArgs               [--input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data]
[36malgo-1    |[0m   jars                    file:/opt/ml/processing/input/jars/json4s-native_2.12-3.6.9.jar
[36malgo-1    |[0m   packages                null
[36malgo-1    |[0m   packagesExclusions      null
[36malgo-1    |[0m   repositories            null
[36malgo-1    |[0m   verbose                 true
[36malgo-1    |[0m 
[36malgo-1    |[0m Spark properties used, including those specified through
[36malgo-1    |[0m  --conf and those from the properties file /usr/lib/spark/conf/spark-defaults.conf:
[36malgo-1    |[0m   (spark.executor.defaultJavaOptions,-verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3)
[36malgo-1    |[0m   (spark.default.parallelism,16)
[36malgo-1    |[0m   (spark.executor.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)
[36malgo-1    |[0m   (spark.executor.memory,2g)
[36malgo-1    |[0m   (spark.driver.memory,2048m)
[36malgo-1    |[0m   (spark.driver.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)
[36malgo-1    |[0m   (spark.executor.instances,2)
[36malgo-1    |[0m   (spark.executor.memoryOverhead,1239m)
[36malgo-1    |[0m   (spark.rpc.askTimeout,300s)
[36malgo-1    |[0m   (spark.driver.memoryOverhead,204m)
[36malgo-1    |[0m   (spark.driver.host,172.21.0.2)
[36malgo-1    |[0m   (spark.driver.defaultJavaOptions,-XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled)
[36malgo-1    |[0m   (spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version,2)
[36malgo-1    |[0m   (spark.executor.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)
[36malgo-1    |[0m   (spark.executor.cores,1)
[36malgo-1    |[0m   (spark.driver.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)
[36malgo-1    |[0m 
[36malgo-1    |[0m     
[36malgo-1    |[0m Main class:
[36malgo-1    |[0m com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp
[36malgo-1    |[0m Arguments:
[36malgo-1    |[0m --input
[36malgo-1    |[0m file:///opt/ml/processing/input/data/data.jsonl
[36malgo-1    |[0m --output
[36malgo-1    |[0m file:///opt/ml/processing/output/data
[36malgo-1    |[0m Spark config:
[36malgo-1    |[0m (spark.driver.host,172.21.0.2)
[36malgo-1    |[0m (spark.executor.memoryOverhead,1239m)
[36malgo-1    |[0m (spark.driver.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)
[36malgo-1    |[0m (spark.jars,file:/opt/ml/processing/input/code/scala/hello-scala-spark/target/scala-2.12/hello-scala-spark_2.12-1.0.jar)
[36malgo-1    |[0m (spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version,2)
[36malgo-1    |[0m (spark.app.name,com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp)
[36malgo-1    |[0m (spark.rpc.askTimeout,300s)
[36malgo-1    |[0m (spark.driver.memory,2048m)
[36malgo-1    |[0m (spark.executor.instances,2)
[36malgo-1    |[0m (spark.submit.pyFiles,)
[36malgo-1    |[0m (spark.driver.memoryOverhead,204m)
[36malgo-1    |[0m (spark.default.parallelism,16)
[36malgo-1    |[0m (spark.executor.defaultJavaOptions,-verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3)
[36malgo-1    |[0m (spark.submit.deployMode,client)
[36malgo-1    |[0m (spark.master,yarn)
[36malgo-1    |[0m (spark.driver.defaultJavaOptions,-XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled)
[36malgo-1    |[0m (spark.executor.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)
[36malgo-1    |[0m (spark.executor.memory,2g)
[36malgo-1    |[0m (spark.driver.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)
[36malgo-1    |[0m (spark.executor.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)
[36malgo-1    |[0m (spark.executor.cores,1)
[36malgo-1    |[0m (spark.repl.local.jars,file:///opt/ml/processing/input/jars/json4s-native_2.12-3.6.9.jar)
[36malgo-1    |[0m (spark.yarn.dist.jars,file:///opt/ml/processing/input/jars/json4s-native_2.12-3.6.9.jar)
[36malgo-1    |[0m Classpath elements:
[36malgo-1    |[0m file:/opt/ml/processing/input/code/scala/hello-scala-spark/target/scala-2.12/hello-scala-spark_2.12-1.0.jar
[36malgo-1    |[0m file:///opt/ml/processing/input/jars/json4s-native_2.12-3.6.9.jar
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m Hello World, this is Scala-Spark!
[36malgo-1    |[0m Parsing command-line args: --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data
[36malgo-1    |[0m Parsed command-line args: {input -> file:///opt/ml/processing/input/data/data.jsonl, output -> file:///opt/ml/processing/output/data}
[36malgo-1    |[0m Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[36malgo-1    |[0m 21/11/14 16:56:16 INFO SparkContext: Running Spark version 3.0.0-amzn-0
[36malgo-1    |[0m 21/11/14 16:56:16 INFO ResourceUtils: ==============================================================
[36malgo-1    |[0m 21/11/14 16:56:16 INFO ResourceUtils: Resources for spark.driver:
[36malgo-1    |[0m 
[36malgo-1    |[0m 21/11/14 16:56:16 INFO ResourceUtils: ==============================================================
[36malgo-1    |[0m 21/11/14 16:56:16 INFO SparkContext: Submitted application: Hello Spark App
[36malgo-1    |[0m 21/11/14 16:56:16 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m 21/11/14 16:56:16 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m 21/11/14 16:56:16 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m 21/11/14 16:56:16 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m 21/11/14 16:56:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m 21/11/14 16:56:16 INFO Utils: Successfully started service 'sparkDriver' on port 44909.
[36malgo-1    |[0m 21/11/14 16:56:16 INFO SparkEnv: Registering MapOutputTracker
[36malgo-1    |[0m 21/11/14 16:56:16 INFO SparkEnv: Registering BlockManagerMaster
[36malgo-1    |[0m 21/11/14 16:56:16 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[36malgo-1    |[0m 21/11/14 16:56:16 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[36malgo-1    |[0m 21/11/14 16:56:17 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[36malgo-1    |[0m 21/11/14 16:56:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-10948434-cbb4-4790-9886-43a03ccc7525
[36malgo-1    |[0m 21/11/14 16:56:17 INFO MemoryStore: MemoryStore started with capacity 1007.8 MiB
[36malgo-1    |[0m 21/11/14 16:56:17 INFO SparkEnv: Registering OutputCommitCoordinator
[36malgo-1    |[0m 21/11/14 16:56:17 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[36malgo-1    |[0m 21/11/14 16:56:17 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.21.0.2:4040
[36malgo-1    |[0m 21/11/14 16:56:17 INFO SparkContext: Added JAR file:/opt/ml/processing/input/code/scala/hello-scala-spark/target/scala-2.12/hello-scala-spark_2.12-1.0.jar at spark://172.21.0.2:44909/jars/hello-scala-spark_2.12-1.0.jar with timestamp 1636908977431
[36malgo-1    |[0m 21/11/14 16:56:17 INFO RMProxy: Connecting to ResourceManager at /172.21.0.2:8032
[36malgo-1    |[0m 21/11/14 16:56:17 INFO Client: Requesting a new application from cluster with 2 NodeManagers
[36malgo-1    |[0m 2021-11-14 16:56:17,959 INFO resourcemanager.ClientRMService: Allocated new applicationId: 1
[36malgo-1    |[0m 21/11/14 16:56:18 INFO Configuration: resource-types.xml not found
[36malgo-1    |[0m 21/11/14 16:56:18 INFO ResourceUtils: Unable to find 'resource-types.xml'.
[36malgo-1    |[0m 21/11/14 16:56:18 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15892 MB per container)
[36malgo-1    |[0m 21/11/14 16:56:18 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
[36malgo-1    |[0m 21/11/14 16:56:18 INFO Client: Setting up container launch context for our AM
[36malgo-1    |[0m 21/11/14 16:56:18 INFO Client: Setting up the launch environment for our AM container
[36malgo-1    |[0m 21/11/14 16:56:18 INFO Client: Preparing resources for our AM container
[36malgo-1    |[0m 21/11/14 16:56:18 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
[36malgo-1    |[0m 21/11/14 16:56:25 INFO Client: Uploading resource file:/tmp/spark-b1d67efe-a586-455c-9d9a-b361b952a4fe/__spark_libs__1335294395360529834.zip -> hdfs://172.21.0.2/user/root/.sparkStaging/application_1636908974098_0001/__spark_libs__1335294395360529834.zip
[36malgo-1    |[0m 2021-11-14 16:56:25,427 INFO hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=172.21.0.2:9866, 172.21.0.3:9866 for /user/root/.sparkStaging/application_1636908974098_0001/__spark_libs__1335294395360529834.zip
[36malgo-1    |[0m 2021-11-14 16:56:25,580 INFO datanode.DataNode: Receiving BP-686721326-172.21.0.2-1636908969933:blk_1073741825_1001 src: /172.21.0.2:37922 dest: /172.21.0.2:9866
[33malgo-2    |[0m 2021-11-14 16:56:25,643 INFO datanode.DataNode: Receiving BP-686721326-172.21.0.2-1636908969933:blk_1073741825_1001 src: /172.21.0.2:36934 dest: /172.21.0.3:9866
[33malgo-2    |[0m 2021-11-14 16:56:26,045 INFO DataNode.clienttrace: src: /172.21.0.2:36934, dest: /172.21.0.3:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_379121905_1, offset: 0, srvID: 47e75933-6556-4116-9d96-6cf0fb8b6171, blockid: BP-686721326-172.21.0.2-1636908969933:blk_1073741825_1001, duration(ns): 380255304
[33malgo-2    |[0m 2021-11-14 16:56:26,046 INFO datanode.DataNode: PacketResponder: BP-686721326-172.21.0.2-1636908969933:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 16:56:26,049 INFO DataNode.clienttrace: src: /172.21.0.2:37922, dest: /172.21.0.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_379121905_1, offset: 0, srvID: e72fe10b-26e1-41a0-b52e-7374580ea20f, blockid: BP-686721326-172.21.0.2-1636908969933:blk_1073741825_1001, duration(ns): 378734974
[36malgo-1    |[0m 2021-11-14 16:56:26,049 INFO datanode.DataNode: PacketResponder: BP-686721326-172.21.0.2-1636908969933:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.21.0.3:9866] terminating
[36malgo-1    |[0m 2021-11-14 16:56:26,056 INFO hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=172.21.0.2:9866, 172.21.0.3:9866 for /user/root/.sparkStaging/application_1636908974098_0001/__spark_libs__1335294395360529834.zip
[36malgo-1    |[0m 2021-11-14 16:56:26,094 INFO datanode.DataNode: Receiving BP-686721326-172.21.0.2-1636908969933:blk_1073741826_1002 src: /172.21.0.2:37926 dest: /172.21.0.2:9866
[33malgo-2    |[0m 2021-11-14 16:56:26,096 INFO datanode.DataNode: Receiving BP-686721326-172.21.0.2-1636908969933:blk_1073741826_1002 src: /172.21.0.2:36938 dest: /172.21.0.3:9866
[33malgo-2    |[0m 2021-11-14 16:56:26,462 INFO DataNode.clienttrace: src: /172.21.0.2:36938, dest: /172.21.0.3:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_379121905_1, offset: 0, srvID: 47e75933-6556-4116-9d96-6cf0fb8b6171, blockid: BP-686721326-172.21.0.2-1636908969933:blk_1073741826_1002, duration(ns): 364019338
[33malgo-2    |[0m 2021-11-14 16:56:26,462 INFO datanode.DataNode: PacketResponder: BP-686721326-172.21.0.2-1636908969933:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 16:56:26,463 INFO DataNode.clienttrace: src: /172.21.0.2:37926, dest: /172.21.0.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_379121905_1, offset: 0, srvID: e72fe10b-26e1-41a0-b52e-7374580ea20f, blockid: BP-686721326-172.21.0.2-1636908969933:blk_1073741826_1002, duration(ns): 364977927
[36malgo-1    |[0m 2021-11-14 16:56:26,463 INFO datanode.DataNode: PacketResponder: BP-686721326-172.21.0.2-1636908969933:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.21.0.3:9866] terminating
[36malgo-1    |[0m 2021-11-14 16:56:26,465 INFO hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=172.21.0.2:9866, 172.21.0.3:9866 for /user/root/.sparkStaging/application_1636908974098_0001/__spark_libs__1335294395360529834.zip
[36malgo-1    |[0m 2021-11-14 16:56:26,507 INFO datanode.DataNode: Receiving BP-686721326-172.21.0.2-1636908969933:blk_1073741827_1003 src: /172.21.0.2:37930 dest: /172.21.0.2:9866
[33malgo-2    |[0m 2021-11-14 16:56:26,509 INFO datanode.DataNode: Receiving BP-686721326-172.21.0.2-1636908969933:blk_1073741827_1003 src: /172.21.0.2:36942 dest: /172.21.0.3:9866
[33malgo-2    |[0m 2021-11-14 16:56:26,801 INFO DataNode.clienttrace: src: /172.21.0.2:36942, dest: /172.21.0.3:9866, bytes: 128628638, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_379121905_1, offset: 0, srvID: 47e75933-6556-4116-9d96-6cf0fb8b6171, blockid: BP-686721326-172.21.0.2-1636908969933:blk_1073741827_1003, duration(ns): 290611171
[33malgo-2    |[0m 2021-11-14 16:56:26,801 INFO datanode.DataNode: PacketResponder: BP-686721326-172.21.0.2-1636908969933:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 16:56:26,802 INFO DataNode.clienttrace: src: /172.21.0.2:37930, dest: /172.21.0.2:9866, bytes: 128628638, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_379121905_1, offset: 0, srvID: e72fe10b-26e1-41a0-b52e-7374580ea20f, blockid: BP-686721326-172.21.0.2-1636908969933:blk_1073741827_1003, duration(ns): 291704359
[36malgo-1    |[0m 2021-11-14 16:56:26,802 INFO datanode.DataNode: PacketResponder: BP-686721326-172.21.0.2-1636908969933:blk_1073741827_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.21.0.3:9866] terminating
[36malgo-1    |[0m 2021-11-14 16:56:26,807 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1636908974098_0001/__spark_libs__1335294395360529834.zip is closed by DFSClient_NONMAPREDUCE_379121905_1
[36malgo-1    |[0m 21/11/14 16:56:26 INFO Client: Uploading resource file:/opt/ml/processing/input/jars/json4s-native_2.12-3.6.9.jar -> hdfs://172.21.0.2/user/root/.sparkStaging/application_1636908974098_0001/json4s-native_2.12-3.6.9.jar
[36malgo-1    |[0m 2021-11-14 16:56:26,904 INFO hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=172.21.0.2:9866, 172.21.0.3:9866 for /user/root/.sparkStaging/application_1636908974098_0001/json4s-native_2.12-3.6.9.jar
[36malgo-1    |[0m 2021-11-14 16:56:26,907 INFO datanode.DataNode: Receiving BP-686721326-172.21.0.2-1636908969933:blk_1073741828_1004 src: /172.21.0.2:37934 dest: /172.21.0.2:9866
[33malgo-2    |[0m 2021-11-14 16:56:26,908 INFO datanode.DataNode: Receiving BP-686721326-172.21.0.2-1636908969933:blk_1073741828_1004 src: /172.21.0.2:36946 dest: /172.21.0.3:9866
[33malgo-2    |[0m 2021-11-14 16:56:26,912 INFO DataNode.clienttrace: src: /172.21.0.2:36946, dest: /172.21.0.3:9866, bytes: 93283, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_379121905_1, offset: 0, srvID: 47e75933-6556-4116-9d96-6cf0fb8b6171, blockid: BP-686721326-172.21.0.2-1636908969933:blk_1073741828_1004, duration(ns): 2272277
[33malgo-2    |[0m 2021-11-14 16:56:26,912 INFO datanode.DataNode: PacketResponder: BP-686721326-172.21.0.2-1636908969933:blk_1073741828_1004, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 16:56:26,913 INFO DataNode.clienttrace: src: /172.21.0.2:37934, dest: /172.21.0.2:9866, bytes: 93283, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_379121905_1, offset: 0, srvID: e72fe10b-26e1-41a0-b52e-7374580ea20f, blockid: BP-686721326-172.21.0.2-1636908969933:blk_1073741828_1004, duration(ns): 3093428
[36malgo-1    |[0m 2021-11-14 16:56:26,913 INFO datanode.DataNode: PacketResponder: BP-686721326-172.21.0.2-1636908969933:blk_1073741828_1004, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.21.0.3:9866] terminating
[36malgo-1    |[0m 2021-11-14 16:56:26,914 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1636908974098_0001/json4s-native_2.12-3.6.9.jar is closed by DFSClient_NONMAPREDUCE_379121905_1
[36malgo-1    |[0m 21/11/14 16:56:27 INFO Client: Uploading resource file:/tmp/spark-b1d67efe-a586-455c-9d9a-b361b952a4fe/__spark_conf__7542882520396414211.zip -> hdfs://172.21.0.2/user/root/.sparkStaging/application_1636908974098_0001/__spark_conf__.zip
[36malgo-1    |[0m 2021-11-14 16:56:27,034 INFO hdfs.StateChange: BLOCK* allocate blk_1073741829_1005, replicas=172.21.0.2:9866, 172.21.0.3:9866 for /user/root/.sparkStaging/application_1636908974098_0001/__spark_conf__.zip
[36malgo-1    |[0m 2021-11-14 16:56:27,038 INFO datanode.DataNode: Receiving BP-686721326-172.21.0.2-1636908969933:blk_1073741829_1005 src: /172.21.0.2:37940 dest: /172.21.0.2:9866
[33malgo-2    |[0m 2021-11-14 16:56:27,039 INFO datanode.DataNode: Receiving BP-686721326-172.21.0.2-1636908969933:blk_1073741829_1005 src: /172.21.0.2:36952 dest: /172.21.0.3:9866
[33malgo-2    |[0m 2021-11-14 16:56:27,043 INFO DataNode.clienttrace: src: /172.21.0.2:36952, dest: /172.21.0.3:9866, bytes: 252264, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_379121905_1, offset: 0, srvID: 47e75933-6556-4116-9d96-6cf0fb8b6171, blockid: BP-686721326-172.21.0.2-1636908969933:blk_1073741829_1005, duration(ns): 2376859
[33malgo-2    |[0m 2021-11-14 16:56:27,043 INFO datanode.DataNode: PacketResponder: BP-686721326-172.21.0.2-1636908969933:blk_1073741829_1005, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 16:56:27,044 INFO DataNode.clienttrace: src: /172.21.0.2:37940, dest: /172.21.0.2:9866, bytes: 252264, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_379121905_1, offset: 0, srvID: e72fe10b-26e1-41a0-b52e-7374580ea20f, blockid: BP-686721326-172.21.0.2-1636908969933:blk_1073741829_1005, duration(ns): 3318542
[36malgo-1    |[0m 2021-11-14 16:56:27,044 INFO datanode.DataNode: PacketResponder: BP-686721326-172.21.0.2-1636908969933:blk_1073741829_1005, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.21.0.3:9866] terminating
[36malgo-1    |[0m 2021-11-14 16:56:27,045 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1636908974098_0001/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_379121905_1
[36malgo-1    |[0m 21/11/14 16:56:27 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m 21/11/14 16:56:27 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m 21/11/14 16:56:27 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m 21/11/14 16:56:27 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m 21/11/14 16:56:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m 21/11/14 16:56:27 INFO Client: Submitting application application_1636908974098_0001 to ResourceManager
[36malgo-1    |[0m 2021-11-14 16:56:27,172 INFO capacity.CapacityScheduler: Application 'application_1636908974098_0001' is submitted without priority hence considering default queue/cluster priority: 0
[36malgo-1    |[0m 2021-11-14 16:56:27,172 INFO capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1636908974098_0001
[36malgo-1    |[0m 2021-11-14 16:56:27,189 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 1]. Use the global max attempts instead.
[36malgo-1    |[0m 2021-11-14 16:56:27,190 INFO resourcemanager.ClientRMService: Application with id 1 submitted by user root
[36malgo-1    |[0m 2021-11-14 16:56:27,191 INFO rmapp.RMAppImpl: Storing application with id application_1636908974098_0001
[36malgo-1    |[0m 2021-11-14 16:56:27,192 INFO resourcemanager.RMAuditLogger: USER=root	IP=172.21.0.2	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1636908974098_0001	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 16:56:27,197 INFO recovery.RMStateStore: Storing info for app: application_1636908974098_0001
[36malgo-1    |[0m 2021-11-14 16:56:27,197 INFO rmapp.RMAppImpl: application_1636908974098_0001 State change from NEW to NEW_SAVING on event = START
[36malgo-1    |[0m 2021-11-14 16:56:27,198 INFO rmapp.RMAppImpl: application_1636908974098_0001 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
[36malgo-1    |[0m 2021-11-14 16:56:27,199 INFO capacity.ParentQueue: Application added - appId: application_1636908974098_0001 user: root leaf-queue of parent: root #applications: 1
[36malgo-1    |[0m 2021-11-14 16:56:27,200 INFO capacity.CapacityScheduler: Accepted application application_1636908974098_0001 from user: root, in queue: default
[36malgo-1    |[0m 2021-11-14 16:56:27,211 INFO rmapp.RMAppImpl: application_1636908974098_0001 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
[36malgo-1    |[0m 2021-11-14 16:56:27,238 INFO resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1636908974098_0001_000001
[36malgo-1    |[0m 2021-11-14 16:56:27,239 INFO attempt.RMAppAttemptImpl: appattempt_1636908974098_0001_000001 State change from NEW to SUBMITTED on event = START
[36malgo-1    |[0m 2021-11-14 16:56:27,289 INFO capacity.LeafQueue: Application application_1636908974098_0001 from user: root activated in queue: default
[36malgo-1    |[0m 2021-11-14 16:56:27,289 INFO capacity.LeafQueue: Application added - appId: application_1636908974098_0001 user: root, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
[36malgo-1    |[0m 2021-11-14 16:56:27,289 INFO capacity.CapacityScheduler: Added Application Attempt appattempt_1636908974098_0001_000001 to scheduler from user root in queue default
[36malgo-1    |[0m 21/11/14 16:56:27 INFO YarnClientImpl: Submitted application application_1636908974098_0001
[36malgo-1    |[0m 2021-11-14 16:56:27,296 INFO attempt.RMAppAttemptImpl: appattempt_1636908974098_0001_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
[36malgo-1    |[0m 2021-11-14 16:56:28,049 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1636908974098_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=
[36malgo-1    |[0m 2021-11-14 16:56:28,053 INFO rmcontainer.RMContainerImpl: container_1636908974098_0001_01_000001 Container Transitioned from NEW to ALLOCATED
[36malgo-1    |[0m 2021-11-14 16:56:28,053 INFO fica.FiCaSchedulerNode: Assigned container container_1636908974098_0001_01_000001 of capacity <memory:896, vCores:1> on host algo-2:45155, which has 1 containers, <memory:896, vCores:1> used and <memory:14996, vCores:3> available after allocation
[36malgo-1    |[0m 2021-11-14 16:56:28,053 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636908974098_0001	CONTAINERID=container_1636908974098_0001_01_000001	RESOURCE=<memory:896, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 16:56:28,071 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-2:45155 for container : container_1636908974098_0001_01_000001
[36malgo-1    |[0m 2021-11-14 16:56:28,078 INFO rmcontainer.RMContainerImpl: container_1636908974098_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
[36malgo-1    |[0m 2021-11-14 16:56:28,078 INFO security.NMTokenSecretManagerInRM: Clear node set for appattempt_1636908974098_0001_000001
[36malgo-1    |[0m 2021-11-14 16:56:28,078 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.028190285 absoluteUsedCapacity=0.028190285 used=<memory:896, vCores:1> cluster=<memory:31784, vCores:8>
[36malgo-1    |[0m 2021-11-14 16:56:28,078 INFO attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1636908974098_0001 AttemptId: appattempt_1636908974098_0001_000001 MasterContainer: Container: [ContainerId: container_1636908974098_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-2:45155, NodeHttpAddress: algo-2:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 172.21.0.3:45155 }, ExecutionType: GUARANTEED, ]
[36malgo-1    |[0m 2021-11-14 16:56:28,078 INFO capacity.CapacityScheduler: Allocation proposal accepted
[36malgo-1    |[0m 2021-11-14 16:56:28,089 INFO attempt.RMAppAttemptImpl: appattempt_1636908974098_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
[36malgo-1    |[0m 2021-11-14 16:56:28,093 INFO attempt.RMAppAttemptImpl: appattempt_1636908974098_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
[36malgo-1    |[0m 2021-11-14 16:56:28,105 INFO amlauncher.AMLauncher: Launching masterappattempt_1636908974098_0001_000001
[36malgo-1    |[0m 2021-11-14 16:56:28,162 INFO amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1636908974098_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-2:45155, NodeHttpAddress: algo-2:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 172.21.0.3:45155 }, ExecutionType: GUARANTEED, ] for AM appattempt_1636908974098_0001_000001
[36malgo-1    |[0m 2021-11-14 16:56:28,162 INFO security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1636908974098_0001_000001
[36malgo-1    |[0m 2021-11-14 16:56:28,165 INFO security.AMRMTokenSecretManager: Creating password for appattempt_1636908974098_0001_000001
[33malgo-2    |[0m 2021-11-14 16:56:28,283 INFO ipc.Server: Auth successful for appattempt_1636908974098_0001_000001 (auth:SIMPLE)
[36malgo-1    |[0m 21/11/14 16:56:28 INFO Client: Application report for application_1636908974098_0001 (state: ACCEPTED)
[36malgo-1    |[0m 21/11/14 16:56:28 INFO Client: 
[36malgo-1    |[0m 	 client token: N/A
[36malgo-1    |[0m 	 diagnostics: [Sun Nov 14 16:56:28 +0000 2021] Scheduler has assigned a container for AM, waiting for AM container to be launched
[36malgo-1    |[0m 	 ApplicationMaster host: N/A
[36malgo-1    |[0m 	 ApplicationMaster RPC port: -1
[36malgo-1    |[0m 	 queue: default
[36malgo-1    |[0m 	 start time: 1636908987189
[36malgo-1    |[0m 	 final status: UNDEFINED
[36malgo-1    |[0m 	 tracking URL: http://algo-1:8088/proxy/application_1636908974098_0001/
[36malgo-1    |[0m 	 user: root
[33malgo-2    |[0m 2021-11-14 16:56:28,367 INFO containermanager.ContainerManagerImpl: Start request for container_1636908974098_0001_01_000001 by user root
[33malgo-2    |[0m 2021-11-14 16:56:28,414 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1636908974098_0001
[33malgo-2    |[0m 2021-11-14 16:56:28,421 INFO application.ApplicationImpl: Application application_1636908974098_0001 transitioned from NEW to INITING
[33malgo-2    |[0m 2021-11-14 16:56:28,421 INFO application.ApplicationImpl: Adding container_1636908974098_0001_01_000001 to application application_1636908974098_0001
[33malgo-2    |[0m 2021-11-14 16:56:28,422 INFO nodemanager.NMAuditLogger: USER=root	IP=172.21.0.2	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1636908974098_0001	CONTAINERID=container_1636908974098_0001_01_000001
[33malgo-2    |[0m 2021-11-14 16:56:28,425 INFO application.ApplicationImpl: Application application_1636908974098_0001 transitioned from INITING to RUNNING
[33malgo-2    |[0m 2021-11-14 16:56:28,428 INFO container.ContainerImpl: Container container_1636908974098_0001_01_000001 transitioned from NEW to LOCALIZING
[33malgo-2    |[0m 2021-11-14 16:56:28,428 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1636908974098_0001
[36malgo-1    |[0m 2021-11-14 16:56:28,434 INFO amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1636908974098_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-2:45155, NodeHttpAddress: algo-2:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 172.21.0.3:45155 }, ExecutionType: GUARANTEED, ] for AM appattempt_1636908974098_0001_000001
[36malgo-1    |[0m 2021-11-14 16:56:28,435 INFO attempt.RMAppAttemptImpl: appattempt_1636908974098_0001_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
[36malgo-1    |[0m 2021-11-14 16:56:28,435 INFO rmapp.RMAppImpl: update the launch time for applicationId: application_1636908974098_0001, attemptId: appattempt_1636908974098_0001_000001launchTime: 1636908988435
[36malgo-1    |[0m 2021-11-14 16:56:28,435 INFO recovery.RMStateStore: Updating info for app: application_1636908974098_0001
[33malgo-2    |[0m 2021-11-14 16:56:28,437 INFO localizer.ResourceLocalizationService: Created localizer for container_1636908974098_0001_01_000001
[33malgo-2    |[0m 2021-11-14 16:56:28,500 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1636908974098_0001_01_000001.tokens
[33malgo-2    |[0m 2021-11-14 16:56:28,510 INFO nodemanager.DefaultContainerExecutor: Initializing user root
[33malgo-2    |[0m 2021-11-14 16:56:28,515 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1636908974098_0001_01_000001.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000001.tokens
[33malgo-2    |[0m 2021-11-14 16:56:28,515 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001
[36malgo-1    |[0m 2021-11-14 16:56:29,044 INFO rmcontainer.RMContainerImpl: container_1636908974098_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING
[36malgo-1    |[0m 21/11/14 16:56:29 INFO Client: Application report for application_1636908974098_0001 (state: ACCEPTED)
[36malgo-1    |[0m 21/11/14 16:56:30 INFO Client: Application report for application_1636908974098_0001 (state: ACCEPTED)
[36malgo-1    |[0m 21/11/14 16:56:31 INFO Client: Application report for application_1636908974098_0001 (state: ACCEPTED)
[33malgo-2    |[0m 2021-11-14 16:56:31,779 INFO container.ContainerImpl: Container container_1636908974098_0001_01_000001 transitioned from LOCALIZING to SCHEDULED
[33malgo-2    |[0m 2021-11-14 16:56:31,780 INFO scheduler.ContainerScheduler: Starting container [container_1636908974098_0001_01_000001]
[33malgo-2    |[0m 2021-11-14 16:56:31,810 INFO container.ContainerImpl: Container container_1636908974098_0001_01_000001 transitioned from SCHEDULED to RUNNING
[33malgo-2    |[0m 2021-11-14 16:56:31,810 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1636908974098_0001_01_000001
[33malgo-2    |[0m 2021-11-14 16:56:31,814 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000001/default_container_executor.sh]
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/prelaunch.out
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/prelaunch.err
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/launch_container.sh
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stdout
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr
[36malgo-1    |[0m 21/11/14 16:56:32 INFO Client: Application report for application_1636908974098_0001 (state: ACCEPTED)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1551128  132 -r-x------   1 root     root       131590 Nov 14 16:56 ./__spark_libs__/hk2-utils-2.6.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1551129  164 -r-x------   1 root     root       167761 Nov 14 16:56 ./__spark_libs__/antlr-runtime-3.5.2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1551130   72 -r-x------   1 root     root        71626 Nov 14 16:56 ./__spark_libs__/commons-compiler-3.0.16.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1551131 1076 -r-x------   1 root     root      1098934 Nov 14 16:56 ./__spark_libs__/parquet-column-1.10.1-spark-amzn-2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553024  700 -r-x------   1 root     root       714238 Nov 14 16:56 ./__spark_libs__/aws-java-sdk-robomaker-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1551132  280 -r-x------   1 root     root       286277 Nov 14 16:56 ./__spark_libs__/parquet-hadoop-1.10.1-spark-amzn-2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1551133 2140 -r-x------   1 root     root      2189117 Nov 14 16:56 ./__spark_libs__/guava-14.0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1551134   36 -r-x------   1 root     root        33786 Nov 14 16:56 ./__spark_libs__/machinist_2.12-0.6.8.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553025  392 -r-x------   1 root     root       398971 Nov 14 16:56 ./__spark_libs__/aws-java-sdk-fms-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1551135  112 -r-x------   1 root     root       112235 Nov 14 16:56 ./__spark_libs__/scala-collection-compat_2.12-2.1.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1551136  304 -r-x------   1 root     root       310891 Nov 14 16:56 ./__spark_libs__/netlib-native_system-linux-armhf-1.1-natives.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553026  148 -r-x------   1 root     root       149482 Nov 14 16:56 ./__spark_libs__/aws-java-sdk-codestarconnections-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1551137  120 -r-x------   1 root     root       120316 Nov 14 16:56 ./__spark_libs__/json-smart-2.3.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1551138   20 -r-x------   1 root     root        16537 Nov 14 16:56 ./__spark_libs__/jcl-over-slf4j-1.7.30.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553027  468 -r-x------   1 root     root       477556 Nov 14 16:56 ./__spark_libs__/aws-java-sdk-machinelearning-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1551139  908 -r-x------   1 root     root       927721 Nov 14 16:56 ./__spark_libs__/jersey-server-2.30.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1551140  100 -r-x------   1 root     root       102174 Nov 14 16:56 ./__spark_libs__/kerby-asn1-1.0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553028  728 -r-x------   1 root     root       741552 Nov 14 16:56 ./__spark_libs__/aws-java-sdk-elasticsearch-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553029  624 -r-x------   1 root     root       637712 Nov 14 16:56 ./__spark_libs__/aws-java-sdk-pinpointemail-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1551141  344 -r-x------   1 root     root       348635 Nov 14 16:56 ./__spark_libs__/jackson-core-2.10.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1551142 1404 -r-x------   1 root     root      1437215 Nov 14 16:56 ./__spark_libs__/arrow-vector-0.15.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553030  388 -r-x------   1 root     root       396097 Nov 14 16:56 ./__spark_libs__/aws-java-sdk-elasticloadbalancing-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1551143   16 -r-x------   1 root     root        15071 Nov 14 16:56 ./__spark_libs__/transaction-api-1.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553031  364 -r-x------   1 root     root       370023 Nov 14 16:56 ./__spark_libs__/aws-java-sdk-ram-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553032  356 -r-x------   1 root     root       364262 Nov 14 16:56 ./__spark_libs__/aws-java-sdk-globalaccelerator-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1551144   48 -r-x------   1 root     root        46646 Nov 14 16:56 ./__spark_libs__/jackson-dataformat-yaml-2.10.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1551145  264 -r-x------   1 root     root       268780 Nov 14 16:56 ./__spark_libs__/jline-2.14.6.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553033  204 -r-x------   1 root     root       205056 Nov 14 16:56 ./__spark_libs__/aws-java-sdk-codestarnotifications-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553035    4 drwx------   3 root     root         4096 Nov 14 16:56 ./__spark_conf__
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553064    4 -r-x------   1 root     root          639 Nov 14 16:56 ./__spark_conf__/__spark_dist_cache__.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553037    4 -r-x------   1 root     root           10 Nov 14 16:56 ./__spark_conf__/metrics.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553062  128 -r-x------   1 root     root       130388 Nov 14 16:56 ./__spark_conf__/__spark_hadoop_conf__.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553036    4 -r-x------   1 root     root           10 Nov 14 16:56 ./__spark_conf__/log4j.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553038    4 drwx------   2 root     root         4096 Nov 14 16:56 ./__spark_conf__/__hadoop_conf__
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553050    4 -r-x------   1 root     root         2979 Nov 14 16:56 ./__spark_conf__/__hadoop_conf__/hdfs-site.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553046    4 -r-x------   1 root     root         1159 Nov 14 16:56 ./__spark_conf__/__hadoop_conf__/core-site.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553055    4 -r-x------   1 root     root         2316 Nov 14 16:56 ./__spark_conf__/__hadoop_conf__/ssl-client.xml.example
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553048    4 -r-x------   1 root     root         3593 Nov 14 16:56 ./__spark_conf__/__hadoop_conf__/container-log4j.properties.default
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553061    8 -r-x------   1 root     root         4113 Nov 14 16:56 ./__spark_conf__/__hadoop_conf__/mapred-queues.xml.template
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553054    4 -r-x------   1 root     root          188 Nov 14 16:56 ./__spark_conf__/__hadoop_conf__/hive-site.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553043    4 -r-x------   1 root     root         1973 Nov 14 16:56 ./__spark_conf__/__hadoop_conf__/yarn-site.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553042    4 -r-x------   1 root     root         3321 Nov 14 16:56 ./__spark_conf__/__hadoop_conf__/hadoop-metrics2.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553051    8 -r-x------   1 root     root         6174 Nov 14 16:56 ./__spark_conf__/__hadoop_conf__/yarn-env.sh
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553059    4 -r-x------   1 root     root         2697 Nov 14 16:56 ./__spark_conf__/__hadoop_conf__/ssl-server.xml.example
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553056   16 -r-x------   1 root     root        14890 Nov 14 16:56 ./__spark_conf__/__hadoop_conf__/log4j.properties.default
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553044   12 -r-x------   1 root     root         8260 Nov 14 16:56 ./__spark_conf__/__hadoop_conf__/capacity-scheduler.xml.default
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553041   20 -r-x------   1 root     root        16403 Nov 14 16:56 ./__spark_conf__/__hadoop_conf__/hadoop-env.sh
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553053    4 -r-x------   1 root     root         1764 Nov 14 16:56 ./__spark_conf__/__hadoop_conf__/mapred-env.sh
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553040    4 -r-x------   1 root     root          758 Nov 14 16:56 ./__spark_conf__/__hadoop_conf__/mapred-site.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553039   16 -r-x------   1 root     root        14900 Nov 14 16:56 ./__spark_conf__/__hadoop_conf__/log4j.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553047   12 -r-x------   1 root     root         8260 Nov 14 16:56 ./__spark_conf__/__hadoop_conf__/capacity-scheduler.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 1553052    4 -r-x------   1 root     root         1940 Nov 14 16:56 ./__spark_conf__/__hadoop_conf__/container-executor.cfg
[36malgo-1    |[0m 21/11/14 16:56:33 INFO Client: Application report for application_1636908974098_0001 (state: ACCEPTED)
[36malgo-1    |[0m 2021-11-14 16:56:34,055 INFO ipc.Server: Auth successful for appattempt_1636908974098_0001_000001 (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 16:56:34,094 INFO resourcemanager.DefaultAMSProcessor: AM registration appattempt_1636908974098_0001_000001
[36malgo-1    |[0m 2021-11-14 16:56:34,095 INFO resourcemanager.RMAuditLogger: USER=root	IP=172.21.0.3	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1636908974098_0001	APPATTEMPTID=appattempt_1636908974098_0001_000001
[36malgo-1    |[0m 2021-11-14 16:56:34,095 INFO attempt.RMAppAttemptImpl: appattempt_1636908974098_0001_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
[36malgo-1    |[0m 2021-11-14 16:56:34,095 INFO rmapp.RMAppImpl: application_1636908974098_0001 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/directory.info] 15530602021-11-14 16:56:34,105 INFO monitor.ContainersMonitorImpl: container_1636908974098_0001_01_000001's ip = 172.21.0.3, and hostname = algo-2
[33malgo-2    |[0m 2021-11-14 16:56:34,111 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1636908974098_0001_01_000001 since CPU usage is not yet available.
[36malgo-1    |[0m 21/11/14 16:56:34 INFO Client: Application report for application_1636908974098_0001 (state: RUNNING)
[36malgo-1    |[0m 21/11/14 16:56:34 INFO Client: 
[36malgo-1    |[0m 	 client token: N/A
[36malgo-1    |[0m 	 diagnostics: N/A
[36malgo-1    |[0m 	 ApplicationMaster host: 172.21.0.3
[36malgo-1    |[0m 	 ApplicationMaster RPC port: -1
[36malgo-1    |[0m 	 queue: default
[36malgo-1    |[0m 	 start time: 1636908987189
[36malgo-1    |[0m 	 final status: UNDEFINED
[36malgo-1    |[0m 	 tracking URL: http://algo-1:8088/proxy/application_1636908974098_0001/
[36malgo-1    |[0m 	 user: root
[36malgo-1    |[0m 21/11/14 16:56:34 INFO YarnClientSchedulerBackend: Application application_1636908974098_0001 has started running.
[36malgo-1    |[0m 21/11/14 16:56:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46499.
[36malgo-1    |[0m 21/11/14 16:56:34 INFO NettyBlockTransferService: Server created on 172.21.0.2:46499
[36malgo-1    |[0m 21/11/14 16:56:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[36malgo-1    |[0m 21/11/14 16:56:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.21.0.2, 46499, None)
[36malgo-1    |[0m 21/11/14 16:56:34 INFO BlockManagerMasterEndpoint: Registering block manager 172.21.0.2:46499 with 1007.8 MiB RAM, BlockManagerId(driver, 172.21.0.2, 46499, None)
[36malgo-1    |[0m 21/11/14 16:56:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.21.0.2, 46499, None)
[36malgo-1    |[0m 21/11/14 16:56:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.21.0.2, 46499, None)
[36malgo-1    |[0m 21/11/14 16:56:34 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1.spark-network, PROXY_URI_BASES -> http://algo-1.spark-network:8088/proxy/application_1636908974098_0001), /proxy/application_1636908974098_0001
[36malgo-1    |[0m 21/11/14 16:56:34 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 16:56:34 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[36malgo-1    |[0m Got a Spark session with version: 3.0.0-amzn-0
[36malgo-1    |[0m 21/11/14 16:56:35 INFO SparkContext: Starting job: foreach at HelloScalaSparkApp.scala:39
[36malgo-1    |[0m 21/11/14 16:56:35 INFO DAGScheduler: Got job 0 (foreach at HelloScalaSparkApp.scala:39) with 16 output partitions
[36malgo-1    |[0m 21/11/14 16:56:35 INFO DAGScheduler: Final stage: ResultStage 0 (foreach at HelloScalaSparkApp.scala:39)
[36malgo-1    |[0m 21/11/14 16:56:35 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:56:35 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:56:35 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at HelloScalaSparkApp.scala:39), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:56:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.3 KiB, free 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:35 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1410.0 B, free 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.21.0.2:46499 (size: 1410.0 B, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:35 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:56:35 INFO DAGScheduler: Submitting 16 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at HelloScalaSparkApp.scala:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[36malgo-1    |[0m 21/11/14 16:56:35 INFO YarnScheduler: Adding task set 0.0 with 16 tasks
[36malgo-1    |[0m 21/11/14 16:56:35 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr] Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr] 21/11/14 16:56:32 INFO SignalUtils: Registered signal handler for TERM
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr] 21/11/14 16:56:32 INFO SignalUtils: Registered signal handler for HUP
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr] 21/11/14 16:56:32 INFO SignalUtils: Registered signal handler for INT
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr] 21/11/14 16:56:33 INFO SecurityManager: Changing view acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr] 21/11/14 16:56:33 INFO SecurityManager: Changing modify acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr] 21/11/14 16:56:33 INFO SecurityManager: Changing view acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr] 21/11/14 16:56:33 INFO SecurityManager: Changing modify acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr] 21/11/14 16:56:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr] 21/11/14 16:56:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr] 21/11/14 16:56:33 INFO ApplicationMaster: ApplicationAttemptId: appattempt_1636908974098_0001_000001
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr] 21/11/14 16:56:33 INFO RMProxy: Connecting to ResourceManager at /172.21.0.2:8030
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr] 21/11/14 16:56:33 INFO YarnRMClient: Registering the ApplicationMaster
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr] 21/11/14 16:56:34 INFO TransportClientFactory: Successfully created connection to /172.21.0.2:44909 after 83 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr] 21/11/14 16:56:34 INFO ApplicationMaster: Preparing Local resources
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr] 21/11/14 16:56:35 INFO ApplicationMaster: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr] ===============================================================================
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr] Default YARN executor launch context:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]   env:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]     CLASSPATH -> /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]     SPARK_YARN_STAGING_DIR -> hdfs://172.21.0.2/user/root/.sparkStaging/application_1636908974098_0001
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]     SPARK_USER -> root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]   command:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]     LD_LIBRARY_PATH=\"/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:$LD_LIBRARY_PATH\" \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       {{JAVA_HOME}}/bin/java \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       -server \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       -Xmx2048m \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       '-verbose:gc' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       '-XX:OnOutOfMemoryError=kill -9 %p' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       '-XX:+PrintGCDetails' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       '-XX:+PrintGCDateStamps' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       '-XX:+UseParallelGC' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       '-XX:InitiatingHeapOccupancyPercent=70' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       '-XX:ConcGCThreads=1' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       '-XX:ParallelGCThreads=3' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       -Djava.io.tmpdir={{PWD}}/tmp \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       '-Dspark.driver.port=44909' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       '-Dspark.rpc.askTimeout=300s' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       -Dspark.yarn.app.container.log.dir=<LOG_DIR> \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       org.apache.spark.executor.YarnCoarseGrainedExecutorBackend \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       --driver-url \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       spark://CoarseGrainedScheduler@172.21.0.2:44909 \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       --executor-id \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       <executorId> \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       --hostname \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       <hostname> \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       --cores \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       1 \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       --app-id \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       application_1636908974098_0001 \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       --resourceProfileId \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       0 \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       --user-class-path \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       file:$PWD/__app__.jar \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       --user-class-path \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       file:$PWD/json4s-native_2.12-3.6.9.jar \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       1><LOG_DIR>/stdout \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]       2><LOG_DIR>/stderr
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]   resources:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]     __spark_libs__ -> resource { scheme: "hdfs" host: "172.21.0.2" port: -1 file: "/user/root/.sparkStaging/application_1636908974098_0001/__spark_libs__1335294395360529834.zip" } size: 397064094 timestamp: 1636908986807 type: ARCHIVE visibility: PRIVATE
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]     __spark_conf__ -> resource { scheme: "hdfs" host: "172.21.0.2" port: -1 file: "/user/root/.sparkStaging/application_1636908974098_0001/__spark_conf__.zip" } size: 252264 timestamp: 1636908987045 type: ARCHIVE visibility: PRIVATE
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr]     json4s-native_2.12-3.6.9.jar -> resource { scheme: "hdfs" host: "172.21.0.2" port: -1 file: "/user/root/.sparkStaging/application_1636908974098_0001/json4s-native_2.12-3.6.9.jar" } size: 93283 timestamp: 1636908986914 type: FILE visibility: PRIVATE
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr] ===============================================================================
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr] 21/11/14 16:56:35 INFO Configuration: resource-types.xml not found
[36malgo-1    |[0m 2021-11-14 16:56:36,062 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1636908974098_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=
[36malgo-1    |[0m 2021-11-14 16:56:36,063 INFO rmcontainer.RMContainerImpl: container_1636908974098_0001_01_000002 Container Transitioned from NEW to ALLOCATED
[36malgo-1    |[0m 2021-11-14 16:56:36,063 INFO fica.FiCaSchedulerNode: Assigned container container_1636908974098_0001_01_000002 of capacity <memory:3287, vCores:1> on host algo-2:45155, which has 2 containers, <memory:4183, vCores:2> used and <memory:11709, vCores:2> available after allocation
[36malgo-1    |[0m 2021-11-14 16:56:36,063 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636908974098_0001	CONTAINERID=container_1636908974098_0001_01_000002	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 16:56:36,063 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.1316071 absoluteUsedCapacity=0.1316071 used=<memory:4183, vCores:2> cluster=<memory:31784, vCores:8>
[36malgo-1    |[0m 2021-11-14 16:56:36,063 INFO capacity.CapacityScheduler: Allocation proposal accepted
[36malgo-1    |[0m 2021-11-14 16:56:36,138 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-2:45155 for container : container_1636908974098_0001_01_000002
[36malgo-1    |[0m 2021-11-14 16:56:36,139 INFO rmcontainer.RMContainerImpl: container_1636908974098_0001_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
[36malgo-1    |[0m 2021-11-14 16:56:36,205 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1636908974098_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=
[36malgo-1    |[0m 2021-11-14 16:56:36,205 INFO rmcontainer.RMContainerImpl: container_1636908974098_0001_01_000003 Container Transitioned from NEW to ALLOCATED
[36malgo-1    |[0m 2021-11-14 16:56:36,205 INFO fica.FiCaSchedulerNode: Assigned container container_1636908974098_0001_01_000003 of capacity <memory:3287, vCores:1> on host algo-1:33635, which has 1 containers, <memory:3287, vCores:1> used and <memory:12605, vCores:3> available after allocation
[36malgo-1    |[0m 2021-11-14 16:56:36,205 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636908974098_0001	CONTAINERID=container_1636908974098_0001_01_000003	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 16:56:36,205 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.23502392 absoluteUsedCapacity=0.23502392 used=<memory:7470, vCores:3> cluster=<memory:31784, vCores:8>
[36malgo-1    |[0m 2021-11-14 16:56:36,205 INFO capacity.CapacityScheduler: Allocation proposal accepted
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000001/stderr2021-11-14 16:56:36,242 INFO ipc.Server: Auth successful for appattempt_1636908974098_0001_000001 (auth:SIMPLE)
[33malgo-2    |[0m 2021-11-14 16:56:36,246 INFO containermanager.ContainerManagerImpl: Start request for container_1636908974098_0001_01_000002 by user root
[33malgo-2    |[0m 2021-11-14 16:56:36,248 INFO nodemanager.NMAuditLogger: USER=root	IP=172.21.0.3	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1636908974098_0001	CONTAINERID=container_1636908974098_0001_01_000002
[33malgo-2    |[0m 2021-11-14 16:56:36,248 INFO application.ApplicationImpl: Adding container_1636908974098_0001_01_000002 to application application_1636908974098_0001
[33malgo-2    |[0m 2021-11-14 16:56:36,249 INFO container.ContainerImpl: Container container_1636908974098_0001_01_000002 transitioned from NEW to LOCALIZING
[33malgo-2    |[0m 2021-11-14 16:56:36,249 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1636908974098_0001
[33malgo-2    |[0m 2021-11-14 16:56:36,249 INFO container.ContainerImpl: Container container_1636908974098_0001_01_000002 transitioned from LOCALIZING to SCHEDULED
[33malgo-2    |[0m 2021-11-14 16:56:36,249 INFO scheduler.ContainerScheduler: Starting container [container_1636908974098_0001_01_000002]
[33malgo-2    |[0m 2021-11-14 16:56:36,263 INFO container.ContainerImpl: Container container_1636908974098_0001_01_000002 transitioned from SCHEDULED to RUNNING
[33malgo-2    |[0m 2021-11-14 16:56:36,263 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1636908974098_0001_01_000002
[33malgo-2    |[0m 2021-11-14 16:56:36,266 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000002/default_container_executor.sh]
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/prelaunch.out
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/prelaunch.err
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/launch_container.sh
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/directory.info
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stdout
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr
[36malgo-1    |[0m 2021-11-14 16:56:36,961 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:33635 for container : container_1636908974098_0001_01_000003
[36malgo-1    |[0m 2021-11-14 16:56:36,962 INFO rmcontainer.RMContainerImpl: container_1636908974098_0001_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
[36malgo-1    |[0m 2021-11-14 16:56:37,046 INFO ipc.Server: Auth successful for appattempt_1636908974098_0001_000001 (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 16:56:37,064 INFO rmcontainer.RMContainerImpl: container_1636908974098_0001_01_000002 Container Transitioned from ACQUIRED to RUNNING
[36malgo-1    |[0m 2021-11-14 16:56:37,065 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1636908974098_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=
[36malgo-1    |[0m 2021-11-14 16:56:37,066 INFO rmcontainer.RMContainerImpl: container_1636908974098_0001_01_000004 Container Transitioned from NEW to ALLOCATED
[36malgo-1    |[0m 2021-11-14 16:56:37,066 INFO fica.FiCaSchedulerNode: Assigned container container_1636908974098_0001_01_000004 of capacity <memory:3287, vCores:1> on host algo-2:45155, which has 3 containers, <memory:7470, vCores:3> used and <memory:8422, vCores:1> available after allocation
[36malgo-1    |[0m 2021-11-14 16:56:37,066 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636908974098_0001	CONTAINERID=container_1636908974098_0001_01_000004	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 16:56:37,066 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.33844072 absoluteUsedCapacity=0.33844072 used=<memory:10757, vCores:4> cluster=<memory:31784, vCores:8>
[36malgo-1    |[0m 2021-11-14 16:56:37,066 INFO capacity.CapacityScheduler: Allocation proposal accepted
[33malgo-2    |[0m 2021-11-14 16:56:37,112 INFO monitor.ContainersMonitorImpl: container_1636908974098_0001_01_000002's ip = 172.21.0.3, and hostname = algo-2
[33malgo-2    |[0m 2021-11-14 16:56:37,117 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1636908974098_0001_01_000002 since CPU usage is not yet available.
[36malgo-1    |[0m 2021-11-14 16:56:37,130 INFO containermanager.ContainerManagerImpl: Start request for container_1636908974098_0001_01_000003 by user root
[36malgo-1    |[0m 2021-11-14 16:56:37,178 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1636908974098_0001
[36malgo-1    |[0m 2021-11-14 16:56:37,186 INFO application.ApplicationImpl: Application application_1636908974098_0001 transitioned from NEW to INITING
[36malgo-1    |[0m 2021-11-14 16:56:37,186 INFO application.ApplicationImpl: Adding container_1636908974098_0001_01_000003 to application application_1636908974098_0001
[36malgo-1    |[0m 2021-11-14 16:56:37,186 INFO nodemanager.NMAuditLogger: USER=root	IP=172.21.0.3	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1636908974098_0001	CONTAINERID=container_1636908974098_0001_01_000003
[36malgo-1    |[0m 2021-11-14 16:56:37,189 INFO application.ApplicationImpl: Application application_1636908974098_0001 transitioned from INITING to RUNNING
[36malgo-1    |[0m 2021-11-14 16:56:37,192 INFO container.ContainerImpl: Container container_1636908974098_0001_01_000003 transitioned from NEW to LOCALIZING
[36malgo-1    |[0m 2021-11-14 16:56:37,192 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1636908974098_0001
[36malgo-1    |[0m 2021-11-14 16:56:37,201 INFO localizer.ResourceLocalizationService: Created localizer for container_1636908974098_0001_01_000003
[36malgo-1    |[0m 2021-11-14 16:56:37,218 INFO rmcontainer.RMContainerImpl: container_1636908974098_0001_01_000003 Container Transitioned from ACQUIRED to RUNNING
[36malgo-1    |[0m 2021-11-14 16:56:37,268 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1636908974098_0001_01_000003.tokens
[36malgo-1    |[0m 2021-11-14 16:56:37,281 INFO nodemanager.DefaultContainerExecutor: Initializing user root
[36malgo-1    |[0m 2021-11-14 16:56:37,287 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1636908974098_0001_01_000003.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000003.tokens
[36malgo-1    |[0m 2021-11-14 16:56:37,287 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001
[36malgo-1    |[0m 21/11/14 16:56:38 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 1239, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[36malgo-1    |[0m 21/11/14 16:56:39 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.21.0.3:37246) with ID 1
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:37 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 507@algo-2
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:37 INFO SignalUtils: Registered signal handler for TERM
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:37 INFO SignalUtils: Registered signal handler for HUP
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:37 INFO SignalUtils: Registered signal handler for INT
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:37 INFO SecurityManager: Changing view acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:37 INFO SecurityManager: Changing modify acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:37 INFO SecurityManager: Changing view acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:37 INFO SecurityManager: Changing modify acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:38 INFO TransportClientFactory: Successfully created connection to /172.21.0.2:44909 after 89 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:38 INFO SecurityManager: Changing view acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:38 INFO SecurityManager: Changing modify acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:38 INFO SecurityManager: Changing view acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:38 INFO SecurityManager: Changing modify acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:38 INFO TransportClientFactory: Successfully created connection to /172.21.0.2:44909 after 1 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:38 INFO DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/blockmgr-90930d24-89b8-419d-aef0-f1137e813f3c
[36malgo-1    |[0m 21/11/14 16:56:39 INFO BlockManagerMasterEndpoint: Registering block manager algo-2:43215 with 912.3 MiB RAM, BlockManagerId(1, algo-2, 43215, None)
[36malgo-1    |[0m 21/11/14 16:56:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, algo-2, executor 1, partition 0, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 16:56:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-2:43215 (size: 1410.0 B, free: 912.3 MiB)
[36malgo-1    |[0m 2021-11-14 16:56:39,968 INFO scheduler.AppSchedulingInfo: checking for deactivate of application :application_1636908974098_0001
[36malgo-1    |[0m 2021-11-14 16:56:39,973 INFO rmcontainer.RMContainerImpl: container_1636908974098_0001_01_000004 Container Transitioned from ALLOCATED to ACQUIRED
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, algo-2, executor 1, partition 1, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 820 ms on algo-2 (executor 1) (1/16)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, algo-2, executor 1, partition 2, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 14 ms on algo-2 (executor 1) (2/16)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, algo-2, executor 1, partition 3, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 13 ms on algo-2 (executor 1) (3/16)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, algo-2, executor 1, partition 4, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 12 ms on algo-2 (executor 1) (4/16)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, algo-2, executor 1, partition 5, PROCESS_LOCAL, 7368 bytes)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 12 ms on algo-2 (executor 1) (5/16)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, algo-2, executor 1, partition 6, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 9 ms on algo-2 (executor 1) (6/16)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, algo-2, executor 1, partition 7, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 9 ms on algo-2 (executor 1) (7/16)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, algo-2, executor 1, partition 8, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 11 ms on algo-2 (executor 1) (8/16)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, algo-2, executor 1, partition 9, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 9 ms on algo-2 (executor 1) (9/16)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10, algo-2, executor 1, partition 10, PROCESS_LOCAL, 7367 bytes)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 7 ms on algo-2 (executor 1) (10/16)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11, algo-2, executor 1, partition 11, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 7 ms on algo-2 (executor 1) (11/16)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12, algo-2, executor 1, partition 12, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 8 ms on algo-2 (executor 1) (12/16)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13, algo-2, executor 1, partition 13, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 7 ms on algo-2 (executor 1) (13/16)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14, algo-2, executor 1, partition 14, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 8 ms on algo-2 (executor 1) (14/16)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15, algo-2, executor 1, partition 15, PROCESS_LOCAL, 7370 bytes)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 8 ms on algo-2 (executor 1) (15/16)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:38 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:38 INFO YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@172.21.0.2:44909
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:38 INFO ResourceUtils: ==============================================================
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:38 INFO ResourceUtils: Resources for spark.executor:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:38 INFO ResourceUtils: ==============================================================
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:39 INFO YarnCoarseGrainedExecutorBackend: Successfully registered with driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:39 INFO Executor: Starting executor ID 1 on host algo-2
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43215.
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:39 INFO NettyBlockTransferService: Server created on algo-2:43215
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:39 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(1, algo-2, 43215, None)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(1, algo-2, 43215, None)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:39 INFO BlockManager: Initialized BlockManager: BlockManagerId(1, algo-2, 43215, None)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:39 INFO MetricsConfig: Loaded properties from hadoop-metrics2.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:39 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:39 INFO MetricsSystemImpl: s3a-file-system metrics system started
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:39 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:39 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:39 INFO Executor: Fetching spark://172.21.0.2:44909/jars/hello-scala-spark_2.12-1.0.jar with timestamp 1636908977431
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:39 INFO TransportClientFactory: Successfully created connection to /172.21.0.2:44909 after 1 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:39 INFO Utils: Fetching spark://172.21.0.2:44909/jars/hello-scala-spark_2.12-1.0.jar to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/spark-9543aacf-534e-4ffb-a358-a1b8ddca3038/fetchFileTemp8695488853250469207.tmp
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:39 INFO Utils: Copying /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/spark-9543aacf-534e-4ffb-a358-a1b8ddca3038/-15009194801636908977431_cache to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000002/./hello-scala-spark_2.12-1.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:39 INFO Executor: Adding file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000002/./hello-scala-spark_2.12-1.0.jar to class loader
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:39 INFO TorrentBroadcast: Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:39 INFO TransportClientFactory: Successfully created connection to /172.21.0.2:46499 after 1 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1410.0 B, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:39 INFO TorrentBroadcast: Reading broadcast variable 0 took 131 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.3 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 880 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 1
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 837 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 2
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 837 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 3
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 837 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 4
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 837 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 5
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] I'm an executor, Hello!
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 837 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 6
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 837 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 7
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 837 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 8
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 837 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 9
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 794 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 10
[36malgo-1    |[0m 21/11/14 16:56:40 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 14 ms on algo-2 (executor 1) (16/16)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:56:40 INFO DAGScheduler: ResultStage 0 (foreach at HelloScalaSparkApp.scala:39) finished in 5.200 s
[36malgo-1    |[0m 21/11/14 16:56:40 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:56:40 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished
[36malgo-1    |[0m 21/11/14 16:56:40 INFO DAGScheduler: Job 0 finished: foreach at HelloScalaSparkApp.scala:39, took 5.256922 s
[36malgo-1    |[0m Reading input from: file:///opt/ml/processing/input/data/data.jsonl
[36malgo-1    |[0m 21/11/14 16:56:40 INFO SharedState: loading hive config file: file:/etc/spark/conf.dist/hive-site.xml
[36malgo-1    |[0m 21/11/14 16:56:40 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/lib/spark/spark-warehouse').
[36malgo-1    |[0m 21/11/14 16:56:40 INFO SharedState: Warehouse path is 'file:/usr/lib/spark/spark-warehouse'.
[36malgo-1    |[0m 21/11/14 16:56:40 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 16:56:40 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 16:56:40 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 16:56:40 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 16:56:40 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 16:56:40 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.21.0.2:46499 in memory (size: 1410.0 B, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:40 INFO BlockManagerInfo: Removed broadcast_0_piece0 on algo-2:43215 in memory (size: 1410.0 B, free: 912.3 MiB)
[36malgo-1    |[0m 2021-11-14 16:56:40,755 INFO container.ContainerImpl: Container container_1636908974098_0001_01_000003 transitioned from LOCALIZING to SCHEDULED
[36malgo-1    |[0m 2021-11-14 16:56:40,755 INFO scheduler.ContainerScheduler: Starting container [container_1636908974098_0001_01_000003]
[36malgo-1    |[0m 2021-11-14 16:56:40,780 INFO container.ContainerImpl: Container container_1636908974098_0001_01_000003 transitioned from SCHEDULED to RUNNING
[36malgo-1    |[0m 2021-11-14 16:56:40,780 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1636908974098_0001_01_000003
[36malgo-1    |[0m 2021-11-14 16:56:40,784 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000003/default_container_executor.sh]
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/prelaunch.out
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/prelaunch.err
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/launch_container.sh
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/directory.info
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stdout
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr
[36malgo-1    |[0m 21/11/14 16:56:41 INFO InMemoryFileIndex: It took 13 ms to list leaf files for 1 paths.
[36malgo-1    |[0m 21/11/14 16:56:41 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[36malgo-1    |[0m 2021-11-14 16:56:42,197 INFO monitor.ContainersMonitorImpl: container_1636908974098_0001_01_000003's ip = 172.21.0.2, and hostname = algo-1
[36malgo-1    |[0m 2021-11-14 16:56:42,204 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1636908974098_0001_01_000003 since CPU usage is not yet available.
[36malgo-1    |[0m 2021-11-14 16:56:42,982 INFO rmcontainer.RMContainerImpl: container_1636908974098_0001_01_000004 Container Transitioned from ACQUIRED to RELEASED
[36malgo-1    |[0m 2021-11-14 16:56:42,983 INFO resourcemanager.RMAuditLogger: USER=root	IP=172.21.0.3	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636908974098_0001	CONTAINERID=container_1636908974098_0001_01_000004	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Runni2021-11-14 16:56:43,079 WARN containermanager.ContainerManagerImpl: couldn't find container container_1636908974098_0001_01_000004 while processing FINISH_CONTAINERS event
[36malgo-1    |[0m 21/11/14 16:56:43 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 16:56:43 INFO FileSourceStrategy: Pushed Filters: 
[36malgo-1    |[0m 21/11/14 16:56:43 INFO FileSourceStrategy: Post-Scan Filters: 
[36malgo-1    |[0m 21/11/14 16:56:43 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[36malgo-1    |[0m 21/11/14 16:56:43 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 317.8 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:56:43 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:56:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.21.0.2:46499 (size: 30.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:43 INFO SparkContext: Created broadcast 1 from json at HelloScalaSparkApp.scala:45
[36malgo-1    |[0m 21/11/14 16:56:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 16:56:43 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 16:56:43 INFO SparkContext: Starting job: json at HelloScalaSparkApp.scala:45
[36malgo-1    |[0m 21/11/14 16:56:43 INFO DAGScheduler: Got job 1 (json at HelloScalaSparkApp.scala:45) with 1 output partitions
[36malgo-1    |[0m 21/11/14 16:56:43 INFO DAGScheduler: Final stage: ResultStage 1 (json at HelloScalaSparkApp.scala:45)
[36malgo-1    |[0m 21/11/14 16:56:43 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:56:43 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:56:43 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at json at HelloScalaSparkApp.scala:45), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:56:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 14.6 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:56:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:56:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.21.0.2:46499 (size: 7.3 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:43 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.21.0.2:44174) with ID 2
[36malgo-1    |[0m 21/11/14 16:56:43 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:56:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at json at HelloScalaSparkApp.scala:45) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 16:56:43 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 16:56:43 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 16, algo-2, executor 1, partition 0, PROCESS_LOCAL, 7755 bytes)
[36malgo-1    |[0m 21/11/14 16:56:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-2:43215 (size: 7.3 KiB, free: 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:41 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 1220@algo-1
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:41 INFO SignalUtils: Registered signal handler for TERM
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:41 INFO SignalUtils: Registered signal handler for HUP
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:41 INFO SignalUtils: Registered signal handler for INT
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:42 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:42 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:42 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:42 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:42 INFO TransportClientFactory: Successfully created connection to /172.21.0.2:44909 after 89 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:43 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:43 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:43 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:43 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:43 INFO TransportClientFactory: Successfully created connection to /172.21.0.2:44909 after 1 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:43 INFO DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/blockmgr-0e56d738-91a2-47c8-bc00-48235de9f2b2
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:43 INFO MemoryStore: MemoryStore started with capacity 912.3 M21/11/14 16:56:43 INFO BlockManagerMasterEndpoint: Registering block manager algo-1:36675 with 912.3 MiB RAM, BlockManagerId(2, algo-1, 36675, None)
[33malgo-2    |[0m ng task 10.0 in stage 0.0 (TID 10)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] I'm an executor, Hola!
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Finished task 10.0 in stage 0.0 (TID 10). 837 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 11
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 794 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 12
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 794 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 13
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Running task 13.0 in stage 0.0 (TID 13)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Finished task 13.0 in stage 0.0 (TID 13). 794 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 14
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Running task 14.0 in stage 0.0 (TID 14)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Finished task 14.0 in stage 0.0 (TID 14). 794 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 15
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Running task 15.0 in stage 0.0 (TID 15)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] I'm an executor, Bonjour!
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:40 INFO Executor: Finished task 15.0 in stage 0.0 (TID 15). 837 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:43 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 16
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:43 INFO Executor: Running task 0.0 in stage 1.0 (TID 16)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:43 INFO TorrentBroadcast: Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m 21/11/14 16:56:44 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-2:43215 (size: 30.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:56:44 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 16) in 1040 ms on algo-2 (executor 1) (1/1)
[36malgo-1    |[0m 21/11/14 16:56:44 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:56:44 INFO DAGScheduler: ResultStage 1 (json at HelloScalaSparkApp.scala:45) finished in 1.082 s
[36malgo-1    |[0m 21/11/14 16:56:44 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:56:44 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
[36malgo-1    |[0m 21/11/14 16:56:44 INFO DAGScheduler: Job 1 finished: json at HelloScalaSparkApp.scala:45, took 1.090455 s
[36malgo-1    |[0m root
[36malgo-1    |[0m  |-- date: string (nullable = true)
[36malgo-1    |[0m  |-- sale: long (nullable = true)
[36malgo-1    |[0m 
[36malgo-1    |[0m 21/11/14 16:56:44 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.21.0.2:46499 in memory (size: 30.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:44 INFO BlockManagerInfo: Removed broadcast_1_piece0 on algo-2:43215 in memory (size: 30.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:56:44 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.21.0.2:46499 in memory (size: 7.3 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:44 INFO BlockManagerInfo: Removed broadcast_2_piece0 on algo-2:43215 in memory (size: 7.3 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:56:44 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 16:56:44 INFO FileSourceStrategy: Pushed Filters: 
[36malgo-1    |[0m 21/11/14 16:56:44 INFO FileSourceStrategy: Post-Scan Filters: 
[36malgo-1    |[0m 21/11/14 16:56:44 INFO FileSourceStrategy: Output Data Schema: struct<date: string, sale: bigint>
[36malgo-1    |[0m 21/11/14 16:56:44 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 317.8 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:56:44 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:56:44 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.21.0.2:46499 (size: 30.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:44 INFO SparkContext: Created broadcast 3 from rdd at HelloScalaSparkApp.scala:52
[36malgo-1    |[0m 21/11/14 16:56:44 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 16:56:44 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 16:56:44 INFO SparkContext: Starting job: first at HelloScalaSparkApp.scala:52
[36malgo-1    |[0m 21/11/14 16:56:44 INFO DAGScheduler: Got job 2 (first at HelloScalaSparkApp.scala:52) with 1 output partitions
[36malgo-1    |[0m 21/11/14 16:56:44 INFO DAGScheduler: Final stage: ResultStage 2 (first at HelloScalaSparkApp.scala:52)
[36malgo-1    |[0m 21/11/14 16:56:44 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:56:44 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:56:44 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at map at HelloScalaSparkApp.scala:52), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:56:44 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 21.5 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:56:44 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.1 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:56:44 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.21.0.2:46499 (size: 10.1 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:44 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:56:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at map at HelloScalaSparkApp.scala:52) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 16:56:44 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 16:56:44 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 17, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7755 bytes)
[36malgo-1    |[0m 21/11/14 16:56:45 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:36675 (size: 10.1 KiB, free: 912.3 MiB)
[36malgo-1    |[0m iB
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:43 INFO YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@172.21.0.2:44909
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:43 INFO ResourceUtils: ==============================================================
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:43 INFO ResourceUtils: Resources for spark.executor:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:43 INFO ResourceUtils: ==============================================================
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:43 INFO YarnCoarseGrainedExecutorBackend: Successfully registered with driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:43 INFO Executor: Starting executor ID 2 on host algo-1
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36675.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:43 INFO NettyBlockTransferService: Server created on algo-1:36675
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(2, algo-1, 36675, None)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(2, algo-1, 36675, None)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(2, algo-1, 36675, None)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:43 INFO MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:43 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:43 INFO MetricsSystemImpl: s3a-file-system metrics system started
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:44 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 17
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:44 INFO Executor: Running task 0.0 in stage 2.0 (TID 17)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:44 INFO Executor: Fetching spark://172.21.0.2:44909/jars/hello-scala-spark_2.12-1.0.jar with timestamp 1636908977431
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:44 INFO TransportClientFactory: Successfully created connection to /172.21.0.2:44909 after 1 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:43 INFO TorrentBroadcast: Reading broadcast variable 2 took 9 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 14.6 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:43 INFO FileScanRDD: TID: 16 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:44 INFO CodeGenerator: Code generated in 381.647009 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:44 INFO TorrentBroadcast: Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:44 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:44 INFO TorrentBroadcast: Reading broadcast variable 1 took 9 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:44 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 445.1 KiB, free 911.8 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:44 INFO Executor: Finished task 0.0 in stage 1.0 (TID 16). 2013 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:46 WARN YarnCoarseGrainedExecutorBackend: eagerFSInit: Unable to eagerly init filesystem s3://does/not/exist
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] java.nio.file.AccessDeniedException: does: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@404810d6: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@68d698f2: Failed to connect to service endpoint: ]
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:187)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:111)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:380)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:314)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3358)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:123)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3407)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3375)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:486)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at org.apache.spark.executor.CoarseGrainedExecutorBackend.$anonfun$eagerFSInit$1(CoarseGrainedExecutorBackend.scala:274)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at scala.collection.parallel.mutable.ParArray$ParArrayIterator.flatmap2combiner(ParArray.scala:419)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at scala.collection.parallel.ParIterableLike$FlatMap.leaf(ParIterableLike.scala:1074)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at scala.collection.parallel.ParIterableLike$FlatMap.tryLeaf(ParIterableLike.scala:1070)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at scala.collection.parallel.FutureTasks.$anonfun$exec$5(Tasks.scala:499)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at scala.util.Success.$anonfun$map$1(Try.scala:255)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at scala.util.Success.map(Try.scala:213)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at java.lang.Thread.run(Thread.java:748)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] Caused by: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@404810d6: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@68d698f2: Failed to connect to service endpoint: ]
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:159)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1257)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:833)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:783)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
[36malgo-1    |[0m [/var/log/yarn/21/11/14 16:56:47 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:36675 (size: 30.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:56:47 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 17) in 3067 ms on algo-1 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 16:56:47 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:56:47 INFO DAGScheduler: ResultStage 2 (first at HelloScalaSparkApp.scala:52) finished in 3.073 s
[36malgo-1    |[0m 21/11/14 16:56:47 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:56:47 INFO YarnScheduler: Killing all running tasks in stage 2: Stage finished
[36malgo-1    |[0m 21/11/14 16:56:47 INFO DAGScheduler: Job 2 finished: first at HelloScalaSparkApp.scala:52, took 3.077466 s
[36malgo-1    |[0m Parsing first line with json4s: JObject(List((date,JString(2020-01-04)), (sale,JInt(283))))
[36malgo-1    |[0m 21/11/14 16:56:48 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.21.0.2:46499 in memory (size: 30.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:48 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 16:56:48 INFO BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:36675 in memory (size: 30.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:56:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(sale),GreaterThan(sale,750)
[36malgo-1    |[0m 21/11/14 16:56:48 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(sale#8L),(sale#8L > 750)
[36malgo-1    |[0m 21/11/14 16:56:48 INFO FileSourceStrategy: Output Data Schema: struct<date: string, sale: bigint>
[36malgo-1    |[0m 21/11/14 16:56:48 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.21.0.2:46499 in memory (size: 10.1 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:48 INFO BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:36675 in memory (size: 10.1 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:56:48 INFO CodeGenerator: Code generated in 247.227987 ms
[36malgo-1    |[0m 21/11/14 16:56:48 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 317.8 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:56:48 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:56:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.21.0.2:46499 (size: 30.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:48 INFO SparkContext: Created broadcast 5 from show at HelloScalaSparkApp.scala:57
[36malgo-1    |[0m 21/11/14 16:56:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 16:56:48 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:44 INFO Utils: Fetching spark://172.21.0.2:44909/jars/hello-scala-spark_2.12-1.0.jar to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/spark-8a8db718-7112-4d75-b092-9358eed9528e/fetchFileTemp3014001727859619979.tmp
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:44 INFO Utils: Copying /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/spark-8a8db718-7112-4d75-b092-9358eed9528e/-15009194801636908977431_cache to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000003/./hello-scala-spark_2.12-1.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:44 INFO Executor: Adding file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000003/./hello-scala-spark_2.12-1.0.jar to class loader
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:45 INFO TorrentBroadcast: Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:45 INFO TransportClientFactory: Successfully created connection to /172.21.0.2:46499 after 1 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:45 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.1 KiB, free 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:45 INFO TorrentBroadcast: Reading broadcast variable 4 took 89 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:45 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 21.5 KiB, free 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:47 INFO CodeGenerator: Code generated in 282.778424 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:47 INFO FileScanRDD: TID: 17 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:47 INFO CodeGenerator: Code generated in 12.200643 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:47 INFO TorrentBroadcast: Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:47 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 912.2 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:47 INFO TorrentBroadcast: Reading broadcast variable 3 took 8 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:47 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 445.1 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:47 INFO CodeGenerator: Code generated in 30.500772 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_16321/11/14 16:56:48 INFO SparkContext: Starting job: show at HelloScalaSparkApp.scala:57
[36malgo-1    |[0m 21/11/14 16:56:48 INFO DAGScheduler: Got job 3 (show at HelloScalaSparkApp.scala:57) with 1 output partitions
[36malgo-1    |[0m 21/11/14 16:56:48 INFO DAGScheduler: Final stage: ResultStage 3 (show at HelloScalaSparkApp.scala:57)
[36malgo-1    |[0m 21/11/14 16:56:48 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:56:48 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:56:48 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[15] at show at HelloScalaSparkApp.scala:57), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:56:48 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 18.4 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:56:48 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 9.1 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:56:48 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.21.0.2:46499 (size: 9.1 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:48 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:56:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at show at HelloScalaSparkApp.scala:57) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 16:56:48 INFO YarnScheduler: Adding task set 3.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 16:56:48 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 18, algo-2, executor 1, partition 0, PROCESS_LOCAL, 7755 bytes)
[36malgo-1    |[0m 21/11/14 16:56:48 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-2:43215 (size: 9.1 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:56:49 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-2:43215 (size: 30.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:56:49 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 18) in 302 ms on algo-2 (executor 1) (1/1)
[36malgo-1    |[0m 21/11/14 16:56:49 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:56:49 INFO DAGScheduler: ResultStage 3 (show at HelloScalaSparkApp.scala:57) finished in 0.309 s
[36malgo-1    |[0m 21/11/14 16:56:49 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:56:49 INFO YarnScheduler: Killing all running tasks in stage 3: Stage finished
[36malgo-1    |[0m 21/11/14 16:56:49 INFO DAGScheduler: Job 3 finished: show at HelloScalaSparkApp.scala:57, took 0.312300 s
[36malgo-1    |[0m 21/11/14 16:56:49 INFO CodeGenerator: Code generated in 13.91261 ms
[36malgo-1    |[0m +----------+----+
[36malgo-1    |[0m |      date|sale|
[36malgo-1    |[0m +----------+----+
[36malgo-1    |[0m |2020-01-03| 999|
[36malgo-1    |[0m |2020-01-02| 999|
[36malgo-1    |[0m |2020-01-06| 998|
[36malgo-1    |[0m |2020-01-05| 998|
[36malgo-1    |[0m |2020-01-07| 996|
[36malgo-1    |[0m |2020-01-07| 994|
[36malgo-1    |[0m |2020-01-07| 993|
[36malgo-1    |[0m |2020-01-01| 993|
[36malgo-1    |[0m |2020-01-02| 991|
[36malgo-1    |[0m |2020-01-05| 990|
[36malgo-1    |[0m |2020-01-07| 990|
[36malgo-1    |[0m |2020-01-03| 989|
[36malgo-1    |[0m |2020-01-04| 988|
[36malgo-1    |[0m |2020-01-05| 988|
[36malgo-1    |[0m |2020-01-07| 985|
[36malgo-1    |[0m |2020-01-03| 985|
[36malgo-1    |[0m |2020-01-04| 982|
[36malgo-1    |[0m |2020-01-02| 981|
[36malgo-1    |[0m |2020-01-03| 980|
[36malgo-1    |[0m |2020-01-05| 979|
[36malgo-1    |[0m +----------+----+
[36malgo-1    |[0m only showing top 20 rows
[36malgo-1    |[0m 
[36malgo-1    |[0m 21/11/14 16:56:49 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.21.0.2:46499 in memory (size: 9.1 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:49 INFO BlockManagerInfo: Removed broadcast_6_piece0 on algo-2:43215 in memory (size: 9.1 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:56:49 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 16:56:49 INFO FileSourceStrategy: Pushed Filters: 
[36malgo-1    |[0m 21/11/14 16:56:49 INFO FileSourceStrategy: Post-Scan Filters: 
[36malgo-1    |[0m 21/11/14 16:56:49 INFO FileSourceStrategy: Output Data Schema: struct<date: string, sale: bigint>
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5062)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:5850)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:5823)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5046)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5008)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1416)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1352)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$verifyBucketExists$1(S3AFileSystem.java:381)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	... 32 more
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] Caused by: com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@404810d6: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@68d698f2: Failed to connect to service endpoint: ]
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:136)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:137)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 	... 50 more
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 18
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:48 INFO Executor: Running task 0.0 in stage 3.0 (TID 18)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:48 INFO TorrentBroadcast: Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m 21/11/14 16:56:49 INFO CodeGenerator: Code generated in 57.224462 ms
[36malgo-1    |[0m 21/11/14 16:56:49 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 317.8 KiB, free 1007.2 MiB)
[36malgo-1    |[0m 21/11/14 16:56:49 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 1007.2 MiB)
[36malgo-1    |[0m 21/11/14 16:56:49 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.21.0.2:46499 (size: 30.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:49 INFO SparkContext: Created broadcast 7 from collect at HelloScalaSparkApp.scala:61
[36malgo-1    |[0m 21/11/14 16:56:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 16:56:49 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 16:56:49 INFO DAGScheduler: Registering RDD 19 (collect at HelloScalaSparkApp.scala:61) as input to shuffle 0
[36malgo-1    |[0m 21/11/14 16:56:49 INFO DAGScheduler: Got map stage job 4 (collect at HelloScalaSparkApp.scala:61) with 1 output partitions
[36malgo-1    |[0m 21/11/14 16:56:49 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (collect at HelloScalaSparkApp.scala:61)
[36malgo-1    |[0m 21/11/14 16:56:49 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:56:49 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:56:49 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[19] at collect at HelloScalaSparkApp.scala:61), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:56:49 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 29.7 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 16:56:49 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 13.9 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 16:56:49 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.21.0.2:46499 (size: 13.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:49 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:56:49 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[19] at collect at HelloScalaSparkApp.scala:61) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 16:56:49 INFO YarnScheduler: Adding task set 4.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 16:56:49 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 19, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7744 bytes)
[36malgo-1    |[0m 21/11/14 16:56:49 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:36675 (size: 13.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:56:49 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:36675 (size: 30.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:56:49 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 19) in 410 ms on algo-1 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 16:56:49 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:56:49 INFO DAGScheduler: ShuffleMapStage 4 (collect at HelloScalaSparkApp.scala:61) finished in 0.429 s
[36malgo-1    |[0m 21/11/14 16:56:49 INFO DAGScheduler: looking for newly runnable stages
[36malgo-1    |[0m 21/11/14 16:56:49 INFO DAGScheduler: running: Set()
[36malgo-1    |[0m 21/11/14 16:56:49 INFO DAGScheduler: waiting: Set()
[36malgo-1    |[0m 21/11/14 16:56:49 INFO DAGScheduler: failed: Set()
[36malgo-1    |[0m 21/11/14 16:56:49 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 16.
[36malgo-1    |[0m 21/11/14 16:56:50 INFO CodeGenerator: Code generated in 18.350143 ms
[36malgo-1    |[0m 21/11/14 16:56:50 INFO CodeGenerator: Code generated in 10.420776 ms
[36malgo-1    |[0m 21/11/14 16:56:50 INFO SparkContext: Starting job: collect at HelloScalaSparkApp.scala:61
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: Got job 5 (collect at HelloScalaSparkApp.scala:61) with 15 output partitions
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: Final stage: ResultStage 6 (collect at HelloScalaSparkApp.scala:61)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[24] at collect at HelloScalaSparkApp.scala:61), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:56:50 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 31.1 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.8 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.21.0.2:46499 (size: 14.8 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: Submitting 15 missing tasks from ResultStage 6 (MapPartitionsRDD[24] at collect at HelloScalaSparkApp.scala:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[36malgo-1    |[0m 21/11/14 16:56:50 INFO YarnScheduler: Adding task set 6.0 with 15 tasks
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 20, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 21, algo-2, executor 1, partition 1, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-2:43215 (size: 14.8 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:36675 (size: 14.8 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.21.0.2:44174
[36malgo-1    |[0m 21/11/14 16:56:50 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.21.0.3:37246
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 2.0 in stage 6.0 (TID 22, algo-1, executor 2, partition 2, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 20) in 173 ms on algo-1 (executor 2) (1/15)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:48 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 9.1 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:48 INFO TorrentBroadcast: Reading broadcast variable 6 took 6 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:48 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 18.4 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:48 INFO CodeGenerator: Code generated in 39.590569 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:48 INFO CodeGenerator: Code generated in 10.956042 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:49 INFO CodeGenerator: Code generated in 8.610676 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:49 INFO FileScanRDD: TID: 18 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:49 INFO CodeGenerator: Code generated in 8.134829 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:49 INFO TorrentBroadcast: Started reading broadcast variable 5 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:49 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:49 INFO TorrentBroadcast: Reading broadcast variable 5 took 8 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:49 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 445.1 KiB, free 911.7 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:49 INFO Executor: Finished task 0.0 in stage 3.0 (TID 18). 2327 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 21
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Running task 1.0 in stage 6.0 (TID 21)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO TorrentBroadcast: Started reading broadcast variable 9 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.8 KiB, free 911.8 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO TorrentBroadcast: Reading broadcast variable 9 took 8 ms
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 23, algo-1, executor 2, partition 3, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 2.0 in stage 6.0 (TID 22) in 26 ms on algo-1 (executor 2) (2/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 4.0 in stage 6.0 (TID 24, algo-1, executor 2, partition 4, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 3.0 in stage 6.0 (TID 23) in 40 ms on algo-1 (executor 2) (3/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 5.0 in stage 6.0 (TID 25, algo-1, executor 2, partition 5, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 4.0 in stage 6.0 (TID 24) in 27 ms on algo-1 (executor 2) (4/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 6.0 in stage 6.0 (TID 26, algo-1, executor 2, partition 6, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 7.0 in stage 6.0 (TID 27, algo-2, executor 1, partition 7, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 5.0 in stage 6.0 (TID 25) in 26 ms on algo-1 (executor 2) (5/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 21) in 287 ms on algo-2 (executor 1) (6/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 8.0 in stage 6.0 (TID 28, algo-1, executor 2, partition 8, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 6.0 in stage 6.0 (TID 26) in 24 ms on algo-1 (executor 2) (7/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 9.0 in stage 6.0 (TID 29, algo-2, executor 1, partition 9, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 7.0 in stage 6.0 (TID 27) in 26 ms on algo-2 (executor 1) (8/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 10.0 in stage 6.0 (TID 30, algo-1, executor 2, partition 10, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 8.0 in stage 6.0 (TID 28) in 24 ms on algo-1 (executor 2) (9/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 11.0 in stage 6.0 (TID 31, algo-2, executor 1, partition 11, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 9.0 in stage 6.0 (TID 29) in 24 ms on algo-2 (executor 1) (10/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 12.0 in stage 6.0 (TID 32, algo-1, executor 2, partition 12, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 10.0 in stage 6.0 (TID 30) in 21 ms on algo-1 (executor 2) (11/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 13.0 in stage 6.0 (TID 33, algo-2, executor 1, partition 13, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 11.0 in stage 6.0 (TID 31) in 30 ms on algo-2 (executor 1) (12/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 14.0 in stage 6.0 (TID 34, algo-1, executor 2, partition 14, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 12.0 in stage 6.0 (TID 32) in 22 ms on algo-1 (executor 2) (13/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 13.0 in stage 6.0 (TID 33) in 31 ms on algo-2 (executor 1) (14/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 14.0 in stage 6.0 (TID 34) in 29 ms on algo-1 (executor 2) (15/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: ResultStage 6 (collect at HelloScalaSparkApp.scala:61) finished in 0.409 s
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:56:50 INFO YarnScheduler: Killing all running tasks in stage 6: Stage finished
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: Job 5 finished: collect at HelloScalaSparkApp.scala:61, took 0.417422 s
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: Registering RDD 25 (collect at HelloScalaSparkApp.scala:61) as input to shuffle 1
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: Got map stage job 6 (collect at HelloScalaSparkApp.scala:61) with 15 output partitions
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (collect at HelloScalaSparkApp.scala:61)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[25] at collect at HelloScalaSparkApp.scala:61), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:56:50 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 31.9 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 15.3 KiB, free 1007.0 MiB)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.21.0.2:46499 (size: 15.3 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: Submitting 15 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[25] at collect at HelloScalaSparkApp.scala:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[36malgo-1    |[0m 21/11/14 16:56:50 INFO YarnScheduler: Adding task set 8.0 with 15 tasks
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 35, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 36, algo-2, executor 1, partition 1, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:36675 (size: 15.3 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-2:43215 (size: 15.3 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 37, algo-1, executor 2, partition 2, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 35) in 76 ms on algo-1 (executor 2) (1/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 3.0 in stage 8.0 (TID 38, algo-1, executor 2, partition 3, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 37) in 28 ms on algo-1 (executor 2) (2/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 4.0 in stage 8.0 (TID 39, algo-2, executor 1, partition 4, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 36) in 129 ms on algo-2 (executor 1) (3/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 5.0 in stage 8.0 (TID 40, algo-1, executor 2, partition 5, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 3.0 in stage 8.0 (TID 38) in 35 ms on algo-1 (executor 2) (4/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 6.0 in stage 8.0 (TID 41, algo-2, executor 1, partition 6, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 4.0 in stage 8.0 (TID 39) in 28 ms on algo-2 (executor 1) (5/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 7.0 in stage 8.0 (TID 42, algo-1, executor 2, partition 7, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 5.0 in stage 8.0 (TID 40) in 28 ms on algo-1 (executor 2) (6/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 8.0 in stage 8.0 (TID 43, algo-2, executor 1, partition 8, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 6.0 in stage 8.0 (TID 41) in 27 ms on algo-2 (executor 1) (7/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 9.0 in stage 8.0 (TID 44, algo-1, executor 2, partition 9, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 7.0 in stage 8.0 (TID 42) in 27 ms on algo-1 (executor 2) (8/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 10.0 in stage 8.0 (TID 45, algo-2, executor 1, partition 10, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 8.0 in stage 8.0 (TID 43) in 27 ms on algo-2 (executor 1) (9/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 11.0 in stage 8.0 (TID 46, algo-1, executor 2, partition 11, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 9.0 in stage 8.0 (TID 44) in 25 ms on algo-1 (executor 2) (10/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 12.0 in stage 8.0 (TID 47, algo-2, executor 1, partition 12, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 10.0 in stage 8.0 (TID 45) in 24 ms on algo-2 (executor 1) (11/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 13.0 in stage 8.0 (TID 48, algo-1, executor 2, partition 13, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 11.0 in stage 8.0 (TID 46) in 25 ms on algo-1 (executor 2) (12/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 14.0 in stage 8.0 (TID 49, algo-2, executor 1, partition 14, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 12.0 in stage 8.0 (TID 47) in 32 ms on algo-2 (executor 1) (13/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 13.0 in stage 8.0 (TID 48) in 32 ms on algo-1 (executor 2) (14/15)
[36malgo-1    |[0m 6908974098_0001_01_000003/stderr] 21/11/14 16:56:47 INFO Executor: Finished task 0.0 in stage 2.0 (TID 17). 2423 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:49 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 19
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:49 INFO Executor: Running task 0.0 in stage 4.0 (TID 19)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:49 INFO TorrentBroadcast: Started reading broadcast variable 8 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:49 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 13.9 KiB, free 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:49 INFO TorrentBroadcast: Reading broadcast variable 8 took 10 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:49 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 29.7 KiB, free 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:49 INFO CodeGenerator: Code generated in 62.226915 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:49 INFO CodeGenerator: Code generated in 13.465127 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:49 INFO CodeGenerator: Code generated in 7.464222 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:49 INFO CodeGenerator: Code generated in 7.662603 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:49 INFO CodeGenerator: Code generated in 7.790347 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:49 INFO FileScanRDD: TID: 19 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:49 INFO TorrentBroadcast: Started reading broadcast variable 7 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:49 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 879.9 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:49 INFO TorrentBroadcast: Reading broadcast variable 7 took 8 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:49 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 445.1 KiB, free 879.5 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:49 INFO Executor: Finished task 0.0 in stage 4.0 (TID 19). 4531 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 20
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Running task 0.0 in stage 6.0 (TID 20)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO TorrentBroadcast: Started reading broadcast variable 9 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.8 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO TorrentBroadcast: Reading broadcast variable 9 took 8 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 31.1 KiB, free 911.7 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@172.21.0.2:44909)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO MapOutputTrackerWorker: Got the output locations
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO CodeGenerator: Code generated in 20.371882 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 0.0 in stage 6.0 (TID 20). 3723 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 22
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Running task 2.0 in stage 6.0 (TID 22)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 2.0 in stage 6.0 (TID 22). 3723 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 23
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Running task 3.0 in stage 6.0 (TID 23)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 3.0 in stage 6.0 (TID 23). 3890 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 24
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Running task 4.0 in stage 6.0 (TID 24)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 4.0 in stage 6.0 (TID 24). 3723 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 25
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Running task 5.0 in stage 6.0 (TID 25)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 5.0 in stage 6.0 (TID 25). 3890 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 26
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Running task 6.0 in stage 6.0 (TID 26)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 6.0 in stage 6.0 (TID 26). 3723 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 28
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Running task 8.0 in stage 6.0 (TID 28)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 8.0 in stage 6.0 (TID 28). 3723 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 30
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Running task 10.0 in stage 6.0 (TID 30)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 10.0 in stage 6.0 (TID 30). 3723 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 32
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Running task 12.0 in stage 6.0 (TID 32)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 12.0 in stage 6.0 (TID 32). 3723 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 34
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Running task 14.0 in stage 6.0 (TID 34)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 WARN YarnCoarseGrainedExecutorBackend: eagerFSInit: Unable to eagerly init filesystem s3://does/not/exist
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] java.nio.file.AccessDeniedException: does: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@344c649f: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@19c1f446: Failed to connect to service endpoint: ]
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:187)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:111)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:380)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:314)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3358)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:123)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3407)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3375)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:486)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at org.apache.spark.executor.CoarseGrainedExecutorBackend.$anonfun$eagerFSInit$1(CoarseGrainedExecutorBackend.scala:274)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at scala.collection.parallel.mutable.ParArray$ParArrayIterator.flatmap2combiner(ParArray.scala:419)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at scala.collection.parallel.ParIterableLike$FlatMap.leaf(ParIterableLike.scala:1074)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at scala.collection.parallel.ParIterableLike$FlatMap.tryLeaf(ParIterableLike.scala:1070)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at scala.collection.parallel.FutureTasks.$anonfun$exec$5(Tasks.scala:499)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at scala.util.Success.$anonfun$map$1(Try.scala:255)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at scala.util.Success.map(Try.scala:213)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at java.lang.Thread.run(Thread.java:748)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] Caused by: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@344c649f: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@19c1f446: Failed to connect to service endpoint: ]
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:159)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1257)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:833)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:783)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5062)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:5850)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:5823)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5046)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5008)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1416)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1352)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$verifyBucketExists$1(S3AFileSystem.java:381)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	... 32 more
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] Caused by: com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@344c649f: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@19c1f446: Failed to connect to service endpoint: ]
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:136)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:137)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 	... 50 more
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 14.0 in stage 6.0 (TID 34). 3723 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 35
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Running task 0.0 in stage 8.0 (TID 35)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO TorrentBroadcast: Started reading broadcast variable 10 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 15.3 KiB, free 911.7 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO TorrentBroadcast: Reading broadcast variable 10 took 9 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 31.9 KiB, free 911.7 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO CodeGenerator: Code generated in 12.586551 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 0.0 in stage 8.0 (TID 35). 3766 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 37
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Running task 2.0 in stage 8.0 (TID 37)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 2.0 in stage 8.0 (TID 37). 3766 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 38
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Running task 3.0 in stage 8.0 (TID 38)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 3.0 in stage 8.0 (TID 38). 3902 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 40
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Running task 5.0 in stage 8.0 (TID 40)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 5.0 in stage 8.0 (TID 40). 3902 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 42
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Running task 7.0 in stage 8.0 (TID 42)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 7.0 in stage 8.0 (TID 42). 3902 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 44
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Running task 9.0 in stage 8.0 (TID 44)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterat21/11/14 16:56:50 INFO TaskSetManager: Finished task 14.0 in stage 8.0 (TID 49) in 27 ms on algo-2 (executor 1) (15/15)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: ShuffleMapStage 8 (collect at HelloScalaSparkApp.scala:61) finished in 0.297 s
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: looking for newly runnable stages
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: running: Set()
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: waiting: Set()
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: failed: Set()
[36malgo-1    |[0m 21/11/14 16:56:50 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 16.
[36malgo-1    |[0m 21/11/14 16:56:50 INFO CodeGenerator: Code generated in 19.26166 ms
[36malgo-1    |[0m 21/11/14 16:56:50 INFO SparkContext: Starting job: collect at HelloScalaSparkApp.scala:61
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: Got job 7 (collect at HelloScalaSparkApp.scala:61) with 7 output partitions
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: Final stage: ResultStage 11 (collect at HelloScalaSparkApp.scala:61)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[28] at collect at HelloScalaSparkApp.scala:61), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:56:50 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 26.7 KiB, free 1007.0 MiB)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 1007.0 MiB)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.21.0.2:46499 (size: 13.4 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:56:50 INFO DAGScheduler: Submitting 7 missing tasks from ResultStage 11 (MapPartitionsRDD[28] at collect at HelloScalaSparkApp.scala:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))
[36malgo-1    |[0m 21/11/14 16:56:50 INFO YarnScheduler: Adding task set 11.0 with 7 tasks
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 50, algo-1, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 51, algo-2, executor 1, partition 1, NODE_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:36675 (size: 13.4 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-2:43215 (size: 13.4 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.21.0.3:37246
[36malgo-1    |[0m 21/11/14 16:56:50 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.21.0.2:44174
[36malgo-1    |[0m 21/11/14 16:56:50 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.21.0.2:46499 in memory (size: 15.3 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:36675 in memory (size: 15.3 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO BlockManagerInfo: Removed broadcast_10_piece0 on algo-2:43215 in memory (size: 15.3 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.21.0.2:46499 in memory (size: 30.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO BlockManagerInfo: Removed broadcast_5_piece0 on algo-2:43215 in memory (size: 30.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.21.0.2:46499 in memory (size: 14.8 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:36675 in memory (size: 14.8 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO BlockManagerInfo: Removed broadcast_9_piece0 on algo-2:43215 in memory (size: 14.8 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Starting task 2.0 in stage 11.0 (TID 52, algo-2, executor 1, partition 2, RACK_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:56:50 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 51) in 91 ms on algo-2 (executor 1) (1/7)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.21.0.2:46499 in memory (size: 13.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:36675 in memory (size: 13.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO TaskSetManager: Starting task 3.0 in stage 11.0 (TID 53, algo-2, executor 1, partition 3, RACK_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO TaskSetManager: Finished task 2.0 in stage 11.0 (TID 52) in 27 ms on algo-2 (executor 1) (2/7)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO TaskSetManager: Starting task 4.0 in stage 11.0 (TID 54, algo-2, executor 1, partition 4, RACK_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO TaskSetManager: Finished task 3.0 in stage 11.0 (TID 53) in 19 ms on algo-2 (executor 1) (3/7)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO TaskSetManager: Starting task 5.0 in stage 11.0 (TID 55, algo-2, executor 1, partition 5, RACK_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO TaskSetManager: Finished task 4.0 in stage 11.0 (TID 54) in 21 ms on algo-2 (executor 1) (4/7)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO TaskSetManager: Starting task 6.0 in stage 11.0 (TID 56, algo-1, executor 2, partition 6, NODE_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 50) in 169 ms on algo-1 (executor 2) (5/7)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO TaskSetManager: Finished task 5.0 in stage 11.0 (TID 55) in 18 ms on algo-2 (executor 1) (6/7)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO TaskSetManager: Finished task 6.0 in stage 11.0 (TID 56) in 17 ms on algo-1 (executor 2) (7/7)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: ResultStage 11 (collect at HelloScalaSparkApp.scala:61) finished in 0.195 s
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:56:51 INFO YarnScheduler: Killing all running tasks in stage 11: Stage finished
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: Job 7 finished: collect at HelloScalaSparkApp.scala:61, took 0.206624 s
[36malgo-1    |[0m 21/11/14 16:56:51 INFO CodeGenerator: Code generated in 11.872293 ms
[36malgo-1    |[0m [Lorg.apache.spark.sql.Row;@45e68fac
[36malgo-1    |[0m 21/11/14 16:56:51 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 16:56:51 INFO FileSourceStrategy: Pushed Filters: 
[36malgo-1    |[0m 21/11/14 16:56:51 INFO FileSourceStrategy: Post-Scan Filters: 
[36malgo-1    |[0m 21/11/14 16:56:51 INFO FileSourceStrategy: Output Data Schema: struct<date: string, sale: bigint>
[36malgo-1    |[0m 21/11/14 16:56:51 INFO CodeGenerator: Code generated in 8.479511 ms
[36malgo-1    |[0m 21/11/14 16:56:51 INFO CodeGenerator: Code generated in 14.059815 ms
[36malgo-1    |[0m 21/11/14 16:56:51 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 317.8 KiB, free 1007.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 31.1 KiB, free 911.8 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@172.21.0.2:44909)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO MapOutputTrackerWorker: Got the output locations
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO TransportClientFactory: Successfully created connection to algo-1/172.21.0.2:36675 after 1 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 21 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO CodeGenerator: Code generated in 32.400547 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO CodeGenerator: Code generated in 8.724622 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO CodeGenerator: Code generated in 9.098653 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 1.0 in stage 6.0 (TID 21). 3890 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 27
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Running task 7.0 in stage 6.0 (TID 27)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 7.0 in stage 6.0 (TID 27). 3890 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 29
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Running task 9.0 in stage 6.0 (TID 29)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 9.0 in stage 6.0 (TID 29). 3890 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 31
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Running task 11.0 in stage 6.0 (TID 31)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 11.0 in stage 6.0 (TID 31). 3890 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 33
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Running task 13.0 in stage 6.0 (TID 33)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 13.0 in stage 6.0 (TID 33). 3890 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 36
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Running task 1.0 in stage 8.0 (TID 36)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO TorrentBroadcast: Started reading broadcast variable 10 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 15.3 KiB, free 911.8 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO TorrentBroadcast: Reading broadcast variable 10 took 11 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 31.9 KiB, free 911.7 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO CodeGenerator: Code generated in 8.677383 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 1.0 in stage 8.0 (TID 36). 3945 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 39
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Running task 4.0 in stage 8.0 (TID 39)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 4.0 in stage 8.0 (TID 39). 3766 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 41
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Running task 6.0 in stage 8.0 (TID 41)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 6.0 in stage 8.0 (TID 41). 3766 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 43
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Running task 8.0 in stage 8.0 (TID 43)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 8.0 in stage 8.0 (TID 43). 3766 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 45
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Running task 10.0 in stage 8.0 (TID 45)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 10.0 in stage 8.0 (TID 45). 3766 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 47
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Running task 12.0 in stage 8.0 (TID 47)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 12.0 in stage 8.0 (TID 47). 3766 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 49
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Running task 14.0 in stage 8.0 (TID 49)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 14.0 in stage 8.0 (TID 49). 3766 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 51
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Running task 1.0 in stage 11.0 (TID 51)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO MapOutputTrackerWorker: Updating epoch to 2 and clearing cache
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO TorrentBroadcast: Started reading broadcast variable 11 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 911.7 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO TorrentBroadcast: Reading broadcast variable 11 took 10 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 26.7 KiB, free 911.7 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@172.21.0.2:44909)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO MapOutputTrackerWorker: Got the output locations
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO CodeGenerator: Code generated in 18.452533 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO CodeGenerator: Code generated in 20.315127 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 1.0 in stage 11.0 (TID 51). 4642 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 52
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO Executor: Running task 2.0 in stage 11.0 (TID 52)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO Executor: Finished task 2.0 in stage 11.0 (TID 52). 4645 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 53
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO Executor: Running task 3.0 in stage 11.0 (TID 53)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO Executor: Finished task 3.0 in stage 11.0 (TID 53). 4646 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 54
[36malgo-1    |[0m 21/11/14 16:56:51 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.21.0.2:46499 (size: 30.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO SparkContext: Created broadcast 12 from show at HelloScalaSparkApp.scala:68
[36malgo-1    |[0m 21/11/14 16:56:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 16:56:51 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 16:56:51 INFO SparkContext: Starting job: show at HelloScalaSparkApp.scala:68
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: Got job 8 (show at HelloScalaSparkApp.scala:68) with 1 output partitions
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: Final stage: ResultStage 12 (show at HelloScalaSparkApp.scala:68)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[33] at show at HelloScalaSparkApp.scala:68), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:56:51 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 15.9 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.21.0.2:46499 (size: 7.7 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[33] at show at HelloScalaSparkApp.scala:68) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 16:56:51 INFO YarnScheduler: Adding task set 12.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 16:56:51 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 57, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7755 bytes)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:36675 (size: 7.7 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:36675 (size: 30.2 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 57) in 135 ms on algo-1 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO YarnScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: ResultStage 12 (show at HelloScalaSparkApp.scala:68) finished in 0.144 s
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:56:51 INFO YarnScheduler: Killing all running tasks in stage 12: Stage finished
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: Job 8 finished: show at HelloScalaSparkApp.scala:68, took 0.146365 s
[36malgo-1    |[0m 21/11/14 16:56:51 INFO CodeGenerator: Code generated in 12.528978 ms
[36malgo-1    |[0m 21/11/14 16:56:51 INFO CodeGenerator: Code generated in 7.818924 ms
[36malgo-1    |[0m +----------+----+-----------+
[36malgo-1    |[0m |      date|sale|sale_double|
[36malgo-1    |[0m +----------+----+-----------+
[36malgo-1    |[0m |2020-01-01|   5|         10|
[36malgo-1    |[0m |2020-01-01|   7|         14|
[36malgo-1    |[0m |2020-01-01|  15|         30|
[36malgo-1    |[0m |2020-01-01|  15|         30|
[36malgo-1    |[0m |2020-01-01|  23|         46|
[36malgo-1    |[0m |2020-01-01|  33|         66|
[36malgo-1    |[0m |2020-01-01|  34|         68|
[36malgo-1    |[0m |2020-01-01|  39|         78|
[36malgo-1    |[0m |2020-01-01|  40|         80|
[36malgo-1    |[0m |2020-01-01|  43|         86|
[36malgo-1    |[0m |2020-01-01|  48|         96|
[36malgo-1    |[0m |2020-01-01|  59|        118|
[36malgo-1    |[0m |2020-01-01|  64|        128|
[36malgo-1    |[0m |2020-01-01|  79|        158|
[36malgo-1    |[0m |2020-01-01|  82|        164|
[36malgo-1    |[0m |2020-01-01|  85|        170|
[36malgo-1    |[0m |2020-01-01| 103|        206|
[36malgo-1    |[0m |2020-01-01| 105|        210|
[36malgo-1    |[0m |2020-01-01| 105|        210|
[36malgo-1    |[0m |2020-01-01| 122|        244|
[36malgo-1    |[0m +----------+----+-----------+
[36malgo-1    |[0m only showing top 20 rows
[36malgo-1    |[0m 
[36malgo-1    |[0m Writing output to: file:///opt/ml/processing/output/data
[36malgo-1    |[0m 21/11/14 16:56:51 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 16:56:51 INFO FileSourceStrategy: Pushed Filters: 
[36malgo-1    |[0m 21/11/14 16:56:51 INFO FileSourceStrategy: Post-Scan Filters: 
[36malgo-1    |[0m 21/11/14 16:56:51 INFO FileSourceStrategy: Output Data Schema: struct<date: string, sale: bigint>
[36malgo-1    |[0m 21/11/14 16:56:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
[36malgo-1    |[0m 21/11/14 16:56:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DirectFileOutputCommitter: Direct Write: DISABLED
[36malgo-1    |[0m 21/11/14 16:56:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter
[36malgo-1    |[0m 21/11/14 16:56:51 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 317.8 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.21.0.2:46499 (size: 30.2 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO SparkContext: Created broadcast 14 from json at HelloScalaSparkApp.scala:72
[36malgo-1    |[0m 21/11/14 16:56:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 16:56:51 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 16:56:51 INFO SparkContext: Starting job: json at HelloScalaSparkApp.scala:72
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: Got job 9 (json at HelloScalaSparkApp.scala:72) with 1 output partitions
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: Final stage: ResultStage 13 (json at HelloScalaSparkApp.scala:72)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[39] at json at HelloScalaSparkApp.scala:72), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:56:51 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 15.4 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 1006.7 MiB)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.21.0.2:46499 (size: 7.6 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[39] at json at HelloScalaSparkApp.scala:72) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 16:56:51 INFO YarnScheduler: Adding task set 13.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 16:56:51 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 58, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7755 bytes)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:36675 (size: 7.6 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:36675 (size: 30.2 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 58) in 39 ms on algo-1 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: ResultStage 13 (json at HelloScalaSparkApp.scala:72) finished in 0.045 s
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:56:51 INFO YarnScheduler: Killing all running tasks in stage 13: Stage finished
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: Job 9 finished: json at HelloScalaSparkApp.scala:72, took 0.047925 s
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: Registering RDD 40 (json at HelloScalaSparkApp.scala:72) as input to shuffle 2
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: Got map stage job 10 (json at HelloScalaSparkApp.scala:72) with 1 output partitions
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: Final stage: ShuffleMapStage 14 (json at HelloScalaSparkApp.scala:72)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[40] at json at HelloScalaSparkApp.scala:72), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:56:51 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 70.6 KiB, free 1006.7 MiB)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 15.2 KiB, free 1006.7 MiB)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.21.0.2:46499 (size: 15.2 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[40] at json at HelloScalaSparkApp.scala:72) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 16:56:51 INFO YarnScheduler: Adding task set 14.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 16:56:51 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 59, algo-2, executor 1, partition 0, PROCESS_LOCAL, 7744 bytes)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-2:43215 (size: 15.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-2:43215 (size: 30.2 KiB, free: 912.2 MiB)
[36malgo-1    |[0m or: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 9.0 in stage 8.0 (TID 44). 3902 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 46
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Running task 11.0 in stage 8.0 (TID 46)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 11.0 in stage 8.0 (TID 46). 3902 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 48
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Running task 13.0 in stage 8.0 (TID 48)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Finished task 13.0 in stage 8.0 (TID 48). 3902 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 50
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO Executor: Running task 0.0 in stage 11.0 (TID 50)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO MapOutputTrackerWorker: Updating epoch to 2 and clearing cache
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO TorrentBroadcast: Started reading broadcast variable 11 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 911.7 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO TorrentBroadcast: Reading broadcast variable 11 took 10 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 26.7 KiB, free 911.7 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@172.21.0.2:44909)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO MapOutputTrackerWorker: Got the output locations
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:50 INFO CodeGenerator: Code generated in 26.816571 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO CodeGenerator: Code generated in 19.681634 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO Executor: Finished task 0.0 in stage 11.0 (TID 50). 4645 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 56
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO Executor: Running task 6.0 in stage 11.0 (TID 56)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO Executor: Finished task 6.0 in stage 11.0 (TID 56). 4646 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 57
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO Executor: Running task 0.0 in stage 12.0 (TID 57)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO TorrentBroadcast: Started reading broadcast variable 13 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO TorrentBroadcast: Reading broadcast variable 13 took 6 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 15.9 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO CodeGenerator: Code generated in 7.829275 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO CodeGenerator: Code generated in 11.734003 ms
[36malgo-1    |[0m [/21/11/14 16:56:51 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 59) in 210 ms on algo-2 (executor 1) (1/1)
[36malgo-1    |[0m 21/11/14 16:56:51 INFO YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: ShuffleMapStage 14 (json at HelloScalaSparkApp.scala:72) finished in 0.218 s
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: looking for newly runnable stages
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: running: Set()
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: waiting: Set()
[36malgo-1    |[0m 21/11/14 16:56:51 INFO DAGScheduler: failed: Set()
[36malgo-1    |[0m 21/11/14 16:56:51 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 2155.
[36malgo-1    |[0m 21/11/14 16:56:51 INFO CodeGenerator: Code generated in 13.29154 ms
[36malgo-1    |[0m 21/11/14 16:56:52 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.21.0.2:46499 in memory (size: 7.6 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:56:52 INFO BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:36675 in memory (size: 7.6 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:56:52 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 172.21.0.2:46499 in memory (size: 15.2 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:56:52 INFO BlockManagerInfo: Removed broadcast_16_piece0 on algo-2:43215 in memory (size: 15.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:56:52 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.21.0.2:46499 in memory (size: 30.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:52 INFO BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:36675 in memory (size: 30.2 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:56:52 INFO SparkContext: Starting job: json at HelloScalaSparkApp.scala:72
[36malgo-1    |[0m 21/11/14 16:56:52 INFO BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:36675 in memory (size: 13.4 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:56:52 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.21.0.2:46499 in memory (size: 13.4 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:52 INFO DAGScheduler: Got job 11 (json at HelloScalaSparkApp.scala:72) with 1 output partitions
[36malgo-1    |[0m 21/11/14 16:56:52 INFO DAGScheduler: Final stage: ResultStage 16 (json at HelloScalaSparkApp.scala:72)
[36malgo-1    |[0m 21/11/14 16:56:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)
[36malgo-1    |[0m 21/11/14 16:56:52 INFO BlockManagerInfo: Removed broadcast_11_piece0 on algo-2:43215 in memory (size: 13.4 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:56:52 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:56:52 INFO DAGScheduler: Submitting ResultStage 16 (CoalescedRDD[44] at json at HelloScalaSparkApp.scala:72), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:56:52 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.21.0.2:46499 in memory (size: 7.7 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:56:52 INFO BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:36675 in memory (size: 7.7 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:56:52 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 196.2 KiB, free 1007.0 MiB)
[36malgo-1    |[0m 21/11/14 16:56:52 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 73.1 KiB, free 1006.9 MiB)
[36malgo-1    |[0m 21/11/14 16:56:52 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.21.0.2:46499 (size: 73.1 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:56:52 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:56:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (CoalescedRDD[44] at json at HelloScalaSparkApp.scala:72) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 16:56:52 INFO YarnScheduler: Adding task set 16.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 16:56:52 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 60, algo-2, executor 1, partition 0, NODE_LOCAL, 8312 bytes)
[36malgo-1    |[0m 21/11/14 16:56:52 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-2:43215 (size: 73.1 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:56:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.21.0.3:37246
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO Executor: Running task 4.0 in stage 11.0 (TID 54)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO Executor: Finished task 4.0 in stage 11.0 (TID 54). 4646 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 55
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO Executor: Running task 5.0 in stage 11.0 (TID 55)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO Executor: Finished task 5.0 in stage 11.0 (TID 55). 4646 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 59
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO Executor: Running task 0.0 in stage 14.0 (TID 59)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO TorrentBroadcast: Started reading broadcast variable 16 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 15.2 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO TorrentBroadcast: Reading broadcast variable 16 took 5 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 70.6 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO CodeGenerator: Code generated in 10.458912 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO CodeGenerator: Code generated in 16.122845 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO FileScanRDD: TID: 59 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO TorrentBroadcast: Started reading broadcast variable 14 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 912.1 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO TorrentBroadcast: Reading broadcast variable 14 took 5 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 445.1 KiB, free 911.7 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:51 INFO Executor: Finished task 0.0 in stage 14.0 (TID 59). 3699 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 60
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO Executor: Running task 0.0 in stage 16.0 (TID 60)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO MapOutputTrackerWorker: Updating epoch to 3 and clearing cache
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO TorrentBroadcast: Started reading broadcast variable 17 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 73.1 KiB, free 911.8 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO TorrentBroadcast: Reading broadcast variable 17 took 5 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 196.2 KiB, free 911.6 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO DirectFileOutputCommitter: Direct Write: DISABLED
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@172.21.0.2:44909)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO MapOutputTrackerWorker: Got the output locations
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m 21/11/14 16:56:52 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 60) in 387 ms on algo-2 (executor 1) (1/1)
[36malgo-1    |[0m 21/11/14 16:56:52 INFO YarnScheduler: Removed TaskSet 16.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:56:52 INFO DAGScheduler: ResultStage 16 (json at HelloScalaSparkApp.scala:72) finished in 0.411 s
[36malgo-1    |[0m 21/11/14 16:56:52 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:56:52 INFO YarnScheduler: Killing all running tasks in stage 16: Stage finished
[36malgo-1    |[0m 21/11/14 16:56:52 INFO DAGScheduler: Job 11 finished: json at HelloScalaSparkApp.scala:72, took 0.420918 s
[36malgo-1    |[0m 21/11/14 16:56:52 INFO FileFormatWriter: Write Job 233ae3cd-a58d-4529-97e6-7a5f5c7a1a9d committed.
[36malgo-1    |[0m 21/11/14 16:56:52 INFO FileFormatWriter: Finished processing stats for write job 233ae3cd-a58d-4529-97e6-7a5f5c7a1a9d.
[36malgo-1    |[0m 21/11/14 16:56:52 INFO SparkUI: Stopped Spark web UI at http://172.21.0.2:4040
[36malgo-1    |[0m 21/11/14 16:56:52 INFO YarnClientSchedulerBackend: Interrupting monitor thread
[36malgo-1    |[0m 21/11/14 16:56:52 INFO YarnClientSchedulerBackend: Shutting down all executors
[36malgo-1    |[0m 21/11/14 16:56:52 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
[36malgo-1    |[0m 21/11/14 16:56:52 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
[36malgo-1    |[0m 21/11/14 16:56:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[36malgo-1    |[0m 21/11/14 16:56:52 INFO MemoryStore: MemoryStore cleared
[36malgo-1    |[0m 21/11/14 16:56:52 INFO BlockManager: BlockManager stopped
[36malgo-1    |[0m 21/11/14 16:56:52 INFO BlockManagerMaster: BlockManagerMaster stopped
[36malgo-1    |[0m 21/11/14 16:56:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[36malgo-1    |[0m 21/11/14 16:56:52 INFO SparkContext: Successfully stopped SparkContext
[36malgo-1    |[0m 21/11/14 16:56:52 INFO ShutdownHookManager: Shutdown hook called
[36malgo-1    |[0m 21/11/14 16:56:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-232c3eae-cfc2-4dad-8665-90cd78d908d1
[36malgo-1    |[0m 21/11/14 16:56:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-b1d67efe-a586-455c-9d9a-b361b952a4fe
[36malgo-1    |[0m 2021-11-14 16:56:52,582 INFO attempt.RMAppAttemptImpl: Updating application attempt appattempt_1636908974098_0001_000001 with final state: FINISHING, and exit status: -1000
[36malgo-1    |[0m 2021-11-14 16:56:52,583 INFO attempt.RMAppAttemptImpl: appattempt_1636908974098_0001_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
[36malgo-1    |[0m 2021-11-14 16:56:52,583 INFO rmapp.RMAppImpl: Updating application application_1636908974098_0001 with final state: FINISHING
[36malgo-1    |[0m 2021-11-14 16:56:52,583 INFO recovery.RMStateStore: Updating info for app: application_1636908974098_0001
[36malgo-1    |[0m 2021-11-14 16:56:52,583 INFO rmapp.RMAppImpl: application_1636908974098_0001 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
[36malgo-1    |[0m 2021-11-14 16:56:52,583 INFO attempt.RMAppAttemptImpl: appattempt_1636908974098_0001_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
[36malgo-1    |[0m 2021-11-14 16:56:52,584 INFO rmapp.RMAppImpl: application_1636908974098_0001 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/con2021-11-14 16:56:52,598 INFO launcher.ContainerLaunch: Container container_1636908974098_0001_01_000002 succeeded 
[33malgo-2    |[0m 2021-11-14 16:56:52,606 INFO container.ContainerImpl: Container container_1636908974098_0001_01_000002 transitioned from RUNNING to EXITED_WITH_SUCCESS
[33malgo-2    |[0m 2021-11-14 16:56:52,607 INFO launcher.ContainerCleanup: Cleaning up container container_1636908974098_0001_01_000002
[33malgo-2    |[0m 2021-11-14 16:56:52,608 INFO nodemanager.NMAuditLogger: USER=root	OPERATION=Container Finished - Succeeded	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1636908974098_0001	CONTAINERID=container_1636908974098_0001_01_000002
[33malgo-2    |[0m 2021-11-14 16:56:52,609 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000002
[33malgo-2    |[0m 2021-11-14 16:56:52,610 INFO container.ContainerImpl: Container container_1636908974098_0001_01_000002 transitioned from EXITED_WITH_SUCCESS to DONE
[33malgo-2    |[0m 2021-11-14 16:56:52,611 INFO application.ApplicationImpl: Removing container_1636908974098_0001_01_000002 from application application_1636908974098_0001
[33malgo-2    |[0m 2021-11-14 16:56:52,611 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1636908974098_0001_01_000002
[33malgo-2    |[0m 2021-11-14 16:56:52,611 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1636908974098_0001
[36malgo-1    |[0m 2021-11-14 16:56:52,614 INFO rmcontainer.RMContainerImpl: container_1636908974098_0001_01_000002 Container Transitioned from RUNNING to COMPLETED
[36malgo-1    |[0m 2021-11-14 16:56:52,614 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636908974098_0001	CONTAINERID=container_1636908974098_0001_01_000002	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 16:56:52,618 INFO launcher.ContainerLaunch: Container container_1636908974098_0001_01_000003 succeeded 
[33malgo-2    |[0m 2021-11-14 16:56:52,624 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000002/launch_container.sh
[33malgo-2    |[0m 2021-11-14 16:56:52,624 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000002/launch_container.sh]
[33malgo-2    |[0m 2021-11-14 16:56:52,624 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000002/container_tokens
[33malgo-2    |[0m 2021-11-14 16:56:52,624 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000002/container_tokens]
[33malgo-2    |[0m 2021-11-14 16:56:52,624 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000002/sysfs
[33malgo-2    |[0m 2021-11-14 16:56:52,624 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000002/sysfs]
[36malgo-1    |[0m 2021-11-14 16:56:52,626 INFO container.ContainerImpl: Container container_1636908974098_0001_01_000003 transitioned from RUNNING to EXITED_WITH_SUCCESS
[36malgo-1    |[0m 2021-11-14 16:56:52,627 INFO launcher.ContainerCleanup: Cleaning up container container_1636908974098_0001_01_000003
[36malgo-1    |[0m 2021-11-14 16:56:52,628 INFO nodemanager.NMAuditLogger: USER=root	OPERATION=Container Finished - Succeeded	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1636908974098_0001	CONTAINERID=container_1636908974098_0001_01_000003
[36malgo-1    |[0m 2021-11-14 16:56:52,629 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000003
[36malgo-1    |[0m 2021-11-14 16:56:52,630 INFO container.ContainerImpl: Container container_1636908974098_0001_01_000003 transitioned from EXITED_WITH_SUCCESS to DONE
[36malgo-1    |[0m 2021-11-14 16:56:52,631 INFO application.ApplicationImpl: Removing container_1636908974098_0001_01_000003 from application application_1636908974098_0001
[36malgo-1    |[0m 2021-11-14 16:56:52,631 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1636908974098_0001_01_000003
[36malgo-1    |[0m 2021-11-14 16:56:52,631 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1636908974098_0001
[36malgo-1    |[0m 2021-11-14 16:56:52,634 INFO rmcontainer.RMContainerImpl: container_1636908974098_0001_01_000003 Container Transitioned from RUNNING to COMPLETED
[36malgo-1    |[0m 2021-11-14 16:56:52,634 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636908974098_0001	CONTAINERID=container_1636908974098_0001_01_000003	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 16:56:52,644 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000003/launch_container.sh
[36malgo-1    |[0m 2021-11-14 16:56:52,644 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000003/launch_container.sh]
[36malgo-1    |[0m 2021-11-14 16:56:52,644 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000003/container_tokens
[36malgo-1    |[0m 2021-11-14 16:56:52,644 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000003/container_tokens]
[36malgo-1    |[0m 2021-11-14 16:56:52,644 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000003/sysfs
[36malgo-1    |[0m 2021-11-14 16:56:52,644 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000003/sysfs]
[36malgo-1    |[0m 2021-11-14 16:56:52,685 INFO resourcemanager.ApplicationMasterService: application_1636908974098_0001 unregistered successfully. 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stdout] 2021-11-14T16:56:41.952+0000: [GC (Allocation Failure) [PSYoungGen: 120320K->9037K(140288K)] 120320K->9053K(461312K), 0.0081300 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stdout] 2021-11-14T16:56:42.428+0000: [GC (Allocation Failure) [PSYoungGen: 129357K->7736K(140288K)] 129373K->7760K(461312K), 0.0058117 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stdout] 2021-11-14T16:56:42.750+0000: [GC (Metadata GC Threshold) [PSYoungGen: 109363K->8648K(140288K)] 109387K->8680K(461312K), 0.0047407 secs] [Times: user=0.02 sys=0.00, real=0.00 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stdout] 2021-11-14T16:56:42.755+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 8648K->0K(140288K)] [ParOldGen: 32K->8404K(190464K)] 8680K->8404K(330752K), [Metaspace: 20379K->20379K(1067008K)], 0.0224069 secs] [Times: user=0.03 sys=0.01, real=0.03 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stdout] 2021-11-14T16:56:43.225+0000: [GC (Allocation Failure) [PSYoungGen: 120320K->4596K(183808K)] 128724K->13009K(374272K), 0.0051615 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stdout] 2021-11-14T16:56:43.659+0000: [GC (Allocation Failure) [PSYoungGen: 183796K->7509K(245248K)] 192209K->15922K(435712K), 0.0080882 secs] [Times: user=0.00 sys=0.01, real=0.00 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stdout] 2021-11-14T16:56:43.758+0000: [GC (Metadata GC Threshold) [PSYoungGen: 45814K->3955K(283648K)] 54227K->12376K(474112K), 0.0040529 secs] [Times: user=0.01 sys=0.01, real=0.00 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stdout] 2021-11-14T16:56:43.762+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 3955K->0K(283648K)] [ParOldGen: 8420K->10099K(277504K)] 12376K->10099K(561152K), [Metaspace: 33949K->33946K(1079296K)], 0.0239685 secs] [Times: user=0.05 sys=0.00, real=0.02 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stdout] 2021-11-14T16:56:45.266+0000: [GC (Allocation Failure) [PSYoungGen: 273408K->10741K(284160K)] 283507K->22159K(561664K), 0.0108562 secs] [Times: user=0.03 sys=0.00, real=0.02 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stdout] 2021-11-14T16:56:45.935+0000: [GC (Metadata GC Threshold) [PSYoungGen: 235151K->12798K(417280K)] 246570K->25538K(694784K), 0.0109928 secs] [Times: user=0.03 sys=0.00, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stdout] 2021-11-14T16:56:45.946+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 12798K->0K(417280K)] [ParOldGen: 12739K->22582K(385536K)] 25538K->22582K(802816K), [Metaspace: 53513K->53513K(1099776K)], 0.0778745 secs] [Times: user=0.20 sys=0.01, real=0.08 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stdout] 2021-11-14T16:56:47.449+0000: [GC (Allocation Failure) [PSYoungGen: 404480K->12699K(419328K)] 427062K->35290K(804864K), 0.0107323 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stdout] 2021-11-14T16:56:52.534+0000: [GC (Allocation Failure) [PSYoungGen: 417179K->14072K(498688K)] 439770K->36671K(884224K), 0.0150626 secs] [Times: user=0.03 sys=0.00, real=0.02 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stvar/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO FileScanRDD: TID: 57 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO TorrentBroadcast: Started reading broadcast variable 12 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 911.7 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO TorrentBroadcast: Reading broadcast variable 12 took 8 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 445.1 KiB, free 911.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO Executor: Finished task 0.0 in stage 12.0 (TID 57). 5505 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 58
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO Executor: Running task 0.0 in stage 13.0 (TID 58)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO TorrentBroadcast: Started reading broadcast variable 15 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 911.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO TorrentBroadcast: Reading broadcast variable 15 took 5 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 15.4 KiB, free 911.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO FileScanRDD: TID: 58 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO TorrentBroadcast: Started reading broadcast variable 14 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 911.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO TorrentBroadcast: Reading broadcast variable 14 took 4 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 445.1 KiB, free 910.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000003/stderr] 21/11/14 16:56:51 INFO Executor: Finished task 0.0 in stage 13.0 (TID 58). 58966 bytes result sent to driver
[36malgo-1    |[0m [/var/lo11-14 16:56 smspark-submit INFO     spark submit was successful. primary node exiting.
[33malgo-2    |[0m 2021-11-14 16:56:53,046 INFO launcher.ContainerLaunch: Container container_1636908974098_0001_01_000001 succeeded 
[33malgo-2    |[0m 2021-11-14 16:56:53,047 INFO container.ContainerImpl: Container container_1636908974098_0001_01_000001 transitioned from RUNNING to EXITED_WITH_SUCCESS
[33malgo-2    |[0m 2021-11-14 16:56:53,047 INFO launcher.ContainerCleanup: Cleaning up container container_1636908974098_0001_01_000001
[33malgo-2    |[0m 2021-11-14 16:56:53,047 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000001
[33malgo-2    |[0m 2021-11-14 16:56:53,047 INFO nodemanager.NMAuditLogger: USER=root	OPERATION=Container Finished - Succeeded	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1636908974098_0001	CONTAINERID=container_1636908974098_0001_01_000001
[33malgo-2    |[0m 2021-11-14 16:56:53,048 INFO container.ContainerImpl: Container container_1636908974098_0001_01_000001 transitioned from EXITED_WITH_SUCCESS to DONE
[33malgo-2    |[0m 2021-11-14 16:56:53,048 INFO application.ApplicationImpl: Removing container_1636908974098_0001_01_000001 from application application_1636908974098_0001
[33malgo-2    |[0m 2021-11-14 16:56:53,048 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1636908974098_0001_01_000001
[33malgo-2    |[0m 2021-11-14 16:56:53,048 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1636908974098_0001
[33malgo-2    |[0m 2021-11-14 16:56:53,053 INFO retry.RetryInvocationHandler: java.io.EOFException: End of File Exception between local host is: "algo-2/172.21.0.3"; destination host is: "algo-1.spark-network":8031; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException, while invoking ResourceTrackerPBClientImpl.nodeHeartbeat over null. Retrying after sleeping for 30000ms.
[33malgo-2    |[0m 2021-11-14 16:56:53,059 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000001/launch_container.sh
[33malgo-2    |[0m 2021-11-14 16:56:53,059 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000001/launch_container.sh]
[33malgo-2    |[0m 2021-11-14 16:56:53,059 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000001/container_tokens
[33malgo-2    |[0m 2021-11-14 16:56:53,059 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000001/container_tokens]
[33malgo-2    |[0m 2021-11-14 16:56:53,059 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000001/sysfs
[33malgo-2    |[0m 2021-11-14 16:56:53,059 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636908974098_0001/container_1636908974098_0001_01_000001/sysfs]
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stdout] 2021-11-14T16:56:37.416+0000: [GC (Allocation Failure) [PSYoungGen: 120320K->9100K(140288K)] 120320K->9116K(461312K), 0.0083867 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stdout] 2021-11-14T16:56:37.876+0000: [GC (Allocation Failure) [PSYoungGen: 129420K->7651K(140288K)] 129436K->7675K(461312K), 0.0058420 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stdout] 2021-11-14T16:56:38.192+0000: [GC (Metadata GC Threshold) [PSYoungGen: 109310K->8690K(140288K)] 109334K->8722K(461312K), 0.0047366 secs] [Times: user=0.01 sys=0.01, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stdout] 2021-11-14T16:56:38.197+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 8690K->0K(140288K)] [ParOldGen: 32K->8403K(190976K)] 8722K->8403K(331264K), [Metaspace: 20377K->20377K(1067008K)], 0.0229700 secs] [Times: user=0.05 sys=0.00, real=0.02 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stdout] 2021-11-14T16:56:38.693+0000: [GC (Allocation Failure) [PSYoungGen: 120320K->4634K(182272K)] 128723K->13046K(373248K), 0.0043202 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stdout] 2021-11-14T16:56:39.125+0000: [GC (Allocation Failure) [PSYoungGen: 181786K->7547K(245760K)] 190198K->15958K(436736K), 0.0070309 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stdout] 2021-11-14T16:56:39.231+0000: [GC (Metadata GC Threshold) [PSYoungGen: 50524K->3940K(275968K)] 58935K->12360K(466944K), 0.0041552 secs] [Times: user=0.01 sys=0.01, real=0.00 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stdout] 2021-11-14T16:56:39.235+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 3940K->0K(275968K)] [ParOldGen: 8419K->10068K(281600K)] 12360K->10068K(557568K), [Metaspace: 33959K->33957K(1079296K)], 0.0242342 secs] [Times: user=0.04 sys=0.00, real=0.03 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stdout] 2021-11-14T16:56:39.943+0000: [GC (GCLocker Initiated GC) [PSYoungGen: 265728K->10746K(276480K)] 275804K->22540K(558080K), 0.0116789 secs] [Times: user=0.07 sys=0.00, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stdout] 2021-11-14T16:56:44.276+0000: [GC (Metadata GC Threshold) [PSYoungGen: 256819K->12797K(397824K)] 268613K->26409K(679424K), 0.0132237 secs] [Times: user=0.04 sys=0.01, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stdout] 2021-11-14T16:56:44.289+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 12797K->0K(397824K)] [ParOldGen: 13612K->23026K(386560K)] 26409K->23026K(784384K), [Metaspace: 54005K->54005K(1099776K)], 0.0927628 secs] [Times: user=0.22 sys=0.01, real=0.09 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stdout] 2021-11-14T16:56:52.153+0000: [GC (Allocation Failure) [PSYoungGen: 385024K->9599K(410624K)] 408059K->32643K(797184K), 0.0095091 secs] [Times: user=0.03 sys=0.00, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stdout] Heap
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stdout]  PSYoungGen      total 410624K, used 126808K [0x00000000d5580000, 0x00000000f4100000, 0x0000000100000000)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/tainer_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO CodeGenerator: Code generated in 12.322837 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.1 KiB) non-empty blocks including 1 (3.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.1 KiB) non-empty blocks including 1 (3.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.0 KiB) non-empty blocks including 1 (3.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.1 KiB) non-empty blocks including 1 (3.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.0 KiB) non-empty blocks including 1 (3.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.0 KiB) non-empty blocks including 1 (3.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.0 KiB) non-empty blocks including 1 (3.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (1408.0 B) non-empty blocks including 1 (1408.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636908974098_0001/container_1636908974098_0001_01_000002/stderr] 21/11/14 16:56:52 INFO FileOutputCommitter: S2021-11-14 16:56:53,324 WARN datanode.DataNode: IOException in offerService
[33malgo-2    |[0m java.io.EOFException: End of File Exception between local host is: "algo-2/172.21.0.3"; destination host is: "algo-1.spark-network":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
[33malgo-2    |[0m 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[33malgo-2    |[0m 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[33malgo-2    |[0m 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[33malgo-2    |[0m 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
[33malgo-2    |[0m 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
[33malgo-2    |[0m 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:791)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
[33malgo-2    |[0m 	at com.sun.proxy.$Proxy16.sendHeartbeat(Unknown Source)
[33malgo-2    |[0m 	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:168)
[33malgo-2    |[0m 	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:517)
[33malgo-2    |[0m 	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
[33malgo-2    |[0m 	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
[33malgo-2    |[0m 	at java.lang.Thread.run(Thread.java:748)
[33malgo-2    |[0m Caused by: java.io.EOFException
[33malgo-2    |[0m 	at java.io.DataInputStream.readInt(DataInputStream.java:392)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)
[33malgo-2    |[0m 11-14 16:56 urllib3.connectionpool WARNING  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd5de1d0e90>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m 11-14 16:56 urllib3.connectionpool WARNING  Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd5de1d07d0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m 11-14 16:57 urllib3.connectionpool WARNING  Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd5de1ad210>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[36malgo-1 exited with code 0
[0m[33malgo-2    |[0m 11-14 16:57 urllib3.connectionpool WARNING  Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd5de1ad610>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m 11-14 16:57 urllib3.connectionpool WARNING  Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd5de1ada10>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m 11-14 16:57 smspark-submit INFO     primary is down, worker now exiting
[33malgo-2 exited with code 0
[0mThe CMD variable is not set. Defaulting to a blank string.
Removing algo-1 ... 
Removing algo-2 ... 
[1A[2KRemoving algo-2 ... [32mdone[0m[1B[2A[2KRemoving algo-1 ... [32mdone[0m[2BRemoving network spark-network


Running docker-compose down ...
PASSED
test/integration/local/test_multinode_container.py::test_java_spark_multinode CMD='--class com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp --verbose /opt/ml/processing/input/code/java/hello-java-spark/target/hello-java-spark-1.0-SNAPSHOT.jar --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data' docker-compose up --force-recreate
Creating network "spark-network" with the default driver
Creating algo-2 ... 
Creating algo-1 ... 
[1A[2K
Creating algo-1 ... [32mdone[0m
[1B[2A[2K
Creating algo-2 ... [32mdone[0m
[2BAttaching to algo-1, algo-2
[33malgo-2    |[0m 11-14 16:57 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--class', 'com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp', '--verbose', '/opt/ml/processing/input/code/java/hello-java-spark/target/hello-java-spark-1.0-SNAPSHOT.jar', '--input', 'file:///opt/ml/processing/input/data/data.jsonl', '--output', 'file:///opt/ml/processing/output/data']
[33malgo-2    |[0m 11-14 16:57 smspark.cli  INFO     Raw spark options before processing: {'class_': 'com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp', 'verbose': True, 'jars': None, 'py_files': None, 'files': None}
[33malgo-2    |[0m 11-14 16:57 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/java/hello-java-spark/target/hello-java-spark-1.0-SNAPSHOT.jar', '--input', 'file:///opt/ml/processing/input/data/data.jsonl', '--output', 'file:///opt/ml/processing/output/data']
[33malgo-2    |[0m 11-14 16:57 smspark.cli  INFO     Rendered spark options: {'class_': 'com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp', 'verbose': True, 'jars': None, 'py_files': None, 'files': None}
[33malgo-2    |[0m 11-14 16:57 smspark.cli  INFO     Initializing processing job.
[33malgo-2    |[0m 11-14 16:57 smspark-submit INFO     {'current_host': 'algo-2', 'hosts': ['algo-1', 'algo-2']}
[33malgo-2    |[0m 11-14 16:57 smspark-submit INFO     {'ProcessingJobArn': 'processing_job_arn', 'ProcessingJobName': 'processing_job_name', 'AppSpecification': {'ImageUri': 'image_uri'}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30}}, 'RoleArn': 'iam_role'}
[33malgo-2    |[0m 11-14 16:57 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client --class com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp --verbose /opt/ml/processing/input/code/java/hello-java-spark/target/hello-java-spark-1.0-SNAPSHOT.jar --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data
[33malgo-2    |[0m 11-14 16:57 smspark-submit INFO     waiting for hosts
[36malgo-1    |[0m 11-14 16:57 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--class', 'com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp', '--verbose', '/opt/ml/processing/input/code/java/hello-java-spark/target/hello-java-spark-1.0-SNAPSHOT.jar', '--input', 'file:///opt/ml/processing/input/data/data.jsonl', '--output', 'file:///opt/ml/processing/output/data']
[36malgo-1    |[0m 11-14 16:57 smspark.cli  INFO     Raw spark options before processing: {'class_': 'com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp', 'verbose': True, 'jars': None, 'py_files': None, 'files': None}
[36malgo-1    |[0m 11-14 16:57 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/java/hello-java-spark/target/hello-java-spark-1.0-SNAPSHOT.jar', '--input', 'file:///opt/ml/processing/input/data/data.jsonl', '--output', 'file:///opt/ml/processing/output/data']
[36malgo-1    |[0m 11-14 16:57 smspark.cli  INFO     Rendered spark options: {'class_': 'com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp', 'verbose': True, 'jars': None, 'py_files': None, 'files': None}
[33malgo-2    |[0m 11-14 16:57 smspark-submit INFO     starting status server
[33malgo-2    |[0m 11-14 16:57 smspark-submit INFO     Status server listening on algo-2:5555
[36malgo-1    |[0m 11-14 16:57 smspark.cli  INFO     Initializing processing job.
[36malgo-1    |[0m 11-14 16:57 smspark-submit INFO     {'current_host': 'algo-1', 'hosts': ['algo-1', 'algo-2']}
[33malgo-2    |[0m 11-14 16:57 smspark-submit INFO     bootstrapping cluster
[36malgo-1    |[0m 11-14 16:57 smspark-submit INFO     {'ProcessingJobArn': 'processing_job_arn', 'ProcessingJobName': 'processing_job_name', 'AppSpecification': {'ImageUri': 'image_uri'}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30}}, 'RoleArn': 'iam_role'}
[36malgo-1    |[0m 11-14 16:57 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client --class com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp --verbose /opt/ml/processing/input/code/java/hello-java-spark/target/hello-java-spark-1.0-SNAPSHOT.jar --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data
[33malgo-2    |[0m 11-14 16:57 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING
[33malgo-2    |[0m 11-14 16:57 smspark-submit INFO     copying aws jars
[36malgo-1    |[0m 11-14 16:57 smspark-submit INFO     waiting for hosts
[36malgo-1    |[0m 11-14 16:57 smspark-submit INFO     starting status server
[36malgo-1    |[0m 11-14 16:57 smspark-submit INFO     Status server listening on algo-1:5555
[36malgo-1    |[0m 11-14 16:57 smspark-submit INFO     bootstrapping cluster
[36malgo-1    |[0m 11-14 16:57 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING
[36malgo-1    |[0m 11-14 16:57 smspark-submit INFO     copying aws jars
[33malgo-2    |[0m Serving on http://algo-2:5555
[36malgo-1    |[0m Serving on http://algo-1:5555
[33malgo-2    |[0m 11-14 16:57 smspark-submit INFO     Found hadoop jar hadoop-aws.jar
[36malgo-1    |[0m 11-14 16:57 smspark-submit INFO     Found hadoop jar hadoop-aws.jar
[36malgo-1    |[0m 11-14 16:57 smspark-submit INFO     Optional jar jets3t-0.9.0.jar in /usr/lib/hadoop/lib does not exist
[33malgo-2    |[0m 11-14 16:57 smspark-submit INFO     Optional jar jets3t-0.9.0.jar in /usr/lib/hadoop/lib does not exist
[36malgo-1    |[0m 11-14 16:57 smspark-submit INFO     copying cluster config
[36malgo-1    |[0m 11-14 16:57 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml
[33malgo-2    |[0m 11-14 16:57 smspark-submit INFO     copying cluster config
[33malgo-2    |[0m 11-14 16:57 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml
[36malgo-1    |[0m 11-14 16:57 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml
[33malgo-2    |[0m 11-14 16:57 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml
[36malgo-1    |[0m 11-14 16:57 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[36malgo-1    |[0m 11-14 16:57 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 16:57 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[36malgo-1    |[0m 11-14 16:57 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh
[33malgo-2    |[0m 11-14 16:57 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 16:57 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh
[36malgo-1    |[0m 11-14 16:57 root         INFO     Detected instance type: m5.xlarge with total memory: 16384M and total cores: 4
[36malgo-1    |[0m 11-14 16:57 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[36malgo-1    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0"?>
[36malgo-1    |[0m <!-- Site specific YARN configuration properties -->
[36malgo-1    |[0m  <configuration>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.resourcemanager.hostname</name>
[36malgo-1    |[0m          <value>172.22.0.3</value>
[36malgo-1    |[0m          <description>The hostname of the RM.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.hostname</name>
[36malgo-1    |[0m          <value>algo-1</value>
[36malgo-1    |[0m          <description>The hostname of the NM.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.webapp.address</name>
[36malgo-1    |[0m          <value>algo-1:8042</value>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.vmem-pmem-ratio</name>
[36malgo-1    |[0m          <value>5</value>
[36malgo-1    |[0m          <description>Ratio between virtual memory to physical memory.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.resourcemanager.am.max-attempts</name>
[36malgo-1    |[0m          <value>1</value>
[36malgo-1    |[0m          <description>The maximum number of application attempts.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.env-whitelist</name>
[36malgo-1    |[0m          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>
[36malgo-1    |[0m          <description>Environment variable whitelist</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m  
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.minimum-allocation-mb</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.maximum-allocation-mb</name>
[36malgo-1    |[0m     <value>15892</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.minimum-allocation-vcores</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.maximum-allocation-vcores</name>
[36malgo-1    |[0m     <value>4</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.nodemanager.resource.memory-mb</name>
[36malgo-1    |[0m     <value>15892</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.nodemanager.resource.cpu-vcores</name>
[36malgo-1    |[0m     <value>4</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 
[33malgo-2    |[0m 11-14 16:57 root         INFO     Detected instance type: m5.xlarge with total memory: 16384M and total cores: 4
[33malgo-2    |[0m 11-14 16:57 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[33malgo-2    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0"?>
[33malgo-2    |[0m <!-- Site specific YARN configuration properties -->
[33malgo-2    |[0m  <configuration>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.resourcemanager.hostname</name>
[33malgo-2    |[0m          <value>172.22.0.3</value>
[33malgo-2    |[0m          <description>The hostname of the RM.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.hostname</name>
[33malgo-2    |[0m          <value>algo-2</value>
[33malgo-2    |[0m          <description>The hostname of the NM.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.webapp.address</name>
[33malgo-2    |[0m          <value>algo-2:8042</value>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.vmem-pmem-ratio</name>
[33malgo-2    |[0m          <value>5</value>
[33malgo-2    |[0m          <description>Ratio between virtual memory to physical memory.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.resourcemanager.am.max-attempts</name>
[33malgo-2    |[0m          <value>1</value>
[33malgo-2    |[0m          <description>The maximum number of application attempts.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.env-whitelist</name>
[33malgo-2    |[0m          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>
[33malgo-2    |[0m          <description>Environment variable whitelist</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m  
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.minimum-allocation-mb</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.maximum-allocation-mb</name>
[33malgo-2    |[0m     <value>15892</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.minimum-allocation-vcores</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.maximum-allocation-vcores</name>
[33malgo-2    |[0m     <value>4</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.nodemanager.resource.memory-mb</name>
[33malgo-2    |[0m     <value>15892</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.nodemanager.resource.cpu-vcores</name>
[33malgo-2    |[0m     <value>4</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 
[36malgo-1    |[0m 11-14 16:57 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 16:57 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[33malgo-2    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.driver.host=172.22.0.3
[33malgo-2    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[33malgo-2    |[0m 
[33malgo-2    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[33malgo-2    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[33malgo-2    |[0m spark.rpc.askTimeout=300s
[33malgo-2    |[0m spark.driver.memory 2048m
[33malgo-2    |[0m spark.driver.memoryOverhead 204m
[33malgo-2    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[33malgo-2    |[0m spark.executor.memory 12399m
[33malgo-2    |[0m spark.executor.memoryOverhead 1239m
[33malgo-2    |[0m spark.executor.cores 4
[33malgo-2    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[33malgo-2    |[0m spark.executor.instances 2
[33malgo-2    |[0m spark.default.parallelism 16
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:57 root         INFO     Finished Yarn configuration files setup.
[36malgo-1    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[36malgo-1    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.driver.host=172.22.0.3
[36malgo-1    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[36malgo-1    |[0m 
[36malgo-1    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[36malgo-1    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[36malgo-1    |[0m spark.rpc.askTimeout=300s
[36malgo-1    |[0m spark.driver.memory 2048m
[36malgo-1    |[0m spark.driver.memoryOverhead 204m
[36malgo-1    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[36malgo-1    |[0m spark.executor.memory 12399m
[36malgo-1    |[0m spark.executor.memoryOverhead 1239m
[36malgo-1    |[0m spark.executor.cores 4
[36malgo-1    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[36malgo-1    |[0m spark.executor.instances 2
[36malgo-1    |[0m spark.default.parallelism 16
[36malgo-1    |[0m 
[33malgo-2    |[0m 11-14 16:57 root         INFO     reading user configuration from /opt/ml/processing/input/conf/configuration.json
[33malgo-2    |[0m 11-14 16:57 root         INFO     User configuration list or dict: [{'Classification': 'spark-defaults', 'Properties': {'spark.executor.memory': '2g', 'spark.executor.cores': '1'}}, {'Classification': 'hadoop-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'core-site', 'Properties': {'spark.executor.memory': '2g', 'spark.executor.cores': '1'}}, {'Classification': 'hadoop-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'hive-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-exec-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-site', 'Properties': {'key': 'value'}}, {'Classification': 'spark-defaults', 'Properties': {'key': 'value'}}, {'Classification': 'spark-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'spark-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'spark-hive-site', 'Properties': {'key': 'value'}}, {'Classification': 'spark-metrics', 'Properties': {'key': 'value'}}, {'Classification': 'yarn-site', 'Properties': {'key': 'value'}}, {'Classification': 'yarn-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}] , type <class 'list'>
[36malgo-1    |[0m 11-14 16:57 root         INFO     Finished Yarn configuration files setup.
[33malgo-2    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m 11-14 16:57 root         INFO     reading user configuration from /opt/ml/processing/input/conf/configuration.json
[33malgo-2    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[33malgo-2    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.driver.host=172.22.0.3
[33malgo-2    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[33malgo-2    |[0m 
[33malgo-2    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[33malgo-2    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[33malgo-2    |[0m spark.rpc.askTimeout=300s
[33malgo-2    |[0m spark.driver.memory 2048m
[33malgo-2    |[0m spark.driver.memoryOverhead 204m
[33malgo-2    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[33malgo-2    |[0m spark.executor.memory 12399m
[33malgo-2    |[0m spark.executor.memoryOverhead 1239m
[33malgo-2    |[0m spark.executor.cores 4
[33malgo-2    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[33malgo-2    |[0m spark.executor.instances 2
[33malgo-2    |[0m spark.default.parallelism 16
[33malgo-2    |[0m spark.executor.memory 2g
[33malgo-2    |[0m spark.executor.cores 1
[33malgo-2    |[0m 
[36malgo-1    |[0m 11-14 16:57 root         INFO     User configuration list or dict: [{'Classification': 'spark-defaults', 'Properties': {'spark.executor.memory': '2g', 'spark.executor.cores': '1'}}, {'Classification': 'hadoop-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'core-site', 'Properties': {'spark.executor.memory': '2g', 'spark.executor.cores': '1'}}, {'Classification': 'hadoop-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'hive-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-exec-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-site', 'Properties': {'key': 'value'}}, {'Classification': 'spark-defaults', 'Properties': {'key': 'value'}}, {'Classification': 'spark-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'spark-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'spark-hive-site', 'Properties': {'key': 'value'}}, {'Classification': 'spark-metrics', 'Properties': {'key': 'value'}}, {'Classification': 'yarn-site', 'Properties': {'key': 'value'}}, {'Classification': 'yarn-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}] , type <class 'list'>
[36malgo-1    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/hadoop-env.sh
[36malgo-1    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[36malgo-1    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.driver.host=172.22.0.3
[36malgo-1    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[36malgo-1    |[0m 
[36malgo-1    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[36malgo-1    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[36malgo-1    |[0m spark.rpc.askTimeout=300s
[36malgo-1    |[0m spark.driver.memory 2048m
[36malgo-1    |[0m spark.driver.memoryOverhead 204m
[36malgo-1    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[36malgo-1    |[0m spark.executor.memory 12399m
[36malgo-1    |[0m spark.executor.memoryOverhead 1239m
[36malgo-1    |[0m spark.executor.cores 4
[36malgo-1    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[36malgo-1    |[0m spark.executor.instances 2
[36malgo-1    |[0m spark.default.parallelism 16
[36malgo-1    |[0m spark.executor.memory 2g
[36malgo-1    |[0m spark.executor.cores 1
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/hadoop-env.sh
[33malgo-2    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/hadoop-env.sh is: 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[33malgo-2    |[0m # or more contributor license agreements.  See the NOTICE file
[33malgo-2    |[0m # distributed with this work for additional information
[33malgo-2    |[0m # regarding copyright ownership.  The ASF licenses this file
[33malgo-2    |[0m # to you under the Apache License, Version 2.0 (the
[33malgo-2    |[0m # "License"); you may not use this file except in compliance
[33malgo-2    |[0m # with the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m # Set Hadoop-specific environment variables here.
[33malgo-2    |[0m 
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## THIS FILE ACTS AS THE MASTER FILE FOR ALL HADOOP PROJECTS.
[33malgo-2    |[0m ## SETTINGS HERE WILL BE READ BY ALL HADOOP COMMANDS.  THEREFORE,
[33malgo-2    |[0m ## ONE CAN USE THIS FILE TO SET YARN, HDFS, AND MAPREDUCE
[33malgo-2    |[0m ## CONFIGURATION OPTIONS INSTEAD OF xxx-env.sh.
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## Precedence rules:
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## {yarn-env.sh|hdfs-env.sh} > hadoop-env.sh > hard-coded defaults
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## {YARN_xyz|HDFS_xyz} > HADOOP_xyz > hard-coded defaults
[33malgo-2    |[0m ##
[33malgo-2    |[0m 
[33malgo-2    |[0m # Many of the options here are built from the perspective that users
[33malgo-2    |[0m # may want to provide OVERWRITING values on the command line.
[33malgo-2    |[0m # For example:
[33malgo-2    |[0m #
[33malgo-2    |[0m #  JAVA_HOME=/usr/java/testing hdfs dfs -ls
[33malgo-2    |[0m #
[33malgo-2    |[0m # Therefore, the vast majority (BUT NOT ALL!) of these defaults
[33malgo-2    |[0m # are configured for substitution and not append.  If append
[33malgo-2    |[0m # is preferable, modify this file accordingly.
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Generic settings for HADOOP
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Technically, the only required environment variable is JAVA_HOME.
[33malgo-2    |[0m # All others are optional.  However, the defaults are probably not
[33malgo-2    |[0m # preferred.  Many sites configure these options outside of Hadoop,
[33malgo-2    |[0m # such as in /etc/profile.d
[33malgo-2    |[0m 
[33malgo-2    |[0m # The java implementation to use. By default, this environment
[33malgo-2    |[0m # variable is REQUIRED on ALL platforms except OS X!
[33malgo-2    |[0m # export JAVA_HOME=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Location of Hadoop.  By default, Hadoop will attempt to determine
[33malgo-2    |[0m # this location based upon its execution path.
[33malgo-2    |[0m # export HADOOP_HOME=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Location of Hadoop's configuration information.  i.e., where this
[33malgo-2    |[0m # file is living. If this is not defined, Hadoop will attempt to
[33malgo-2    |[0m # locate it based upon its execution path.
[33malgo-2    |[0m #
[33malgo-2    |[0m # NOTE: It is recommend that this variable not be set here but in
[33malgo-2    |[0m # /etc/profile.d or equivalent.  Some options (such as
[33malgo-2    |[0m # --config) may react strangely otherwise.
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
[33malgo-2    |[0m 
[33malgo-2    |[0m # The maximum amount of heap to use (Java -Xmx).  If no unit
[33malgo-2    |[0m # is provided, it will be converted to MB.  Daemons will
[33malgo-2    |[0m # prefer any Xmx setting in their respective _OPT variable.
[33malgo-2    |[0m # There is no default; the JVM will autoscale based upon machine
[33malgo-2    |[0m # memory size.
[33malgo-2    |[0m # export HADOOP_HEAPSIZE_MAX=
[33malgo-2    |[0m 
[33malgo-2    |[0m # The minimum amount of heap to use (Java -Xms).  If no unit
[33malgo-2    |[0m # is provided, it will be converted to MB.  Daemons will
[33malgo-2    |[0m # prefer any Xms setting in their respective _OPT variable.
[33malgo-2    |[0m # There is no default; the JVM will autoscale based upon machine
[33malgo-2    |[0m # memory size.
[33malgo-2    |[0m # export HADOOP_HEAPSIZE_MIN=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Enable extra debugging of Hadoop's JAAS binding, used to set up
[33malgo-2    |[0m # Kerberos security.
[33malgo-2    |[0m # export HADOOP_JAAS_DEBUG=true
[33malgo-2    |[0m 
[33malgo-2    |[0m # Extra Java runtime options for all Hadoop commands. We don't support
[33malgo-2    |[0m # IPv6 yet/still, so by default the preference is set to IPv4.
[33malgo-2    |[0m # export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true"
[33malgo-2    |[0m # For Kerberos debugging, an extended option set logs more information
[33malgo-2    |[0m # export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug"
[33malgo-2    |[0m 
[33malgo-2    |[0m # Some parts of the shell code may do special things dependent upon
[33malgo-2    |[0m # the operating system.  We have to set this here. See the next
[33malgo-2    |[0m # section as to why....
[33malgo-2    |[0m #export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}
[33malgo-2    |[0m 
[33malgo-2    |[0m # Extra Java runtime options for some Hadoop commands
[33malgo-2    |[0m # and clients (i.e., hdfs dfs -blah).  These get appended to HADOOP_OPTS for
[33malgo-2    |[0m # such commands.  In most cases, # this should be left empty and
[33malgo-2    |[0m # let users supply it on the command line.
[33malgo-2    |[0m # export HADOOP_CLIENT_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # A note about classpaths.
[33malgo-2    |[0m #
[33malgo-2    |[0m # By default, Apache Hadoop overrides Java's CLASSPATH
[33malgo-2    |[0m # environment variable.  It is configured such
[33malgo-2    |[0m # that it starts out blank with new entries added after passing
[33malgo-2    |[0m # a series of checks (file/dir exists, not already listed aka
[33malgo-2    |[0m # de-deduplication).  During de-deduplication, wildcards and/or
[33malgo-2    |[0m # directories are *NOT* expanded to keep it simple. Therefore,
[33malgo-2    |[0m # if the computed classpath has two specific mentions of
[33malgo-2    |[0m # awesome-methods-1.0.jar, only the first one added will be seen.
[33malgo-2    |[0m # If two directories are in the classpath that both contain
[33malgo-2    |[0m # awesome-methods-1.0.jar, then Java will pick up both versions.
[33malgo-2    |[0m 
[33malgo-2    |[0m # An additional, custom CLASSPATH. Site-wide configs should be
[33malgo-2    |[0m # handled via the shellprofile functionality, utilizing the
[33malgo-2    |[0m # hadoop_add_classpath function for greater control and much
[33malgo-2    |[0m # harder for apps/end-users to accidentally override.
[33malgo-2    |[0m # Similarly, end users should utilize ${HOME}/.hadooprc .
[33malgo-2    |[0m # This variable should ideally only be used as a short-cut,
[33malgo-2    |[0m # interactive way for temporary additions on the command line.
[33malgo-2    |[0m # export HADOOP_CLASSPATH="/some/cool/path/on/your/machine"
[33malgo-2    |[0m 
[33malgo-2    |[0m # Should HADOOP_CLASSPATH be first in the official CLASSPATH?
[33malgo-2    |[0m # export HADOOP_USER_CLASSPATH_FIRST="yes"
[33malgo-2    |[0m 
[33malgo-2    |[0m # If HADOOP_USE_CLIENT_CLASSLOADER is set, the classpath along
[33malgo-2    |[0m # with the main jar are handled by a separate isolated
[33malgo-2    |[0m # client classloader when 'hadoop jar', 'yarn jar', or 'mapred job'
[33malgo-2    |[0m # is utilized. If it is set, HADOOP_CLASSPATH and
[33malgo-2    |[0m # HADOOP_USER_CLASSPATH_FIRST are ignored.
[33malgo-2    |[0m # export HADOOP_USE_CLIENT_CLASSLOADER=true
[33malgo-2    |[0m 
[33malgo-2    |[0m # HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES overrides the default definition of
[33malgo-2    |[0m # system classes for the client classloader when HADOOP_USE_CLIENT_CLASSLOADER
[33malgo-2    |[0m # is enabled. Names ending in '.' (period) are treated as package names, and
[33malgo-2    |[0m # names starting with a '-' are treated as negative matches. For example,
[33malgo-2    |[0m # export HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES="-org.apache.hadoop.UserClass,java.,javax.,org.apache.hadoop."
[33malgo-2    |[0m 
[33malgo-2    |[0m # Enable optional, bundled Hadoop features
[33malgo-2    |[0m # This is a comma delimited list.  It may NOT be overridden via .hadooprc
[33malgo-2    |[0m # Entries may be added/removed as needed.
[33malgo-2    |[0m # export HADOOP_OPTIONAL_TOOLS="hadoop-azure,hadoop-azure-datalake,hadoop-aws,hadoop-openstack,hadoop-aliyun,hadoop-kafka"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Options for remote shell connectivity
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # There are some optional components of hadoop that allow for
[33malgo-2    |[0m # command and control of remote hosts.  For example,
[33malgo-2    |[0m # start-dfs.sh will attempt to bring up all NNs, DNS, etc.
[33malgo-2    |[0m 
[33malgo-2    |[0m # Options to pass to SSH when one of the "log into a host and
[33malgo-2    |[0m # start/stop daemons" scripts is executed
[33malgo-2    |[0m # export HADOOP_SSH_OPTS="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10s"
[33malgo-2    |[0m 
[33malgo-2    |[0m # The built-in ssh handler will limit itself to 10 simultaneous connections.
[33malgo-2    |[0m # For pdsh users, this sets the fanout size ( -f )
[33malgo-2    |[0m # Change this to increase/decrease as necessary.
[33malgo-2    |[0m # export HADOOP_SSH_PARALLEL=10
[33malgo-2    |[0m 
[33malgo-2    |[0m # Filename which contains all of the hosts for any remote execution
[33malgo-2    |[0m # helper scripts # such as workers.sh, start-dfs.sh, etc.
[33malgo-2    |[0m # export HADOOP_WORKERS="${HADOOP_CONF_DIR}/workers"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Options for all daemons
[33malgo-2    |[0m ###
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Many options may also be specified as Java properties.  It is
[33malgo-2    |[0m # very common, and in many cases, desirable, to hard-set these
[33malgo-2    |[0m # in daemon _OPTS variables.  Where applicable, the appropriate
[33malgo-2    |[0m # Java property is also identified.  Note that many are re-used
[33malgo-2    |[0m # or set differently in certain contexts (e.g., secure vs
[33malgo-2    |[0m # non-secure)
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m # Where (primarily) daemon log files are stored.
[33malgo-2    |[0m # ${HADOOP_HOME}/logs by default.
[33malgo-2    |[0m # Java property: hadoop.log.dir
[33malgo-2    |[0m # export HADOOP_LOG_DIR=${HADOOP_HOME}/logs
[33malgo-2    |[0m 
[33malgo-2    |[0m # A string representing this instance of hadoop. $USER by default.
[33malgo-2    |[0m # This is used in writing log and pid files, so keep that in mind!
[33malgo-2    |[0m # Java property: hadoop.id.str
[33malgo-2    |[0m # export HADOOP_IDENT_STRING=$USER
[33malgo-2    |[0m 
[33malgo-2    |[0m # How many seconds to pause after stopping a daemon
[33malgo-2    |[0m # export HADOOP_STOP_TIMEOUT=5
[33malgo-2    |[0m 
[33malgo-2    |[0m # Where pid files are stored.  /tmp by default.
[33malgo-2    |[0m # export HADOOP_PID_DIR=/tmp
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default log4j setting for interactive commands
[33malgo-2    |[0m # Java property: hadoop.root.logger
[33malgo-2    |[0m # export HADOOP_ROOT_LOGGER=INFO,console
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default log4j setting for daemons spawned explicitly by
[33malgo-2    |[0m # --daemon option of hadoop, hdfs, mapred and yarn command.
[33malgo-2    |[0m # Java property: hadoop.root.logger
[33malgo-2    |[0m # export HADOOP_DAEMON_ROOT_LOGGER=INFO,RFA
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default log level and output location for security-related messages.
[33malgo-2    |[0m # You will almost certainly want to change this on a per-daemon basis via
[33malgo-2    |[0m # the Java property (i.e., -Dhadoop.security.logger=foo). (Note that the
[33malgo-2    |[0m # defaults for the NN and 2NN override this by default.)
[33malgo-2    |[0m # Java property: hadoop.security.logger
[33malgo-2    |[0m # export HADOOP_SECURITY_LOGGER=INFO,NullAppender
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default process priority level
[33malgo-2    |[0m # Note that sub-processes will also run at this level!
[33malgo-2    |[0m # export HADOOP_NICENESS=0
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default name for the service level authorization file
[33malgo-2    |[0m # Java property: hadoop.policy.file
[33malgo-2    |[0m # export HADOOP_POLICYFILE="hadoop-policy.xml"
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # NOTE: this is not used by default!  <-----
[33malgo-2    |[0m # You can define variables right here and then re-use them later on.
[33malgo-2    |[0m # For example, it is common to use the same garbage collection settings
[33malgo-2    |[0m # for all the daemons.  So one could define:
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HADOOP_GC_SETTINGS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps"
[33malgo-2    |[0m #
[33malgo-2    |[0m # .. and then use it as per the b option under the namenode.
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Secure/privileged execution
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Out of the box, Hadoop uses jsvc from Apache Commons to launch daemons
[33malgo-2    |[0m # on privileged ports.  This functionality can be replaced by providing
[33malgo-2    |[0m # custom functions.  See hadoop-functions.sh for more information.
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m # The jsvc implementation to use. Jsvc is required to run secure datanodes
[33malgo-2    |[0m # that bind to privileged ports to provide authentication of data transfer
[33malgo-2    |[0m # protocol.  Jsvc is not required if SASL is configured for authentication of
[33malgo-2    |[0m # data transfer protocol using non-privileged ports.
[33malgo-2    |[0m # export JSVC_HOME=/usr/bin
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # This directory contains pids for secure and privileged processes.
[33malgo-2    |[0m #export HADOOP_SECURE_PID_DIR=${HADOOP_PID_DIR}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # This directory contains the logs for secure and privileged processes.
[33malgo-2    |[0m # Java property: hadoop.log.dir
[33malgo-2    |[0m # export HADOOP_SECURE_LOG=${HADOOP_LOG_DIR}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # When running a secure daemon, the default value of HADOOP_IDENT_STRING
[33malgo-2    |[0m # ends up being a bit bogus.  Therefore, by default, the code will
[33malgo-2    |[0m # replace HADOOP_IDENT_STRING with HADOOP_xx_SECURE_USER.  If one wants
[33malgo-2    |[0m # to keep HADOOP_IDENT_STRING untouched, then uncomment this line.
[33malgo-2    |[0m # export HADOOP_SECURE_IDENT_PRESERVE="true"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # NameNode specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default log level and output location for file system related change
[33malgo-2    |[0m # messages. For non-namenode daemons, the Java property must be set in
[33malgo-2    |[0m # the appropriate _OPTS if one wants something other than INFO,NullAppender
[33malgo-2    |[0m # Java property: hdfs.audit.logger
[33malgo-2    |[0m # export HDFS_AUDIT_LOGGER=INFO,NullAppender
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the NameNode.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # a) Set JMX options
[33malgo-2    |[0m # export HDFS_NAMENODE_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
[33malgo-2    |[0m #
[33malgo-2    |[0m # b) Set garbage collection logs
[33malgo-2    |[0m # export HDFS_NAMENODE_OPTS="${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[33malgo-2    |[0m #
[33malgo-2    |[0m # c) ... or set them directly
[33malgo-2    |[0m # export HDFS_NAMENODE_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[33malgo-2    |[0m 
[33malgo-2    |[0m # this is the default:
[33malgo-2    |[0m # export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # SecondaryNameNode specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the SecondaryNameNode.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # This is the default:
[33malgo-2    |[0m # export HDFS_SECONDARYNAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # DataNode specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the DataNode.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # This is the default:
[33malgo-2    |[0m # export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS"
[33malgo-2    |[0m 
[33malgo-2    |[0m # On secure datanodes, user to run the datanode as after dropping privileges.
[33malgo-2    |[0m # This **MUST** be uncommented to enable secure HDFS if using privileged ports
[33malgo-2    |[0m # to provide authentication of data transfer protocol.  This **MUST NOT** be
[33malgo-2    |[0m # defined if SASL is configured for authentication of data transfer protocol
[33malgo-2    |[0m # using non-privileged ports.
[33malgo-2    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[33malgo-2    |[0m # export HDFS_DATANODE_SECURE_USER=hdfs
[33malgo-2    |[0m 
[33malgo-2    |[0m # Supplemental options for secure datanodes
[33malgo-2    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[33malgo-2    |[0m # server jvm.
[33malgo-2    |[0m # export HDFS_DATANODE_SECURE_EXTRA_OPTS="-jvm server"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # NFS3 Gateway specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the NFS3 Gateway.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_NFS3_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the Hadoop portmapper.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_PORTMAP_OPTS="-Xmx512m"
[33malgo-2    |[0m 
[33malgo-2    |[0m # Supplemental options for priviliged gateways
[33malgo-2    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[33malgo-2    |[0m # server jvm.
[33malgo-2    |[0m # export HDFS_NFS3_SECURE_EXTRA_OPTS="-jvm server"
[33malgo-2    |[0m 
[33malgo-2    |[0m # On privileged gateways, user to run the gateway as after dropping privileges
[33malgo-2    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[33malgo-2    |[0m # export HDFS_NFS3_SECURE_USER=nfsserver
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # ZKFailoverController specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the ZKFailoverController.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_ZKFC_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # QuorumJournalNode specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the QuorumJournalNode.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_JOURNALNODE_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # HDFS Balancer specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the HDFS Balancer.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_BALANCER_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # HDFS Mover specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the HDFS Mover.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_MOVER_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Router-based HDFS Federation specific parameters
[33malgo-2    |[0m # Specify the JVM options to be used when starting the RBF Routers.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_DFSROUTER_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # HDFS StorageContainerManager specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the HDFS Storage Container Manager.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_STORAGECONTAINERMANAGER_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Advanced Users Only!
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # When building Hadoop, one can add the class paths to the commands
[33malgo-2    |[0m # via this special env var:
[33malgo-2    |[0m # export HADOOP_ENABLE_BUILD_PATHS="true"
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # To prevent accidents, shell commands be (superficially) locked
[33malgo-2    |[0m # to only allow certain users to execute certain subcommands.
[33malgo-2    |[0m # It uses the format of (command)_(subcommand)_USER.
[33malgo-2    |[0m #
[33malgo-2    |[0m # For example, to limit who can execute the namenode command,
[33malgo-2    |[0m # export HDFS_NAMENODE_USER=hdfs
[33malgo-2    |[0m export SPARK_MASTER_HOST=172.22.0.3
[33malgo-2    |[0m export AWS_CONTAINER_CREDENTIALS_RELATIVE_URI=
[33malgo-2    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[33malgo-2    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/core-site.xml
[36malgo-1    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/hadoop-env.sh is: 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[36malgo-1    |[0m # or more contributor license agreements.  See the NOTICE file
[36malgo-1    |[0m # distributed with this work for additional information
[36malgo-1    |[0m # regarding copyright ownership.  The ASF licenses this file
[36malgo-1    |[0m # to you under the Apache License, Version 2.0 (the
[36malgo-1    |[0m # "License"); you may not use this file except in compliance
[36malgo-1    |[0m # with the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m # Set Hadoop-specific environment variables here.
[36malgo-1    |[0m 
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## THIS FILE ACTS AS THE MASTER FILE FOR ALL HADOOP PROJECTS.
[36malgo-1    |[0m ## SETTINGS HERE WILL BE READ BY ALL HADOOP COMMANDS.  THEREFORE,
[36malgo-1    |[0m ## ONE CAN USE THIS FILE TO SET YARN, HDFS, AND MAPREDUCE
[36malgo-1    |[0m ## CONFIGURATION OPTIONS INSTEAD OF xxx-env.sh.
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## Precedence rules:
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## {yarn-env.sh|hdfs-env.sh} > hadoop-env.sh > hard-coded defaults
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## {YARN_xyz|HDFS_xyz} > HADOOP_xyz > hard-coded defaults
[36malgo-1    |[0m ##
[36malgo-1    |[0m 
[36malgo-1    |[0m # Many of the options here are built from the perspective that users
[36malgo-1    |[0m # may want to provide OVERWRITING values on the command line.
[36malgo-1    |[0m # For example:
[36malgo-1    |[0m #
[36malgo-1    |[0m #  JAVA_HOME=/usr/java/testing hdfs dfs -ls
[36malgo-1    |[0m #
[36malgo-1    |[0m # Therefore, the vast majority (BUT NOT ALL!) of these defaults
[36malgo-1    |[0m # are configured for substitution and not append.  If append
[36malgo-1    |[0m # is preferable, modify this file accordingly.
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Generic settings for HADOOP
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Technically, the only required environment variable is JAVA_HOME.
[36malgo-1    |[0m # All others are optional.  However, the defaults are probably not
[36malgo-1    |[0m # preferred.  Many sites configure these options outside of Hadoop,
[36malgo-1    |[0m # such as in /etc/profile.d
[36malgo-1    |[0m 
[36malgo-1    |[0m # The java implementation to use. By default, this environment
[36malgo-1    |[0m # variable is REQUIRED on ALL platforms except OS X!
[36malgo-1    |[0m # export JAVA_HOME=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Location of Hadoop.  By default, Hadoop will attempt to determine
[36malgo-1    |[0m # this location based upon its execution path.
[36malgo-1    |[0m # export HADOOP_HOME=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Location of Hadoop's configuration information.  i.e., where this
[36malgo-1    |[0m # file is living. If this is not defined, Hadoop will attempt to
[36malgo-1    |[0m # locate it based upon its execution path.
[36malgo-1    |[0m #
[36malgo-1    |[0m # NOTE: It is recommend that this variable not be set here but in
[36malgo-1    |[0m # /etc/profile.d or equivalent.  Some options (such as
[36malgo-1    |[0m # --config) may react strangely otherwise.
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
[36malgo-1    |[0m 
[36malgo-1    |[0m # The maximum amount of heap to use (Java -Xmx).  If no unit
[36malgo-1    |[0m # is provided, it will be converted to MB.  Daemons will
[36malgo-1    |[0m # prefer any Xmx setting in their respective _OPT variable.
[36malgo-1    |[0m # There is no default; the JVM will autoscale based upon machine
[36malgo-1    |[0m # memory size.
[36malgo-1    |[0m # export HADOOP_HEAPSIZE_MAX=
[36malgo-1    |[0m 
[36malgo-1    |[0m # The minimum amount of heap to use (Java -Xms).  If no unit
[36malgo-1    |[0m # is provided, it will be converted to MB.  Daemons will
[36malgo-1    |[0m # prefer any Xms setting in their respective _OPT variable.
[36malgo-1    |[0m # There is no default; the JVM will autoscale based upon machine
[36malgo-1    |[0m # memory size.
[36malgo-1    |[0m # export HADOOP_HEAPSIZE_MIN=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Enable extra debugging of Hadoop's JAAS binding, used to set up
[36malgo-1    |[0m # Kerberos security.
[36malgo-1    |[0m # export HADOOP_JAAS_DEBUG=true
[36malgo-1    |[0m 
[36malgo-1    |[0m # Extra Java runtime options for all Hadoop commands. We don't support
[36malgo-1    |[0m # IPv6 yet/still, so by default the preference is set to IPv4.
[36malgo-1    |[0m # export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true"
[36malgo-1    |[0m # For Kerberos debugging, an extended option set logs more information
[36malgo-1    |[0m # export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug"
[36malgo-1    |[0m 
[36malgo-1    |[0m # Some parts of the shell code may do special things dependent upon
[36malgo-1    |[0m # the operating system.  We have to set this here. See the next
[36malgo-1    |[0m # section as to why....
[36malgo-1    |[0m #export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}
[36malgo-1    |[0m 
[36malgo-1    |[0m # Extra Java runtime options for some Hadoop commands
[36malgo-1    |[0m # and clients (i.e., hdfs dfs -blah).  These get appended to HADOOP_OPTS for
[36malgo-1    |[0m # such commands.  In most cases, # this should be left empty and
[36malgo-1    |[0m # let users supply it on the command line.
[36malgo-1    |[0m # export HADOOP_CLIENT_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # A note about classpaths.
[36malgo-1    |[0m #
[36malgo-1    |[0m # By default, Apache Hadoop overrides Java's CLASSPATH
[36malgo-1    |[0m # environment variable.  It is configured such
[36malgo-1    |[0m # that it starts out blank with new entries added after passing
[36malgo-1    |[0m # a series of checks (file/dir exists, not already listed aka
[36malgo-1    |[0m # de-deduplication).  During de-deduplication, wildcards and/or
[36malgo-1    |[0m # directories are *NOT* expanded to keep it simple. Therefore,
[36malgo-1    |[0m # if the computed classpath has two specific mentions of
[36malgo-1    |[0m # awesome-methods-1.0.jar, only the first one added will be seen.
[36malgo-1    |[0m # If two directories are in the classpath that both contain
[36malgo-1    |[0m # awesome-methods-1.0.jar, then Java will pick up both versions.
[36malgo-1    |[0m 
[36malgo-1    |[0m # An additional, custom CLASSPATH. Site-wide configs should be
[36malgo-1    |[0m # handled via the shellprofile functionality, utilizing the
[36malgo-1    |[0m # hadoop_add_classpath function for greater control and much
[36malgo-1    |[0m # harder for apps/end-users to accidentally override.
[36malgo-1    |[0m # Similarly, end users should utilize ${HOME}/.hadooprc .
[36malgo-1    |[0m # This variable should ideally only be used as a short-cut,
[36malgo-1    |[0m # interactive way for temporary additions on the command line.
[36malgo-1    |[0m # export HADOOP_CLASSPATH="/some/cool/path/on/your/machine"
[36malgo-1    |[0m 
[36malgo-1    |[0m # Should HADOOP_CLASSPATH be first in the official CLASSPATH?
[36malgo-1    |[0m # export HADOOP_USER_CLASSPATH_FIRST="yes"
[36malgo-1    |[0m 
[36malgo-1    |[0m # If HADOOP_USE_CLIENT_CLASSLOADER is set, the classpath along
[36malgo-1    |[0m # with the main jar are handled by a separate isolated
[36malgo-1    |[0m # client classloader when 'hadoop jar', 'yarn jar', or 'mapred job'
[36malgo-1    |[0m # is utilized. If it is set, HADOOP_CLASSPATH and
[36malgo-1    |[0m # HADOOP_USER_CLASSPATH_FIRST are ignored.
[36malgo-1    |[0m # export HADOOP_USE_CLIENT_CLASSLOADER=true
[36malgo-1    |[0m 
[36malgo-1    |[0m # HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES overrides the default definition of
[36malgo-1    |[0m # system classes for the client classloader when HADOOP_USE_CLIENT_CLASSLOADER
[36malgo-1    |[0m # is enabled. Names ending in '.' (period) are treated as package names, and
[36malgo-1    |[0m # names starting with a '-' are treated as negative matches. For example,
[36malgo-1    |[0m # export HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES="-org.apache.hadoop.UserClass,java.,javax.,org.apache.hadoop."
[36malgo-1    |[0m 
[36malgo-1    |[0m # Enable optional, bundled Hadoop features
[36malgo-1    |[0m # This is a comma delimited list.  It may NOT be overridden via .hadooprc
[36malgo-1    |[0m # Entries may be added/removed as needed.
[36malgo-1    |[0m # export HADOOP_OPTIONAL_TOOLS="hadoop-azure,hadoop-azure-datalake,hadoop-aws,hadoop-openstack,hadoop-aliyun,hadoop-kafka"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Options for remote shell connectivity
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # There are some optional components of hadoop that allow for
[36malgo-1    |[0m # command and control of remote hosts.  For example,
[36malgo-1    |[0m # start-dfs.sh will attempt to bring up all NNs, DNS, etc.
[36malgo-1    |[0m 
[36malgo-1    |[0m # Options to pass to SSH when one of the "log into a host and
[36malgo-1    |[0m # start/stop daemons" scripts is executed
[36malgo-1    |[0m # export HADOOP_SSH_OPTS="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10s"
[36malgo-1    |[0m 
[36malgo-1    |[0m # The built-in ssh handler will limit itself to 10 simultaneous connections.
[36malgo-1    |[0m # For pdsh users, this sets the fanout size ( -f )
[36malgo-1    |[0m # Change this to increase/decrease as necessary.
[36malgo-1    |[0m # export HADOOP_SSH_PARALLEL=10
[36malgo-1    |[0m 
[36malgo-1    |[0m # Filename which contains all of the hosts for any remote execution
[36malgo-1    |[0m # helper scripts # such as workers.sh, start-dfs.sh, etc.
[36malgo-1    |[0m # export HADOOP_WORKERS="${HADOOP_CONF_DIR}/workers"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Options for all daemons
[36malgo-1    |[0m ###
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Many options may also be specified as Java properties.  It is
[36malgo-1    |[0m # very common, and in many cases, desirable, to hard-set these
[36malgo-1    |[0m # in daemon _OPTS variables.  Where applicable, the appropriate
[36malgo-1    |[0m # Java property is also identified.  Note that many are re-used
[36malgo-1    |[0m # or set differently in certain contexts (e.g., secure vs
[36malgo-1    |[0m # non-secure)
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m # Where (primarily) daemon log files are stored.
[36malgo-1    |[0m # ${HADOOP_HOME}/logs by default.
[36malgo-1    |[0m # Java property: hadoop.log.dir
[36malgo-1    |[0m # export HADOOP_LOG_DIR=${HADOOP_HOME}/logs
[36malgo-1    |[0m 
[36malgo-1    |[0m # A string representing this instance of hadoop. $USER by default.
[36malgo-1    |[0m # This is used in writing log and pid files, so keep that in mind!
[36malgo-1    |[0m # Java property: hadoop.id.str
[36malgo-1    |[0m # export HADOOP_IDENT_STRING=$USER
[36malgo-1    |[0m 
[36malgo-1    |[0m # How many seconds to pause after stopping a daemon
[36malgo-1    |[0m # export HADOOP_STOP_TIMEOUT=5
[36malgo-1    |[0m 
[36malgo-1    |[0m # Where pid files are stored.  /tmp by default.
[36malgo-1    |[0m # export HADOOP_PID_DIR=/tmp
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default log4j setting for interactive commands
[36malgo-1    |[0m # Java property: hadoop.root.logger
[36malgo-1    |[0m # export HADOOP_ROOT_LOGGER=INFO,console
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default log4j setting for daemons spawned explicitly by
[36malgo-1    |[0m # --daemon option of hadoop, hdfs, mapred and yarn command.
[36malgo-1    |[0m # Java property: hadoop.root.logger
[36malgo-1    |[0m # export HADOOP_DAEMON_ROOT_LOGGER=INFO,RFA
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default log level and output location for security-related messages.
[36malgo-1    |[0m # You will almost certainly want to change this on a per-daemon basis via
[36malgo-1    |[0m # the Java property (i.e., -Dhadoop.security.logger=foo). (Note that the
[36malgo-1    |[0m # defaults for the NN and 2NN override this by default.)
[36malgo-1    |[0m # Java property: hadoop.security.logger
[36malgo-1    |[0m # export HADOOP_SECURITY_LOGGER=INFO,NullAppender
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default process priority level
[36malgo-1    |[0m # Note that sub-processes will also run at this level!
[36malgo-1    |[0m # export HADOOP_NICENESS=0
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default name for the service level authorization file
[36malgo-1    |[0m # Java property: hadoop.policy.file
[36malgo-1    |[0m # export HADOOP_POLICYFILE="hadoop-policy.xml"
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # NOTE: this is not used by default!  <-----
[36malgo-1    |[0m # You can define variables right here and then re-use them later on.
[36malgo-1    |[0m # For example, it is common to use the same garbage collection settings
[36malgo-1    |[0m # for all the daemons.  So one could define:
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HADOOP_GC_SETTINGS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps"
[36malgo-1    |[0m #
[36malgo-1    |[0m # .. and then use it as per the b option under the namenode.
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Secure/privileged execution
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Out of the box, Hadoop uses jsvc from Apache Commons to launch daemons
[36malgo-1    |[0m # on privileged ports.  This functionality can be replaced by providing
[36malgo-1    |[0m # custom functions.  See hadoop-functions.sh for more information.
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m # The jsvc implementation to use. Jsvc is required to run secure datanodes
[36malgo-1    |[0m # that bind to privileged ports to provide authentication of data transfer
[36malgo-1    |[0m # protocol.  Jsvc is not required if SASL is configured for authentication of
[36malgo-1    |[0m # data transfer protocol using non-privileged ports.
[36malgo-1    |[0m # export JSVC_HOME=/usr/bin
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # This directory contains pids for secure and privileged processes.
[36malgo-1    |[0m #export HADOOP_SECURE_PID_DIR=${HADOOP_PID_DIR}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # This directory contains the logs for secure and privileged processes.
[36malgo-1    |[0m # Java property: hadoop.log.dir
[36malgo-1    |[0m # export HADOOP_SECURE_LOG=${HADOOP_LOG_DIR}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # When running a secure daemon, the default value of HADOOP_IDENT_STRING
[36malgo-1    |[0m # ends up being a bit bogus.  Therefore, by default, the code will
[36malgo-1    |[0m # replace HADOOP_IDENT_STRING with HADOOP_xx_SECURE_USER.  If one wants
[36malgo-1    |[0m # to keep HADOOP_IDENT_STRING untouched, then uncomment this line.
[36malgo-1    |[0m # export HADOOP_SECURE_IDENT_PRESERVE="true"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # NameNode specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default log level and output location for file system related change
[36malgo-1    |[0m # messages. For non-namenode daemons, the Java property must be set in
[36malgo-1    |[0m # the appropriate _OPTS if one wants something other than INFO,NullAppender
[36malgo-1    |[0m # Java property: hdfs.audit.logger
[36malgo-1    |[0m # export HDFS_AUDIT_LOGGER=INFO,NullAppender
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the NameNode.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # a) Set JMX options
[36malgo-1    |[0m # export HDFS_NAMENODE_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
[36malgo-1    |[0m #
[36malgo-1    |[0m # b) Set garbage collection logs
[36malgo-1    |[0m # export HDFS_NAMENODE_OPTS="${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[36malgo-1    |[0m #
[36malgo-1    |[0m # c) ... or set them directly
[36malgo-1    |[0m # export HDFS_NAMENODE_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[36malgo-1    |[0m 
[36malgo-1    |[0m # this is the default:
[36malgo-1    |[0m # export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # SecondaryNameNode specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the SecondaryNameNode.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # This is the default:
[36malgo-1    |[0m # export HDFS_SECONDARYNAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # DataNode specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the DataNode.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # This is the default:
[36malgo-1    |[0m # export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS"
[36malgo-1    |[0m 
[36malgo-1    |[0m # On secure datanodes, user to run the datanode as after dropping privileges.
[36malgo-1    |[0m # This **MUST** be uncommented to enable secure HDFS if using privileged ports
[36malgo-1    |[0m # to provide authentication of data transfer protocol.  This **MUST NOT** be
[36malgo-1    |[0m # defined if SASL is configured for authentication of data transfer protocol
[36malgo-1    |[0m # using non-privileged ports.
[36malgo-1    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[36malgo-1    |[0m # export HDFS_DATANODE_SECURE_USER=hdfs
[36malgo-1    |[0m 
[36malgo-1    |[0m # Supplemental options for secure datanodes
[36malgo-1    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[36malgo-1    |[0m # server jvm.
[36malgo-1    |[0m # export HDFS_DATANODE_SECURE_EXTRA_OPTS="-jvm server"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # NFS3 Gateway specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the NFS3 Gateway.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_NFS3_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the Hadoop portmapper.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_PORTMAP_OPTS="-Xmx512m"
[36malgo-1    |[0m 
[36malgo-1    |[0m # Supplemental options for priviliged gateways
[36malgo-1    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[36malgo-1    |[0m # server jvm.
[36malgo-1    |[0m # export HDFS_NFS3_SECURE_EXTRA_OPTS="-jvm server"
[36malgo-1    |[0m 
[36malgo-1    |[0m # On privileged gateways, user to run the gateway as after dropping privileges
[36malgo-1    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[36malgo-1    |[0m # export HDFS_NFS3_SECURE_USER=nfsserver
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # ZKFailoverController specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the ZKFailoverController.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_ZKFC_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # QuorumJournalNode specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the QuorumJournalNode.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_JOURNALNODE_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # HDFS Balancer specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the HDFS Balancer.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_BALANCER_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # HDFS Mover specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the HDFS Mover.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_MOVER_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Router-based HDFS Federation specific parameters
[36malgo-1    |[0m # Specify the JVM options to be used when starting the RBF Routers.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_DFSROUTER_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # HDFS StorageContainerManager specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the HDFS Storage Container Manager.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_STORAGECONTAINERMANAGER_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Advanced Users Only!
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # When building Hadoop, one can add the class paths to the commands
[36malgo-1    |[0m # via this special env var:
[36malgo-1    |[0m # export HADOOP_ENABLE_BUILD_PATHS="true"
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # To prevent accidents, shell commands be (superficially) locked
[36malgo-1    |[0m # to only allow certain users to execute certain subcommands.
[36malgo-1    |[0m # It uses the format of (command)_(subcommand)_USER.
[36malgo-1    |[0m #
[36malgo-1    |[0m # For example, to limit who can execute the namenode command,
[36malgo-1    |[0m # export HDFS_NAMENODE_USER=hdfs
[36malgo-1    |[0m export SPARK_MASTER_HOST=172.22.0.3
[36malgo-1    |[0m export AWS_CONTAINER_CREDENTIALS_RELATIVE_URI=
[36malgo-1    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[36malgo-1    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/core-site.xml
[36malgo-1    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/core-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0" encoding="UTF-8"?>
[36malgo-1    |[0m  <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[36malgo-1    |[0m  <!-- Put site-specific property overrides in this file. -->
[36malgo-1    |[0m 
[36malgo-1    |[0m  <configuration>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>fs.defaultFS</name>
[36malgo-1    |[0m          <value>hdfs://172.22.0.3/</value>
[36malgo-1    |[0m          <description>NameNode URI</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>fs.s3a.aws.credentials.provider</name>
[36malgo-1    |[0m          <value>com.amazonaws.auth.DefaultAWSCredentialsProviderChain</value>
[36malgo-1    |[0m          <description>AWS S3 credential provider</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>fs.s3.impl</name>
[36malgo-1    |[0m          <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
[36malgo-1    |[0m          <description>s3a filesystem implementation</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>fs.AbstractFileSystem.s3a.imp</name>
[36malgo-1    |[0m          <value>org.apache.hadoop.fs.s3a.S3A</value>
[36malgo-1    |[0m          <description>s3a filesystem implementation</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m  
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>spark.executor.memory</name>
[36malgo-1    |[0m     <value>2g</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>spark.executor.cores</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 
[33malgo-2    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/core-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0" encoding="UTF-8"?>
[33malgo-2    |[0m  <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[33malgo-2    |[0m  <!-- Put site-specific property overrides in this file. -->
[33malgo-2    |[0m 
[33malgo-2    |[0m  <configuration>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>fs.defaultFS</name>
[33malgo-2    |[0m          <value>hdfs://172.22.0.3/</value>
[33malgo-2    |[0m          <description>NameNode URI</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>fs.s3a.aws.credentials.provider</name>
[33malgo-2    |[0m          <value>com.amazonaws.auth.DefaultAWSCredentialsProviderChain</value>
[33malgo-2    |[0m          <description>AWS S3 credential provider</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>fs.s3.impl</name>
[33malgo-2    |[0m          <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
[33malgo-2    |[0m          <description>s3a filesystem implementation</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>fs.AbstractFileSystem.s3a.imp</name>
[33malgo-2    |[0m          <value>org.apache.hadoop.fs.s3a.S3A</value>
[33malgo-2    |[0m          <description>s3a filesystem implementation</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m  
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>spark.executor.memory</name>
[33malgo-2    |[0m     <value>2g</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>spark.executor.cores</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 
[36malgo-1    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/log4j.properties
[33malgo-2    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/log4j.properties
[36malgo-1    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/log4j.properties is: 
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[36malgo-1    |[0m # or more contributor license agreements.  See the NOTICE file
[36malgo-1    |[0m # distributed with this work for additional information
[36malgo-1    |[0m # regarding copyright ownership.  The ASF licenses this file
[36malgo-1    |[0m # to you under the Apache License, Version 2.0 (the
[36malgo-1    |[0m # "License"); you may not use this file except in compliance
[36malgo-1    |[0m # with the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m # Define some default values that can be overridden by system properties
[36malgo-1    |[0m hadoop.root.logger=INFO,console
[36malgo-1    |[0m hadoop.log.dir=.
[36malgo-1    |[0m hadoop.log.file=hadoop.log
[36malgo-1    |[0m 
[36malgo-1    |[0m # Define the root logger to the system property "hadoop.root.logger".
[36malgo-1    |[0m log4j.rootLogger=${hadoop.root.logger}, EventCounter
[36malgo-1    |[0m 
[36malgo-1    |[0m # Logging Threshold
[36malgo-1    |[0m log4j.threshold=ALL
[36malgo-1    |[0m 
[36malgo-1    |[0m # Null Appender
[36malgo-1    |[0m log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Rolling File Appender - cap space usage at 5gb.
[36malgo-1    |[0m #
[36malgo-1    |[0m hadoop.log.maxfilesize=256MB
[36malgo-1    |[0m hadoop.log.maxbackupindex=20
[36malgo-1    |[0m log4j.appender.RFA=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.RFA.MaxFileSize=${hadoop.log.maxfilesize}
[36malgo-1    |[0m log4j.appender.RFA.MaxBackupIndex=${hadoop.log.maxbackupindex}
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.RFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m 
[36malgo-1    |[0m # Pattern format: Date LogLevel LoggerName LogMessage
[36malgo-1    |[0m log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[36malgo-1    |[0m # Debugging Pattern format
[36malgo-1    |[0m #log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Daily Rolling File Appender
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.DRFA.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m 
[36malgo-1    |[0m # Rollver at midnight
[36malgo-1    |[0m #log4j.appender.DRFA.DatePattern=.yyyy-MM-dd
[36malgo-1    |[0m # Rollver at every hour
[36malgo-1    |[0m log4j.appender.DRFA.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m 
[36malgo-1    |[0m # Pattern format: Date LogLevel LoggerName LogMessage
[36malgo-1    |[0m log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m # Debugging Pattern format
[36malgo-1    |[0m #log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # console
[36malgo-1    |[0m # Add "console" to rootlogger above if you want to use this
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.console=org.apache.log4j.ConsoleAppender
[36malgo-1    |[0m log4j.appender.console.target=System.err
[36malgo-1    |[0m log4j.appender.console.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # TaskLog Appender
[36malgo-1    |[0m #
[36malgo-1    |[0m log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.TLA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # HDFS block state change log from block manager
[36malgo-1    |[0m #
[36malgo-1    |[0m # Uncomment the following to log normal block state change
[36malgo-1    |[0m # messages from BlockManager in NameNode.
[36malgo-1    |[0m #log4j.logger.BlockStateChange=DEBUG
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m #Security appender
[36malgo-1    |[0m #
[36malgo-1    |[0m hadoop.security.logger=INFO,NullAppender
[36malgo-1    |[0m hadoop.security.log.maxfilesize=256MB
[36malgo-1    |[0m hadoop.security.log.maxbackupindex=20
[36malgo-1    |[0m log4j.category.SecurityLogger=${hadoop.security.logger}
[36malgo-1    |[0m hadoop.security.log.file=SecurityAuth-${user.name}.audit
[36malgo-1    |[0m log4j.appender.RFAS=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.RFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
[36malgo-1    |[0m log4j.appender.RFAS.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[36malgo-1    |[0m log4j.appender.RFAS.MaxFileSize=${hadoop.security.log.maxfilesize}
[36malgo-1    |[0m log4j.appender.RFAS.MaxBackupIndex=${hadoop.security.log.maxbackupindex}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Daily Rolling Security appender
[36malgo-1    |[0m #
[36malgo-1    |[0m log4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.DRFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
[36malgo-1    |[0m log4j.appender.DRFAS.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.DRFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[36malgo-1    |[0m log4j.appender.DRFAS.DatePattern=.yyyy-MM-dd
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # hadoop configuration logging
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m # Uncomment the following line to turn off configuration deprecation warnings.
[36malgo-1    |[0m # log4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # hdfs audit logging
[36malgo-1    |[0m #
[36malgo-1    |[0m hdfs.audit.logger=INFO,NullAppender
[36malgo-1    |[0m hdfs.audit.log.maxfilesize=256MB
[36malgo-1    |[0m hdfs.audit.log.maxbackupindex=20
[36malgo-1    |[0m log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}
[36malgo-1    |[0m log4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false
[36malgo-1    |[0m log4j.appender.RFAAUDIT=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.RFAAUDIT.File=${hadoop.log.dir}/hdfs-audit.log
[36malgo-1    |[0m log4j.appender.RFAAUDIT.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.RFAAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[36malgo-1    |[0m log4j.appender.RFAAUDIT.MaxFileSize=${hdfs.audit.log.maxfilesize}
[36malgo-1    |[0m log4j.appender.RFAAUDIT.MaxBackupIndex=${hdfs.audit.log.maxbackupindex}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # NameNode metrics logging.
[36malgo-1    |[0m # The default is to retain two namenode-metrics.log files up to 64MB each.
[36malgo-1    |[0m #
[36malgo-1    |[0m namenode.metrics.logger=INFO,NullAppender
[36malgo-1    |[0m log4j.logger.NameNodeMetricsLog=${namenode.metrics.logger}
[36malgo-1    |[0m log4j.additivity.NameNodeMetricsLog=false
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.File=${hadoop.log.dir}/namenode-metrics.log
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.MaxBackupIndex=1
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.MaxFileSize=64MB
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # DataNode metrics logging.
[36malgo-1    |[0m # The default is to retain two datanode-metrics.log files up to 64MB each.
[36malgo-1    |[0m #
[36malgo-1    |[0m datanode.metrics.logger=INFO,NullAppender
[36malgo-1    |[0m log4j.logger.DataNodeMetricsLog=${datanode.metrics.logger}
[36malgo-1    |[0m log4j.additivity.DataNodeMetricsLog=false
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.File=${hadoop.log.dir}/datanode-metrics.log
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.MaxBackupIndex=1
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.MaxFileSize=64MB
[36malgo-1    |[0m 
[36malgo-1    |[0m # Custom Logging levels
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=DEBUG
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m # AWS SDK & S3A FileSystem
[36malgo-1    |[0m #log4j.logger.com.amazonaws=ERROR
[36malgo-1    |[0m log4j.logger.com.amazonaws.http.AmazonHttpClient=ERROR
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.fs.s3a.S3AFileSystem=WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Event Counter Appender
[36malgo-1    |[0m # Sends counts of logging messages at different severity levels to Hadoop Metrics.
[36malgo-1    |[0m #
[36malgo-1    |[0m log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Job Summary Appender
[36malgo-1    |[0m #
[36malgo-1    |[0m # Use following logger to send summary to separate file defined by
[36malgo-1    |[0m # hadoop.mapreduce.jobsummary.log.file :
[36malgo-1    |[0m # hadoop.mapreduce.jobsummary.logger=INFO,JSA
[36malgo-1    |[0m # 
[36malgo-1    |[0m hadoop.mapreduce.jobsummary.logger=${hadoop.root.logger}
[36malgo-1    |[0m hadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log
[36malgo-1    |[0m hadoop.mapreduce.jobsummary.log.maxfilesize=256MB
[36malgo-1    |[0m hadoop.mapreduce.jobsummary.log.maxbackupindex=20
[36malgo-1    |[0m log4j.appender.JSA=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.JSA.File=${hadoop.log.dir}/${hadoop.mapreduce.jobsummary.log.file}
[36malgo-1    |[0m log4j.appender.JSA.MaxFileSize=${hadoop.mapreduce.jobsummary.log.maxfilesize}
[36malgo-1    |[0m log4j.appender.JSA.MaxBackupIndex=${hadoop.mapreduce.jobsummary.log.maxbackupindex}
[36malgo-1    |[0m log4j.appender.JSA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.JSA.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
[36malgo-1    |[0m log4j.appender.JSA.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.logger.org.apache.hadoop.mapred.JobInProgress$JobSummary=${hadoop.mapreduce.jobsummary.logger}
[36malgo-1    |[0m log4j.additivity.org.apache.hadoop.mapred.JobInProgress$JobSummary=false
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # shuffle connection log from shuffleHandler
[36malgo-1    |[0m # Uncomment the following line to enable logging of shuffle connections
[36malgo-1    |[0m # log4j.logger.org.apache.hadoop.mapred.ShuffleHandler.audit=DEBUG
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Yarn ResourceManager Application Summary Log
[36malgo-1    |[0m #
[36malgo-1    |[0m # Set the ResourceManager summary log filename
[36malgo-1    |[0m yarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log
[36malgo-1    |[0m # Set the ResourceManager summary log level and appender
[36malgo-1    |[0m yarn.server.resourcemanager.appsummary.logger=${hadoop.root.logger}
[36malgo-1    |[0m #yarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY
[36malgo-1    |[0m 
[36malgo-1    |[0m # To enable AppSummaryLogging for the RM,
[36malgo-1    |[0m # set yarn.server.resourcemanager.appsummary.logger to
[36malgo-1    |[0m # <LEVEL>,RMSUMMARY in hadoop-env.sh
[36malgo-1    |[0m 
[36malgo-1    |[0m # Appender for ResourceManager Application Summary Log
[36malgo-1    |[0m # Requires the following properties to be set
[36malgo-1    |[0m #    - hadoop.log.dir (Hadoop Log directory)
[36malgo-1    |[0m #    - yarn.server.resourcemanager.appsummary.log.file (resource manager app summary log filename)
[36malgo-1    |[0m #    - yarn.server.resourcemanager.appsummary.logger (resource manager app summary log level and appender)
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=${yarn.server.resourcemanager.appsummary.logger}
[36malgo-1    |[0m log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=false
[36malgo-1    |[0m log4j.appender.RMSUMMARY=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.RMSUMMARY.File=${hadoop.log.dir}/${yarn.server.resourcemanager.appsummary.log.file}
[36malgo-1    |[0m log4j.appender.RMSUMMARY.MaxFileSize=256MB
[36malgo-1    |[0m log4j.appender.RMSUMMARY.MaxBackupIndex=20
[36malgo-1    |[0m log4j.appender.RMSUMMARY.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.RMSUMMARY.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m # HS audit log configs
[36malgo-1    |[0m #mapreduce.hs.audit.logger=INFO,HSAUDIT
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=${mapreduce.hs.audit.logger}
[36malgo-1    |[0m #log4j.additivity.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=false
[36malgo-1    |[0m #log4j.appender.HSAUDIT=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m #log4j.appender.HSAUDIT.File=${hadoop.log.dir}/hs-audit.log
[36malgo-1    |[0m #log4j.appender.HSAUDIT.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m #log4j.appender.HSAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[36malgo-1    |[0m #log4j.appender.HSAUDIT.DatePattern=.yyyy-MM-dd
[36malgo-1    |[0m 
[36malgo-1    |[0m # Http Server Request Logs
[36malgo-1    |[0m #log4j.logger.http.requests.namenode=INFO,namenoderequestlog
[36malgo-1    |[0m #log4j.appender.namenoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.namenoderequestlog.Filename=${hadoop.log.dir}/jetty-namenode-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.namenoderequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.http.requests.datanode=INFO,datanoderequestlog
[36malgo-1    |[0m #log4j.appender.datanoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.datanoderequestlog.Filename=${hadoop.log.dir}/jetty-datanode-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.datanoderequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.http.requests.resourcemanager=INFO,resourcemanagerrequestlog
[36malgo-1    |[0m #log4j.appender.resourcemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.resourcemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-resourcemanager-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.resourcemanagerrequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.http.requests.jobhistory=INFO,jobhistoryrequestlog
[36malgo-1    |[0m #log4j.appender.jobhistoryrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.jobhistoryrequestlog.Filename=${hadoop.log.dir}/jetty-jobhistory-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.jobhistoryrequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.http.requests.nodemanager=INFO,nodemanagerrequestlog
[36malgo-1    |[0m #log4j.appender.nodemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.nodemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-nodemanager-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.nodemanagerrequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m # WebHdfs request log on datanodes
[36malgo-1    |[0m # Specify -Ddatanode.webhdfs.logger=INFO,HTTPDRFA on datanode startup to
[36malgo-1    |[0m # direct the log to a separate file.
[36malgo-1    |[0m #datanode.webhdfs.logger=INFO,console
[36malgo-1    |[0m #log4j.logger.datanode.webhdfs=${datanode.webhdfs.logger}
[36malgo-1    |[0m #log4j.appender.HTTPDRFA=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m #log4j.appender.HTTPDRFA.File=${hadoop.log.dir}/hadoop-datanode-webhdfs.log
[36malgo-1    |[0m #log4j.appender.HTTPDRFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m #log4j.appender.HTTPDRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[36malgo-1    |[0m #log4j.appender.HTTPDRFA.DatePattern=.yyyy-MM-dd
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m # Appender for viewing information for errors and warnings
[36malgo-1    |[0m yarn.ewma.cleanupInterval=300
[36malgo-1    |[0m yarn.ewma.messageAgeLimitSeconds=86400
[36malgo-1    |[0m yarn.ewma.maxUniqueMessages=250
[36malgo-1    |[0m log4j.appender.EWMA=org.apache.hadoop.yarn.util.Log4jWarningErrorMetricsAppender
[36malgo-1    |[0m log4j.appender.EWMA.cleanupInterval=${yarn.ewma.cleanupInterval}
[36malgo-1    |[0m log4j.appender.EWMA.messageAgeLimitSeconds=${yarn.ewma.messageAgeLimitSeconds}
[36malgo-1    |[0m log4j.appender.EWMA.maxUniqueMessages=${yarn.ewma.maxUniqueMessages}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Fair scheduler state dump
[36malgo-1    |[0m #
[36malgo-1    |[0m # Use following logger to dump the state to a separate file
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=DEBUG,FSSTATEDUMP
[36malgo-1    |[0m #log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=false
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.File=${hadoop.log.dir}/fairscheduler-statedump.log
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.MaxFileSize=${hadoop.log.maxfilesize}
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.MaxBackupIndex=${hadoop.log.maxbackupindex}
[36malgo-1    |[0m 
[36malgo-1    |[0m # Log levels of third-party libraries
[36malgo-1    |[0m log4j.logger.org.apache.commons.beanutils=WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m #AWS SDK Logging
[36malgo-1    |[0m log4j.logger.com.amazonaws=WARN
[36malgo-1    |[0m log4j.logger.org.apache.zookeeper=ERROR
[36malgo-1    |[0m log4j.logger.org.apache.hadoop.hbase=WARN
[36malgo-1    |[0m log4j.logger.amazon.emr.metrics=WARN
[36malgo-1    |[0m log4j.logger.emr=INFO
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.HADOOP=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.HADOOP.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m log4j.appender.HADOOP.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.appender.HADOOP.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.HADOOP.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.JHS=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.JHS.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m log4j.appender.JHS.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.appender.JHS.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.JHS.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.MAPRED=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.MAPRED.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m log4j.appender.MAPRED.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.appender.MAPRED.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.MAPRED.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.YARN=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.YARN.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m log4j.appender.YARN.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.appender.YARN.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.YARN.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/hive/conf/hive-env.sh
[33malgo-2    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/log4j.properties is: 
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[33malgo-2    |[0m # or more contributor license agreements.  See the NOTICE file
[33malgo-2    |[0m # distributed with this work for additional information
[33malgo-2    |[0m # regarding copyright ownership.  The ASF licenses this file
[33malgo-2    |[0m # to you under the Apache License, Version 2.0 (the
[33malgo-2    |[0m # "License"); you may not use this file except in compliance
[33malgo-2    |[0m # with the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m # Define some default values that can be overridden by system properties
[33malgo-2    |[0m hadoop.root.logger=INFO,console
[33malgo-2    |[0m hadoop.log.dir=.
[33malgo-2    |[0m hadoop.log.file=hadoop.log
[33malgo-2    |[0m 
[33malgo-2    |[0m # Define the root logger to the system property "hadoop.root.logger".
[33malgo-2    |[0m log4j.rootLogger=${hadoop.root.logger}, EventCounter
[33malgo-2    |[0m 
[33malgo-2    |[0m # Logging Threshold
[33malgo-2    |[0m log4j.threshold=ALL
[33malgo-2    |[0m 
[33malgo-2    |[0m # Null Appender
[33malgo-2    |[0m log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Rolling File Appender - cap space usage at 5gb.
[33malgo-2    |[0m #
[33malgo-2    |[0m hadoop.log.maxfilesize=256MB
[33malgo-2    |[0m hadoop.log.maxbackupindex=20
[33malgo-2    |[0m log4j.appender.RFA=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.RFA.MaxFileSize=${hadoop.log.maxfilesize}
[33malgo-2    |[0m log4j.appender.RFA.MaxBackupIndex=${hadoop.log.maxbackupindex}
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.RFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m 
[33malgo-2    |[0m # Pattern format: Date LogLevel LoggerName LogMessage
[33malgo-2    |[0m log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[33malgo-2    |[0m # Debugging Pattern format
[33malgo-2    |[0m #log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Daily Rolling File Appender
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.DRFA.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m 
[33malgo-2    |[0m # Rollver at midnight
[33malgo-2    |[0m #log4j.appender.DRFA.DatePattern=.yyyy-MM-dd
[33malgo-2    |[0m # Rollver at every hour
[33malgo-2    |[0m log4j.appender.DRFA.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m 
[33malgo-2    |[0m # Pattern format: Date LogLevel LoggerName LogMessage
[33malgo-2    |[0m log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m # Debugging Pattern format
[33malgo-2    |[0m #log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # console
[33malgo-2    |[0m # Add "console" to rootlogger above if you want to use this
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.console=org.apache.log4j.ConsoleAppender
[33malgo-2    |[0m log4j.appender.console.target=System.err
[33malgo-2    |[0m log4j.appender.console.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # TaskLog Appender
[33malgo-2    |[0m #
[33malgo-2    |[0m log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.TLA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # HDFS block state change log from block manager
[33malgo-2    |[0m #
[33malgo-2    |[0m # Uncomment the following to log normal block state change
[33malgo-2    |[0m # messages from BlockManager in NameNode.
[33malgo-2    |[0m #log4j.logger.BlockStateChange=DEBUG
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m #Security appender
[33malgo-2    |[0m #
[33malgo-2    |[0m hadoop.security.logger=INFO,NullAppender
[33malgo-2    |[0m hadoop.security.log.maxfilesize=256MB
[33malgo-2    |[0m hadoop.security.log.maxbackupindex=20
[33malgo-2    |[0m log4j.category.SecurityLogger=${hadoop.security.logger}
[33malgo-2    |[0m hadoop.security.log.file=SecurityAuth-${user.name}.audit
[33malgo-2    |[0m log4j.appender.RFAS=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.RFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
[33malgo-2    |[0m log4j.appender.RFAS.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[33malgo-2    |[0m log4j.appender.RFAS.MaxFileSize=${hadoop.security.log.maxfilesize}
[33malgo-2    |[0m log4j.appender.RFAS.MaxBackupIndex=${hadoop.security.log.maxbackupindex}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Daily Rolling Security appender
[33malgo-2    |[0m #
[33malgo-2    |[0m log4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.DRFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
[33malgo-2    |[0m log4j.appender.DRFAS.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.DRFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[33malgo-2    |[0m log4j.appender.DRFAS.DatePattern=.yyyy-MM-dd
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # hadoop configuration logging
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m # Uncomment the following line to turn off configuration deprecation warnings.
[33malgo-2    |[0m # log4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # hdfs audit logging
[33malgo-2    |[0m #
[33malgo-2    |[0m hdfs.audit.logger=INFO,NullAppender
[33malgo-2    |[0m hdfs.audit.log.maxfilesize=256MB
[33malgo-2    |[0m hdfs.audit.log.maxbackupindex=20
[33malgo-2    |[0m log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}
[33malgo-2    |[0m log4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false
[33malgo-2    |[0m log4j.appender.RFAAUDIT=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.RFAAUDIT.File=${hadoop.log.dir}/hdfs-audit.log
[33malgo-2    |[0m log4j.appender.RFAAUDIT.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.RFAAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[33malgo-2    |[0m log4j.appender.RFAAUDIT.MaxFileSize=${hdfs.audit.log.maxfilesize}
[33malgo-2    |[0m log4j.appender.RFAAUDIT.MaxBackupIndex=${hdfs.audit.log.maxbackupindex}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # NameNode metrics logging.
[33malgo-2    |[0m # The default is to retain two namenode-metrics.log files up to 64MB each.
[33malgo-2    |[0m #
[33malgo-2    |[0m namenode.metrics.logger=INFO,NullAppender
[33malgo-2    |[0m log4j.logger.NameNodeMetricsLog=${namenode.metrics.logger}
[33malgo-2    |[0m log4j.additivity.NameNodeMetricsLog=false
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.File=${hadoop.log.dir}/namenode-metrics.log
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.MaxBackupIndex=1
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.MaxFileSize=64MB
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # DataNode metrics logging.
[33malgo-2    |[0m # The default is to retain two datanode-metrics.log files up to 64MB each.
[33malgo-2    |[0m #
[33malgo-2    |[0m datanode.metrics.logger=INFO,NullAppender
[33malgo-2    |[0m log4j.logger.DataNodeMetricsLog=${datanode.metrics.logger}
[33malgo-2    |[0m log4j.additivity.DataNodeMetricsLog=false
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.File=${hadoop.log.dir}/datanode-metrics.log
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.MaxBackupIndex=1
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.MaxFileSize=64MB
[33malgo-2    |[0m 
[33malgo-2    |[0m # Custom Logging levels
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=DEBUG
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m # AWS SDK & S3A FileSystem
[33malgo-2    |[0m #log4j.logger.com.amazonaws=ERROR
[33malgo-2    |[0m log4j.logger.com.amazonaws.http.AmazonHttpClient=ERROR
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.fs.s3a.S3AFileSystem=WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Event Counter Appender
[33malgo-2    |[0m # Sends counts of logging messages at different severity levels to Hadoop Metrics.
[33malgo-2    |[0m #
[33malgo-2    |[0m log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Job Summary Appender
[33malgo-2    |[0m #
[33malgo-2    |[0m # Use following logger to send summary to separate file defined by
[33malgo-2    |[0m # hadoop.mapreduce.jobsummary.log.file :
[33malgo-2    |[0m # hadoop.mapreduce.jobsummary.logger=INFO,JSA
[33malgo-2    |[0m # 
[33malgo-2    |[0m hadoop.mapreduce.jobsummary.logger=${hadoop.root.logger}
[33malgo-2    |[0m hadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log
[33malgo-2    |[0m hadoop.mapreduce.jobsummary.log.maxfilesize=256MB
[33malgo-2    |[0m hadoop.mapreduce.jobsummary.log.maxbackupindex=20
[33malgo-2    |[0m log4j.appender.JSA=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.JSA.File=${hadoop.log.dir}/${hadoop.mapreduce.jobsummary.log.file}
[33malgo-2    |[0m log4j.appender.JSA.MaxFileSize=${hadoop.mapreduce.jobsummary.log.maxfilesize}
[33malgo-2    |[0m log4j.appender.JSA.MaxBackupIndex=${hadoop.mapreduce.jobsummary.log.maxbackupindex}
[33malgo-2    |[0m log4j.appender.JSA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.JSA.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
[33malgo-2    |[0m log4j.appender.JSA.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.logger.org.apache.hadoop.mapred.JobInProgress$JobSummary=${hadoop.mapreduce.jobsummary.logger}
[33malgo-2    |[0m log4j.additivity.org.apache.hadoop.mapred.JobInProgress$JobSummary=false
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # shuffle connection log from shuffleHandler
[33malgo-2    |[0m # Uncomment the following line to enable logging of shuffle connections
[33malgo-2    |[0m # log4j.logger.org.apache.hadoop.mapred.ShuffleHandler.audit=DEBUG
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Yarn ResourceManager Application Summary Log
[33malgo-2    |[0m #
[33malgo-2    |[0m # Set the ResourceManager summary log filename
[33malgo-2    |[0m yarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log
[33malgo-2    |[0m # Set the ResourceManager summary log level and appender
[33malgo-2    |[0m yarn.server.resourcemanager.appsummary.logger=${hadoop.root.logger}
[33malgo-2    |[0m #yarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY
[33malgo-2    |[0m 
[33malgo-2    |[0m # To enable AppSummaryLogging for the RM,
[33malgo-2    |[0m # set yarn.server.resourcemanager.appsummary.logger to
[33malgo-2    |[0m # <LEVEL>,RMSUMMARY in hadoop-env.sh
[33malgo-2    |[0m 
[33malgo-2    |[0m # Appender for ResourceManager Application Summary Log
[33malgo-2    |[0m # Requires the following properties to be set
[33malgo-2    |[0m #    - hadoop.log.dir (Hadoop Log directory)
[33malgo-2    |[0m #    - yarn.server.resourcemanager.appsummary.log.file (resource manager app summary log filename)
[33malgo-2    |[0m #    - yarn.server.resourcemanager.appsummary.logger (resource manager app summary log level and appender)
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=${yarn.server.resourcemanager.appsummary.logger}
[33malgo-2    |[0m log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=false
[33malgo-2    |[0m log4j.appender.RMSUMMARY=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.RMSUMMARY.File=${hadoop.log.dir}/${yarn.server.resourcemanager.appsummary.log.file}
[33malgo-2    |[0m log4j.appender.RMSUMMARY.MaxFileSize=256MB
[33malgo-2    |[0m log4j.appender.RMSUMMARY.MaxBackupIndex=20
[33malgo-2    |[0m log4j.appender.RMSUMMARY.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.RMSUMMARY.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m # HS audit log configs
[33malgo-2    |[0m #mapreduce.hs.audit.logger=INFO,HSAUDIT
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=${mapreduce.hs.audit.logger}
[33malgo-2    |[0m #log4j.additivity.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=false
[33malgo-2    |[0m #log4j.appender.HSAUDIT=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m #log4j.appender.HSAUDIT.File=${hadoop.log.dir}/hs-audit.log
[33malgo-2    |[0m #log4j.appender.HSAUDIT.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m #log4j.appender.HSAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[33malgo-2    |[0m #log4j.appender.HSAUDIT.DatePattern=.yyyy-MM-dd
[33malgo-2    |[0m 
[33malgo-2    |[0m # Http Server Request Logs
[33malgo-2    |[0m #log4j.logger.http.requests.namenode=INFO,namenoderequestlog
[33malgo-2    |[0m #log4j.appender.namenoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.namenoderequestlog.Filename=${hadoop.log.dir}/jetty-namenode-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.namenoderequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.http.requests.datanode=INFO,datanoderequestlog
[33malgo-2    |[0m #log4j.appender.datanoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.datanoderequestlog.Filename=${hadoop.log.dir}/jetty-datanode-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.datanoderequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.http.requests.resourcemanager=INFO,resourcemanagerrequestlog
[33malgo-2    |[0m #log4j.appender.resourcemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.resourcemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-resourcemanager-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.resourcemanagerrequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.http.requests.jobhistory=INFO,jobhistoryrequestlog
[33malgo-2    |[0m #log4j.appender.jobhistoryrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.jobhistoryrequestlog.Filename=${hadoop.log.dir}/jetty-jobhistory-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.jobhistoryrequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.http.requests.nodemanager=INFO,nodemanagerrequestlog
[33malgo-2    |[0m #log4j.appender.nodemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.nodemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-nodemanager-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.nodemanagerrequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m # WebHdfs request log on datanodes
[33malgo-2    |[0m # Specify -Ddatanode.webhdfs.logger=INFO,HTTPDRFA on datanode startup to
[33malgo-2    |[0m # direct the log to a separate file.
[33malgo-2    |[0m #datanode.webhdfs.logger=INFO,console
[33malgo-2    |[0m #log4j.logger.datanode.webhdfs=${datanode.webhdfs.logger}
[33malgo-2    |[0m #log4j.appender.HTTPDRFA=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m #log4j.appender.HTTPDRFA.File=${hadoop.log.dir}/hadoop-datanode-webhdfs.log
[33malgo-2    |[0m #log4j.appender.HTTPDRFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m #log4j.appender.HTTPDRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[33malgo-2    |[0m #log4j.appender.HTTPDRFA.DatePattern=.yyyy-MM-dd
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m # Appender for viewing information for errors and warnings
[33malgo-2    |[0m yarn.ewma.cleanupInterval=300
[33malgo-2    |[0m yarn.ewma.messageAgeLimitSeconds=86400
[33malgo-2    |[0m yarn.ewma.maxUniqueMessages=250
[33malgo-2    |[0m log4j.appender.EWMA=org.apache.hadoop.yarn.util.Log4jWarningErrorMetricsAppender
[33malgo-2    |[0m log4j.appender.EWMA.cleanupInterval=${yarn.ewma.cleanupInterval}
[33malgo-2    |[0m log4j.appender.EWMA.messageAgeLimitSeconds=${yarn.ewma.messageAgeLimitSeconds}
[33malgo-2    |[0m log4j.appender.EWMA.maxUniqueMessages=${yarn.ewma.maxUniqueMessages}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Fair scheduler state dump
[33malgo-2    |[0m #
[33malgo-2    |[0m # Use following logger to dump the state to a separate file
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=DEBUG,FSSTATEDUMP
[33malgo-2    |[0m #log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=false
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.File=${hadoop.log.dir}/fairscheduler-statedump.log
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.MaxFileSize=${hadoop.log.maxfilesize}
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.MaxBackupIndex=${hadoop.log.maxbackupindex}
[33malgo-2    |[0m 
[33malgo-2    |[0m # Log levels of third-party libraries
[33malgo-2    |[0m log4j.logger.org.apache.commons.beanutils=WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m #AWS SDK Logging
[33malgo-2    |[0m log4j.logger.com.amazonaws=WARN
[33malgo-2    |[0m log4j.logger.org.apache.zookeeper=ERROR
[33malgo-2    |[0m log4j.logger.org.apache.hadoop.hbase=WARN
[33malgo-2    |[0m log4j.logger.amazon.emr.metrics=WARN
[33malgo-2    |[0m log4j.logger.emr=INFO
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.HADOOP=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.HADOOP.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m log4j.appender.HADOOP.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.appender.HADOOP.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.HADOOP.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.JHS=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.JHS.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m log4j.appender.JHS.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.appender.JHS.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.JHS.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.MAPRED=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.MAPRED.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m log4j.appender.MAPRED.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.appender.MAPRED.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.MAPRED.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.YARN=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.YARN.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m log4j.appender.YARN.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.appender.YARN.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.YARN.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[36malgo-1    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/hive/conf/hive-env.sh is: 
[36malgo-1    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[36malgo-1    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[36malgo-1    |[0m 
[33malgo-2    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/hive/conf/hive-env.sh
[33malgo-2    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/hive/conf/hive-env.sh is: 
[33malgo-2    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[33malgo-2    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[33malgo-2    |[0m 
[36malgo-1    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/hive/conf/hive-log4j2.properties
[33malgo-2    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/hive/conf/hive-log4j2.properties
[33malgo-2    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/hive/conf/hive-log4j2.properties is: 
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[33malgo-2    |[0m # or more contributor license agreements.  See the NOTICE file
[33malgo-2    |[0m # distributed with this work for additional information
[33malgo-2    |[0m # regarding copyright ownership.  The ASF licenses this file
[33malgo-2    |[0m # to you under the Apache License, Version 2.0 (the
[33malgo-2    |[0m # "License"); you may not use this file except in compliance
[33malgo-2    |[0m # with the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m status = INFO
[33malgo-2    |[0m name = HiveLog4j2
[33malgo-2    |[0m packages = org.apache.hadoop.hive.ql.log
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of properties
[33malgo-2    |[0m property.hive.log.level = INFO
[33malgo-2    |[0m property.hive.root.logger = DRFA
[33malgo-2    |[0m property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
[33malgo-2    |[0m property.hive.log.file = hive.log
[33malgo-2    |[0m property.hive.perflogger.log.level = INFO
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of all appenders
[33malgo-2    |[0m appenders = console, DRFA
[33malgo-2    |[0m 
[33malgo-2    |[0m # console appender
[33malgo-2    |[0m appender.console.type = Console
[33malgo-2    |[0m appender.console.name = console
[33malgo-2    |[0m appender.console.target = SYSTEM_ERR
[33malgo-2    |[0m appender.console.layout.type = PatternLayout
[33malgo-2    |[0m appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m # daily rolling file appender
[33malgo-2    |[0m appender.DRFA.type = RollingRandomAccessFile
[33malgo-2    |[0m appender.DRFA.name = DRFA
[33malgo-2    |[0m appender.DRFA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
[33malgo-2    |[0m # Use %pid in the filePattern to append <process-id>@<host-name> to the filename if you want separate log files for different CLI session
[33malgo-2    |[0m appender.DRFA.filePattern = ${sys:hive.log.dir}/${sys:hive.log.file}.%d{yyyy-MM-dd}
[33malgo-2    |[0m appender.DRFA.layout.type = PatternLayout
[33malgo-2    |[0m appender.DRFA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[33malgo-2    |[0m appender.DRFA.policies.type = Policies
[33malgo-2    |[0m appender.DRFA.policies.time.type = TimeBasedTriggeringPolicy
[33malgo-2    |[0m appender.DRFA.policies.time.interval = 1
[33malgo-2    |[0m appender.DRFA.policies.time.modulate = true
[33malgo-2    |[0m appender.DRFA.strategy.type = DefaultRolloverStrategy
[33malgo-2    |[0m appender.DRFA.strategy.max = 30
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of all loggers
[33malgo-2    |[0m loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, PerfLogger, AmazonAws, ApacheHttp
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
[33malgo-2    |[0m logger.NIOServerCnxn.level = WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
[33malgo-2    |[0m logger.ClientCnxnSocketNIO.level = WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.DataNucleus.name = DataNucleus
[33malgo-2    |[0m logger.DataNucleus.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.Datastore.name = Datastore
[33malgo-2    |[0m logger.Datastore.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.JPOX.name = JPOX
[33malgo-2    |[0m logger.JPOX.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.AmazonAws.name=com.amazonaws
[33malgo-2    |[0m logger.AmazonAws.level = INFO
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.ApacheHttp.name=org.apache.http
[33malgo-2    |[0m logger.ApacheHttp.level = INFO
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.PerfLogger.name = org.apache.hadoop.hive.ql.log.PerfLogger
[33malgo-2    |[0m logger.PerfLogger.level = ${sys:hive.perflogger.log.level}
[33malgo-2    |[0m 
[33malgo-2    |[0m # root logger
[33malgo-2    |[0m rootLogger.level = ${sys:hive.log.level}
[33malgo-2    |[0m rootLogger.appenderRefs = root
[33malgo-2    |[0m rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[36malgo-1    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/hive/conf/hive-log4j2.properties is: 
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[36malgo-1    |[0m # or more contributor license agreements.  See the NOTICE file
[36malgo-1    |[0m # distributed with this work for additional information
[36malgo-1    |[0m # regarding copyright ownership.  The ASF licenses this file
[36malgo-1    |[0m # to you under the Apache License, Version 2.0 (the
[36malgo-1    |[0m # "License"); you may not use this file except in compliance
[36malgo-1    |[0m # with the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m status = INFO
[36malgo-1    |[0m name = HiveLog4j2
[36malgo-1    |[0m packages = org.apache.hadoop.hive.ql.log
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of properties
[36malgo-1    |[0m property.hive.log.level = INFO
[36malgo-1    |[0m property.hive.root.logger = DRFA
[36malgo-1    |[0m property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
[36malgo-1    |[0m property.hive.log.file = hive.log
[36malgo-1    |[0m property.hive.perflogger.log.level = INFO
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of all appenders
[36malgo-1    |[0m appenders = console, DRFA
[36malgo-1    |[0m 
[36malgo-1    |[0m # console appender
[36malgo-1    |[0m appender.console.type = Console
[36malgo-1    |[0m appender.console.name = console
[36malgo-1    |[0m appender.console.target = SYSTEM_ERR
[36malgo-1    |[0m appender.console.layout.type = PatternLayout
[36malgo-1    |[0m appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m # daily rolling file appender
[36malgo-1    |[0m appender.DRFA.type = RollingRandomAccessFile
[36malgo-1    |[0m appender.DRFA.name = DRFA
[36malgo-1    |[0m appender.DRFA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
[36malgo-1    |[0m # Use %pid in the filePattern to append <process-id>@<host-name> to the filename if you want separate log files for different CLI session
[36malgo-1    |[0m appender.DRFA.filePattern = ${sys:hive.log.dir}/${sys:hive.log.file}.%d{yyyy-MM-dd}
[36malgo-1    |[0m appender.DRFA.layout.type = PatternLayout
[36malgo-1    |[0m appender.DRFA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[36malgo-1    |[0m appender.DRFA.policies.type = Policies
[36malgo-1    |[0m appender.DRFA.policies.time.type = TimeBasedTriggeringPolicy
[36malgo-1    |[0m appender.DRFA.policies.time.interval = 1
[36malgo-1    |[0m appender.DRFA.policies.time.modulate = true
[36malgo-1    |[0m appender.DRFA.strategy.type = DefaultRolloverStrategy
[36malgo-1    |[0m appender.DRFA.strategy.max = 30
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of all loggers
[36malgo-1    |[0m loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, PerfLogger, AmazonAws, ApacheHttp
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
[36malgo-1    |[0m logger.NIOServerCnxn.level = WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
[36malgo-1    |[0m logger.ClientCnxnSocketNIO.level = WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.DataNucleus.name = DataNucleus
[36malgo-1    |[0m logger.DataNucleus.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.Datastore.name = Datastore
[36malgo-1    |[0m logger.Datastore.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.JPOX.name = JPOX
[36malgo-1    |[0m logger.JPOX.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.AmazonAws.name=com.amazonaws
[36malgo-1    |[0m logger.AmazonAws.level = INFO
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.ApacheHttp.name=org.apache.http
[36malgo-1    |[0m logger.ApacheHttp.level = INFO
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.PerfLogger.name = org.apache.hadoop.hive.ql.log.PerfLogger
[36malgo-1    |[0m logger.PerfLogger.level = ${sys:hive.perflogger.log.level}
[36malgo-1    |[0m 
[36malgo-1    |[0m # root logger
[36malgo-1    |[0m rootLogger.level = ${sys:hive.log.level}
[36malgo-1    |[0m rootLogger.appenderRefs = root
[36malgo-1    |[0m rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[33malgo-2    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/hive/conf/hive-exec-log4j2.properties
[36malgo-1    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/hive/conf/hive-exec-log4j2.properties
[33malgo-2    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/hive/conf/hive-exec-log4j2.properties is: 
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[33malgo-2    |[0m # or more contributor license agreements.  See the NOTICE file
[33malgo-2    |[0m # distributed with this work for additional information
[33malgo-2    |[0m # regarding copyright ownership.  The ASF licenses this file
[33malgo-2    |[0m # to you under the Apache License, Version 2.0 (the
[33malgo-2    |[0m # "License"); you may not use this file except in compliance
[33malgo-2    |[0m # with the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m status = INFO
[33malgo-2    |[0m name = HiveExecLog4j2
[33malgo-2    |[0m packages = org.apache.hadoop.hive.ql.log
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of properties
[33malgo-2    |[0m property.hive.log.level = INFO
[33malgo-2    |[0m property.hive.root.logger = FA
[33malgo-2    |[0m property.hive.query.id = hadoop
[33malgo-2    |[0m property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
[33malgo-2    |[0m property.hive.log.file = ${sys:hive.query.id}.log
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of all appenders
[33malgo-2    |[0m appenders = console, FA
[33malgo-2    |[0m 
[33malgo-2    |[0m # console appender
[33malgo-2    |[0m appender.console.type = Console
[33malgo-2    |[0m appender.console.name = console
[33malgo-2    |[0m appender.console.target = SYSTEM_ERR
[33malgo-2    |[0m appender.console.layout.type = PatternLayout
[33malgo-2    |[0m appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m # simple file appender
[33malgo-2    |[0m appender.FA.type = RandomAccessFile
[33malgo-2    |[0m appender.FA.name = FA
[33malgo-2    |[0m appender.FA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
[33malgo-2    |[0m appender.FA.layout.type = PatternLayout
[33malgo-2    |[0m appender.FA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of all loggers
[33malgo-2    |[0m loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
[33malgo-2    |[0m logger.NIOServerCnxn.level = WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
[33malgo-2    |[0m logger.ClientCnxnSocketNIO.level = WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.DataNucleus.name = DataNucleus
[33malgo-2    |[0m logger.DataNucleus.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.Datastore.name = Datastore
[33malgo-2    |[0m logger.Datastore.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.JPOX.name = JPOX
[33malgo-2    |[0m logger.JPOX.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m # root logger
[33malgo-2    |[0m rootLogger.level = ${sys:hive.log.level}
[33malgo-2    |[0m rootLogger.appenderRefs = root
[33malgo-2    |[0m rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[36malgo-1    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/hive/conf/hive-exec-log4j2.properties is: 
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[36malgo-1    |[0m # or more contributor license agreements.  See the NOTICE file
[36malgo-1    |[0m # distributed with this work for additional information
[36malgo-1    |[0m # regarding copyright ownership.  The ASF licenses this file
[36malgo-1    |[0m # to you under the Apache License, Version 2.0 (the
[36malgo-1    |[0m # "License"); you may not use this file except in compliance
[36malgo-1    |[0m # with the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m status = INFO
[36malgo-1    |[0m name = HiveExecLog4j2
[36malgo-1    |[0m packages = org.apache.hadoop.hive.ql.log
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of properties
[36malgo-1    |[0m property.hive.log.level = INFO
[36malgo-1    |[0m property.hive.root.logger = FA
[36malgo-1    |[0m property.hive.query.id = hadoop
[36malgo-1    |[0m property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
[36malgo-1    |[0m property.hive.log.file = ${sys:hive.query.id}.log
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of all appenders
[36malgo-1    |[0m appenders = console, FA
[36malgo-1    |[0m 
[36malgo-1    |[0m # console appender
[36malgo-1    |[0m appender.console.type = Console
[36malgo-1    |[0m appender.console.name = console
[36malgo-1    |[0m appender.console.target = SYSTEM_ERR
[36malgo-1    |[0m appender.console.layout.type = PatternLayout
[36malgo-1    |[0m appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m # simple file appender
[36malgo-1    |[0m appender.FA.type = RandomAccessFile
[36malgo-1    |[0m appender.FA.name = FA
[36malgo-1    |[0m appender.FA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
[36malgo-1    |[0m appender.FA.layout.type = PatternLayout
[36malgo-1    |[0m appender.FA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of all loggers
[36malgo-1    |[0m loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
[36malgo-1    |[0m logger.NIOServerCnxn.level = WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
[36malgo-1    |[0m logger.ClientCnxnSocketNIO.level = WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.DataNucleus.name = DataNucleus
[36malgo-1    |[0m logger.DataNucleus.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.Datastore.name = Datastore
[36malgo-1    |[0m logger.Datastore.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.JPOX.name = JPOX
[36malgo-1    |[0m logger.JPOX.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m # root logger
[36malgo-1    |[0m rootLogger.level = ${sys:hive.log.level}
[36malgo-1    |[0m rootLogger.appenderRefs = root
[36malgo-1    |[0m rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[33malgo-2    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/hive/conf/hive-site.xml
[36malgo-1    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/hive/conf/hive-site.xml
[36malgo-1    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/hive/conf/hive-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0"?>
[36malgo-1    |[0m <!--
[36malgo-1    |[0m   Licensed to the Apache Software Foundation (ASF) under one or more
[36malgo-1    |[0m   contributor license agreements.  See the NOTICE file distributed with
[36malgo-1    |[0m   this work for additional information regarding copyright ownership.
[36malgo-1    |[0m   The ASF licenses this file to You under the Apache License, Version 2.0
[36malgo-1    |[0m   (the "License"); you may not use this file except in compliance with
[36malgo-1    |[0m   the License.  You may obtain a copy of the License at
[36malgo-1    |[0m 
[36malgo-1    |[0m       http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m 
[36malgo-1    |[0m   Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m   distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m   See the License for the specific language governing permissions and
[36malgo-1    |[0m   limitations under the License.
[36malgo-1    |[0m -->
[36malgo-1    |[0m <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[36malgo-1    |[0m 
[36malgo-1    |[0m <configuration>
[36malgo-1    |[0m 
[36malgo-1    |[0m <!-- Hive Configuration can either be stored in this file or in the hadoop configuration files  -->
[36malgo-1    |[0m <!-- that are implied by Hadoop setup variables.                                                -->
[36malgo-1    |[0m <!-- Aside from Hadoop setup variables - this file is provided as a convenience so that Hive    -->
[36malgo-1    |[0m <!-- users do not have to edit hadoop configuration files (that may be managed as a centralized -->
[36malgo-1    |[0m <!-- resource).                                                                                 -->
[36malgo-1    |[0m 
[36malgo-1    |[0m <!-- Hive Execution Parameters -->
[36malgo-1    |[0m 
[36malgo-1    |[0m <property>
[36malgo-1    |[0m   <name>javax.jdo.option.ConnectionURL</name>
[36malgo-1    |[0m   <value>jdbc:derby:;databaseName=/var/lib/hive/metastore/metastore_db;create=true</value>
[36malgo-1    |[0m   <description>JDBC connect string for a JDBC metastore</description>
[36malgo-1    |[0m </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m <property>
[36malgo-1    |[0m   <name>javax.jdo.option.ConnectionDriverName</name>
[36malgo-1    |[0m   <value>org.apache.derby.jdbc.EmbeddedDriver</value>
[36malgo-1    |[0m   <description>Driver class name for a JDBC metastore</description>
[36malgo-1    |[0m </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>key</name>
[36malgo-1    |[0m     <value>value</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 
[33malgo-2    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/hive/conf/hive-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0"?>
[33malgo-2    |[0m <!--
[33malgo-2    |[0m   Licensed to the Apache Software Foundation (ASF) under one or more
[33malgo-2    |[0m   contributor license agreements.  See the NOTICE file distributed with
[33malgo-2    |[0m   this work for additional information regarding copyright ownership.
[33malgo-2    |[0m   The ASF licenses this file to You under the Apache License, Version 2.0
[33malgo-2    |[0m   (the "License"); you may not use this file except in compliance with
[33malgo-2    |[0m   the License.  You may obtain a copy of the License at
[33malgo-2    |[0m 
[33malgo-2    |[0m       http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m 
[33malgo-2    |[0m   Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m   distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m   See the License for the specific language governing permissions and
[33malgo-2    |[0m   limitations under the License.
[33malgo-2    |[0m -->
[33malgo-2    |[0m <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[33malgo-2    |[0m 
[33malgo-2    |[0m <configuration>
[33malgo-2    |[0m 
[33malgo-2    |[0m <!-- Hive Configuration can either be stored in this file or in the hadoop configuration files  -->
[33malgo-2    |[0m <!-- that are implied by Hadoop setup variables.                                                -->
[33malgo-2    |[0m <!-- Aside from Hadoop setup variables - this file is provided as a convenience so that Hive    -->
[33malgo-2    |[0m <!-- users do not have to edit hadoop configuration files (that may be managed as a centralized -->
[33malgo-2    |[0m <!-- resource).                                                                                 -->
[33malgo-2    |[0m 
[33malgo-2    |[0m <!-- Hive Execution Parameters -->
[33malgo-2    |[0m 
[33malgo-2    |[0m <property>
[33malgo-2    |[0m   <name>javax.jdo.option.ConnectionURL</name>
[33malgo-2    |[0m   <value>jdbc:derby:;databaseName=/var/lib/hive/metastore/metastore_db;create=true</value>
[33malgo-2    |[0m   <description>JDBC connect string for a JDBC metastore</description>
[33malgo-2    |[0m </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m <property>
[33malgo-2    |[0m   <name>javax.jdo.option.ConnectionDriverName</name>
[33malgo-2    |[0m   <value>org.apache.derby.jdbc.EmbeddedDriver</value>
[33malgo-2    |[0m   <description>Driver class name for a JDBC metastore</description>
[33malgo-2    |[0m </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>key</name>
[33malgo-2    |[0m     <value>value</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[33malgo-2    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.driver.host=172.22.0.3
[33malgo-2    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[33malgo-2    |[0m 
[33malgo-2    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[33malgo-2    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[33malgo-2    |[0m spark.rpc.askTimeout=300s
[33malgo-2    |[0m spark.driver.memory 2048m
[33malgo-2    |[0m spark.driver.memoryOverhead 204m
[33malgo-2    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[33malgo-2    |[0m spark.executor.memory 12399m
[33malgo-2    |[0m spark.executor.memoryOverhead 1239m
[33malgo-2    |[0m spark.executor.cores 4
[33malgo-2    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[33malgo-2    |[0m spark.executor.instances 2
[33malgo-2    |[0m spark.default.parallelism 16
[33malgo-2    |[0m spark.executor.memory 2g
[33malgo-2    |[0m spark.executor.cores 1
[33malgo-2    |[0m key value
[33malgo-2    |[0m 
[36malgo-1    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[36malgo-1    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.driver.host=172.22.0.3
[36malgo-1    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[36malgo-1    |[0m 
[36malgo-1    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[36malgo-1    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[36malgo-1    |[0m spark.rpc.askTimeout=300s
[36malgo-1    |[0m spark.driver.memory 2048m
[36malgo-1    |[0m spark.driver.memoryOverhead 204m
[36malgo-1    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[36malgo-1    |[0m spark.executor.memory 12399m
[36malgo-1    |[0m spark.executor.memoryOverhead 1239m
[36malgo-1    |[0m spark.executor.cores 4
[36malgo-1    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[36malgo-1    |[0m spark.executor.instances 2
[36malgo-1    |[0m spark.default.parallelism 16
[36malgo-1    |[0m spark.executor.memory 2g
[36malgo-1    |[0m spark.executor.cores 1
[36malgo-1    |[0m key value
[36malgo-1    |[0m 
[33malgo-2    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/spark/conf/spark-env.sh
[36malgo-1    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/spark/conf/spark-env.sh
[33malgo-2    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/spark/conf/spark-env.sh is: 
[33malgo-2    |[0m #EMPTY FILE AVOID OVERRIDDING ENV VARS
[33malgo-2    |[0m # Specifically, without copying the empty file, SPARK_HISTORY_OPTS will be overriden, 
[33malgo-2    |[0m # spark.history.ui.port defaults to 18082, and spark.eventLog.dir defaults to local fs
[33malgo-2    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[33malgo-2    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/spark/conf/log4j.properties
[36malgo-1    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/spark/conf/spark-env.sh is: 
[36malgo-1    |[0m #EMPTY FILE AVOID OVERRIDDING ENV VARS
[36malgo-1    |[0m # Specifically, without copying the empty file, SPARK_HISTORY_OPTS will be overriden, 
[36malgo-1    |[0m # spark.history.ui.port defaults to 18082, and spark.eventLog.dir defaults to local fs
[36malgo-1    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[36malgo-1    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/spark/conf/log4j.properties
[36malgo-1    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/spark/conf/log4j.properties is: 
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[33malgo-2    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/spark/conf/log4j.properties is: 
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[36malgo-1    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/spark/conf/hive-site.xml
[33malgo-2    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/spark/conf/hive-site.xml
[36malgo-1    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/spark/conf/hive-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0"?>
[36malgo-1    |[0m <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[36malgo-1    |[0m <configuration>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>key</name>
[36malgo-1    |[0m     <value>value</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m </configuration>
[33malgo-2    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/spark/conf/hive-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0"?>
[33malgo-2    |[0m <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[33malgo-2    |[0m <configuration>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>key</name>
[33malgo-2    |[0m     <value>value</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/spark/conf/metrics.properties
[36malgo-1    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/spark/conf/metrics.properties
[33malgo-2    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/spark/conf/metrics.properties is: 
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[36malgo-1    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/spark/conf/metrics.properties is: 
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[33malgo-2    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[36malgo-1    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[36malgo-1    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0"?>
[36malgo-1    |[0m <!-- Site specific YARN configuration properties -->
[36malgo-1    |[0m  <configuration>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.resourcemanager.hostname</name>
[36malgo-1    |[0m          <value>172.22.0.3</value>
[36malgo-1    |[0m          <description>The hostname of the RM.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.hostname</name>
[36malgo-1    |[0m          <value>algo-1</value>
[36malgo-1    |[0m          <description>The hostname of the NM.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.webapp.address</name>
[36malgo-1    |[0m          <value>algo-1:8042</value>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.vmem-pmem-ratio</name>
[36malgo-1    |[0m          <value>5</value>
[36malgo-1    |[0m          <description>Ratio between virtual memory to physical memory.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.resourcemanager.am.max-attempts</name>
[36malgo-1    |[0m          <value>1</value>
[36malgo-1    |[0m          <description>The maximum number of application attempts.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.env-whitelist</name>
[36malgo-1    |[0m          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>
[36malgo-1    |[0m          <description>Environment variable whitelist</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m  
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.minimum-allocation-mb</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.maximum-allocation-mb</name>
[36malgo-1    |[0m     <value>15892</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.minimum-allocation-vcores</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.maximum-allocation-vcores</name>
[36malgo-1    |[0m     <value>4</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.nodemanager.resource.memory-mb</name>
[36malgo-1    |[0m     <value>15892</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.nodemanager.resource.cpu-vcores</name>
[36malgo-1    |[0m     <value>4</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>key</name>
[36malgo-1    |[0m     <value>value</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 
[33malgo-2    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0"?>
[33malgo-2    |[0m <!-- Site specific YARN configuration properties -->
[33malgo-2    |[0m  <configuration>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.resourcemanager.hostname</name>
[33malgo-2    |[0m          <value>172.22.0.3</value>
[33malgo-2    |[0m          <description>The hostname of the RM.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.hostname</name>
[33malgo-2    |[0m          <value>algo-2</value>
[33malgo-2    |[0m          <description>The hostname of the NM.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.webapp.address</name>
[33malgo-2    |[0m          <value>algo-2:8042</value>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.vmem-pmem-ratio</name>
[33malgo-2    |[0m          <value>5</value>
[33malgo-2    |[0m          <description>Ratio between virtual memory to physical memory.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.resourcemanager.am.max-attempts</name>
[33malgo-2    |[0m          <value>1</value>
[33malgo-2    |[0m          <description>The maximum number of application attempts.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.env-whitelist</name>
[33malgo-2    |[0m          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>
[33malgo-2    |[0m          <description>Environment variable whitelist</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m  
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.minimum-allocation-mb</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.maximum-allocation-mb</name>
[33malgo-2    |[0m     <value>15892</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.minimum-allocation-vcores</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.maximum-allocation-vcores</name>
[33malgo-2    |[0m     <value>4</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.nodemanager.resource.memory-mb</name>
[33malgo-2    |[0m     <value>15892</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.nodemanager.resource.cpu-vcores</name>
[33malgo-2    |[0m     <value>4</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>key</name>
[33malgo-2    |[0m     <value>value</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/yarn-env.sh
[36malgo-1    |[0m 11-14 16:57 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/yarn-env.sh
[33malgo-2    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-env.sh is: 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one or more
[33malgo-2    |[0m # contributor license agreements.  See the NOTICE file distributed with
[33malgo-2    |[0m # this work for additional information regarding copyright ownership.
[33malgo-2    |[0m # The ASF licenses this file to You under the Apache License, Version 2.0
[33malgo-2    |[0m # (the "License"); you may not use this file except in compliance with
[33malgo-2    |[0m # the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## THIS FILE ACTS AS AN OVERRIDE FOR hadoop-env.sh FOR ALL
[33malgo-2    |[0m ## WORK DONE BY THE yarn AND RELATED COMMANDS.
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## Precedence rules:
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## yarn-env.sh > hadoop-env.sh > hard-coded defaults
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## YARN_xyz > HADOOP_xyz > hard-coded defaults
[33malgo-2    |[0m ##
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Resource Manager specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the max heapsize for the ResourceManager.  If no units are
[33malgo-2    |[0m # given, it will be assumed to be in MB.
[33malgo-2    |[0m # This value will be overridden by an Xmx setting specified in either
[33malgo-2    |[0m # HADOOP_OPTS and/or YARN_RESOURCEMANAGER_OPTS.
[33malgo-2    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX
[33malgo-2    |[0m #export YARN_RESOURCEMANAGER_HEAPSIZE=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the ResourceManager.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # Examples for a Sun/Oracle JDK:
[33malgo-2    |[0m # a) override the appsummary log file:
[33malgo-2    |[0m # export YARN_RESOURCEMANAGER_OPTS="-Dyarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log -Dyarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY"
[33malgo-2    |[0m #
[33malgo-2    |[0m # b) Set JMX options
[33malgo-2    |[0m # export YARN_RESOURCEMANAGER_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
[33malgo-2    |[0m #
[33malgo-2    |[0m # c) Set garbage collection logs from hadoop-env.sh
[33malgo-2    |[0m # export YARN_RESOURCE_MANAGER_OPTS="${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[33malgo-2    |[0m #
[33malgo-2    |[0m # d) ... or set them directly
[33malgo-2    |[0m # export YARN_RESOURCEMANAGER_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[33malgo-2    |[0m #
[33malgo-2    |[0m #
[33malgo-2    |[0m # export YARN_RESOURCEMANAGER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Node Manager specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the max heapsize for the NodeManager.  If no units are
[33malgo-2    |[0m # given, it will be assumed to be in MB.
[33malgo-2    |[0m # This value will be overridden by an Xmx setting specified in either
[33malgo-2    |[0m # HADOOP_OPTS and/or YARN_NODEMANAGER_OPTS.
[33malgo-2    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[33malgo-2    |[0m #export YARN_NODEMANAGER_HEAPSIZE=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the NodeManager.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_NODEMANAGER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # TimeLineServer specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the max heapsize for the timelineserver.  If no units are
[33malgo-2    |[0m # given, it will be assumed to be in MB.
[33malgo-2    |[0m # This value will be overridden by an Xmx setting specified in either
[33malgo-2    |[0m # HADOOP_OPTS and/or YARN_TIMELINESERVER_OPTS.
[33malgo-2    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[33malgo-2    |[0m #export YARN_TIMELINE_HEAPSIZE=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the TimeLineServer.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_TIMELINESERVER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # TimeLineReader specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the TimeLineReader.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_TIMELINEREADER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Web App Proxy Server specifc parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the max heapsize for the web app proxy server.  If no units are
[33malgo-2    |[0m # given, it will be assumed to be in MB.
[33malgo-2    |[0m # This value will be overridden by an Xmx setting specified in either
[33malgo-2    |[0m # HADOOP_OPTS and/or YARN_PROXYSERVER_OPTS.
[33malgo-2    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[33malgo-2    |[0m #export YARN_PROXYSERVER_HEAPSIZE=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the proxy server.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_PROXYSERVER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Shared Cache Manager specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the
[33malgo-2    |[0m # shared cache manager server.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_SHAREDCACHEMANAGER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Router specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the Router.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_ROUTER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Registry DNS specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # For privileged registry DNS, user to run as after dropping privileges
[33malgo-2    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[33malgo-2    |[0m # export YARN_REGISTRYDNS_SECURE_USER=yarn
[33malgo-2    |[0m 
[33malgo-2    |[0m # Supplemental options for privileged registry DNS
[33malgo-2    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[33malgo-2    |[0m # server jvm.
[33malgo-2    |[0m # export YARN_REGISTRYDNS_SECURE_EXTRA_OPTS="-jvm server"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # YARN Services parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Directory containing service examples
[33malgo-2    |[0m # export YARN_SERVICE_EXAMPLES_DIR = $HADOOP_YARN_HOME/share/hadoop/yarn/yarn-service-examples
[33malgo-2    |[0m # export YARN_CONTAINER_RUNTIME_DOCKER_RUN_OVERRIDE_DISABLE=true
[33malgo-2    |[0m export YARN_LOG_DIR=/var/log/yarn/export HADOOP_DATANODE_HEAPSIZE=2048
[33malgo-2    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[33malgo-2    |[0m 
[36malgo-1    |[0m 11-14 16:57 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-env.sh is: 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one or more
[36malgo-1    |[0m # contributor license agreements.  See the NOTICE file distributed with
[36malgo-1    |[0m # this work for additional information regarding copyright ownership.
[36malgo-1    |[0m # The ASF licenses this file to You under the Apache License, Version 2.0
[36malgo-1    |[0m # (the "License"); you may not use this file except in compliance with
[36malgo-1    |[0m # the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## THIS FILE ACTS AS AN OVERRIDE FOR hadoop-env.sh FOR ALL
[36malgo-1    |[0m ## WORK DONE BY THE yarn AND RELATED COMMANDS.
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## Precedence rules:
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## yarn-env.sh > hadoop-env.sh > hard-coded defaults
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## YARN_xyz > HADOOP_xyz > hard-coded defaults
[36malgo-1    |[0m ##
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Resource Manager specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the max heapsize for the ResourceManager.  If no units are
[36malgo-1    |[0m # given, it will be assumed to be in MB.
[36malgo-1    |[0m # This value will be overridden by an Xmx setting specified in either
[36malgo-1    |[0m # HADOOP_OPTS and/or YARN_RESOURCEMANAGER_OPTS.
[36malgo-1    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX
[36malgo-1    |[0m #export YARN_RESOURCEMANAGER_HEAPSIZE=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the ResourceManager.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # Examples for a Sun/Oracle JDK:
[36malgo-1    |[0m # a) override the appsummary log file:
[36malgo-1    |[0m # export YARN_RESOURCEMANAGER_OPTS="-Dyarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log -Dyarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY"
[36malgo-1    |[0m #
[36malgo-1    |[0m # b) Set JMX options
[36malgo-1    |[0m # export YARN_RESOURCEMANAGER_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
[36malgo-1    |[0m #
[36malgo-1    |[0m # c) Set garbage collection logs from hadoop-env.sh
[36malgo-1    |[0m # export YARN_RESOURCE_MANAGER_OPTS="${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[36malgo-1    |[0m #
[36malgo-1    |[0m # d) ... or set them directly
[36malgo-1    |[0m # export YARN_RESOURCEMANAGER_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[36malgo-1    |[0m #
[36malgo-1    |[0m #
[36malgo-1    |[0m # export YARN_RESOURCEMANAGER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Node Manager specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the max heapsize for the NodeManager.  If no units are
[36malgo-1    |[0m # given, it will be assumed to be in MB.
[36malgo-1    |[0m # This value will be overridden by an Xmx setting specified in either
[36malgo-1    |[0m # HADOOP_OPTS and/or YARN_NODEMANAGER_OPTS.
[36malgo-1    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[36malgo-1    |[0m #export YARN_NODEMANAGER_HEAPSIZE=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the NodeManager.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_NODEMANAGER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # TimeLineServer specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the max heapsize for the timelineserver.  If no units are
[36malgo-1    |[0m # given, it will be assumed to be in MB.
[36malgo-1    |[0m # This value will be overridden by an Xmx setting specified in either
[36malgo-1    |[0m # HADOOP_OPTS and/or YARN_TIMELINESERVER_OPTS.
[36malgo-1    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[36malgo-1    |[0m #export YARN_TIMELINE_HEAPSIZE=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the TimeLineServer.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_TIMELINESERVER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # TimeLineReader specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the TimeLineReader.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_TIMELINEREADER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Web App Proxy Server specifc parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the max heapsize for the web app proxy server.  If no units are
[36malgo-1    |[0m # given, it will be assumed to be in MB.
[36malgo-1    |[0m # This value will be overridden by an Xmx setting specified in either
[36malgo-1    |[0m # HADOOP_OPTS and/or YARN_PROXYSERVER_OPTS.
[36malgo-1    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[36malgo-1    |[0m #export YARN_PROXYSERVER_HEAPSIZE=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the proxy server.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_PROXYSERVER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Shared Cache Manager specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the
[36malgo-1    |[0m # shared cache manager server.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_SHAREDCACHEMANAGER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Router specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the Router.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_ROUTER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Registry DNS specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # For privileged registry DNS, user to run as after dropping privileges
[36malgo-1    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[36malgo-1    |[0m # export YARN_REGISTRYDNS_SECURE_USER=yarn
[36malgo-1    |[0m 
[36malgo-1    |[0m # Supplemental options for privileged registry DNS
[36malgo-1    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[36malgo-1    |[0m # server jvm.
[36malgo-1    |[0m # export YARN_REGISTRYDNS_SECURE_EXTRA_OPTS="-jvm server"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # YARN Services parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Directory containing service examples
[36malgo-1    |[0m # export YARN_SERVICE_EXAMPLES_DIR = $HADOOP_YARN_HOME/share/hadoop/yarn/yarn-service-examples
[36malgo-1    |[0m # export YARN_CONTAINER_RUNTIME_DOCKER_RUN_OVERRIDE_DISABLE=true
[36malgo-1    |[0m export YARN_LOG_DIR=/var/log/yarn/export HADOOP_DATANODE_HEAPSIZE=2048
[36malgo-1    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[36malgo-1    |[0m 
[33malgo-2    |[0m 11-14 16:57 smspark-submit INFO     waiting for cluster to be up
[33malgo-2    |[0m WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.
[36malgo-1    |[0m WARNING: HADOOP_NAMENODE_OPTS has been replaced by HDFS_NAMENODE_OPTS. Using value of HADOOP_NAMENODE_OPTS.
[36malgo-1    |[0m WARNING: /usr/lib/hadoop/logs does not exist. Creating.
[33malgo-2    |[0m WARNING: /var/log/yarn/export does not exist. Creating.
[33malgo-2    |[0m WARNING: /usr/lib/hadoop/logs does not exist. Creating.
[33malgo-2    |[0m 2021-11-14 16:57:15,359 INFO nodemanager.NodeManager: STARTUP_MSG: 
[33malgo-2    |[0m /************************************************************
[33malgo-2    |[0m STARTUP_MSG: Starting NodeManager
[33malgo-2    |[0m STARTUP_MSG:   host = algo-2/172.22.0.2
[33malgo-2    |[0m STARTUP_MSG:   args = []
[33malgo-2    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[33malgo-2    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar
[33malgo-2    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[33malgo-2    |[0m STARTUP_MSG:   java = 1.8.0_302
[33malgo-2    |[0m ************************************************************/
[33malgo-2    |[0m 2021-11-14 16:57:15,371 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 16:57:15,409 INFO namenode.NameNode: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting NameNode
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/172.22.0.3
[36malgo-1    |[0m STARTUP_MSG:   args = [-format, -force]
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 16:57:15,423 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
[33malgo-2    |[0m 2021-11-14 16:57:15,451 INFO datanode.DataNode: STARTUP_MSG: 
[33malgo-2    |[0m /************************************************************
[33malgo-2    |[0m STARTUP_MSG: Starting DataNode
[33malgo-2    |[0m STARTUP_MSG:   host = algo-2/172.22.0.2
[33malgo-2    |[0m STARTUP_MSG:   args = []
[33malgo-2    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[33malgo-2    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar
[33malgo-2    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[33malgo-2    |[0m STARTUP_MSG:   java = 1.8.0_302
[33malgo-2    |[0m ************************************************************/
[33malgo-2    |[0m 2021-11-14 16:57:15,460 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 16:57:15,495 INFO namenode.NameNode: createNameNode [-format, -force]
[33malgo-2    |[0m 2021-11-14 16:57:15,757 INFO resourceplugin.ResourcePluginManager: No Resource plugins found from configuration!
[33malgo-2    |[0m 2021-11-14 16:57:15,757 INFO resourceplugin.ResourcePluginManager: Found Resource plugins from configuration: null
[33malgo-2    |[0m 2021-11-14 16:57:15,797 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode
[33malgo-2    |[0m 2021-11-14 16:57:15,834 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.
[33malgo-2    |[0m 2021-11-14 16:57:15,881 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher
[33malgo-2    |[0m 2021-11-14 16:57:15,882 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher
[33malgo-2    |[0m 2021-11-14 16:57:15,883 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper
[33malgo-2    |[0m 2021-11-14 16:57:15,884 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices
[33malgo-2    |[0m 2021-11-14 16:57:15,884 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl
[33malgo-2    |[0m 2021-11-14 16:57:15,885 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher
[33malgo-2    |[0m 2021-11-14 16:57:15,885 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler
[33malgo-2    |[0m 2021-11-14 16:57:15,887 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[33malgo-2    |[0m 2021-11-14 16:57:15,887 INFO tracker.NMLogAggregationStatusTracker: the rolling interval seconds for the NodeManager Cached Log aggregation status is 600
[36malgo-1    |[0m Formatting using clusterid: CID-e9b272d1-7023-4183-af32-e1af31503e0e
[33malgo-2    |[0m 2021-11-14 16:57:15,906 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl
[33malgo-2    |[0m 2021-11-14 16:57:15,906 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager
[36malgo-1    |[0m 2021-11-14 16:57:15,932 INFO namenode.FSEditLog: Edit logging is async:true
[36malgo-1    |[0m 2021-11-14 16:57:15,947 INFO namenode.FSNamesystem: KeyProvider: null
[36malgo-1    |[0m 2021-11-14 16:57:15,948 INFO namenode.FSNamesystem: fsLock is fair: true
[36malgo-1    |[0m 2021-11-14 16:57:15,949 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
[36malgo-1    |[0m 2021-11-14 16:57:15,954 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 16:57:15,954 INFO namenode.FSNamesystem: supergroup          = supergroup
[36malgo-1    |[0m 2021-11-14 16:57:15,954 INFO namenode.FSNamesystem: isPermissionEnabled = true
[36malgo-1    |[0m 2021-11-14 16:57:15,954 INFO namenode.FSNamesystem: HA Enabled: false
[33malgo-2    |[0m 2021-11-14 16:57:15,959 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[33malgo-2    |[0m 2021-11-14 16:57:15,959 INFO impl.MetricsSystemImpl: DataNode metrics system started
[33malgo-2    |[0m 2021-11-14 16:57:15,961 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m 2021-11-14 16:57:16,000 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[36malgo-1    |[0m 2021-11-14 16:57:16,015 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
[36malgo-1    |[0m 2021-11-14 16:57:16,015 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
[36malgo-1    |[0m 2021-11-14 16:57:16,020 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
[36malgo-1    |[0m 2021-11-14 16:57:16,020 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Nov 14 16:57:16
[36malgo-1    |[0m 2021-11-14 16:57:16,022 INFO util.GSet: Computing capacity for map BlocksMap
[36malgo-1    |[0m 2021-11-14 16:57:16,022 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 16:57:16,023 INFO util.GSet: 2.0% max memory 6.5 GB = 133.6 MB
[36malgo-1    |[0m 2021-11-14 16:57:16,023 INFO util.GSet: capacity      = 2^24 = 16777216 entries
[33malgo-2    |[0m 2021-11-14 16:57:16,034 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[33malgo-2    |[0m 2021-11-14 16:57:16,034 INFO impl.MetricsSystemImpl: NodeManager metrics system started
[33malgo-2    |[0m 2021-11-14 16:57:16,052 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.
[33malgo-2    |[0m 2021-11-14 16:57:16,060 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.
[36malgo-1    |[0m 2021-11-14 16:57:16,066 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
[36malgo-1    |[0m 2021-11-14 16:57:16,066 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
[36malgo-1    |[0m 2021-11-14 16:57:16,074 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
[36malgo-1    |[0m 2021-11-14 16:57:16,074 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
[36malgo-1    |[0m 2021-11-14 16:57:16,074 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
[36malgo-1    |[0m 2021-11-14 16:57:16,074 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
[36malgo-1    |[0m 2021-11-14 16:57:16,075 INFO blockmanagement.BlockManager: defaultReplication         = 3
[36malgo-1    |[0m 2021-11-14 16:57:16,075 INFO blockmanagement.BlockManager: maxReplication             = 512
[36malgo-1    |[0m 2021-11-14 16:57:16,075 INFO blockmanagement.BlockManager: minReplication             = 1
[36malgo-1    |[0m 2021-11-14 16:57:16,075 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
[36malgo-1    |[0m 2021-11-14 16:57:16,075 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
[36malgo-1    |[0m 2021-11-14 16:57:16,075 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
[36malgo-1    |[0m 2021-11-14 16:57:16,075 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
[33malgo-2    |[0m 2021-11-14 16:57:16,091 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@4c60d6e9
[33malgo-2    |[0m 2021-11-14 16:57:16,092 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler
[33malgo-2    |[0m 2021-11-14 16:57:16,093 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService
[33malgo-2    |[0m 2021-11-14 16:57:16,094 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled
[33malgo-2    |[0m 2021-11-14 16:57:16,094 INFO localizer.ResourceLocalizationService: per directory file limit = 8192
[36malgo-1    |[0m 2021-11-14 16:57:16,102 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
[36malgo-1    |[0m 2021-11-14 16:57:16,102 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 16:57:16,102 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 16:57:16,102 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
[33malgo-2    |[0m 2021-11-14 16:57:16,112 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker
[36malgo-1    |[0m 2021-11-14 16:57:16,116 INFO util.GSet: Computing capacity for map INodeMap
[36malgo-1    |[0m 2021-11-14 16:57:16,116 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 16:57:16,117 INFO util.GSet: 1.0% max memory 6.5 GB = 66.8 MB
[36malgo-1    |[0m 2021-11-14 16:57:16,117 INFO util.GSet: capacity      = 2^23 = 8388608 entries
[33malgo-2    |[0m 2021-11-14 16:57:16,117 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[33malgo-2    |[0m 2021-11-14 16:57:16,118 INFO resources.ResourceHandlerModule: Using traffic control bandwidth handler
[33malgo-2    |[0m 2021-11-14 16:57:16,120 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
[33malgo-2    |[0m 2021-11-14 16:57:16,121 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@221a3fa4
[33malgo-2    |[0m 2021-11-14 16:57:16,122 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null
[33malgo-2    |[0m 2021-11-14 16:57:16,123 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true
[33malgo-2    |[0m 2021-11-14 16:57:16,123 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true
[33malgo-2    |[0m 2021-11-14 16:57:16,123 INFO monitor.ContainersMonitorImpl: Elastic memory control enabled: false
[33malgo-2    |[0m 2021-11-14 16:57:16,123 INFO monitor.ContainersMonitorImpl: Strict memory control enabled: true
[33malgo-2    |[0m 2021-11-14 16:57:16,123 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true
[33malgo-2    |[0m 2021-11-14 16:57:16,125 INFO datanode.DataNode: Configured hostname is algo-2
[33malgo-2    |[0m 2021-11-14 16:57:16,125 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.
[33malgo-2    |[0m 2021-11-14 16:57:16,125 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[33malgo-2    |[0m 2021-11-14 16:57:16,129 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0
[36malgo-1    |[0m 2021-11-14 16:57:16,135 INFO namenode.FSDirectory: ACLs enabled? false
[36malgo-1    |[0m 2021-11-14 16:57:16,136 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
[36malgo-1    |[0m 2021-11-14 16:57:16,136 INFO namenode.FSDirectory: XAttrs enabled? true
[36malgo-1    |[0m 2021-11-14 16:57:16,136 INFO namenode.NameNode: Caching file names occurring more than 10 times
[36malgo-1    |[0m 2021-11-14 16:57:16,142 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
[36malgo-1    |[0m 2021-11-14 16:57:16,144 INFO snapshot.SnapshotManager: SkipList is disabled
[36malgo-1    |[0m 2021-11-14 16:57:16,149 INFO util.GSet: Computing capacity for map cachedBlocks
[36malgo-1    |[0m 2021-11-14 16:57:16,149 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 16:57:16,149 INFO util.GSet: 0.25% max memory 6.5 GB = 16.7 MB
[36malgo-1    |[0m 2021-11-14 16:57:16,149 INFO util.GSet: capacity      = 2^21 = 2097152 entries
[33malgo-2    |[0m 2021-11-14 16:57:16,154 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866
[33malgo-2    |[0m 2021-11-14 16:57:16,154 INFO conf.Configuration: resource-types.xml not found
[33malgo-2    |[0m 2021-11-14 16:57:16,154 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
[33malgo-2    |[0m 2021-11-14 16:57:16,156 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s
[33malgo-2    |[0m 2021-11-14 16:57:16,156 INFO datanode.DataNode: Number threads for balancing is 50
[36malgo-1    |[0m 2021-11-14 16:57:16,157 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
[36malgo-1    |[0m 2021-11-14 16:57:16,157 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
[36malgo-1    |[0m 2021-11-14 16:57:16,157 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
[33malgo-2    |[0m 2021-11-14 16:57:16,160 INFO conf.Configuration: node-resources.xml not found
[33malgo-2    |[0m 2021-11-14 16:57:16,160 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.
[36malgo-1    |[0m 2021-11-14 16:57:16,160 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
[36malgo-1    |[0m 2021-11-14 16:57:16,161 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
[33malgo-2    |[0m 2021-11-14 16:57:16,162 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 16:57:16,162 INFO util.GSet: Computing capacity for map NameNodeRetryCache
[36malgo-1    |[0m 2021-11-14 16:57:16,162 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 16:57:16,162 INFO util.GSet: 0.029999999329447746% max memory 6.5 GB = 2.0 MB
[36malgo-1    |[0m 2021-11-14 16:57:16,163 INFO util.GSet: capacity      = 2^18 = 262144 entries
[33malgo-2    |[0m 2021-11-14 16:57:16,166 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=15892 virtual-memory=79460 virtual-cores=4
[36malgo-1    |[0m 2021-11-14 16:57:16,189 INFO namenode.FSImage: Allocated new BlockPoolId: BP-464411801-172.22.0.3-1636909036182
[33malgo-2    |[0m 2021-11-14 16:57:16,196 INFO util.log: Logging initialized @1264ms to org.eclipse.jetty.util.log.Slf4jLog
[33malgo-2    |[0m 2021-11-14 16:57:16,203 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 16:57:16,206 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.
[33malgo-2    |[0m 2021-11-14 16:57:16,219 INFO ipc.Server: Starting Socket Reader #1 for port 0
[36malgo-1    |[0m 2021-11-14 16:57:16,229 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression
[36malgo-1    |[0m 2021-11-14 16:57:16,321 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .
[36malgo-1    |[0m 2021-11-14 16:57:16,329 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
[33malgo-2    |[0m 2021-11-14 16:57:16,331 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 16:57:16,334 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
[36malgo-1    |[0m 2021-11-14 16:57:16,334 INFO namenode.NameNode: SHUTDOWN_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m SHUTDOWN_MSG: Shutting down NameNode at algo-1/172.22.0.3
[36malgo-1    |[0m ************************************************************/
[33malgo-2    |[0m 2021-11-14 16:57:16,337 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined
[33malgo-2    |[0m 2021-11-14 16:57:16,342 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[33malgo-2    |[0m 2021-11-14 16:57:16,343 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
[33malgo-2    |[0m 2021-11-14 16:57:16,344 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[33malgo-2    |[0m 2021-11-14 16:57:16,344 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[33malgo-2    |[0m 2021-11-14 16:57:16,369 INFO http.HttpServer2: Jetty bound to port 40515
[33malgo-2    |[0m 2021-11-14 16:57:16,370 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[36malgo-1    |[0m 11-14 16:57 smspark-submit INFO     waiting for cluster to be up
[33malgo-2    |[0m 2021-11-14 16:57:16,396 INFO server.session: DefaultSessionIdManager workerName=node0
[33malgo-2    |[0m 2021-11-14 16:57:16,396 INFO server.session: No SessionScavenger set, using defaults
[33malgo-2    |[0m 2021-11-14 16:57:16,397 INFO server.session: node0 Scavenging every 600000ms
[33malgo-2    |[0m 2021-11-14 16:57:16,407 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server
[33malgo-2    |[0m 2021-11-14 16:57:16,407 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a3793c7{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}
[33malgo-2    |[0m 2021-11-14 16:57:16,407 INFO ipc.Server: IPC Server Responder: starting
[33malgo-2    |[0m 2021-11-14 16:57:16,407 INFO ipc.Server: IPC Server listener on 0: starting
[33malgo-2    |[0m 2021-11-14 16:57:16,407 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@bb9e6dc{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}
[33malgo-2    |[0m 2021-11-14 16:57:16,415 INFO security.NMContainerTokenSecretManager: Updating node address : algo-2:37609
[33malgo-2    |[0m 2021-11-14 16:57:16,420 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 500, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[33malgo-2    |[0m 2021-11-14 16:57:16,421 INFO ipc.Server: Starting Socket Reader #1 for port 8040
[33malgo-2    |[0m 2021-11-14 16:57:16,424 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server
[33malgo-2    |[0m 2021-11-14 16:57:16,424 INFO ipc.Server: IPC Server Responder: starting
[33malgo-2    |[0m 2021-11-14 16:57:16,425 INFO ipc.Server: IPC Server listener on 8040: starting
[33malgo-2    |[0m 2021-11-14 16:57:16,427 INFO localizer.ResourceLocalizationService: Localizer started on port 8040
[33malgo-2    |[0m 2021-11-14 16:57:16,429 INFO containermanager.ContainerManagerImpl: ContainerManager started at /172.22.0.2:37609
[33malgo-2    |[0m 2021-11-14 16:57:16,429 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-2/172.22.0.2:0
[33malgo-2    |[0m 2021-11-14 16:57:16,430 WARN tracker.NMLogAggregationStatusTracker: Log Aggregation is disabled.So is the LogAggregationStatusTracker.
[33malgo-2    |[0m 2021-11-14 16:57:16,433 INFO webapp.WebServer: Instantiating NMWebApp at algo-2:8042
[36malgo-1    |[0m WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.
[36malgo-1    |[0m WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.
[36malgo-1    |[0m WARNING: HADOOP_NAMENODE_OPTS has been replaced by HDFS_NAMENODE_OPTS. Using value of HADOOP_NAMENODE_OPTS.
[33malgo-2    |[0m 2021-11-14 16:57:16,456 INFO util.log: Logging initialized @1531ms to org.eclipse.jetty.util.log.Slf4jLog
[33malgo-2    |[0m 2021-11-14 16:57:16,463 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m WARNING: /var/log/yarn/export does not exist. Creating.
[33malgo-2    |[0m 2021-11-14 16:57:16,472 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@338c99c8{datanode,/,file:///usr/lib/hadoop-hdfs/webapps/datanode/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/datanode}
[33malgo-2    |[0m 2021-11-14 16:57:16,479 INFO server.AbstractConnector: Started ServerConnector@6c372fe6{HTTP/1.1,[http/1.1]}{localhost:40515}
[33malgo-2    |[0m 2021-11-14 16:57:16,480 INFO server.Server: Started @1547ms
[33malgo-2    |[0m 2021-11-14 16:57:16,546 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[33malgo-2    |[0m 2021-11-14 16:57:16,549 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined
[33malgo-2    |[0m 2021-11-14 16:57:16,554 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[33malgo-2    |[0m 2021-11-14 16:57:16,555 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node
[33malgo-2    |[0m 2021-11-14 16:57:16,555 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[33malgo-2    |[0m 2021-11-14 16:57:16,555 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[33malgo-2    |[0m 2021-11-14 16:57:16,556 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context node
[33malgo-2    |[0m 2021-11-14 16:57:16,556 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
[33malgo-2    |[0m 2021-11-14 16:57:16,556 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
[33malgo-2    |[0m 2021-11-14 16:57:16,558 INFO http.HttpServer2: adding path spec: /node/*
[33malgo-2    |[0m 2021-11-14 16:57:16,558 INFO http.HttpServer2: adding path spec: /ws/*
[33malgo-2    |[0m 2021-11-14 16:57:16,628 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864
[33malgo-2    |[0m 2021-11-14 16:57:16,635 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[33malgo-2    |[0m 2021-11-14 16:57:16,636 INFO datanode.DataNode: dnUserName = root
[33malgo-2    |[0m 2021-11-14 16:57:16,636 INFO datanode.DataNode: supergroup = supergroup
[33malgo-2    |[0m 2021-11-14 16:57:16,680 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[33malgo-2    |[0m 2021-11-14 16:57:16,697 INFO ipc.Server: Starting Socket Reader #1 for port 9867
[33malgo-2    |[0m 2021-11-14 16:57:16,921 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:9867
[33malgo-2    |[0m 2021-11-14 16:57:16,922 INFO webapp.WebApps: Registered webapp guice modules
[33malgo-2    |[0m 2021-11-14 16:57:16,925 INFO http.HttpServer2: Jetty bound to port 8042
[33malgo-2    |[0m 2021-11-14 16:57:16,927 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[33malgo-2    |[0m 2021-11-14 16:57:16,940 INFO datanode.DataNode: Refresh request received for nameservices: null
[33malgo-2    |[0m 2021-11-14 16:57:16,952 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>
[33malgo-2    |[0m 2021-11-14 16:57:16,957 INFO server.session: DefaultSessionIdManager workerName=node0
[33malgo-2    |[0m 2021-11-14 16:57:16,957 INFO server.session: No SessionScavenger set, using defaults
[33malgo-2    |[0m 2021-11-14 16:57:16,958 INFO server.session: node0 Scavenging every 600000ms
[33malgo-2    |[0m 2021-11-14 16:57:16,962 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1.spark-network/172.22.0.3:8020 starting to offer service
[33malgo-2    |[0m 2021-11-14 16:57:16,968 INFO ipc.Server: IPC Server Responder: starting
[33malgo-2    |[0m 2021-11-14 16:57:16,968 INFO ipc.Server: IPC Server listener on 9867: starting
[33malgo-2    |[0m 2021-11-14 16:57:16,969 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[33malgo-2    |[0m 2021-11-14 16:57:16,972 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4bff64c2{logs,/logs,file:///var/log/yarn/export/,AVAILABLE}
[33malgo-2    |[0m 2021-11-14 16:57:16,973 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3cc41abc{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/static,AVAILABLE}
[33malgo-2    |[0m 2021-11-14 16:57:16,985 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException
[36malgo-1    |[0m 2021-11-14 16:57:17,034 INFO resourcemanager.ResourceManager: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting ResourceManager
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/172.22.0.3
[36malgo-1    |[0m STARTUP_MSG:   args = []
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 16:57:17,040 INFO namenode.NameNode: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting NameNode
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/172.22.0.3
[36malgo-1    |[0m STARTUP_MSG:   args = []
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 16:57:17,046 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 16:57:17,049 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 16:57:17,067 INFO datanode.DataNode: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting DataNode
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/172.22.0.3
[36malgo-1    |[0m STARTUP_MSG:   args = []
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 16:57:17,078 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 16:57:17,081 INFO nodemanager.NodeManager: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting NodeManager
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/172.22.0.3
[36malgo-1    |[0m STARTUP_MSG:   args = []
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 16:57:17,090 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]
[33malgo-2    |[0m 2021-11-14 16:57:17,096 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m 2021-11-14 16:57:17,141 INFO namenode.NameNode: createNameNode []
[33malgo-2    |[0m Nov 14, 2021 4:57:17 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[33malgo-2    |[0m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class
[33malgo-2    |[0m Nov 14, 2021 4:57:17 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[33malgo-2    |[0m INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
[33malgo-2    |[0m Nov 14, 2021 4:57:17 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[33malgo-2    |[0m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class
[33malgo-2    |[0m Nov 14, 2021 4:57:17 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
[33malgo-2    |[0m INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'
[33malgo-2    |[0m Nov 14, 2021 4:57:17 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[33malgo-2    |[0m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 16:57:17,331 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m 2021-11-14 16:57:17,456 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m 2021-11-14 16:57:17,457 INFO impl.MetricsSystemImpl: NameNode metrics system started
[36malgo-1    |[0m 2021-11-14 16:57:17,463 INFO conf.Configuration: found resource core-site.xml at file:/etc/hadoop/conf.empty/core-site.xml
[36malgo-1    |[0m 2021-11-14 16:57:17,484 INFO namenode.NameNodeUtils: fs.defaultFS is hdfs://172.22.0.3/
[33malgo-2    |[0m Nov 14, 2021 4:57:17 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[33malgo-2    |[0m INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 16:57:17,499 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode
[36malgo-1    |[0m 2021-11-14 16:57:17,506 INFO conf.Configuration: resource-types.xml not found
[36malgo-1    |[0m 2021-11-14 16:57:17,507 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
[36malgo-1    |[0m 2021-11-14 16:57:17,559 INFO conf.Configuration: found resource yarn-site.xml at file:/etc/hadoop/conf.empty/yarn-site.xml
[36malgo-1    |[0m 2021-11-14 16:57:17,576 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher
[36malgo-1    |[0m 2021-11-14 16:57:17,581 INFO resourceplugin.ResourcePluginManager: No Resource plugins found from configuration!
[36malgo-1    |[0m 2021-11-14 16:57:17,582 INFO resourceplugin.ResourcePluginManager: Found Resource plugins from configuration: null
[36malgo-1    |[0m 2021-11-14 16:57:17,612 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m 2021-11-14 16:57:17,620 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms
[36malgo-1    |[0m 2021-11-14 16:57:17,625 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms
[36malgo-1    |[0m 2021-11-14 16:57:17,630 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms
[36malgo-1    |[0m 2021-11-14 16:57:17,643 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 16:57:17,675 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:9870
[36malgo-1    |[0m 2021-11-14 16:57:17,680 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.
[36malgo-1    |[0m 2021-11-14 16:57:17,694 INFO util.log: Logging initialized @1224ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m 2021-11-14 16:57:17,697 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler
[36malgo-1    |[0m 2021-11-14 16:57:17,702 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager
[36malgo-1    |[0m 2021-11-14 16:57:17,702 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
[36malgo-1    |[0m 2021-11-14 16:57:17,731 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m 2021-11-14 16:57:17,731 INFO impl.MetricsSystemImpl: DataNode metrics system started
[36malgo-1    |[0m 2021-11-14 16:57:17,747 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher
[36malgo-1    |[0m 2021-11-14 16:57:17,749 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher
[36malgo-1    |[0m 2021-11-14 16:57:17,750 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper
[36malgo-1    |[0m 2021-11-14 16:57:17,750 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher
[36malgo-1    |[0m 2021-11-14 16:57:17,751 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices
[36malgo-1    |[0m 2021-11-14 16:57:17,752 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher
[36malgo-1    |[0m 2021-11-14 16:57:17,752 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl
[36malgo-1    |[0m 2021-11-14 16:57:17,753 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher
[36malgo-1    |[0m 2021-11-14 16:57:17,753 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher
[36malgo-1    |[0m 2021-11-14 16:57:17,753 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler
[36malgo-1    |[0m 2021-11-14 16:57:17,755 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher
[36malgo-1    |[0m 2021-11-14 16:57:17,757 INFO tracker.NMLogAggregationStatusTracker: the rolling interval seconds for the NodeManager Cached Log aggregation status is 600
[36malgo-1    |[0m 2021-11-14 16:57:17,782 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl
[36malgo-1    |[0m 2021-11-14 16:57:17,783 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager
[36malgo-1    |[0m 2021-11-14 16:57:17,841 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 16:57:17,844 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m 2021-11-14 16:57:17,859 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m 2021-11-14 16:57:17,860 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined
[33malgo-2    |[0m Nov 14, 2021 4:57:17 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[33malgo-2    |[0m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 16:57:17,876 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[36malgo-1    |[0m 2021-11-14 16:57:17,880 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
[36malgo-1    |[0m 2021-11-14 16:57:17,881 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[36malgo-1    |[0m 2021-11-14 16:57:17,881 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[33malgo-2    |[0m 2021-11-14 16:57:17,911 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5d5b5fa7{node,/,file:///tmp/jetty-algo-2-8042-_-any-208339415650820057.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/node}
[36malgo-1    |[0m 2021-11-14 16:57:17,920 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
[36malgo-1    |[0m 2021-11-14 16:57:17,921 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
[33malgo-2    |[0m 2021-11-14 16:57:17,933 INFO server.AbstractConnector: Started ServerConnector@3f390d63{HTTP/1.1,[http/1.1]}{algo-2:8042}
[33malgo-2    |[0m 2021-11-14 16:57:17,933 INFO server.Server: Started @3009ms
[36malgo-1    |[0m 2021-11-14 16:57:17,933 INFO http.HttpServer2: Jetty bound to port 9870
[33malgo-2    |[0m 2021-11-14 16:57:17,934 INFO webapp.WebApps: Web app node started at 8042
[36malgo-1    |[0m 2021-11-14 16:57:17,935 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[33malgo-2    |[0m 2021-11-14 16:57:17,944 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-2:37609
[33malgo-2    |[0m 2021-11-14 16:57:17,946 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[33malgo-2    |[0m 2021-11-14 16:57:17,954 INFO client.RMProxy: Connecting to ResourceManager at /172.22.0.3:8031
[36malgo-1    |[0m 2021-11-14 16:57:17,956 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m 2021-11-14 16:57:17,957 INFO impl.MetricsSystemImpl: ResourceManager metrics system started
[36malgo-1    |[0m 2021-11-14 16:57:17,993 INFO server.session: DefaultSessionIdManager workerName=node0
[36malgo-1    |[0m 2021-11-14 16:57:17,994 INFO server.session: No SessionScavenger set, using defaults
[36malgo-1    |[0m 2021-11-14 16:57:17,995 INFO server.session: node0 Scavenging every 660000ms
[36malgo-1    |[0m 2021-11-14 16:57:17,998 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m 2021-11-14 16:57:17,999 INFO impl.MetricsSystemImpl: NodeManager metrics system started
[36malgo-1    |[0m 2021-11-14 16:57:18,007 INFO security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.
[33malgo-2    |[0m 2021-11-14 16:57:18,007 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []
[36malgo-1    |[0m 2021-11-14 16:57:18,010 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6d60fe40{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 16:57:18,011 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager
[36malgo-1    |[0m 2021-11-14 16:57:18,011 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@73eb439a{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}
[33malgo-2    |[0m 2021-11-14 16:57:18,019 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]
[36malgo-1    |[0m 2021-11-14 16:57:18,020 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher
[36malgo-1    |[0m 2021-11-14 16:57:18,026 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.
[36malgo-1    |[0m 2021-11-14 16:57:18,028 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[36malgo-1    |[0m 2021-11-14 16:57:18,029 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean
[36malgo-1    |[0m 2021-11-14 16:57:18,029 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.
[36malgo-1    |[0m 2021-11-14 16:57:18,032 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
[36malgo-1    |[0m 2021-11-14 16:57:18,038 INFO placement.MultiNodeSortingManager: Initializing NodeSortingService=MultiNodeSortingManager
[36malgo-1    |[0m 2021-11-14 16:57:18,038 INFO datanode.DataNode: Configured hostname is algo-1
[36malgo-1    |[0m 2021-11-14 16:57:18,039 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[36malgo-1    |[0m 2021-11-14 16:57:18,040 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list
[36malgo-1    |[0m 2021-11-14 16:57:18,045 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0
[36malgo-1    |[0m 2021-11-14 16:57:18,048 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.
[36malgo-1    |[0m 2021-11-14 16:57:18,051 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/etc/hadoop/conf.empty/capacity-scheduler.xml
[33malgo-2    |[0m 2021-11-14 16:57:18,052 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/172.22.0.3:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 16:57:18,066 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1, vCores:1>
[36malgo-1    |[0m 2021-11-14 16:57:18,066 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 16:57:18,072 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866
[36malgo-1    |[0m 2021-11-14 16:57:18,076 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s
[36malgo-1    |[0m 2021-11-14 16:57:18,076 INFO datanode.DataNode: Number threads for balancing is 50
[36malgo-1    |[0m 2021-11-14 16:57:18,088 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m 2021-11-14 16:57:18,099 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@4c60d6e9
[36malgo-1    |[0m 2021-11-14 16:57:18,102 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler
[36malgo-1    |[0m 2021-11-14 16:57:18,103 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4dd6fd0a{hdfs,/,file:///usr/lib/hadoop-hdfs/webapps/hdfs/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/hdfs}
[36malgo-1    |[0m 2021-11-14 16:57:18,103 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService
[36malgo-1    |[0m 2021-11-14 16:57:18,104 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled
[36malgo-1    |[0m 2021-11-14 16:57:18,104 INFO localizer.ResourceLocalizationService: per directory file limit = 8192
[36malgo-1    |[0m 2021-11-14 16:57:18,110 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined
[36malgo-1    |[0m 2021-11-14 16:57:18,111 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined
[36malgo-1    |[0m 2021-11-14 16:57:18,123 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,
[36malgo-1    |[0m , reservationsContinueLooking=true, orderingPolicy=utilization, priority=0
[36malgo-1    |[0m 2021-11-14 16:57:18,124 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root
[36malgo-1    |[0m 2021-11-14 16:57:18,126 INFO util.log: Logging initialized @1660ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m 2021-11-14 16:57:18,127 INFO server.AbstractConnector: Started ServerConnector@4d14b6c2{HTTP/1.1,[http/1.1]}{0.0.0.0:9870}
[36malgo-1    |[0m 2021-11-14 16:57:18,127 INFO server.Server: Started @1657ms
[36malgo-1    |[0m 2021-11-14 16:57:18,131 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker
[36malgo-1    |[0m 2021-11-14 16:57:18,138 INFO resources.ResourceHandlerModule: Using traffic control bandwidth handler
[36malgo-1    |[0m 2021-11-14 16:57:18,140 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined
[36malgo-1    |[0m 2021-11-14 16:57:18,140 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined
[36malgo-1    |[0m 2021-11-14 16:57:18,142 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@221a3fa4
[36malgo-1    |[0m 2021-11-14 16:57:18,143 INFO capacity.LeafQueue: Initializing default
[36malgo-1    |[0m capacity = 1.0 [= (float) configuredCapacity / 100 ]
[36malgo-1    |[0m absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]
[36malgo-1    |[0m maxCapacity = 1.0 [= configuredMaxCapacity ]
[36malgo-1    |[0m absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]
[36malgo-1    |[0m effectiveMinResource=<memory:0, vCores:0>
[36malgo-1    |[0m  , effectiveMaxResource=<memory:0, vCores:0>
[36malgo-1    |[0m userLimit = 100 [= configuredUserLimit ]
[36malgo-1    |[0m userLimitFactor = 1.0 [= configuredUserLimitFactor ]
[36malgo-1    |[0m maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]
[36malgo-1    |[0m maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]
[36malgo-1    |[0m usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]
[36malgo-1    |[0m absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]
[36malgo-1    |[0m maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]
[36malgo-1    |[0m minimumAllocationFactor = 0.99993706 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]
[36malgo-1    |[0m maximumAllocation = <memory:15892, vCores:4> [= configuredMaxAllocation ]
[36malgo-1    |[0m numContainers = 0 [= currentNumContainers ]
[36malgo-1    |[0m state = RUNNING [= configuredState ]
[36malgo-1    |[0m acls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= configuredAcls ]
[36malgo-1    |[0m nodeLocalityDelay = 40
[36malgo-1    |[0m rackLocalityAdditionalDelay = -1
[36malgo-1    |[0m labels=*,
[36malgo-1    |[0m reservationsContinueLooking = true
[36malgo-1    |[0m preemptionDisabled = true
[36malgo-1    |[0m defaultAppPriorityPerQueue = 0
[36malgo-1    |[0m priority = 0
[36malgo-1    |[0m maxLifetime = -1 seconds
[36malgo-1    |[0m defaultLifetime = -1 seconds
[36malgo-1    |[0m 2021-11-14 16:57:18,143 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null
[36malgo-1    |[0m 2021-11-14 16:57:18,143 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>
[36malgo-1    |[0m 2021-11-14 16:57:18,143 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
[36malgo-1    |[0m 2021-11-14 16:57:18,144 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true
[36malgo-1    |[0m 2021-11-14 16:57:18,144 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true
[36malgo-1    |[0m 2021-11-14 16:57:18,144 INFO monitor.ContainersMonitorImpl: Elastic memory control enabled: false
[36malgo-1    |[0m 2021-11-14 16:57:18,144 INFO monitor.ContainersMonitorImpl: Strict memory control enabled: true
[36malgo-1    |[0m 2021-11-14 16:57:18,145 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true
[36malgo-1    |[0m 2021-11-14 16:57:18,145 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
[36malgo-1    |[0m 2021-11-14 16:57:18,146 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false
[36malgo-1    |[0m 2021-11-14 16:57:18,147 INFO placement.MultiNodeSortingManager: MultiNode scheduling is 'false', and configured policies are 
[36malgo-1    |[0m 2021-11-14 16:57:18,147 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1, vCores:1>>, maximumAllocation=<<memory:15892, vCores:4>>, asynchronousScheduling=false, asyncScheduleInterval=5ms,multiNodePlacementEnabled=false
[36malgo-1    |[0m 2021-11-14 16:57:18,148 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.
[36malgo-1    |[0m 2021-11-14 16:57:18,150 INFO conf.Configuration: dynamic-resources.xml not found
[36malgo-1    |[0m 2021-11-14 16:57:18,152 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].
[36malgo-1    |[0m 2021-11-14 16:57:18,152 INFO resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.
[36malgo-1    |[0m 2021-11-14 16:57:18,154 INFO resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. 
[36malgo-1    |[0m 2021-11-14 16:57:18,160 INFO resourcemanager.ResourceManager: TimelineServicePublisher is not configured
[36malgo-1    |[0m 2021-11-14 16:57:18,193 INFO conf.Configuration: resource-types.xml not found
[36malgo-1    |[0m 2021-11-14 16:57:18,194 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
[36malgo-1    |[0m 2021-11-14 16:57:18,201 INFO conf.Configuration: node-resources.xml not found
[36malgo-1    |[0m 2021-11-14 16:57:18,201 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.
[36malgo-1    |[0m 2021-11-14 16:57:18,203 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 16:57:18,208 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=15892 virtual-memory=79460 virtual-cores=4
[36malgo-1    |[0m 2021-11-14 16:57:18,219 INFO util.log: Logging initialized @1747ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m 2021-11-14 16:57:18,273 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[33malgo-2    |[0m 11-14 16:57 smspark-submit INFO     cluster is up
[33malgo-2    |[0m 11-14 16:57 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING
[33malgo-2    |[0m 11-14 16:57 smspark-submit INFO     starting executor logs watcher
[33malgo-2    |[0m 11-14 16:57 smspark-submit INFO     waiting for the primary to come up
[33malgo-2    |[0m Starting executor logs watcher on log_dir: /var/log/yarn
[33malgo-2    |[0m 11-14 16:57 smspark-submit INFO     waiting for the primary to go down
[36malgo-1    |[0m 2021-11-14 16:57:18,296 INFO ipc.Server: Starting Socket Reader #1 for port 0
[36malgo-1    |[0m 2021-11-14 16:57:18,337 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 16:57:18,340 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 16:57:18,345 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined
[36malgo-1    |[0m 2021-11-14 16:57:18,345 INFO http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined
[36malgo-1    |[0m 2021-11-14 16:57:18,351 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[36malgo-1    |[0m 2021-11-14 16:57:18,352 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[36malgo-1    |[0m 2021-11-14 16:57:18,354 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
[36malgo-1    |[0m 2021-11-14 16:57:18,354 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[36malgo-1    |[0m 2021-11-14 16:57:18,354 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 2021-11-14 16:57:18,355 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster
[36malgo-1    |[0m 2021-11-14 16:57:18,356 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs
[36malgo-1    |[0m 2021-11-14 16:57:18,356 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static
[36malgo-1    |[0m 2021-11-14 16:57:18,356 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster
[36malgo-1    |[0m 2021-11-14 16:57:18,356 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[36malgo-1    |[0m 2021-11-14 16:57:18,356 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 2021-11-14 16:57:18,358 INFO http.HttpServer2: adding path spec: /cluster/*
[36malgo-1    |[0m 2021-11-14 16:57:18,358 INFO http.HttpServer2: adding path spec: /ws/*
[36malgo-1    |[0m 2021-11-14 16:57:18,358 INFO http.HttpServer2: adding path spec: /app/*
[36malgo-1    |[0m 2021-11-14 16:57:18,391 INFO http.HttpServer2: Jetty bound to port 36515
[36malgo-1    |[0m 2021-11-14 16:57:18,392 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[36malgo-1    |[0m 2021-11-14 16:57:18,406 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
[36malgo-1    |[0m 2021-11-14 16:57:18,407 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
[36malgo-1    |[0m 2021-11-14 16:57:18,425 INFO server.session: DefaultSessionIdManager workerName=node0
[36malgo-1    |[0m 2021-11-14 16:57:18,426 INFO server.session: No SessionScavenger set, using defaults
[36malgo-1    |[0m 2021-11-14 16:57:18,428 INFO server.session: node0 Scavenging every 660000ms
[36malgo-1    |[0m 2021-11-14 16:57:18,442 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a3793c7{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 16:57:18,443 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@bb9e6dc{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 16:57:18,467 INFO namenode.FSEditLog: Edit logging is async:true
[36malgo-1    |[0m 2021-11-14 16:57:18,483 INFO namenode.FSNamesystem: KeyProvider: null
[36malgo-1    |[0m 2021-11-14 16:57:18,485 INFO namenode.FSNamesystem: fsLock is fair: true
[36malgo-1    |[0m 2021-11-14 16:57:18,485 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
[36malgo-1    |[0m 2021-11-14 16:57:18,492 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 16:57:18,492 INFO namenode.FSNamesystem: supergroup          = supergroup
[36malgo-1    |[0m 2021-11-14 16:57:18,492 INFO namenode.FSNamesystem: isPermissionEnabled = true
[36malgo-1    |[0m 2021-11-14 16:57:18,492 INFO namenode.FSNamesystem: HA Enabled: false
[36malgo-1    |[0m 2021-11-14 16:57:18,516 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 16:57:18,517 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 16:57:18,517 INFO ipc.Server: IPC Server listener on 0: starting
[36malgo-1    |[0m 2021-11-14 16:57:18,521 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m 2021-11-14 16:57:18,524 INFO security.NMContainerTokenSecretManager: Updating node address : algo-1:35169
[36malgo-1    |[0m 2021-11-14 16:57:18,531 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 500, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 16:57:18,531 INFO ipc.Server: Starting Socket Reader #1 for port 8040
[36malgo-1    |[0m 2021-11-14 16:57:18,534 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 16:57:18,534 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@338c99c8{datanode,/,file:///usr/lib/hadoop-hdfs/webapps/datanode/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/datanode}
[36malgo-1    |[0m 2021-11-14 16:57:18,535 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 16:57:18,535 INFO ipc.Server: IPC Server listener on 8040: starting
[36malgo-1    |[0m 2021-11-14 16:57:18,535 INFO localizer.ResourceLocalizationService: Localizer started on port 8040
[36malgo-1    |[0m 2021-11-14 16:57:18,537 INFO containermanager.ContainerManagerImpl: ContainerManager started at /172.22.0.3:35169
[36malgo-1    |[0m 2021-11-14 16:57:18,537 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-1/172.22.0.3:0
[36malgo-1    |[0m 2021-11-14 16:57:18,538 WARN tracker.NMLogAggregationStatusTracker: Log Aggregation is disabled.So is the LogAggregationStatusTracker.
[36malgo-1    |[0m 2021-11-14 16:57:18,538 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[36malgo-1    |[0m 2021-11-14 16:57:18,541 INFO webapp.WebServer: Instantiating NMWebApp at algo-1:8042
[36malgo-1    |[0m 2021-11-14 16:57:18,544 INFO server.AbstractConnector: Started ServerConnector@6c372fe6{HTTP/1.1,[http/1.1]}{localhost:36515}
[36malgo-1    |[0m 2021-11-14 16:57:18,544 INFO server.Server: Started @2078ms
[36malgo-1    |[0m 2021-11-14 16:57:18,549 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
[36malgo-1    |[0m 2021-11-14 16:57:18,550 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
[36malgo-1    |[0m 2021-11-14 16:57:18,555 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
[36malgo-1    |[0m 2021-11-14 16:57:18,555 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Nov 14 16:57:18
[36malgo-1    |[0m 2021-11-14 16:57:18,557 INFO util.GSet: Computing capacity for map BlocksMap
[36malgo-1    |[0m 2021-11-14 16:57:18,557 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 16:57:18,559 INFO util.GSet: 2.0% max memory 6.5 GB = 133.6 MB
[36malgo-1    |[0m 2021-11-14 16:57:18,559 INFO util.GSet: capacity      = 2^24 = 16777216 entries
[36malgo-1    |[0m 2021-11-14 16:57:18,564 INFO util.log: Logging initialized @2090ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m 2021-11-14 16:57:18,582 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
[36malgo-1    |[0m 2021-11-14 16:57:18,582 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
[36malgo-1    |[0m 2021-11-14 16:57:18,588 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
[36malgo-1    |[0m 2021-11-14 16:57:18,588 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
[36malgo-1    |[0m 2021-11-14 16:57:18,588 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
[36malgo-1    |[0m 2021-11-14 16:57:18,588 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
[36malgo-1    |[0m 2021-11-14 16:57:18,589 INFO blockmanagement.BlockManager: defaultReplication         = 3
[36malgo-1    |[0m 2021-11-14 16:57:18,589 INFO blockmanagement.BlockManager: maxReplication             = 512
[36malgo-1    |[0m 2021-11-14 16:57:18,589 INFO blockmanagement.BlockManager: minReplication             = 1
[36malgo-1    |[0m 2021-11-14 16:57:18,589 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
[36malgo-1    |[0m 2021-11-14 16:57:18,589 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
[36malgo-1    |[0m 2021-11-14 16:57:18,589 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
[36malgo-1    |[0m 2021-11-14 16:57:18,589 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
[36malgo-1    |[0m 2021-11-14 16:57:18,613 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
[36malgo-1    |[0m 2021-11-14 16:57:18,613 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 16:57:18,613 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 16:57:18,613 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 16:57:18,626 INFO util.GSet: Computing capacity for map INodeMap
[36malgo-1    |[0m 2021-11-14 16:57:18,626 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 16:57:18,626 INFO util.GSet: 1.0% max memory 6.5 GB = 66.8 MB
[36malgo-1    |[0m 2021-11-14 16:57:18,627 INFO util.GSet: capacity      = 2^23 = 8388608 entries
[36malgo-1    |[0m 2021-11-14 16:57:18,663 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 16:57:18,667 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined
[36malgo-1    |[0m 2021-11-14 16:57:18,675 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[36malgo-1    |[0m 2021-11-14 16:57:18,677 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node
[36malgo-1    |[0m 2021-11-14 16:57:18,677 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 2021-11-14 16:57:18,677 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[36malgo-1    |[0m 2021-11-14 16:57:18,677 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context node
[36malgo-1    |[0m 2021-11-14 16:57:18,677 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
[36malgo-1    |[0m 2021-11-14 16:57:18,678 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
[36malgo-1    |[0m 2021-11-14 16:57:18,679 INFO http.HttpServer2: adding path spec: /node/*
[36malgo-1    |[0m 2021-11-14 16:57:18,680 INFO http.HttpServer2: adding path spec: /ws/*
[36malgo-1    |[0m 2021-11-14 16:57:18,752 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864
[36malgo-1    |[0m 2021-11-14 16:57:18,763 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 16:57:18,764 INFO datanode.DataNode: dnUserName = root
[36malgo-1    |[0m 2021-11-14 16:57:18,764 INFO datanode.DataNode: supergroup = supergroup
[36malgo-1    |[0m 2021-11-14 16:57:18,769 INFO namenode.FSDirectory: ACLs enabled? false
[36malgo-1    |[0m 2021-11-14 16:57:18,769 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
[36malgo-1    |[0m 2021-11-14 16:57:18,770 INFO namenode.FSDirectory: XAttrs enabled? true
[36malgo-1    |[0m 2021-11-14 16:57:18,770 INFO namenode.NameNode: Caching file names occurring more than 10 times
[36malgo-1    |[0m 2021-11-14 16:57:18,774 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
[36malgo-1    |[0m 2021-11-14 16:57:18,776 INFO snapshot.SnapshotManager: SkipList is disabled
[36malgo-1    |[0m 2021-11-14 16:57:18,781 INFO util.GSet: Computing capacity for map cachedBlocks
[36malgo-1    |[0m 2021-11-14 16:57:18,781 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 16:57:18,781 INFO util.GSet: 0.25% max memory 6.5 GB = 16.7 MB
[36malgo-1    |[0m 2021-11-14 16:57:18,781 INFO util.GSet: capacity      = 2^21 = 2097152 entries
[36malgo-1    |[0m 2021-11-14 16:57:18,786 INFO webapp.WebApps: Registered webapp guice modules
[36malgo-1    |[0m 2021-11-14 16:57:18,790 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
[36malgo-1    |[0m 2021-11-14 16:57:18,790 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
[36malgo-1    |[0m 2021-11-14 16:57:18,790 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
[36malgo-1    |[0m 2021-11-14 16:57:18,794 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
[36malgo-1    |[0m 2021-11-14 16:57:18,794 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
[36malgo-1    |[0m 2021-11-14 16:57:18,794 INFO http.HttpServer2: Jetty bound to port 8088
[36malgo-1    |[0m 2021-11-14 16:57:18,796 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[36malgo-1    |[0m 2021-11-14 16:57:18,796 INFO util.GSet: Computing capacity for map NameNodeRetryCache
[36malgo-1    |[0m 2021-11-14 16:57:18,796 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 16:57:18,797 INFO util.GSet: 0.029999999329447746% max memory 6.5 GB = 2.0 MB
[36malgo-1    |[0m 2021-11-14 16:57:18,797 INFO util.GSet: capacity      = 2^18 = 262144 entries
[36malgo-1    |[0m 2021-11-14 16:57:18,800 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 16:57:18,812 INFO ipc.Server: Starting Socket Reader #1 for port 9867
[36malgo-1    |[0m 2021-11-14 16:57:18,815 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/namenode/in_use.lock acquired by nodename 134@algo-1
[36malgo-1    |[0m 2021-11-14 16:57:18,822 INFO server.session: DefaultSessionIdManager workerName=node0
[36malgo-1    |[0m 2021-11-14 16:57:18,822 INFO server.session: No SessionScavenger set, using defaults
[36malgo-1    |[0m 2021-11-14 16:57:18,823 INFO server.session: node0 Scavenging every 660000ms
[36malgo-1    |[0m 2021-11-14 16:57:18,834 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 16:57:18,837 INFO namenode.FileJournalManager: Recovering unfinalized segments in /opt/amazon/hadoop/hdfs/namenode/current
[36malgo-1    |[0m 2021-11-14 16:57:18,837 INFO namenode.FSImage: No edit log streams selected.
[36malgo-1    |[0m 2021-11-14 16:57:18,838 INFO namenode.FSImage: Planning to load image: FSImageFile(file=/opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
[36malgo-1    |[0m 2021-11-14 16:57:18,841 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
[36malgo-1    |[0m 2021-11-14 16:57:18,843 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
[36malgo-1    |[0m 2021-11-14 16:57:18,843 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
[36malgo-1    |[0m 2021-11-14 16:57:18,863 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@352c1b98{logs,/logs,file:///var/log/yarn/export/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 16:57:18,863 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d898981{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/static,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 16:57:18,873 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException
[36malgo-1    |[0m 2021-11-14 16:57:18,897 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.
[36malgo-1    |[0m 2021-11-14 16:57:18,923 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
[36malgo-1    |[0m 2021-11-14 16:57:18,923 INFO namenode.FSImage: Loaded image for txid 0 from /opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000
[36malgo-1    |[0m 2021-11-14 16:57:18,928 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
[36malgo-1    |[0m 2021-11-14 16:57:18,928 INFO namenode.FSEditLog: Starting log segment at 1
[36malgo-1    |[0m 2021-11-14 16:57:18,980 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m 2021-11-14 16:57:19,005 INFO webapp.WebApps: Registered webapp guice modules
[36malgo-1    |[0m 2021-11-14 16:57:19,008 INFO http.HttpServer2: Jetty bound to port 8042
[36malgo-1    |[0m 2021-11-14 16:57:19,009 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[36malgo-1    |[0m 2021-11-14 16:57:19,026 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:9867
[36malgo-1    |[0m 2021-11-14 16:57:19,033 INFO namenode.NameCache: initialized with 0 entries 0 lookups
[36malgo-1    |[0m 2021-11-14 16:57:19,033 INFO namenode.FSNamesystem: Finished loading FSImage in 233 msecs
[36malgo-1    |[0m 2021-11-14 16:57:19,034 INFO server.session: DefaultSessionIdManager workerName=node0
[36malgo-1    |[0m 2021-11-14 16:57:19,034 INFO server.session: No SessionScavenger set, using defaults
[36malgo-1    |[0m 2021-11-14 16:57:19,036 INFO server.session: node0 Scavenging every 600000ms
[36malgo-1    |[0m 2021-11-14 16:57:19,045 INFO datanode.DataNode: Refresh request received for nameservices: null
[36malgo-1    |[0m 2021-11-14 16:57:19,045 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 16:57:19,047 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4bff64c2{logs,/logs,file:///var/log/yarn/export/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 16:57:19,048 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3cc41abc{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/static,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 16:57:19,053 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>
[33malgo-2    |[0m 2021-11-14 16:57:19,053 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/172.22.0.3:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 16:57:19,058 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException
[36malgo-1    |[0m 2021-11-14 16:57:19,062 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1/172.22.0.3:8020 starting to offer service
[33malgo-2    |[0m 2021-11-14 16:57:19,062 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/172.22.0.3:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m Nov 14, 2021 4:57:19 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class
[36malgo-1    |[0m Nov 14, 2021 4:57:19 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class
[36malgo-1    |[0m Nov 14, 2021 4:57:19 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
[36malgo-1    |[0m Nov 14, 2021 4:57:19 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
[36malgo-1    |[0m INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'
[36malgo-1    |[0m 2021-11-14 16:57:19,067 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 16:57:19,067 INFO ipc.Server: IPC Server listener on 9867: starting
[36malgo-1    |[0m Nov 14, 2021 4:57:19 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 16:57:19,143 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m Nov 14, 2021 4:57:19 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class
[36malgo-1    |[0m Nov 14, 2021 4:57:19 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
[36malgo-1    |[0m Nov 14, 2021 4:57:19 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class
[36malgo-1    |[0m Nov 14, 2021 4:57:19 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
[36malgo-1    |[0m INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'
[36malgo-1    |[0m Nov 14, 2021 4:57:19 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 16:57:19,367 INFO namenode.NameNode: RPC server is binding to algo-1:8020
[36malgo-1    |[0m 2021-11-14 16:57:19,397 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 16:57:19,406 INFO ipc.Server: Starting Socket Reader #1 for port 8020
[36malgo-1    |[0m Nov 14, 2021 4:57:19 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m Nov 14, 2021 4:57:19 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 16:57:19,579 INFO namenode.NameNode: Clients are to use algo-1:8020 to access this namenode/service.
[36malgo-1    |[0m 2021-11-14 16:57:19,582 INFO namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
[36malgo-1    |[0m 2021-11-14 16:57:19,592 INFO namenode.LeaseManager: Number of blocks under construction: 0
[36malgo-1    |[0m 2021-11-14 16:57:19,611 INFO blockmanagement.BlockManager: initializing replication queues
[36malgo-1    |[0m 2021-11-14 16:57:19,622 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs
[36malgo-1    |[0m 2021-11-14 16:57:19,622 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
[36malgo-1    |[0m 2021-11-14 16:57:19,622 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
[36malgo-1    |[0m 2021-11-14 16:57:19,647 INFO blockmanagement.BlockManager: Total number of blocks            = 0
[36malgo-1    |[0m 2021-11-14 16:57:19,647 INFO blockmanagement.BlockManager: Number of invalid blocks          = 0
[36malgo-1    |[0m 2021-11-14 16:57:19,647 INFO blockmanagement.BlockManager: Number of under-replicated blocks = 0
[36malgo-1    |[0m 2021-11-14 16:57:19,647 INFO blockmanagement.BlockManager: Number of  over-replicated blocks = 0
[36malgo-1    |[0m 2021-11-14 16:57:19,647 INFO blockmanagement.BlockManager: Number of blocks being written    = 0
[36malgo-1    |[0m 2021-11-14 16:57:19,647 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 25 msec
[36malgo-1    |[0m 2021-11-14 16:57:19,651 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 16:57:19,651 INFO ipc.Server: IPC Server listener on 8020: starting
[36malgo-1    |[0m 2021-11-14 16:57:19,677 INFO namenode.NameNode: NameNode RPC up at: algo-1/172.22.0.3:8020
[36malgo-1    |[0m 2021-11-14 16:57:19,683 INFO namenode.FSNamesystem: Starting services required for active state
[36malgo-1    |[0m 2021-11-14 16:57:19,683 INFO namenode.FSDirectory: Initializing quota with 4 thread(s)
[36malgo-1    |[0m 2021-11-14 16:57:19,696 INFO namenode.FSDirectory: Quota initialization completed in 13 milliseconds
[36malgo-1    |[0m name space=1
[36malgo-1    |[0m storage space=0
[36malgo-1    |[0m storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
[36malgo-1    |[0m 2021-11-14 16:57:19,705 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
[36malgo-1    |[0m Nov 14, 2021 4:57:19 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 16:57:20,019 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@499683c4{cluster,/,file:///tmp/jetty-172_22_0_3-8088-_-any-6759503629791010014.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/cluster}
[36malgo-1    |[0m 2021-11-14 16:57:20,033 INFO server.AbstractConnector: Started ServerConnector@51549490{HTTP/1.1,[http/1.1]}{172.22.0.3:8088}
[36malgo-1    |[0m 2021-11-14 16:57:20,033 INFO server.Server: Started @3562ms
[36malgo-1    |[0m 2021-11-14 16:57:20,033 INFO webapp.WebApps: Web app cluster started at 8088
[33malgo-2    |[0m 2021-11-14 16:57:20,054 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/172.22.0.3:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[33malgo-2    |[0m 2021-11-14 16:57:20,064 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/172.22.0.3:8031. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m Nov 14, 2021 4:57:20 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 16:57:20,114 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 16:57:20,126 INFO ipc.Server: Starting Socket Reader #1 for port 8033
[36malgo-1    |[0m 2021-11-14 16:57:20,130 INFO ipc.Client: Retrying connect to server: algo-1/172.22.0.3:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 16:57:20,138 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5d5b5fa7{node,/,file:///tmp/jetty-algo-1-8042-_-any-8818536295193617878.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/node}
[36malgo-1    |[0m 2021-11-14 16:57:20,149 INFO server.AbstractConnector: Started ServerConnector@3f390d63{HTTP/1.1,[http/1.1]}{algo-1:8042}
[36malgo-1    |[0m 2021-11-14 16:57:20,150 INFO server.Server: Started @3675ms
[36malgo-1    |[0m 2021-11-14 16:57:20,150 INFO webapp.WebApps: Web app node started at 8042
[36malgo-1    |[0m 2021-11-14 16:57:20,151 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-1:35169
[36malgo-1    |[0m 2021-11-14 16:57:20,152 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 16:57:20,159 INFO client.RMProxy: Connecting to ResourceManager at /172.22.0.3:8031
[36malgo-1    |[0m 2021-11-14 16:57:20,218 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []
[36malgo-1    |[0m 2021-11-14 16:57:20,243 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]
[36malgo-1    |[0m 2021-11-14 16:57:20,270 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1/172.22.0.3:8020
[36malgo-1    |[0m 2021-11-14 16:57:20,272 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
[36malgo-1    |[0m 2021-11-14 16:57:20,279 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 135@algo-1
[36malgo-1    |[0m 2021-11-14 16:57:20,281 INFO common.Storage: Storage directory with location [DISK]file:/opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 542165568. Formatting...
[36malgo-1    |[0m 2021-11-14 16:57:20,282 INFO common.Storage: Generated new storageID DS-f764e4b6-0e96-4347-83c3-ecd0c9ec439e for directory /opt/amazon/hadoop/hdfs/datanode 
[33malgo-2    |[0m 2021-11-14 16:57:20,284 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1.spark-network/172.22.0.3:8020
[33malgo-2    |[0m 2021-11-14 16:57:20,287 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
[33malgo-2    |[0m 2021-11-14 16:57:20,294 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 18@algo-2
[33malgo-2    |[0m 2021-11-14 16:57:20,296 INFO common.Storage: Storage directory with location [DISK]file:/opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 542165568. Formatting...
[33malgo-2    |[0m 2021-11-14 16:57:20,297 INFO common.Storage: Generated new storageID DS-d3e74b7e-3d92-4048-8f6d-3a653b9233c1 for directory /opt/amazon/hadoop/hdfs/datanode 
[36malgo-1    |[0m 2021-11-14 16:57:20,311 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 16:57:20,312 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 16:57:20,312 INFO common.Storage: Analyzing storage directories for bpid BP-464411801-172.22.0.3-1636909036182
[36malgo-1    |[0m 2021-11-14 16:57:20,313 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-464411801-172.22.0.3-1636909036182
[36malgo-1    |[0m 2021-11-14 16:57:20,314 INFO common.Storage: Block pool storage directory for location [DISK]file:/opt/amazon/hadoop/hdfs/datanode and block pool id BP-464411801-172.22.0.3-1636909036182 is not formatted. Formatting ...
[36malgo-1    |[0m 2021-11-14 16:57:20,314 INFO common.Storage: Formatting block pool BP-464411801-172.22.0.3-1636909036182 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-464411801-172.22.0.3-1636909036182/current
[36malgo-1    |[0m 2021-11-14 16:57:20,321 INFO datanode.DataNode: Setting up storage: nsid=542165568;bpid=BP-464411801-172.22.0.3-1636909036182;lv=-57;nsInfo=lv=-65;cid=CID-e9b272d1-7023-4183-af32-e1af31503e0e;nsid=542165568;c=1636909036182;bpid=BP-464411801-172.22.0.3-1636909036182;dnuuid=null
[36malgo-1    |[0m 2021-11-14 16:57:20,323 INFO datanode.DataNode: Generated and persisted new Datanode UUID a1e69296-e00d-4d5d-88f9-c6f89d8d373f
[33malgo-2    |[0m 2021-11-14 16:57:20,325 INFO common.Storage: Analyzing storage directories for bpid BP-464411801-172.22.0.3-1636909036182
[33malgo-2    |[0m 2021-11-14 16:57:20,325 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-464411801-172.22.0.3-1636909036182
[33malgo-2    |[0m 2021-11-14 16:57:20,326 INFO common.Storage: Block pool storage directory for location [DISK]file:/opt/amazon/hadoop/hdfs/datanode and block pool id BP-464411801-172.22.0.3-1636909036182 is not formatted. Formatting ...
[33malgo-2    |[0m 2021-11-14 16:57:20,326 INFO common.Storage: Formatting block pool BP-464411801-172.22.0.3-1636909036182 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-464411801-172.22.0.3-1636909036182/current
[36malgo-1    |[0m 2021-11-14 16:57:20,335 INFO ipc.Server: IPC Server listener on 8033: starting
[36malgo-1    |[0m 2021-11-14 16:57:20,336 INFO resourcemanager.ResourceManager: Transitioning to active state
[33malgo-2    |[0m 2021-11-14 16:57:20,336 INFO datanode.DataNode: Setting up storage: nsid=542165568;bpid=BP-464411801-172.22.0.3-1636909036182;lv=-57;nsInfo=lv=-65;cid=CID-e9b272d1-7023-4183-af32-e1af31503e0e;nsid=542165568;c=1636909036182;bpid=BP-464411801-172.22.0.3-1636909036182;dnuuid=null
[33malgo-2    |[0m 2021-11-14 16:57:20,340 INFO datanode.DataNode: Generated and persisted new Datanode UUID ff27e906-a896-43b8-b902-49ab636000e3
[36malgo-1    |[0m 2021-11-14 16:57:20,355 INFO recovery.RMStateStore: Updating AMRMToken
[36malgo-1    |[0m 2021-11-14 16:57:20,355 INFO security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
[36malgo-1    |[0m 2021-11-14 16:57:20,356 INFO security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
[36malgo-1    |[0m 2021-11-14 16:57:20,356 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
[36malgo-1    |[0m 2021-11-14 16:57:20,356 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 1
[36malgo-1    |[0m 2021-11-14 16:57:20,356 INFO recovery.RMStateStore: Storing RMDTMasterKey.
[36malgo-1    |[0m 2021-11-14 16:57:20,356 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
[36malgo-1    |[0m 2021-11-14 16:57:20,357 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
[36malgo-1    |[0m 2021-11-14 16:57:20,357 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 2
[36malgo-1    |[0m 2021-11-14 16:57:20,357 INFO recovery.RMStateStore: Storing RMDTMasterKey.
[36malgo-1    |[0m 2021-11-14 16:57:20,359 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler
[36malgo-1    |[0m 2021-11-14 16:57:20,443 INFO impl.FsDatasetImpl: Added new volume: DS-f764e4b6-0e96-4347-83c3-ecd0c9ec439e
[36malgo-1    |[0m 2021-11-14 16:57:20,443 INFO impl.FsDatasetImpl: Added volume - [DISK]file:/opt/amazon/hadoop/hdfs/datanode, StorageType: DISK
[36malgo-1    |[0m 2021-11-14 16:57:20,449 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean
[33malgo-2    |[0m 2021-11-14 16:57:20,454 INFO impl.FsDatasetImpl: Added new volume: DS-d3e74b7e-3d92-4048-8f6d-3a653b9233c1
[33malgo-2    |[0m 2021-11-14 16:57:20,455 INFO impl.FsDatasetImpl: Added volume - [DISK]file:/opt/amazon/hadoop/hdfs/datanode, StorageType: DISK
[33malgo-2    |[0m 2021-11-14 16:57:20,458 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean
[36malgo-1    |[0m 2021-11-14 16:57:20,459 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode
[33malgo-2    |[0m 2021-11-14 16:57:20,466 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode
[36malgo-1    |[0m 2021-11-14 16:57:20,474 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode
[33malgo-2    |[0m 2021-11-14 16:57:20,474 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode
[33malgo-2    |[0m 2021-11-14 16:57:20,476 INFO impl.FsDatasetImpl: Adding block pool BP-464411801-172.22.0.3-1636909036182
[36malgo-1    |[0m 2021-11-14 16:57:20,476 INFO impl.FsDatasetImpl: Adding block pool BP-464411801-172.22.0.3-1636909036182
[33malgo-2    |[0m 2021-11-14 16:57:20,477 INFO impl.FsDatasetImpl: Scanning block pool BP-464411801-172.22.0.3-1636909036182 on volume /opt/amazon/hadoop/hdfs/datanode...
[36malgo-1    |[0m 2021-11-14 16:57:20,477 INFO impl.FsDatasetImpl: Scanning block pool BP-464411801-172.22.0.3-1636909036182 on volume /opt/amazon/hadoop/hdfs/datanode...
[36malgo-1    |[0m 11-14 16:57 smspark-submit INFO     cluster is up
[36malgo-1    |[0m 11-14 16:57 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING
[36malgo-1    |[0m 11-14 16:57 smspark-submit INFO     starting executor logs watcher
[36malgo-1    |[0m 11-14 16:57 smspark-submit INFO     start log event log publisher
[36malgo-1    |[0m Starting executor logs watcher on log_dir: /var/log/yarn
[36malgo-1    |[0m 11-14 16:57 sagemaker-spark-event-logs-publisher INFO     Spark event log not enabled.
[36malgo-1    |[0m 11-14 16:57 smspark-submit INFO     Waiting for hosts to bootstrap: ['algo-1', 'algo-2']
[36malgo-1    |[0m 2021-11-14 16:57:20,510 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-464411801-172.22.0.3-1636909036182 on /opt/amazon/hadoop/hdfs/datanode: 32ms
[36malgo-1    |[0m 2021-11-14 16:57:20,510 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-464411801-172.22.0.3-1636909036182: 34ms
[36malgo-1    |[0m 2021-11-14 16:57:20,511 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-464411801-172.22.0.3-1636909036182 on volume /opt/amazon/hadoop/hdfs/datanode...
[36malgo-1    |[0m 11-14 16:57 smspark-submit INFO     Received host statuses: dict_items([('algo-1', StatusMessage(status='WAITING', timestamp='2021-11-14T16:57:20.503072')), ('algo-2', StatusMessage(status='WAITING', timestamp='2021-11-14T16:57:20.509505'))])
[36malgo-1    |[0m 2021-11-14 16:57:20,512 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-464411801-172.22.0.3-1636909036182/current/replicas doesn't exist 
[36malgo-1    |[0m 2021-11-14 16:57:20,513 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-464411801-172.22.0.3-1636909036182 on volume /opt/amazon/hadoop/hdfs/datanode: 2ms
[36malgo-1    |[0m 2021-11-14 16:57:20,513 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-464411801-172.22.0.3-1636909036182: 2ms
[33malgo-2    |[0m 2021-11-14 16:57:20,514 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-464411801-172.22.0.3-1636909036182 on /opt/amazon/hadoop/hdfs/datanode: 37ms
[33malgo-2    |[0m 2021-11-14 16:57:20,515 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-464411801-172.22.0.3-1636909036182: 38ms
[36malgo-1    |[0m 2021-11-14 16:57:20,515 INFO datanode.VolumeScanner: Now scanning bpid BP-464411801-172.22.0.3-1636909036182 on volume /opt/amazon/hadoop/hdfs/datanode
[33malgo-2    |[0m 2021-11-14 16:57:20,516 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-464411801-172.22.0.3-1636909036182 on volume /opt/amazon/hadoop/hdfs/datanode...
[33malgo-2    |[0m 2021-11-14 16:57:20,517 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-464411801-172.22.0.3-1636909036182/current/replicas doesn't exist 
[36malgo-1    |[0m 2021-11-14 16:57:20,518 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-f764e4b6-0e96-4347-83c3-ecd0c9ec439e): finished scanning block pool BP-464411801-172.22.0.3-1636909036182
[33malgo-2    |[0m 2021-11-14 16:57:20,518 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-464411801-172.22.0.3-1636909036182 on volume /opt/amazon/hadoop/hdfs/datanode: 2ms
[33malgo-2    |[0m 2021-11-14 16:57:20,518 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-464411801-172.22.0.3-1636909036182: 3ms
[33malgo-2    |[0m 2021-11-14 16:57:20,520 INFO datanode.VolumeScanner: Now scanning bpid BP-464411801-172.22.0.3-1636909036182 on volume /opt/amazon/hadoop/hdfs/datanode
[33malgo-2    |[0m 2021-11-14 16:57:20,522 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-d3e74b7e-3d92-4048-8f6d-3a653b9233c1): finished scanning block pool BP-464411801-172.22.0.3-1636909036182
[36malgo-1    |[0m 2021-11-14 16:57:20,531 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 11/14/21 5:13 PM with interval of 21600000ms
[36malgo-1    |[0m 2021-11-14 16:57:20,532 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-f764e4b6-0e96-4347-83c3-ecd0c9ec439e): no suitable block pools found to scan.  Waiting 1814399983 ms.
[33malgo-2    |[0m 2021-11-14 16:57:20,534 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 11/14/21 9:10 PM with interval of 21600000ms
[33malgo-2    |[0m 2021-11-14 16:57:20,535 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-d3e74b7e-3d92-4048-8f6d-3a653b9233c1): no suitable block pools found to scan.  Waiting 1814399985 ms.
[36malgo-1    |[0m 2021-11-14 16:57:20,538 INFO datanode.DataNode: Block pool BP-464411801-172.22.0.3-1636909036182 (Datanode Uuid a1e69296-e00d-4d5d-88f9-c6f89d8d373f) service to algo-1/172.22.0.3:8020 beginning handshake with NN
[33malgo-2    |[0m 2021-11-14 16:57:20,539 INFO datanode.DataNode: Block pool BP-464411801-172.22.0.3-1636909036182 (Datanode Uuid ff27e906-a896-43b8-b902-49ab636000e3) service to algo-1.spark-network/172.22.0.3:8020 beginning handshake with NN
[36malgo-1    |[0m 2021-11-14 16:57:20,561 INFO store.AbstractFSNodeStore: Created store directory :file:/tmp/hadoop-yarn-root/node-attribute
[36malgo-1    |[0m 2021-11-14 16:57:20,576 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.22.0.3:9866, datanodeUuid=a1e69296-e00d-4d5d-88f9-c6f89d8d373f, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-e9b272d1-7023-4183-af32-e1af31503e0e;nsid=542165568;c=1636909036182) storage a1e69296-e00d-4d5d-88f9-c6f89d8d373f
[36malgo-1    |[0m 2021-11-14 16:57:20,578 INFO net.NetworkTopology: Adding a new node: /default-rack/172.22.0.3:9866
[36malgo-1    |[0m 2021-11-14 16:57:20,578 INFO blockmanagement.BlockReportLeaseManager: Registered DN a1e69296-e00d-4d5d-88f9-c6f89d8d373f (172.22.0.3:9866).
[36malgo-1    |[0m 2021-11-14 16:57:20,578 INFO store.AbstractFSNodeStore: Finished write mirror at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.mirror
[36malgo-1    |[0m 2021-11-14 16:57:20,578 INFO store.AbstractFSNodeStore: Finished create editlog file at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.editlog
[36malgo-1    |[0m 2021-11-14 16:57:20,580 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.22.0.2:9866, datanodeUuid=ff27e906-a896-43b8-b902-49ab636000e3, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-e9b272d1-7023-4183-af32-e1af31503e0e;nsid=542165568;c=1636909036182) storage ff27e906-a896-43b8-b902-49ab636000e3
[36malgo-1    |[0m 2021-11-14 16:57:20,580 INFO net.NetworkTopology: Adding a new node: /default-rack/172.22.0.2:9866
[36malgo-1    |[0m 2021-11-14 16:57:20,580 INFO blockmanagement.BlockReportLeaseManager: Registered DN ff27e906-a896-43b8-b902-49ab636000e3 (172.22.0.2:9866).
[36malgo-1    |[0m 2021-11-14 16:57:20,590 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl$ForwardingEventHandler
[33malgo-2    |[0m 2021-11-14 16:57:20,590 INFO datanode.DataNode: Block pool Block pool BP-464411801-172.22.0.3-1636909036182 (Datanode Uuid ff27e906-a896-43b8-b902-49ab636000e3) service to algo-1.spark-network/172.22.0.3:8020 successfully registered with NN
[36malgo-1    |[0m 2021-11-14 16:57:20,591 INFO placement.MultiNodeSortingManager: Starting NodeSortingService=MultiNodeSortingManager
[36malgo-1    |[0m 2021-11-14 16:57:20,591 INFO datanode.DataNode: Block pool Block pool BP-464411801-172.22.0.3-1636909036182 (Datanode Uuid a1e69296-e00d-4d5d-88f9-c6f89d8d373f) service to algo-1/172.22.0.3:8020 successfully registered with NN
[33malgo-2    |[0m 2021-11-14 16:57:20,590 INFO datanode.DataNode: For namenode algo-1.spark-network/172.22.0.3:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
[36malgo-1    |[0m 2021-11-14 16:57:20,591 INFO datanode.DataNode: For namenode algo-1/172.22.0.3:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
[36malgo-1    |[0m 2021-11-14 16:57:20,601 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 16:57:20,602 INFO ipc.Server: Starting Socket Reader #1 for port 8031
[36malgo-1    |[0m 2021-11-14 16:57:20,604 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server
[36malgo-1    |[0m 2021-11-14 16:57:20,605 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 16:57:20,605 INFO ipc.Server: IPC Server listener on 8031: starting
[36malgo-1    |[0m 2021-11-14 16:57:20,613 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 16:57:20,622 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 16:57:20,628 INFO ipc.Server: Starting Socket Reader #1 for port 8030
[36malgo-1    |[0m 2021-11-14 16:57:20,634 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 16:57:20,635 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 16:57:20,635 INFO ipc.Server: IPC Server listener on 8030: starting
[36malgo-1    |[0m 2021-11-14 16:57:20,656 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-f764e4b6-0e96-4347-83c3-ecd0c9ec439e for DN 172.22.0.3:9866
[36malgo-1    |[0m 2021-11-14 16:57:20,658 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-d3e74b7e-3d92-4048-8f6d-3a653b9233c1 for DN 172.22.0.2:9866
[36malgo-1    |[0m 2021-11-14 16:57:20,697 INFO BlockStateChange: BLOCK* processReport 0xaf9eb0548f0553a3: Processing first storage report for DS-d3e74b7e-3d92-4048-8f6d-3a653b9233c1 from datanode ff27e906-a896-43b8-b902-49ab636000e3
[36malgo-1    |[0m 2021-11-14 16:57:20,698 INFO BlockStateChange: BLOCK* processReport 0xaf9eb0548f0553a3: from storage DS-d3e74b7e-3d92-4048-8f6d-3a653b9233c1 node DatanodeRegistration(172.22.0.2:9866, datanodeUuid=ff27e906-a896-43b8-b902-49ab636000e3, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-e9b272d1-7023-4183-af32-e1af31503e0e;nsid=542165568;c=1636909036182), blocks: 0, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0
[36malgo-1    |[0m 2021-11-14 16:57:20,731 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 16:57:20,736 INFO ipc.Server: Starting Socket Reader #1 for port 8032
[36malgo-1    |[0m 2021-11-14 16:57:20,750 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 16:57:20,758 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 16:57:20,767 INFO ipc.Server: IPC Server listener on 8032: starting
[36malgo-1    |[0m 2021-11-14 16:57:20,784 INFO BlockStateChange: BLOCK* processReport 0x29af6c66e1f7438: Processing first storage report for DS-f764e4b6-0e96-4347-83c3-ecd0c9ec439e from datanode a1e69296-e00d-4d5d-88f9-c6f89d8d373f
[36malgo-1    |[0m 2021-11-14 16:57:20,784 INFO BlockStateChange: BLOCK* processReport 0x29af6c66e1f7438: from storage DS-f764e4b6-0e96-4347-83c3-ecd0c9ec439e node DatanodeRegistration(172.22.0.3:9866, datanodeUuid=a1e69296-e00d-4d5d-88f9-c6f89d8d373f, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-e9b272d1-7023-4183-af32-e1af31503e0e;nsid=542165568;c=1636909036182), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
[33malgo-2    |[0m 2021-11-14 16:57:20,795 INFO datanode.DataNode: Successfully sent block report 0xaf9eb0548f0553a3,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 116 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
[33malgo-2    |[0m 2021-11-14 16:57:20,795 INFO datanode.DataNode: Got finalize command for block pool BP-464411801-172.22.0.3-1636909036182
[36malgo-1    |[0m 2021-11-14 16:57:20,797 INFO resourcemanager.ResourceManager: Transitioned to active state
[36malgo-1    |[0m 2021-11-14 16:57:20,801 INFO datanode.DataNode: Successfully sent block report 0x29af6c66e1f7438,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 121 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
[36malgo-1    |[0m 2021-11-14 16:57:20,801 INFO datanode.DataNode: Got finalize command for block pool BP-464411801-172.22.0.3-1636909036182
[33malgo-2    |[0m 2021-11-14 16:57:21,065 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/172.22.0.3:8031. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 16:57:21,243 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-2(cmPort: 37609 httpPort: 8042) registered with capability: <memory:15892, vCores:4>, assigned nodeId algo-2:37609
[36malgo-1    |[0m 2021-11-14 16:57:21,247 INFO rmnode.RMNodeImpl: algo-2:37609 Node Transitioned from NEW to RUNNING
[33malgo-2    |[0m 2021-11-14 16:57:21,260 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id 2030061424
[33malgo-2    |[0m 2021-11-14 16:57:21,261 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id 1383142793
[33malgo-2    |[0m 2021-11-14 16:57:21,262 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-2:37609 with total resource of <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 16:57:21,268 INFO capacity.CapacityScheduler: Added node algo-2:37609 clusterResource: <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 16:57:21,345 INFO ipc.Client: Retrying connect to server: algo-1/172.22.0.3:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 16:57:21,354 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-1(cmPort: 35169 httpPort: 8042) registered with capability: <memory:15892, vCores:4>, assigned nodeId algo-1:35169
[36malgo-1    |[0m 2021-11-14 16:57:21,355 INFO rmnode.RMNodeImpl: algo-1:35169 Node Transitioned from NEW to RUNNING
[36malgo-1    |[0m 2021-11-14 16:57:21,356 INFO capacity.CapacityScheduler: Added node algo-1:35169 clusterResource: <memory:31784, vCores:8>
[36malgo-1    |[0m 2021-11-14 16:57:21,364 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id 2030061424
[36malgo-1    |[0m 2021-11-14 16:57:21,365 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id 1383142793
[36malgo-1    |[0m 2021-11-14 16:57:21,366 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-1:35169 with total resource of <memory:15892, vCores:4>
[36malgo-1    |[0m Using properties file: /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m Adding default property: spark.driver.host=172.22.0.3
[36malgo-1    |[0m Adding default property: spark.executor.memoryOverhead=1239m
[36malgo-1    |[0m Adding default property: spark.driver.extraLibraryPath=/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m Adding default property: spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[36malgo-1    |[0m Adding default property: spark.rpc.askTimeout=300s
[36malgo-1    |[0m Adding default property: spark.driver.memory=2048m
[36malgo-1    |[0m Adding default property: spark.executor.instances=2
[36malgo-1    |[0m Adding default property: spark.driver.memoryOverhead=204m
[36malgo-1    |[0m Adding default property: key=value
[36malgo-1    |[0m Adding default property: spark.default.parallelism=16
[36malgo-1    |[0m Adding default property: spark.executor.defaultJavaOptions=-verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3
[36malgo-1    |[0m Adding default property: spark.driver.defaultJavaOptions=-XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[36malgo-1    |[0m Adding default property: spark.executor.extraLibraryPath=/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m Adding default property: spark.executor.memory=2g
[36malgo-1    |[0m Adding default property: spark.driver.extraClassPath=/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m Adding default property: spark.executor.extraClassPath=/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m Adding default property: spark.executor.cores=1
[36malgo-1    |[0m Warning: Ignoring non-Spark config property: key
[36malgo-1    |[0m Parsed arguments:
[36malgo-1    |[0m   master                  yarn
[36malgo-1    |[0m   deployMode              client
[36malgo-1    |[0m   executorMemory          2g
[36malgo-1    |[0m   executorCores           1
[36malgo-1    |[0m   totalExecutorCores      null
[36malgo-1    |[0m   propertiesFile          /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m   driverMemory            2048m
[36malgo-1    |[0m   driverCores             null
[36malgo-1    |[0m   driverExtraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m   driverExtraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m   driverExtraJavaOptions  null
[36malgo-1    |[0m   supervise               false
[36malgo-1    |[0m   queue                   null
[36malgo-1    |[0m   numExecutors            2
[36malgo-1    |[0m   files                   null
[36malgo-1    |[0m   pyFiles                 null
[36malgo-1    |[0m   archives                null
[36malgo-1    |[0m   mainClass               com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp
[36malgo-1    |[0m   primaryResource         file:/opt/ml/processing/input/code/java/hello-java-spark/target/hello-java-spark-1.0-SNAPSHOT.jar
[36malgo-1    |[0m   name                    com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp
[36malgo-1    |[0m   childArgs               [--input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data]
[36malgo-1    |[0m   jars                    null
[36malgo-1    |[0m   packages                null
[36malgo-1    |[0m   packagesExclusions      null
[36malgo-1    |[0m   repositories            null
[36malgo-1    |[0m   verbose                 true
[36malgo-1    |[0m 
[36malgo-1    |[0m Spark properties used, including those specified through
[36malgo-1    |[0m  --conf and those from the properties file /usr/lib/spark/conf/spark-defaults.conf:
[36malgo-1    |[0m   (spark.executor.defaultJavaOptions,-verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3)
[36malgo-1    |[0m   (spark.default.parallelism,16)
[36malgo-1    |[0m   (spark.executor.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)
[36malgo-1    |[0m   (spark.executor.memory,2g)
[36malgo-1    |[0m   (spark.driver.memory,2048m)
[36malgo-1    |[0m   (spark.driver.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)
[36malgo-1    |[0m   (spark.executor.instances,2)
[36malgo-1    |[0m   (spark.executor.memoryOverhead,1239m)
[36malgo-1    |[0m   (spark.rpc.askTimeout,300s)
[36malgo-1    |[0m   (spark.driver.memoryOverhead,204m)
[36malgo-1    |[0m   (spark.driver.host,172.22.0.3)
[36malgo-1    |[0m   (spark.driver.defaultJavaOptions,-XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled)
[36malgo-1    |[0m   (spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version,2)
[36malgo-1    |[0m   (spark.executor.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)
[36malgo-1    |[0m   (spark.executor.cores,1)
[36malgo-1    |[0m   (spark.driver.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)
[36malgo-1    |[0m 
[36malgo-1    |[0m     
[36malgo-1    |[0m Main class:
[36malgo-1    |[0m com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp
[36malgo-1    |[0m Arguments:
[36malgo-1    |[0m --input
[36malgo-1    |[0m file:///opt/ml/processing/input/data/data.jsonl
[36malgo-1    |[0m --output
[36malgo-1    |[0m file:///opt/ml/processing/output/data
[36malgo-1    |[0m Spark config:
[36malgo-1    |[0m (spark.driver.host,172.22.0.3)
[36malgo-1    |[0m (spark.executor.memoryOverhead,1239m)
[36malgo-1    |[0m (spark.driver.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)
[36malgo-1    |[0m (spark.jars,file:/opt/ml/processing/input/code/java/hello-java-spark/target/hello-java-spark-1.0-SNAPSHOT.jar)
[36malgo-1    |[0m (spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version,2)
[36malgo-1    |[0m (spark.app.name,com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp)
[36malgo-1    |[0m (spark.rpc.askTimeout,300s)
[36malgo-1    |[0m (spark.driver.memory,2048m)
[36malgo-1    |[0m (spark.executor.instances,2)
[36malgo-1    |[0m (spark.submit.pyFiles,)
[36malgo-1    |[0m (spark.driver.memoryOverhead,204m)
[36malgo-1    |[0m (spark.default.parallelism,16)
[36malgo-1    |[0m (spark.executor.defaultJavaOptions,-verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3)
[36malgo-1    |[0m (spark.submit.deployMode,client)
[36malgo-1    |[0m (spark.master,yarn)
[36malgo-1    |[0m (spark.driver.defaultJavaOptions,-XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled)
[36malgo-1    |[0m (spark.executor.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)
[36malgo-1    |[0m (spark.executor.memory,2g)
[36malgo-1    |[0m (spark.driver.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)
[36malgo-1    |[0m (spark.executor.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)
[36malgo-1    |[0m (spark.executor.cores,1)
[36malgo-1    |[0m Classpath elements:
[36malgo-1    |[0m file:/opt/ml/processing/input/code/java/hello-java-spark/target/hello-java-spark-1.0-SNAPSHOT.jar
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m Hello World, this is Java-Spark!
[36malgo-1    |[0m Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[36malgo-1    |[0m 21/11/14 16:57:22 INFO SparkContext: Running Spark version 3.0.0-amzn-0
[36malgo-1    |[0m 21/11/14 16:57:22 INFO ResourceUtils: ==============================================================
[36malgo-1    |[0m 21/11/14 16:57:22 INFO ResourceUtils: Resources for spark.driver:
[36malgo-1    |[0m 
[36malgo-1    |[0m 21/11/14 16:57:22 INFO ResourceUtils: ==============================================================
[36malgo-1    |[0m 21/11/14 16:57:22 INFO SparkContext: Submitted application: Hello Spark App
[36malgo-1    |[0m 21/11/14 16:57:22 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m 21/11/14 16:57:22 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m 21/11/14 16:57:22 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m 21/11/14 16:57:22 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m 21/11/14 16:57:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m 21/11/14 16:57:22 INFO Utils: Successfully started service 'sparkDriver' on port 46583.
[36malgo-1    |[0m 21/11/14 16:57:22 INFO SparkEnv: Registering MapOutputTracker
[36malgo-1    |[0m 21/11/14 16:57:23 INFO SparkEnv: Registering BlockManagerMaster
[36malgo-1    |[0m 21/11/14 16:57:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[36malgo-1    |[0m 21/11/14 16:57:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[36malgo-1    |[0m 21/11/14 16:57:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[36malgo-1    |[0m 21/11/14 16:57:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c873397b-8849-4864-a639-a8df0da68547
[36malgo-1    |[0m 21/11/14 16:57:23 INFO MemoryStore: MemoryStore started with capacity 1007.8 MiB
[36malgo-1    |[0m 21/11/14 16:57:23 INFO SparkEnv: Registering OutputCommitCoordinator
[36malgo-1    |[0m 21/11/14 16:57:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[36malgo-1    |[0m 21/11/14 16:57:23 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.22.0.3:4040
[36malgo-1    |[0m 21/11/14 16:57:23 INFO SparkContext: Added JAR file:/opt/ml/processing/input/code/java/hello-java-spark/target/hello-java-spark-1.0-SNAPSHOT.jar at spark://172.22.0.3:46583/jars/hello-java-spark-1.0-SNAPSHOT.jar with timestamp 1636909043485
[36malgo-1    |[0m 21/11/14 16:57:23 INFO RMProxy: Connecting to ResourceManager at /172.22.0.3:8032
[36malgo-1    |[0m 21/11/14 16:57:24 INFO Client: Requesting a new application from cluster with 2 NodeManagers
[36malgo-1    |[0m 2021-11-14 16:57:24,023 INFO resourcemanager.ClientRMService: Allocated new applicationId: 1
[36malgo-1    |[0m 21/11/14 16:57:24 INFO Configuration: resource-types.xml not found
[36malgo-1    |[0m 21/11/14 16:57:24 INFO ResourceUtils: Unable to find 'resource-types.xml'.
[36malgo-1    |[0m 21/11/14 16:57:24 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15892 MB per container)
[36malgo-1    |[0m 21/11/14 16:57:24 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
[36malgo-1    |[0m 21/11/14 16:57:24 INFO Client: Setting up container launch context for our AM
[36malgo-1    |[0m 21/11/14 16:57:24 INFO Client: Setting up the launch environment for our AM container
[36malgo-1    |[0m 21/11/14 16:57:24 INFO Client: Preparing resources for our AM container
[36malgo-1    |[0m 21/11/14 16:57:24 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
[36malgo-1    |[0m 21/11/14 16:57:30 INFO Client: Uploading resource file:/tmp/spark-8a8db27a-a8d4-4f99-91b5-59856e4140cd/__spark_libs__296927782304977530.zip -> hdfs://172.22.0.3/user/root/.sparkStaging/application_1636909040339_0001/__spark_libs__296927782304977530.zip
[36malgo-1    |[0m 2021-11-14 16:57:30,565 INFO hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=172.22.0.3:9866, 172.22.0.2:9866 for /user/root/.sparkStaging/application_1636909040339_0001/__spark_libs__296927782304977530.zip
[36malgo-1    |[0m 2021-11-14 16:57:30,690 INFO datanode.DataNode: Receiving BP-464411801-172.22.0.3-1636909036182:blk_1073741825_1001 src: /172.22.0.3:59916 dest: /172.22.0.3:9866
[33malgo-2    |[0m 2021-11-14 16:57:30,753 INFO datanode.DataNode: Receiving BP-464411801-172.22.0.3-1636909036182:blk_1073741825_1001 src: /172.22.0.3:39508 dest: /172.22.0.2:9866
[33malgo-2    |[0m 2021-11-14 16:57:31,175 INFO DataNode.clienttrace: src: /172.22.0.3:39508, dest: /172.22.0.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_954643853_1, offset: 0, srvID: ff27e906-a896-43b8-b902-49ab636000e3, blockid: BP-464411801-172.22.0.3-1636909036182:blk_1073741825_1001, duration(ns): 399588667
[33malgo-2    |[0m 2021-11-14 16:57:31,175 INFO datanode.DataNode: PacketResponder: BP-464411801-172.22.0.3-1636909036182:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 16:57:31,179 INFO DataNode.clienttrace: src: /172.22.0.3:59916, dest: /172.22.0.3:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_954643853_1, offset: 0, srvID: a1e69296-e00d-4d5d-88f9-c6f89d8d373f, blockid: BP-464411801-172.22.0.3-1636909036182:blk_1073741825_1001, duration(ns): 397708014
[36malgo-1    |[0m 2021-11-14 16:57:31,180 INFO datanode.DataNode: PacketResponder: BP-464411801-172.22.0.3-1636909036182:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.22.0.2:9866] terminating
[36malgo-1    |[0m 2021-11-14 16:57:31,186 INFO hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=172.22.0.3:9866, 172.22.0.2:9866 for /user/root/.sparkStaging/application_1636909040339_0001/__spark_libs__296927782304977530.zip
[36malgo-1    |[0m 2021-11-14 16:57:31,190 INFO datanode.DataNode: Receiving BP-464411801-172.22.0.3-1636909036182:blk_1073741826_1002 src: /172.22.0.3:59920 dest: /172.22.0.3:9866
[33malgo-2    |[0m 2021-11-14 16:57:31,192 INFO datanode.DataNode: Receiving BP-464411801-172.22.0.3-1636909036182:blk_1073741826_1002 src: /172.22.0.3:39512 dest: /172.22.0.2:9866
[33malgo-2    |[0m 2021-11-14 16:57:31,535 INFO DataNode.clienttrace: src: /172.22.0.3:39512, dest: /172.22.0.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_954643853_1, offset: 0, srvID: ff27e906-a896-43b8-b902-49ab636000e3, blockid: BP-464411801-172.22.0.3-1636909036182:blk_1073741826_1002, duration(ns): 341768591
[33malgo-2    |[0m 2021-11-14 16:57:31,536 INFO datanode.DataNode: PacketResponder: BP-464411801-172.22.0.3-1636909036182:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 16:57:31,537 INFO DataNode.clienttrace: src: /172.22.0.3:59920, dest: /172.22.0.3:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_954643853_1, offset: 0, srvID: a1e69296-e00d-4d5d-88f9-c6f89d8d373f, blockid: BP-464411801-172.22.0.3-1636909036182:blk_1073741826_1002, duration(ns): 342960866
[36malgo-1    |[0m 2021-11-14 16:57:31,537 INFO datanode.DataNode: PacketResponder: BP-464411801-172.22.0.3-1636909036182:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.22.0.2:9866] terminating
[36malgo-1    |[0m 2021-11-14 16:57:31,539 INFO hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=172.22.0.3:9866, 172.22.0.2:9866 for /user/root/.sparkStaging/application_1636909040339_0001/__spark_libs__296927782304977530.zip
[36malgo-1    |[0m 2021-11-14 16:57:31,543 INFO datanode.DataNode: Receiving BP-464411801-172.22.0.3-1636909036182:blk_1073741827_1003 src: /172.22.0.3:59924 dest: /172.22.0.3:9866
[33malgo-2    |[0m 2021-11-14 16:57:31,544 INFO datanode.DataNode: Receiving BP-464411801-172.22.0.3-1636909036182:blk_1073741827_1003 src: /172.22.0.3:39516 dest: /172.22.0.2:9866
[33malgo-2    |[0m 2021-11-14 16:57:31,867 INFO DataNode.clienttrace: src: /172.22.0.3:39516, dest: /172.22.0.2:9866, bytes: 128628638, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_954643853_1, offset: 0, srvID: ff27e906-a896-43b8-b902-49ab636000e3, blockid: BP-464411801-172.22.0.3-1636909036182:blk_1073741827_1003, duration(ns): 320890563
[33malgo-2    |[0m 2021-11-14 16:57:31,867 INFO datanode.DataNode: PacketResponder: BP-464411801-172.22.0.3-1636909036182:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 16:57:31,868 INFO DataNode.clienttrace: src: /172.22.0.3:59924, dest: /172.22.0.3:9866, bytes: 128628638, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_954643853_1, offset: 0, srvID: a1e69296-e00d-4d5d-88f9-c6f89d8d373f, blockid: BP-464411801-172.22.0.3-1636909036182:blk_1073741827_1003, duration(ns): 322012582
[36malgo-1    |[0m 2021-11-14 16:57:31,868 INFO datanode.DataNode: PacketResponder: BP-464411801-172.22.0.3-1636909036182:blk_1073741827_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.22.0.2:9866] terminating
[36malgo-1    |[0m 2021-11-14 16:57:31,873 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1636909040339_0001/__spark_libs__296927782304977530.zip is closed by DFSClient_NONMAPREDUCE_954643853_1
[36malgo-1    |[0m 21/11/14 16:57:32 INFO Client: Uploading resource file:/tmp/spark-8a8db27a-a8d4-4f99-91b5-59856e4140cd/__spark_conf__2659374692550917010.zip -> hdfs://172.22.0.3/user/root/.sparkStaging/application_1636909040339_0001/__spark_conf__.zip
[36malgo-1    |[0m 2021-11-14 16:57:32,080 INFO hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=172.22.0.3:9866, 172.22.0.2:9866 for /user/root/.sparkStaging/application_1636909040339_0001/__spark_conf__.zip
[36malgo-1    |[0m 2021-11-14 16:57:32,083 INFO datanode.DataNode: Receiving BP-464411801-172.22.0.3-1636909036182:blk_1073741828_1004 src: /172.22.0.3:59928 dest: /172.22.0.3:9866
[33malgo-2    |[0m 2021-11-14 16:57:32,085 INFO datanode.DataNode: Receiving BP-464411801-172.22.0.3-1636909036182:blk_1073741828_1004 src: /172.22.0.3:39520 dest: /172.22.0.2:9866
[33malgo-2    |[0m 2021-11-14 16:57:32,089 INFO DataNode.clienttrace: src: /172.22.0.3:39520, dest: /172.22.0.2:9866, bytes: 251855, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_954643853_1, offset: 0, srvID: ff27e906-a896-43b8-b902-49ab636000e3, blockid: BP-464411801-172.22.0.3-1636909036182:blk_1073741828_1004, duration(ns): 2386557
[33malgo-2    |[0m 2021-11-14 16:57:32,089 INFO datanode.DataNode: PacketResponder: BP-464411801-172.22.0.3-1636909036182:blk_1073741828_1004, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 16:57:32,090 INFO DataNode.clienttrace: src: /172.22.0.3:59928, dest: /172.22.0.3:9866, bytes: 251855, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_954643853_1, offset: 0, srvID: a1e69296-e00d-4d5d-88f9-c6f89d8d373f, blockid: BP-464411801-172.22.0.3-1636909036182:blk_1073741828_1004, duration(ns): 3309958
[36malgo-1    |[0m 2021-11-14 16:57:32,090 INFO datanode.DataNode: PacketResponder: BP-464411801-172.22.0.3-1636909036182:blk_1073741828_1004, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.22.0.2:9866] terminating
[36malgo-1    |[0m 2021-11-14 16:57:32,091 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1636909040339_0001/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_954643853_1
[36malgo-1    |[0m 21/11/14 16:57:32 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m 21/11/14 16:57:32 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m 21/11/14 16:57:32 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m 21/11/14 16:57:32 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m 21/11/14 16:57:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m 21/11/14 16:57:32 INFO Client: Submitting application application_1636909040339_0001 to ResourceManager
[36malgo-1    |[0m 2021-11-14 16:57:32,216 INFO capacity.CapacityScheduler: Application 'application_1636909040339_0001' is submitted without priority hence considering default queue/cluster priority: 0
[36malgo-1    |[0m 2021-11-14 16:57:32,216 INFO capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1636909040339_0001
[36malgo-1    |[0m 2021-11-14 16:57:32,232 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 1]. Use the global max attempts instead.
[36malgo-1    |[0m 2021-11-14 16:57:32,234 INFO resourcemanager.ClientRMService: Application with id 1 submitted by user root
[36malgo-1    |[0m 2021-11-14 16:57:32,235 INFO rmapp.RMAppImpl: Storing application with id application_1636909040339_0001
[36malgo-1    |[0m 2021-11-14 16:57:32,236 INFO resourcemanager.RMAuditLogger: USER=root	IP=172.22.0.3	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1636909040339_0001	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 16:57:32,242 INFO recovery.RMStateStore: Storing info for app: application_1636909040339_0001
[36malgo-1    |[0m 2021-11-14 16:57:32,242 INFO rmapp.RMAppImpl: application_1636909040339_0001 State change from NEW to NEW_SAVING on event = START
[36malgo-1    |[0m 2021-11-14 16:57:32,243 INFO rmapp.RMAppImpl: application_1636909040339_0001 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
[36malgo-1    |[0m 2021-11-14 16:57:32,244 INFO capacity.ParentQueue: Application added - appId: application_1636909040339_0001 user: root leaf-queue of parent: root #applications: 1
[36malgo-1    |[0m 2021-11-14 16:57:32,244 INFO capacity.CapacityScheduler: Accepted application application_1636909040339_0001 from user: root, in queue: default
[36malgo-1    |[0m 2021-11-14 16:57:32,256 INFO rmapp.RMAppImpl: application_1636909040339_0001 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
[36malgo-1    |[0m 2021-11-14 16:57:32,282 INFO resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1636909040339_0001_000001
[36malgo-1    |[0m 2021-11-14 16:57:32,283 INFO attempt.RMAppAttemptImpl: appattempt_1636909040339_0001_000001 State change from NEW to SUBMITTED on event = START
[36malgo-1    |[0m 21/11/14 16:57:32 INFO YarnClientImpl: Submitted application application_1636909040339_0001
[36malgo-1    |[0m 2021-11-14 16:57:32,352 INFO capacity.LeafQueue: Application application_1636909040339_0001 from user: root activated in queue: default
[36malgo-1    |[0m 2021-11-14 16:57:32,352 INFO capacity.LeafQueue: Application added - appId: application_1636909040339_0001 user: root, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
[36malgo-1    |[0m 2021-11-14 16:57:32,352 INFO capacity.CapacityScheduler: Added Application Attempt appattempt_1636909040339_0001_000001 to scheduler from user root in queue default
[36malgo-1    |[0m 2021-11-14 16:57:32,359 INFO attempt.RMAppAttemptImpl: appattempt_1636909040339_0001_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
[36malgo-1    |[0m 2021-11-14 16:57:32,445 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1636909040339_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=
[36malgo-1    |[0m 2021-11-14 16:57:32,449 INFO rmcontainer.RMContainerImpl: container_1636909040339_0001_01_000001 Container Transitioned from NEW to ALLOCATED
[36malgo-1    |[0m 2021-11-14 16:57:32,449 INFO fica.FiCaSchedulerNode: Assigned container container_1636909040339_0001_01_000001 of capacity <memory:896, vCores:1> on host algo-1:35169, which has 1 containers, <memory:896, vCores:1> used and <memory:14996, vCores:3> available after allocation
[36malgo-1    |[0m 2021-11-14 16:57:32,449 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636909040339_0001	CONTAINERID=container_1636909040339_0001_01_000001	RESOURCE=<memory:896, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 16:57:32,467 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:35169 for container : container_1636909040339_0001_01_000001
[36malgo-1    |[0m 2021-11-14 16:57:32,475 INFO rmcontainer.RMContainerImpl: container_1636909040339_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
[36malgo-1    |[0m 2021-11-14 16:57:32,475 INFO security.NMTokenSecretManagerInRM: Clear node set for appattempt_1636909040339_0001_000001
[36malgo-1    |[0m 2021-11-14 16:57:32,475 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.028190285 absoluteUsedCapacity=0.028190285 used=<memory:896, vCores:1> cluster=<memory:31784, vCores:8>
[36malgo-1    |[0m 2021-11-14 16:57:32,475 INFO attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1636909040339_0001 AttemptId: appattempt_1636909040339_0001_000001 MasterContainer: Container: [ContainerId: container_1636909040339_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-1:35169, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 172.22.0.3:35169 }, ExecutionType: GUARANTEED, ]
[36malgo-1    |[0m 2021-11-14 16:57:32,476 INFO capacity.CapacityScheduler: Allocation proposal accepted
[36malgo-1    |[0m 2021-11-14 16:57:32,486 INFO attempt.RMAppAttemptImpl: appattempt_1636909040339_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
[36malgo-1    |[0m 2021-11-14 16:57:32,490 INFO attempt.RMAppAttemptImpl: appattempt_1636909040339_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
[36malgo-1    |[0m 2021-11-14 16:57:32,502 INFO amlauncher.AMLauncher: Launching masterappattempt_1636909040339_0001_000001
[36malgo-1    |[0m 2021-11-14 16:57:32,559 INFO amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1636909040339_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-1:35169, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 172.22.0.3:35169 }, ExecutionType: GUARANTEED, ] for AM appattempt_1636909040339_0001_000001
[36malgo-1    |[0m 2021-11-14 16:57:32,560 INFO security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1636909040339_0001_000001
[36malgo-1    |[0m 2021-11-14 16:57:32,562 INFO security.AMRMTokenSecretManager: Creating password for appattempt_1636909040339_0001_000001
[36malgo-1    |[0m 2021-11-14 16:57:32,688 INFO ipc.Server: Auth successful for appattempt_1636909040339_0001_000001 (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 16:57:32,771 INFO containermanager.ContainerManagerImpl: Start request for container_1636909040339_0001_01_000001 by user root
[36malgo-1    |[0m 2021-11-14 16:57:32,825 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1636909040339_0001
[36malgo-1    |[0m 2021-11-14 16:57:32,833 INFO application.ApplicationImpl: Application application_1636909040339_0001 transitioned from NEW to INITING
[36malgo-1    |[0m 2021-11-14 16:57:32,833 INFO application.ApplicationImpl: Adding container_1636909040339_0001_01_000001 to application application_1636909040339_0001
[36malgo-1    |[0m 2021-11-14 16:57:32,833 INFO nodemanager.NMAuditLogger: USER=root	IP=172.22.0.3	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1636909040339_0001	CONTAINERID=container_1636909040339_0001_01_000001
[36malgo-1    |[0m 2021-11-14 16:57:32,837 INFO application.ApplicationImpl: Application application_1636909040339_0001 transitioned from INITING to RUNNING
[36malgo-1    |[0m 2021-11-14 16:57:32,839 INFO container.ContainerImpl: Container container_1636909040339_0001_01_000001 transitioned from NEW to LOCALIZING
[36malgo-1    |[0m 2021-11-14 16:57:32,839 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1636909040339_0001
[36malgo-1    |[0m 2021-11-14 16:57:32,846 INFO amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1636909040339_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-1:35169, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 172.22.0.3:35169 }, ExecutionType: GUARANTEED, ] for AM appattempt_1636909040339_0001_000001
[36malgo-1    |[0m 2021-11-14 16:57:32,846 INFO attempt.RMAppAttemptImpl: appattempt_1636909040339_0001_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
[36malgo-1    |[0m 2021-11-14 16:57:32,846 INFO rmapp.RMAppImpl: update the launch time for applicationId: application_1636909040339_0001, attemptId: appattempt_1636909040339_0001_000001launchTime: 1636909052846
[36malgo-1    |[0m 2021-11-14 16:57:32,847 INFO recovery.RMStateStore: Updating info for app: application_1636909040339_0001
[36malgo-1    |[0m 2021-11-14 16:57:32,849 INFO localizer.ResourceLocalizationService: Created localizer for container_1636909040339_0001_01_000001
[36malgo-1    |[0m 2021-11-14 16:57:32,912 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1636909040339_0001_01_000001.tokens
[36malgo-1    |[0m 2021-11-14 16:57:32,922 INFO nodemanager.DefaultContainerExecutor: Initializing user root
[36malgo-1    |[0m 2021-11-14 16:57:32,927 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1636909040339_0001_01_000001.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/container_1636909040339_0001_01_000001.tokens
[36malgo-1    |[0m 2021-11-14 16:57:32,927 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001
[36malgo-1    |[0m 21/11/14 16:57:33 INFO Client: Application report for application_1636909040339_0001 (state: ACCEPTED)
[36malgo-1    |[0m 21/11/14 16:57:33 INFO Client: 
[36malgo-1    |[0m 	 client token: N/A
[36malgo-1    |[0m 	 diagnostics: AM container is launched, waiting for AM container to Register with RM
[36malgo-1    |[0m 	 ApplicationMaster host: N/A
[36malgo-1    |[0m 	 ApplicationMaster RPC port: -1
[36malgo-1    |[0m 	 queue: default
[36malgo-1    |[0m 	 start time: 1636909052232
[36malgo-1    |[0m 	 final status: UNDEFINED
[36malgo-1    |[0m 	 tracking URL: http://algo-1:8088/proxy/application_1636909040339_0001/
[36malgo-1    |[0m 	 user: root
[36malgo-1    |[0m 2021-11-14 16:57:33,443 INFO rmcontainer.RMContainerImpl: container_1636909040339_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING
[36malgo-1    |[0m 21/11/14 16:57:34 INFO Client: Application report for application_1636909040339_0001 (state: ACCEPTED)
[36malgo-1    |[0m 21/11/14 16:57:35 INFO Client: Application report for application_1636909040339_0001 (state: ACCEPTED)
[36malgo-1    |[0m 2021-11-14 16:57:36,257 INFO container.ContainerImpl: Container container_1636909040339_0001_01_000001 transitioned from LOCALIZING to SCHEDULED
[36malgo-1    |[0m 2021-11-14 16:57:36,257 INFO scheduler.ContainerScheduler: Starting container [container_1636909040339_0001_01_000001]
[36malgo-1    |[0m 2021-11-14 16:57:36,281 INFO container.ContainerImpl: Container container_1636909040339_0001_01_000001 transitioned from SCHEDULED to RUNNING
[36malgo-1    |[0m 2021-11-14 16:57:36,281 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1636909040339_0001_01_000001
[36malgo-1    |[0m 2021-11-14 16:57:36,285 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/container_1636909040339_0001_01_000001/default_container_executor.sh]
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/prelaunch.out
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/prelaunch.err
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/launch_container.sh
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/directory.info
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stdout
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr
[36malgo-1    |[0m 21/11/14 16:57:36 INFO Client: Application report for application_1636909040339_0001 (state: ACCEPTED)
[36malgo-1    |[0m 2021-11-14 16:57:36,542 INFO monitor.ContainersMonitorImpl: container_1636909040339_0001_01_000001's ip = 172.22.0.3, and hostname = algo-1
[36malgo-1    |[0m 2021-11-14 16:57:36,548 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1636909040339_0001_01_000001 since CPU usage is not yet available.
[36malgo-1    |[0m 21/11/14 16:57:37 INFO Client: Application report for application_1636909040339_0001 (state: ACCEPTED)
[36malgo-1    |[0m 21/11/14 16:57:38 INFO Client: Application report for application_1636909040339_0001 (state: ACCEPTED)
[36malgo-1    |[0m 2021-11-14 16:57:38,526 INFO ipc.Server: Auth successful for appattempt_1636909040339_0001_000001 (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 16:57:38,552 INFO resourcemanager.DefaultAMSProcessor: AM registration appattempt_1636909040339_0001_000001
[36malgo-1    |[0m 2021-11-14 16:57:38,552 INFO resourcemanager.RMAuditLogger: USER=root	IP=172.22.0.3	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1636909040339_0001	APPATTEMPTID=appattempt_1636909040339_0001_000001
[36malgo-1    |[0m 2021-11-14 16:57:38,553 INFO attempt.RMAppAttemptImpl: appattempt_1636909040339_0001_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
[36malgo-1    |[0m 2021-11-14 16:57:38,553 INFO rmapp.RMAppImpl: application_1636909040339_0001 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
[36malgo-1    |[0m 21/11/14 16:57:38 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1636909040339_0001), /proxy/application_1636909040339_0001
[36malgo-1    |[0m 21/11/14 16:57:39 INFO Client: Application report for application_1636909040339_0001 (state: RUNNING)
[36malgo-1    |[0m 21/11/14 16:57:39 INFO Client: 
[36malgo-1    |[0m 	 client token: N/A
[36malgo-1    |[0m 	 diagnostics: N/A
[36malgo-1    |[0m 	 ApplicationMaster host: 172.22.0.3
[36malgo-1    |[0m 	 ApplicationMaster RPC port: -1
[36malgo-1    |[0m 	 queue: default
[36malgo-1    |[0m 	 start time: 1636909052232
[36malgo-1    |[0m 	 final status: UNDEFINED
[36malgo-1    |[0m 	 tracking URL: http://algo-1:8088/proxy/application_1636909040339_0001/
[36malgo-1    |[0m 	 user: root
[36malgo-1    |[0m 21/11/14 16:57:39 INFO YarnClientSchedulerBackend: Application application_1636909040339_0001 has started running.
[36malgo-1    |[0m 21/11/14 16:57:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41209.
[36malgo-1    |[0m 21/11/14 16:57:39 INFO NettyBlockTransferService: Server created on 172.22.0.3:41209
[36malgo-1    |[0m 21/11/14 16:57:39 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[36malgo-1    |[0m 21/11/14 16:57:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.22.0.3, 41209, None)
[36malgo-1    |[0m 21/11/14 16:57:39 INFO BlockManagerMasterEndpoint: Registering block manager 172.22.0.3:41209 with 1007.8 MiB RAM, BlockManagerId(driver, 172.22.0.3, 41209, None)
[36malgo-1    |[0m 21/11/14 16:57:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.22.0.3, 41209, None)
[36malgo-1    |[0m 21/11/14 16:57:39 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.22.0.3, 41209, None)
[36malgo-1    |[0m 21/11/14 16:57:39 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 16:57:39 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[36malgo-1    |[0m 21/11/14 16:57:39 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
[36malgo-1    |[0m Got a Spark session with version: 3.0.0-amzn-0
[36malgo-1    |[0m Reading input from: file:///opt/ml/processing/input/data/data.jsonl
[36malgo-1    |[0m 21/11/14 16:57:39 INFO SharedState: loading hive config file: file:/etc/spark/conf.dist/hive-site.xml
[36malgo-1    |[0m 21/11/14 16:57:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/lib/spark/spark-warehouse').
[36malgo-1    |[0m 21/11/14 16:57:39 INFO SharedState: Warehouse path is 'file:/usr/lib/spark/spark-warehouse'.
[36malgo-1    |[0m 21/11/14 16:57:39 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 16:57:39 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 16:57:39 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 16:57:39 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 16:57:39 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] 21/11/14 16:57:37 INFO SignalUtils: Registered signal handler for TERM
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] 21/11/14 16:57:37 INFO SignalUtils: Registered signal handler for HUP
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] 21/11/14 16:57:37 INFO SignalUtils: Registered signal handler for INT
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] 21/11/14 16:57:37 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] 21/11/14 16:57:37 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] 21/11/14 16:57:37 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] 21/11/14 16:57:37 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] 21/11/14 16:57:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] 21/11/14 16:57:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] 21/11/14 16:57:38 INFO ApplicationMaster: ApplicationAttemptId: appattempt_1636909040339_0001_000001
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] 21/11/14 16:57:38 INFO RMProxy: Connecting to ResourceManager at /172.22.0.3:8030
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] 21/11/14 16:57:38 INFO YarnRMClient: Registering the ApplicationMaster
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] 21/11/14 16:57:38 INFO TransportClientFactory: Successfully created connection to /172.22.0.3:46583 after 86 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] 21/11/14 16:57:38 INFO ApplicationMaster: Preparing Local resources
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] 21/11/14 16:57:39 INFO ApplicationMaster: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] ===============================================================================
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] Default YARN executor launch context:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]   env:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]     CLASSPATH -> /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]     SPARK_YARN_STAGING_DIR -> hdfs://172.22.0.3/user/root/.sparkStaging/application_1636909040339_0001
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]     SPARK_USER -> root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]   command:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]     LD_LIBRARY_PATH=\"/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:$LD_LIBRARY_PATH\" \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       {{JAVA_HOME}}/bin/java \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       -server \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       -Xmx2048m \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       '-verbose:gc' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       '-XX:OnOutOfMemoryError=kill -9 %p' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       '-XX:+PrintGCDetails' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       '-XX:+PrintGCDateStamps' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       '-XX:+UseParallelGC' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       '-XX:InitiatingHeapOccupancyPercent=70' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       '-XX:ConcGCThreads=1' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       '-XX:ParallelGCThreads=3' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       -Djava.io.tmpdir={{PWD}}/tmp \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       '-Dspark.rpc.askTimeout=300s' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       '-Dspark.driver.port=46583' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       -Dspark.yarn.app.container.log.dir=<LOG_DIR> \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       org.apache.spark.executor.YarnCoarseGrainedExecutorBackend \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       --driver-url \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       spark://CoarseGrainedScheduler@172.22.0.3:46583 \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       --executor-id \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       <executorId> \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       --hostname \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       <hostname> \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       --cores \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       1 \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       --app-id \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       application_1636909040339_0001 \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       --resourceProfileId \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       0 \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       --user-class-path \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       file:$PWD/__app__.jar \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       1><LOG_DIR>/stdout \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]       2><LOG_DIR>/stderr
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]   resources:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]     __spark_libs__ -> resource { scheme: "hdfs" host: "172.22.0.3" port: -1 file: "/user/root/.sparkStaging/application_1636909040339_0001/__spark_libs__296927782304977530.zip" } size: 397064094 timestamp: 1636909051873 type: ARCHIVE visibility: PRIVATE
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr]     __spark_conf__ -> resource { scheme: "hdfs" host: "172.22.0.3" port: -1 file: "/user/root/.sparkStaging/application_1636909040339_0001/__spark_conf__.zip" } size: 251855 timestamp: 1636909052091 type: ARCHIVE visibility: PRIVATE
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] ===============================================================================
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] 21/11/14 16:57:39 INFO Configuration: resource-types.xml not found
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] 21/11/14 16:57:39 INFO ResourceUtils: Unable to find 'resource-types.xml'.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] 21/11/14 16:57:39 INFO YarnAllocator: Will request 2 executor container(s), each with 1 core(s) and 3287 MB memory (including 1239 MB of overhead)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] 21/11/14 16:57:39 INFO YarnAllocator: Submitted 2 unlocalized container requests.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000001/stderr] 21/11/14 16:57:39 INFO 2021-11-14 16:57:40,357 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1636909040339_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=
[36malgo-1    |[0m 2021-11-14 16:57:40,357 INFO rmcontainer.RMContainerImpl: container_1636909040339_0001_01_000002 Container Transitioned from NEW to ALLOCATED
[36malgo-1    |[0m 2021-11-14 16:57:40,357 INFO fica.FiCaSchedulerNode: Assigned container container_1636909040339_0001_01_000002 of capacity <memory:3287, vCores:1> on host algo-2:37609, which has 1 containers, <memory:3287, vCores:1> used and <memory:12605, vCores:3> available after allocation
[36malgo-1    |[0m 2021-11-14 16:57:40,357 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636909040339_0001	CONTAINERID=container_1636909040339_0001_01_000002	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 16:57:40,357 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.1316071 absoluteUsedCapacity=0.1316071 used=<memory:4183, vCores:2> cluster=<memory:31784, vCores:8>
[36malgo-1    |[0m 2021-11-14 16:57:40,357 INFO capacity.CapacityScheduler: Allocation proposal accepted
[36malgo-1    |[0m 2021-11-14 16:57:40,462 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1636909040339_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=
[36malgo-1    |[0m 2021-11-14 16:57:40,463 INFO rmcontainer.RMContainerImpl: container_1636909040339_0001_01_000003 Container Transitioned from NEW to ALLOCATED
[36malgo-1    |[0m 2021-11-14 16:57:40,463 INFO fica.FiCaSchedulerNode: Assigned container container_1636909040339_0001_01_000003 of capacity <memory:3287, vCores:1> on host algo-1:35169, which has 2 containers, <memory:4183, vCores:2> used and <memory:11709, vCores:2> available after allocation
[36malgo-1    |[0m 2021-11-14 16:57:40,463 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636909040339_0001	CONTAINERID=container_1636909040339_0001_01_000003	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 16:57:40,463 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.23502392 absoluteUsedCapacity=0.23502392 used=<memory:7470, vCores:3> cluster=<memory:31784, vCores:8>
[36malgo-1    |[0m 2021-11-14 16:57:40,463 INFO capacity.CapacityScheduler: Allocation proposal accepted
[36malgo-1    |[0m 21/11/14 16:57:40 INFO InMemoryFileIndex: It took 27 ms to list leaf files for 1 paths.
[36malgo-1    |[0m 2021-11-14 16:57:40,593 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-2:37609 for container : container_1636909040339_0001_01_000002
[36malgo-1    |[0m 2021-11-14 16:57:40,594 INFO rmcontainer.RMContainerImpl: container_1636909040339_0001_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
[36malgo-1    |[0m 2021-11-14 16:57:40,595 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:35169 for container : container_1636909040339_0001_01_000003
[36malgo-1    |[0m 2021-11-14 16:57:40,596 INFO rmcontainer.RMContainerImpl: container_1636909040339_0001_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
[36malgo-1    |[0m 21/11/14 16:57:40 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[36malgo-1    |[0m 2021-11-14 16:57:40,716 INFO ipc.Server: Auth successful for appattempt_1636909040339_0001_000001 (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 16:57:40,721 INFO containermanager.ContainerManagerImpl: Start request for container_1636909040339_0001_01_000003 by user root
[36malgo-1    |[0m 2021-11-14 16:57:40,722 INFO nodemanager.NMAuditLogger: USER=root	IP=172.22.0.3	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1636909040339_0001	CONTAINERID=container_1636909040339_0001_01_000003
[36malgo-1    |[0m 2021-11-14 16:57:40,722 INFO application.ApplicationImpl: Adding container_1636909040339_0001_01_000003 to application application_1636909040339_0001
[36malgo-1    |[0m 2021-11-14 16:57:40,723 INFO container.ContainerImpl: Container container_1636909040339_0001_01_000003 transitioned from NEW to LOCALIZING
[36malgo-1    |[0m 2021-11-14 16:57:40,723 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1636909040339_0001
[36malgo-1    |[0m 2021-11-14 16:57:40,724 INFO container.ContainerImpl: Container container_1636909040339_0001_01_000003 transitioned from LOCALIZING to SCHEDULED
[36malgo-1    |[0m 2021-11-14 16:57:40,724 INFO scheduler.ContainerScheduler: Starting container [container_1636909040339_0001_01_000003]
[36malgo-1    |[0m 2021-11-14 16:57:40,738 INFO container.ContainerImpl: Container container_1636909040339_0001_01_000003 transitioned from SCHEDULED to RUNNING
[36malgo-1    |[0m 2021-11-14 16:57:40,738 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1636909040339_0001_01_000003
[36malgo-1    |[0m 2021-11-14 16:57:40,741 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/container_1636909040339_0001_01_000003/default_container_executor.sh]
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/prelaunch.out
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/prelaunch.err
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/launch_container.sh
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/directory.info
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stdout
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr
[33malgo-2    |[0m 2021-11-14 16:57:40,795 INFO ipc.Server: Auth successful for appattempt_1636909040339_0001_000001 (auth:SIMPLE)
[33malgo-2    |[0m 2021-11-14 16:57:40,877 INFO containermanager.ContainerManagerImpl: Start request for container_1636909040339_0001_01_000002 by user root
[33malgo-2    |[0m 2021-11-14 16:57:40,926 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1636909040339_0001
[33malgo-2    |[0m 2021-11-14 16:57:40,934 INFO nodemanager.NMAuditLogger: USER=root	IP=172.22.0.3	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1636909040339_0001	CONTAINERID=container_1636909040339_0001_01_000002
[33malgo-2    |[0m 2021-11-14 16:57:40,934 INFO application.ApplicationImpl: Application application_1636909040339_0001 transitioned from NEW to INITING
[33malgo-2    |[0m 2021-11-14 16:57:40,935 INFO application.ApplicationImpl: Adding container_1636909040339_0001_01_000002 to application application_1636909040339_0001
[33malgo-2    |[0m 2021-11-14 16:57:40,939 INFO application.ApplicationImpl: Application application_1636909040339_0001 transitioned from INITING to RUNNING
[33malgo-2    |[0m 2021-11-14 16:57:40,943 INFO container.ContainerImpl: Container container_1636909040339_0001_01_000002 transitioned from NEW to LOCALIZING
[33malgo-2    |[0m 2021-11-14 16:57:40,943 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1636909040339_0001
[33malgo-2    |[0m 2021-11-14 16:57:40,953 INFO localizer.ResourceLocalizationService: Created localizer for container_1636909040339_0001_01_000002
[33malgo-2    |[0m 2021-11-14 16:57:41,021 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1636909040339_0001_01_000002.tokens
[33malgo-2    |[0m 2021-11-14 16:57:41,032 INFO nodemanager.DefaultContainerExecutor: Initializing user root
[33malgo-2    |[0m 2021-11-14 16:57:41,037 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1636909040339_0001_01_000002.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/container_1636909040339_0001_01_000002.tokens
[33malgo-2    |[0m 2021-11-14 16:57:41,038 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001
[36malgo-1    |[0m 2021-11-14 16:57:41,374 INFO rmcontainer.RMContainerImpl: container_1636909040339_0001_01_000002 Container Transitioned from ACQUIRED to RUNNING
[36malgo-1    |[0m 2021-11-14 16:57:41,464 INFO rmcontainer.RMContainerImpl: container_1636909040339_0001_01_000003 Container Transitioned from ACQUIRED to RUNNING
[36malgo-1    |[0m 2021-11-14 16:57:42,558 INFO monitor.ContainersMonitorImpl: container_1636909040339_0001_01_000003's ip = 172.22.0.3, and hostname = algo-1
[36malgo-1    |[0m 2021-11-14 16:57:42,564 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1636909040339_0001_01_000003 since CPU usage is not yet available.
[36malgo-1    |[0m 21/11/14 16:57:42 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 16:57:42 INFO FileSourceStrategy: Pushed Filters: 
[36malgo-1    |[0m 21/11/14 16:57:42 INFO FileSourceStrategy: Post-Scan Filters: 
[36malgo-1    |[0m 21/11/14 16:57:42 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[36malgo-1    |[0m 21/11/14 16:57:43 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 1239, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[36malgo-1    |[0m 21/11/14 16:57:43 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 317.1 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:57:43 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:57:43 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.22.0.3:41209 (size: 29.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:57:43 INFO SparkContext: Created broadcast 0 from json at HelloJavaSparkApp.java:46
[36malgo-1    |[0m 21/11/14 16:57:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 16:57:43 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 16:57:43 INFO SparkContext: Starting job: json at HelloJavaSparkApp.java:46
[36malgo-1    |[0m 21/11/14 16:57:43 INFO DAGScheduler: Got job 0 (json at HelloJavaSparkApp.java:46) with 1 output partitions
[36malgo-1    |[0m 21/11/14 16:57:43 INFO DAGScheduler: Final stage: ResultStage 0 (json at HelloJavaSparkApp.java:46)
[36malgo-1    |[0m 21/11/14 16:57:43 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:57:43 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:57:43 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at HelloJavaSparkApp.java:46), which has no missing parents
[36malgo-1    |[0m 2021-11-14 16:57:43,622 INFO scheduler.AppSchedulingInfo: checking for deactivate of application :application_1636909040339_0001
[36malgo-1    |[0m 21/11/14 16:57:43 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.4 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:57:43 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:57:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.22.0.3:41209 (size: 7.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:57:43 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:57:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at HelloJavaSparkApp.java:46) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 16:57:43 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:41 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 1268@algo-1
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:41 INFO SignalUtils: Registered signal handler for TERM
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:41 INFO SignalUtils: Registered signal handler for HUP
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:41 INFO SignalUtils: Registered signal handler for INT
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:42 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:42 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:42 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:42 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:43 INFO TransportClientFactory: Successfully created connection to /172.22.0.3:46583 after 99 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:43 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:43 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:43 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:43 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:43 INFO TransportClientFactory: Successfully created connection to /172.22.0.3:46583 after 1 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:43 INFO DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/blockmgr-a4ebc28a-f4da-46dc-8189-82ce2d350b75
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:43 INFO MemoryStore: MemoryStore started with capacity 912.3 M21/11/14 16:57:43 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.22.0.3:35260) with ID 2
[36malgo-1    |[0m 21/11/14 16:57:44 INFO BlockManagerMasterEndpoint: Registering block manager algo-1:41109 with 912.3 MiB RAM, BlockManagerId(2, algo-1, 41109, None)
[36malgo-1    |[0m 21/11/14 16:57:44 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7755 bytes)
[36malgo-1    |[0m 21/11/14 16:57:44 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:41109 (size: 7.2 KiB, free: 912.3 MiB)
[33malgo-2    |[0m 2021-11-14 16:57:44,628 INFO container.ContainerImpl: Container container_1636909040339_0001_01_000002 transitioned from LOCALIZING to SCHEDULED
[33malgo-2    |[0m 2021-11-14 16:57:44,629 INFO scheduler.ContainerScheduler: Starting container [container_1636909040339_0001_01_000002]
[33malgo-2    |[0m 2021-11-14 16:57:44,655 INFO container.ContainerImpl: Container container_1636909040339_0001_01_000002 transitioned from SCHEDULED to RUNNING
[33malgo-2    |[0m 2021-11-14 16:57:44,656 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1636909040339_0001_01_000002
[33malgo-2    |[0m 2021-11-14 16:57:44,660 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/container_1636909040339_0001_01_000002/default_container_executor.sh]
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/prelaunch.out
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/prelaunch.err
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/launch_container.sh
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/directory.info
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stdout
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr
[36malgo-1    |[0m iB
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:43 INFO YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@172.22.0.3:46583
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:43 INFO ResourceUtils: ==============================================================
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:43 INFO ResourceUtils: Resources for spark.executor:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:43 INFO ResourceUtils: ==============================================================
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:43 INFO YarnCoarseGrainedExecutorBackend: Successfully registered with driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:43 INFO Executor: Starting executor ID 2 on host algo-1
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41109.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:44 INFO NettyBlockTransferService: Server created on algo-1:41109
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(2, algo-1, 41109, None)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(2, algo-1, 41109, None)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(2, algo-1, 41109, None)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:44 INFO MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:44 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:44 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:44 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:44 INFO MetricsSystemImpl: s3a-file-system metrics system started
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:44 INFO Executor: Fetching spark://172.22.0.3:46583/jars/hello-java-spark-1.0-SNAPSHOT.jar with timestamp 1636909043485
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:44 INFO TransportClientFactory: Successfully created connection to /172.22.0.3:46583 after 1 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m [/var/log/yarn21/11/14 16:57:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:41109 (size: 29.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:57:45 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1783 ms on algo-1 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 16:57:45 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:57:45 INFO DAGScheduler: ResultStage 0 (json at HelloJavaSparkApp.java:46) finished in 2.337 s
[36malgo-1    |[0m 21/11/14 16:57:45 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:57:45 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished
[36malgo-1    |[0m 21/11/14 16:57:45 INFO DAGScheduler: Job 0 finished: json at HelloJavaSparkApp.java:46, took 2.382899 s
[36malgo-1    |[0m root
[36malgo-1    |[0m  |-- date: string (nullable = true)
[36malgo-1    |[0m  |-- sale: long (nullable = true)
[36malgo-1    |[0m 
[36malgo-1    |[0m 21/11/14 16:57:46 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.22.0.3:41209 in memory (size: 7.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:57:46 INFO BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:41109 in memory (size: 7.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:57:46 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.22.0.3:41209 in memory (size: 29.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:57:46 INFO BlockManagerInfo: Removed broadcast_0_piece0 on algo-1:41109 in memory (size: 29.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:57:46 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 16:57:46 INFO FileSourceStrategy: Pushed Filters: IsNotNull(sale),GreaterThan(sale,750)
[36malgo-1    |[0m 21/11/14 16:57:46 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(sale#8L),(sale#8L > 750)
[33malgo-2    |[0m 2021-11-14 16:57:46,435 INFO monitor.ContainersMonitorImpl: container_1636909040339_0001_01_000002's ip = 172.22.0.2, and hostname = algo-2
[36malgo-1    |[0m 21/11/14 16:57:46 INFO FileSourceStrategy: Output Data Schema: struct<date: string, sale: bigint>
[33malgo-2    |[0m 2021-11-14 16:57:46,441 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1636909040339_0001_01_000002 since CPU usage is not yet available.
[36malgo-1    |[0m 21/11/14 16:57:46 INFO CodeGenerator: Code generated in 279.888394 ms
[36malgo-1    |[0m 21/11/14 16:57:46 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 317.1 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:57:46 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:57:46 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.22.0.3:41209 (size: 29.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:57:46 INFO SparkContext: Created broadcast 2 from show at HelloJavaSparkApp.java:52
[36malgo-1    |[0m 21/11/14 16:57:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 16:57:46 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 16:57:46 INFO SparkContext: Starting job: show at HelloJavaSparkApp.java:52
[36malgo-1    |[0m 21/11/14 16:57:46 INFO DAGScheduler: Got job 1 (show at HelloJavaSparkApp.java:52) with 1 output partitions
[36malgo-1    |[0m 21/11/14 16:57:46 INFO DAGScheduler: Final stage: ResultStage 1 (show at HelloJavaSparkApp.java:52)
[36malgo-1    |[0m 21/11/14 16:57:46 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:57:46 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:57:46 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at show at HelloJavaSparkApp.java:52), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:57:46 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 18.4 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:57:46 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.1 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:57:46 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.22.0.3:41209 (size: 9.1 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:57:46 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:57:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at show at HelloJavaSparkApp.java:52) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 16:57:46 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 16:57:46 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7755 bytes)
[36malgo-1    |[0m 21/11/14 16:57:46 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:41109 (size: 9.1 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:57:47 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:41109 (size: 29.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:57:47 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 339 ms on algo-1 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 16:57:47 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:57:47 INFO DAGScheduler: ResultStage 1 (show at HelloJavaSparkApp.java:52) finished in 0.348 s
[36malgo-1    |[0m 21/11/14 16:57:47 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:57:47 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
[36malgo-1    |[0m 21/11/14 16:57:47 INFO DAGScheduler: Job 1 finished: show at HelloJavaSparkApp.java:52, took 0.351869 s
[36malgo-1    |[0m 21/11/14 16:57:47 INFO CodeGenerator: Code generated in 18.630828 ms
[36malgo-1    |[0m +----------+----+
[36malgo-1    |[0m |      date|sale|
[36malgo-1    |[0m +----------+----+
[36malgo-1    |[0m |2020-01-03| 999|
[36malgo-1    |[0m |2020-01-02| 999|
[36malgo-1    |[0m |2020-01-06| 998|
[36malgo-1    |[0m |2020-01-05| 998|
[36malgo-1    |[0m |2020-01-07| 996|
[36malgo-1    |[0m |2020-01-07| 994|
[36malgo-1    |[0m |2020-01-07| 993|
[36malgo-1    |[0m |2020-01-01| 993|
[36malgo-1    |[0m |2020-01-02| 991|
[36malgo-1    |[0m |2020-01-05| 990|
[36malgo-1    |[0m |2020-01-07| 990|
[36malgo-1    |[0m |2020-01-03| 989|
[36malgo-1    |[0m |2020-01-04| 988|
[36malgo-1    |[0m |2020-01-05| 988|
[36malgo-1    |[0m |2020-01-07| 985|
[36malgo-1    |[0m |2020-01-03| 985|
[36malgo-1    |[0m |2020-01-04| 982|
[36malgo-1    |[0m |2020-01-02| 981|
[36malgo-1    |[0m |2020-01-03| 980|
[36malgo-1    |[0m |2020-01-05| 979|
[36malgo-1    |[0m +----------+----+
[36malgo-1    |[0m only showing top 20 rows
[36malgo-1    |[0m 
[36malgo-1    |[0m 21/11/14 16:57:47 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 16:57:47 INFO FileSourceStrategy: Pushed Filters: 
[36malgo-1    |[0m 21/11/14 16:57:47 INFO FileSourceStrategy: Post-Scan Filters: 
[36malgo-1    |[0m 21/11/14 16:57:47 INFO FileSourceStrategy: Output Data Schema: struct<date: string, sale: bigint>
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:45 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 420@algo-2
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:45 INFO SignalUtils: Registered signal handler for TERM
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:45 INFO SignalUtils: Registered signal handler for HUP
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:45 INFO SignalUtils: Registered signal handler for INT
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:46 INFO SecurityManager: Changing view acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:46 INFO SecurityManager: Changing modify acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:46 INFO SecurityManager: Changing view acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:46 INFO SecurityManager: Changing modify acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:46 INFO TransportClientFactory: Successfully created connection to /172.22.0.3:46583 after 98 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:47 INFO SecurityManager: Changing view acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:47 INFO SecurityManager: Changing modify acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:47 INFO SecurityManager: Changing view acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:47 INFO SecurityManager: Changing modify acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:47 INFO TransportClientFactory: Successfully created connection to /172.22.0.3:46583 after 2 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:47 INFO DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/blockmgr-5126ecab-a9d3-4fff-824d-9cb8dc993e38
[36malgo-1    |[0m 21/11/14 16:57:47 INFO CodeGenerator: Code generated in 75.044084 ms
[36malgo-1    |[0m 21/11/14 16:57:47 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 317.1 KiB, free 1007.2 MiB)
[36malgo-1    |[0m 21/11/14 16:57:47 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 16:57:47 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.22.0.3:41209 (size: 29.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:57:47 INFO SparkContext: Created broadcast 4 from collectAsList at HelloJavaSparkApp.java:55
[36malgo-1    |[0m 21/11/14 16:57:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 16:57:47 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 16:57:47 INFO DAGScheduler: Registering RDD 11 (collectAsList at HelloJavaSparkApp.java:55) as input to shuffle 0
[36malgo-1    |[0m 21/11/14 16:57:47 INFO DAGScheduler: Got map stage job 2 (collectAsList at HelloJavaSparkApp.java:55) with 1 output partitions
[36malgo-1    |[0m 21/11/14 16:57:47 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (collectAsList at HelloJavaSparkApp.java:55)
[36malgo-1    |[0m 21/11/14 16:57:47 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:57:47 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:57:47 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at collectAsList at HelloJavaSparkApp.java:55), which has no missing parents
[36malgo-1    |[0m /export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:44 INFO Utils: Fetching spark://172.22.0.3:46583/jars/hello-java-spark-1.0-SNAPSHOT.jar to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/spark-5f73534b-a455-45f8-b69f-945e8dc3daf2/fetchFileTemp6438222838383939490.tmp
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:44 INFO Utils: Copying /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/spark-5f73534b-a455-45f8-b69f-945e8dc3daf2/12720616091636909043485_cache to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/container_1636909040339_0001_01_000003/./hello-java-spark-1.0-SNAPSHOT.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:44 INFO Executor: Adding file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/container_1636909040339_0001_01_000003/./hello-java-spark-1.0-SNAPSHOT.jar to class loader
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:44 INFO TorrentBroadcast: Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:44 INFO TransportClientFactory: Successfully created connection to /172.22.0.3:41209 after 1 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:44 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:44 INFO TorrentBroadcast: Reading broadcast variable 1 took 124 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:44 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.4 KiB, free 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:45 INFO FileScanRDD: TID: 0 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:45 INFO CodeGenerator: Code generated in 358.309847 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:45 INFO TorrentBroadcast: Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 912.2 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:45 INFO TorrentBroadcast: Reading broadcast variable 0 took 10 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:45 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 445.1 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:45 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2013 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:46 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 1
[36malgo-1    |[0m [/var/log/yarn/export21/11/14 16:57:47 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 29.7 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 16:57:47 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.9 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 16:57:47 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.22.0.3:41209 (size: 13.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:57:47 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:57:47 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at collectAsList at HelloJavaSparkApp.java:55) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 16:57:47 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 16:57:47 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7744 bytes)
[36malgo-1    |[0m 21/11/14 16:57:47 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:41109 (size: 13.9 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:57:47 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.22.0.2:45370) with ID 1
[36malgo-1    |[0m 21/11/14 16:57:47 INFO BlockManagerMasterEndpoint: Registering block manager algo-2:38733 with 912.3 MiB RAM, BlockManagerId(1, algo-2, 38733, None)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:41109 (size: 29.9 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 372 ms on algo-1 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:57:48 INFO DAGScheduler: ShuffleMapStage 2 (collectAsList at HelloJavaSparkApp.java:55) finished in 0.394 s
[36malgo-1    |[0m 21/11/14 16:57:48 INFO DAGScheduler: looking for newly runnable stages
[36malgo-1    |[0m 21/11/14 16:57:48 INFO DAGScheduler: running: Set()
[36malgo-1    |[0m 21/11/14 16:57:48 INFO DAGScheduler: waiting: Set()
[36malgo-1    |[0m 21/11/14 16:57:48 INFO DAGScheduler: failed: Set()
[36malgo-1    |[0m 21/11/14 16:57:48 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.22.0.3:41209 in memory (size: 13.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:41109 in memory (size: 13.9 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 16.
[36malgo-1    |[0m 21/11/14 16:57:48 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.22.0.3:41209 in memory (size: 9.1 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:41109 in memory (size: 9.1 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.22.0.3:41209 in memory (size: 29.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO BlockManagerInfo: Removed broadcast_2_piece0 on algo-1:41109 in memory (size: 29.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO CodeGenerator: Code generated in 29.370055 ms
[36malgo-1    |[0m 21/11/14 16:57:48 INFO CodeGenerator: Code generated in 12.507853 ms
[36malgo-1    |[0m 21/11/14 16:57:48 INFO SparkContext: Starting job: collectAsList at HelloJavaSparkApp.java:55
[36malgo-1    |[0m 21/11/14 16:57:48 INFO DAGScheduler: Got job 3 (collectAsList at HelloJavaSparkApp.java:55) with 15 output partitions
[36malgo-1    |[0m 21/11/14 16:57:48 INFO DAGScheduler: Final stage: ResultStage 4 (collectAsList at HelloJavaSparkApp.java:55)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:57:48 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[16] at collectAsList at HelloJavaSparkApp.java:55), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:57:48 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 31.1 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.8 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.22.0.3:41209 (size: 14.8 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:57:48 INFO DAGScheduler: Submitting 15 missing tasks from ResultStage 4 (MapPartitionsRDD[16] at collectAsList at HelloJavaSparkApp.java:55) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[36malgo-1    |[0m 21/11/14 16:57:48 INFO YarnScheduler: Adding task set 4.0 with 15 tasks
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3, algo-2, executor 1, partition 0, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 4, algo-1, executor 2, partition 1, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:41109 (size: 14.8 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.22.0.3:35260
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 5, algo-1, executor 2, partition 2, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 4) in 258 ms on algo-1 (executor 2) (1/15)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 6, algo-1, executor 2, partition 3, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 5) in 34 ms on algo-1 (executor 2) (2/15)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 7, algo-1, executor 2, partition 4, PROCESS_LOCAL, 7336 bytes)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:47 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:47 INFO YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@172.22.0.3:46583
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:47 INFO ResourceUtils: ==============================================================
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:47 INFO ResourceUtils: Resources for spark.executor:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:47 INFO ResourceUtils: ==============================================================
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:47 INFO YarnCoarseGrainedExecutorBackend: Successfully registered with driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:47 INFO Executor: Starting executor ID 1 on host algo-2
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38733.
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:47 INFO NettyBlockTransferService: Server created on algo-2:38733
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:47 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:47 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(1, algo-2, 38733, None)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:47 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(1, algo-2, 38733, None)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:47 INFO BlockManager: Initialized BlockManager: BlockManagerId(1, algo-2, 38733, None)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:48 INFO MetricsConfig: Loaded properties from hadoop-metrics2.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:48 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:48 INFO MetricsSystemImpl: s3a-file-system metrics system started
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 3
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:48 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:48 INFO Executor: Fetching spark://172.22.0.3:46583/jars/hello-java-spark-1.0-SNAPSHOT.jar with timestamp 1636909043485
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:48 INFO TransportClientFactory: Successfully created connection to /172.22.0.3:46583 after 1 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 6) in 26 ms on algo-1 (executor 2) (3/15)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 8, algo-1, executor 2, partition 5, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 7) in 25 ms on algo-1 (executor 2) (4/15)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-2:38733 (size: 14.8 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 9, algo-1, executor 2, partition 6, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 8) in 21 ms on algo-1 (executor 2) (5/15)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 10, algo-1, executor 2, partition 7, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 9) in 26 ms on algo-1 (executor 2) (6/15)
[36malgo-1    |[0m /userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:46 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:46 INFO TorrentBroadcast: Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:46 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.1 KiB, free 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:46 INFO TorrentBroadcast: Reading broadcast variable 3 took 10 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:46 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 18.4 KiB, free 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:47 INFO CodeGenerator: Code generated in 46.200395 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:47 INFO CodeGenerator: Code generated in 12.273882 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:47 INFO CodeGenerator: Code generated in 9.026271 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:47 INFO FileScanRDD: TID: 1 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:47 INFO CodeGenerator: Code generated in 9.30203 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:47 INFO TorrentBroadcast: Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:47 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 912.2 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:47 INFO TorrentBroadcast: Reading broadcast variable 2 took 10 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:47 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 445.1 KiB, free 911.7 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:47 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2327 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:47 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 2
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:47 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:47 INFO TorrentBroadcast: Started reading broadcast variable 5 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:47 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.9 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:47 INFO TorrentBroadcast: Reading broadcast variable 5 took 11 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:47 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 29.7 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:47 INFO CodeGenerator: Code generated in 58.028747 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO CodeGenerator: Code generated in 20.598156 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO CodeGenerator: Code generated in 8.021823 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO CodeGenerator: Code generated in 11.771162 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO FileScanRDD: TID: 2 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO TorrentBroadcast: Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 879.5 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO TorrentBroadcast: Reading broadcast variable 4 took 10 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 445.1 KiB, free 879.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 4574 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 4
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Running task 1.0 in stage 4.0 (TID 4)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO TorrentBroadcast: Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.8 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO TorrentBroadcast: Reading broadcast variable 6 took 10 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 31.1 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@172.22.0.3:46583)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO MapOutputTrackerWorker: Got the output locations
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO CodeGenerator: Code generated in 29.22313 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Finished task 1.0 in stage 4.0 (TID 4). 3890 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 5
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Running task 2.0 in stage 4.0 (TID 5)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Finished task 2.0 in stage 4.0 (TID 5). 3723 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 6
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Running task 3.0 in stage 4.0 (TID 6)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Finished task 3.0 in stage 4.0 (TID 6). 3890 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 7
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Running task 4.0 in stage 4.0 (TID 7)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application21/11/14 16:57:48 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 11, algo-1, executor 2, partition 8, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 10) in 20 ms on algo-1 (executor 2) (7/15)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 12, algo-1, executor 2, partition 9, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 11) in 17 ms on algo-1 (executor 2) (8/15)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 13, algo-1, executor 2, partition 10, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 12) in 21 ms on algo-1 (executor 2) (9/15)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 14, algo-1, executor 2, partition 11, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 13) in 18 ms on algo-1 (executor 2) (10/15)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 15, algo-1, executor 2, partition 12, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 14) in 18 ms on algo-1 (executor 2) (11/15)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 16, algo-1, executor 2, partition 13, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 15) in 19 ms on algo-1 (executor 2) (12/15)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 17, algo-1, executor 2, partition 14, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 16) in 25 ms on algo-1 (executor 2) (13/15)
[36malgo-1    |[0m 21/11/14 16:57:48 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 17) in 24 ms on algo-1 (executor 2) (14/15)
[36malgo-1    |[0m 21/11/14 16:57:49 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.22.0.2:45370
[36malgo-1    |[0m _1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Finished task 4.0 in stage 4.0 (TID 7). 3723 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 8
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Running task 5.0 in stage 4.0 (TID 8)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Finished task 5.0 in stage 4.0 (TID 8). 3890 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 9
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Running task 6.0 in stage 4.0 (TID 9)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Finished task 6.0 in stage 4.0 (TID 9). 3723 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 10
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Running task 7.0 in stage 4.0 (TID 10)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Finished task 7.0 in stage 4.0 (TID 10). 3890 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 11
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Running task 8.0 in stage 4.0 (TID 11)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Finished task 8.0 in stage 4.0 (TID 11). 3723 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 12
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Running task 9.0 in stage 4.0 (TID 12)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Finished task 9.0 in stage 4.0 (TID 12). 3890 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 13
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Running task 10.0 in stage 4.0 (TID 13)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Finished task 10.0 in stage 4.0 (TID 13). 3723 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 14
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Running task 11.0 in stage 4.0 (TID 14)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Finished task 11.0 in stage 4.0 (TID 14). 3890 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 15
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Running task 12.0 in stage 4.0 (TID 15)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_163690921/11/14 16:57:50 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 1805 ms on algo-2 (executor 1) (15/15)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: ResultStage 4 (collectAsList at HelloJavaSparkApp.java:55) finished in 1.814 s
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:57:50 INFO YarnScheduler: Killing all running tasks in stage 4: Stage finished
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: Job 3 finished: collectAsList at HelloJavaSparkApp.java:55, took 1.823953 s
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: Registering RDD 17 (collectAsList at HelloJavaSparkApp.java:55) as input to shuffle 1
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: Got map stage job 4 (collectAsList at HelloJavaSparkApp.java:55) with 15 output partitions
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (collectAsList at HelloJavaSparkApp.java:55)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[17] at collectAsList at HelloJavaSparkApp.java:55), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:57:50 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 31.9 KiB, free 1007.4 MiB)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 15.2 KiB, free 1007.4 MiB)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.22.0.3:41209 (size: 15.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: Submitting 15 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[17] at collectAsList at HelloJavaSparkApp.java:55) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[36malgo-1    |[0m 21/11/14 16:57:50 INFO YarnScheduler: Adding task set 6.0 with 15 tasks
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 18, algo-2, executor 1, partition 0, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 19, algo-1, executor 2, partition 1, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:41109 (size: 15.2 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-2:38733 (size: 15.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Starting task 2.0 in stage 6.0 (TID 20, algo-1, executor 2, partition 2, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 19) in 72 ms on algo-1 (executor 2) (1/15)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 21, algo-1, executor 2, partition 3, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Finished task 2.0 in stage 6.0 (TID 20) in 18 ms on algo-1 (executor 2) (2/15)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Starting task 4.0 in stage 6.0 (TID 22, algo-1, executor 2, partition 4, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Finished task 3.0 in stage 6.0 (TID 21) in 20 ms on algo-1 (executor 2) (3/15)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Starting task 5.0 in stage 6.0 (TID 23, algo-1, executor 2, partition 5, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Finished task 4.0 in stage 6.0 (TID 22) in 16 ms on algo-1 (executor 2) (4/15)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Starting task 6.0 in stage 6.0 (TID 24, algo-2, executor 1, partition 6, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 18) in 132 ms on algo-2 (executor 1) (5/15)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Starting task 7.0 in stage 6.0 (TID 25, algo-1, executor 2, partition 7, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Finished task 5.0 in stage 6.0 (TID 23) in 29 ms on algo-1 (executor 2) (6/15)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Starting task 8.0 in stage 6.0 (TID 26, algo-1, executor 2, partition 8, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Finished task 7.0 in stage 6.0 (TID 25) in 33 ms on algo-1 (executor 2) (7/15)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Starting task 9.0 in stage 6.0 (TID 27, algo-2, executor 1, partition 9, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Finished task 6.0 in stage 6.0 (TID 24) in 60 ms on algo-2 (executor 1) (8/15)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Starting task 10.0 in stage 6.0 (TID 28, algo-1, executor 2, partition 10, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Finished task 8.0 in stage 6.0 (TID 26) in 20 ms on algo-1 (executor 2) (9/15)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Starting task 11.0 in stage 6.0 (TID 29, algo-1, executor 2, partition 11, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Finished task 10.0 in stage 6.0 (TID 28) in 18 ms on algo-1 (executor 2) (10/15)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Starting task 12.0 in stage 6.0 (TID 30, algo-1, executor 2, partition 12, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Finished task 11.0 in stage 6.0 (TID 29) in 19 ms on algo-1 (executor 2) (11/15)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Starting task 13.0 in stage 6.0 (TID 31, algo-1, executor 2, partition 13, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Finished task 12.0 in stage 6.0 (TID 30) in 18 ms on algo-1 (executor 2) (12/15)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Starting task 14.0 in stage 6.0 (TID 32, algo-2, executor 1, partition 14, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Finished task 9.0 in stage 6.0 (TID 27) in 80 ms on algo-2 (executor 1) (13/15)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Finished task 13.0 in stage 6.0 (TID 31) in 20 ms on algo-1 (executor 2) (14/15)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Finished task 14.0 in stage 6.0 (TID 32) in 26 ms on algo-2 (executor 1) (15/15)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: ShuffleMapStage 6 (collectAsList at HelloJavaSparkApp.java:55) finished in 0.309 s
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: looking for newly runnable stages
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: running: Set()
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: waiting: Set()
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: failed: Set()
[36malgo-1    |[0m 21/11/14 16:57:50 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 16.
[36malgo-1    |[0m 21/11/14 16:57:50 INFO CodeGenerator: Code generated in 14.699833 ms
[36malgo-1    |[0m 21/11/14 16:57:50 INFO SparkContext: Starting job: collectAsList at HelloJavaSparkApp.java:55
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: Got job 5 (collectAsList at HelloJavaSparkApp.java:55) with 7 output partitions
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: Final stage: ResultStage 9 (collectAsList at HelloJavaSparkApp.java:55)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[20] at collectAsList at HelloJavaSparkApp.java:55), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:57:50 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 26.7 KiB, free 1007.4 MiB)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 1007.4 MiB)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.22.0.3:41209 (size: 13.4 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: Submitting 7 missing tasks from ResultStage 9 (MapPartitionsRDD[20] at collectAsList at HelloJavaSparkApp.java:55) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))
[36malgo-1    |[0m 21/11/14 16:57:50 INFO YarnScheduler: Adding task set 9.0 with 7 tasks
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 33, algo-1, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Starting task 3.0 in stage 9.0 (TID 34, algo-2, executor 1, partition 3, NODE_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:41109 (size: 13.4 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-2:38733 (size: 13.4 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.22.0.3:35260
[36malgo-1    |[0m 21/11/14 16:57:50 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.22.0.2:45370
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 35, algo-1, executor 2, partition 1, NODE_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 33) in 80 ms on algo-1 (executor 2) (1/7)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 36, algo-1, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 35) in 18 ms on algo-1 (executor 2) (2/7)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Starting task 4.0 in stage 9.0 (TID 37, algo-1, executor 2, partition 4, NODE_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Finished task 2.0 in stage 9.0 (TID 36) in 19 ms on algo-1 (executor 2) (3/7)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:48 INFO Utils: Fetching spark://172.22.0.3:46583/jars/hello-java-spark-1.0-SNAPSHOT.jar to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/spark-5fd69e9a-de02-4929-8c4c-4040bff099a3/fetchFileTemp3607921157109161727.tmp
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:48 INFO Utils: Copying /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/spark-5fd69e9a-de02-4929-8c4c-4040bff099a3/12720616091636909043485_cache to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/container_1636909040339_0001_01_000002/./hello-java-spark-1.0-SNAPSHOT.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:48 INFO Executor: Adding file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/container_1636909040339_0001_01_000002/./hello-java-spark-1.0-SNAPSHOT.jar to class loader
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:48 INFO MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:48 INFO TorrentBroadcast: Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:48 INFO TransportClientFactory: Successfully created connection to algo-1/172.22.0.3:41109 after 1 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:48 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.8 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:48 INFO TorrentBroadcast: Reading broadcast variable 6 took 133 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:48 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 31.1 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:49 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:49 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@172.22.0.3:46583)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:49 INFO MapOutputTrackerWorker: Got the output locations
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:49 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO CodeGenerator: Code generated in 382.349399 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO CodeGenerator: Code generated in 11.084386 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO CodeGenerator: Code generated in 6.896897 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO CodeGenerator: Code generated in 8.368356 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 3766 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 18
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO Executor: Running task 0.0 in stage 6.0 (TID 18)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO TorrentBroadcast: Started reading broadcast variable 7 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO TransportClientFactory: Successfully created connection to /172.22.0.3:41209 after 1 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 15.2 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO TorrentBroadcast: Reading broadcast variable 7 took 11 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 31.9 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO CodeGenerator: Code generated in 13.089992 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO Executor: Finished task 0.0 in stage 6.0 (TID 18). 3766 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 24
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO Executor: Running task 6.0 in stage 6.0 (TID 24)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO Executor: Finished task 6.0 in stage 6.0 (TID 24). 3766 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 27
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO Executor: Running task 9.0 in stage 6.0 (TID 27)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 12 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO Executor: Finished task 9.0 in stage 6.0 (TID 27). 3902 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 32
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO Executor: Running task 14.0 in stage 6.0 (TID 32)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO Executor: Finished task 14.0 in stage 6.0 (TID 32). 3766 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 34
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO Executor: Running task 3.0 in stage 9.0 (TID 34)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO MapOutputTrackerWorker: Updating epoch to 2 and clearing cache
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO TorrentBroadcast: Started reading broadcast variable 8 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO TorrentBroadcast: Reading broadcast variable 8 took 14 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 26.7 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@172.22.0.3:46583)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO MapOutputTrackerWorker: Got the output locations
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Starting task 5.0 in stage 9.0 (TID 38, algo-1, executor 2, partition 5, NODE_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Finished task 4.0 in stage 9.0 (TID 37) in 14 ms on algo-1 (executor 2) (4/7)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Starting task 6.0 in stage 9.0 (TID 39, algo-1, executor 2, partition 6, NODE_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Finished task 5.0 in stage 9.0 (TID 38) in 34 ms on algo-1 (executor 2) (5/7)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Finished task 6.0 in stage 9.0 (TID 39) in 13 ms on algo-1 (executor 2) (6/7)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Finished task 3.0 in stage 9.0 (TID 34) in 180 ms on algo-2 (executor 1) (7/7)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: ResultStage 9 (collectAsList at HelloJavaSparkApp.java:55) finished in 0.190 s
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:57:50 INFO YarnScheduler: Killing all running tasks in stage 9: Stage finished
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: Job 5 finished: collectAsList at HelloJavaSparkApp.java:55, took 0.201031 s
[36malgo-1    |[0m 21/11/14 16:57:50 INFO CodeGenerator: Code generated in 11.082238 ms
[36malgo-1    |[0m Collected average sales: [[2020-01-01,490.1985294117647], [2020-01-02,472.625], [2020-01-03,474.6470588235294], [2020-01-04,491.67973856209153], [2020-01-05,471.24691358024694], [2020-01-06,518.7692307692307], [2020-01-07,533.9855072463768]]
[36malgo-1    |[0m 21/11/14 16:57:50 WARN SimpleFunctionRegistry: The function double replaced a previously registered function.
[36malgo-1    |[0m 040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Finished task 12.0 in stage 4.0 (TID 15). 3723 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 16
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Running task 13.0 in stage 4.0 (TID 16)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Finished task 13.0 in stage 4.0 (TID 16). 3890 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 17
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Running task 14.0 in stage 4.0 (TID 17)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:48 INFO Executor: Finished task 14.0 in stage 4.0 (TID 17). 3723 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 19
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Running task 1.0 in stage 6.0 (TID 19)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO TorrentBroadcast: Started reading broadcast variable 7 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 15.2 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO TorrentBroadcast: Reading broadcast variable 7 took 7 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 31.9 KiB, free 911.7 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO CodeGenerator: Code generated in 7.859564 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Finished task 1.0 in stage 6.0 (TID 19). 3902 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 20
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Running task 2.0 in stage 6.0 (TID 20)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Finished task 2.0 in stage 6.0 (TID 20). 3766 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 21
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Running task 3.0 in stage 6.0 (TID 21)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Finished task 3.0 in stage 6.0 (TID 21). 3902 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 22
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Running task 4.0 in stage 6.0 (TID 22)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Finished task 4.0 in stage 6.0 (TID 22). 3766 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 23
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Running task 5.0 in stage 6.0 (TID 23)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Finished task 5.0 in stage 6.0 (TID 23). 3902 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 25
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Running task 7.0 in stage 6.0 (TID 25)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Finished task 7.0 in stage 6.0 (TID 25). 3902 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 26
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Running task 8.0 in stage 6.0 (TID 26)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Finished task 8.0 in stage 6.0 (TID 26). 3766 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 28
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Running task 10.0 in stage 6.0 (TID 28)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Finished task 10.0 in stage 6.0 (TID 28). 3766 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 29
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Running task 11.0 in stage 6.0 (TID 29)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Finished task 11.0 in stage 6.0 (TID 29). 3902 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 30
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Running task 12.0 in stage 6.0 (TID 30)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Finished task 12.0 in stage 6.0 (TID 30). 3766 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 31
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Running task 13.0 in stage 6.0 (TID 31)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Finished task 13.0 in stage 6.0 (TID 31). 3902 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 33
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Running task 0.0 in stage 9.0 (TID 33)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO MapOutputTrackerWorker: Updating epoch to 2 and clearing cache
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO TorrentBroadcast: Started reading broadcast variable 8 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 911.7 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO TorrentBroadcast: Reading broadcast variable 8 took 9 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 26.7 KiB, free 911.7 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@172.22.0.3:46583)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO MapOutputTrackerWorker: Got the output locations
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO CodeGenerator: Code generated in 19.616921 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO CodeGenerator: Code generated in 17.589955 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Finished task 0.0 in stage 9.0 (TID 33). 4645 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 35
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Running task 1.0 in stage 9.0 (TID 35)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Finished task 1.0 in stage 9.0 (TID 35). 4642 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 36
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Running task 2.0 in stage 9.0 (TID 36)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_16369090403321/11/14 16:57:50 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 16:57:50 INFO FileSourceStrategy: Pushed Filters: 
[36malgo-1    |[0m 21/11/14 16:57:50 INFO FileSourceStrategy: Post-Scan Filters: 
[36malgo-1    |[0m 21/11/14 16:57:50 INFO FileSourceStrategy: Output Data Schema: struct<date: string, sale: bigint>
[36malgo-1    |[0m 21/11/14 16:57:50 INFO CodeGenerator: Code generated in 9.459407 ms
[36malgo-1    |[0m 21/11/14 16:57:50 INFO CodeGenerator: Code generated in 15.848737 ms
[36malgo-1    |[0m 21/11/14 16:57:50 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 317.1 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 1007.0 MiB)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.22.0.3:41209 (size: 29.9 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO SparkContext: Created broadcast 9 from show at HelloJavaSparkApp.java:63
[36malgo-1    |[0m 21/11/14 16:57:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 16:57:50 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 16:57:50 INFO SparkContext: Starting job: show at HelloJavaSparkApp.java:63
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: Got job 6 (show at HelloJavaSparkApp.java:63) with 1 output partitions
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: Final stage: ResultStage 10 (show at HelloJavaSparkApp.java:63)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[25] at show at HelloJavaSparkApp.java:63), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:57:50 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 15.8 KiB, free 1007.0 MiB)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 1007.0 MiB)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.22.0.3:41209 (size: 7.6 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:57:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[25] at show at HelloJavaSparkApp.java:63) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 16:57:50 INFO YarnScheduler: Adding task set 10.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 16:57:50 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 40, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7755 bytes)
[36malgo-1    |[0m 21/11/14 16:57:50 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:41109 (size: 7.6 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:41109 (size: 29.9 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 40) in 316 ms on algo-1 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:57:51 INFO DAGScheduler: ResultStage 10 (show at HelloJavaSparkApp.java:63) finished in 0.327 s
[36malgo-1    |[0m 21/11/14 16:57:51 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:57:51 INFO YarnScheduler: Killing all running tasks in stage 10: Stage finished
[36malgo-1    |[0m 21/11/14 16:57:51 INFO DAGScheduler: Job 6 finished: show at HelloJavaSparkApp.java:63, took 0.330142 s
[36malgo-1    |[0m 21/11/14 16:57:51 INFO CodeGenerator: Code generated in 14.900642 ms
[36malgo-1    |[0m 21/11/14 16:57:51 INFO CodeGenerator: Code generated in 11.500205 ms
[36malgo-1    |[0m +----------+----+-----------+
[36malgo-1    |[0m |      date|sale|sale_double|
[36malgo-1    |[0m +----------+----+-----------+
[36malgo-1    |[0m |2020-01-01|   5|         10|
[36malgo-1    |[0m |2020-01-01|   7|         14|
[36malgo-1    |[0m |2020-01-01|  15|         30|
[36malgo-1    |[0m |2020-01-01|  15|         30|
[36malgo-1    |[0m |2020-01-01|  23|         46|
[36malgo-1    |[0m |2020-01-01|  33|         66|
[36malgo-1    |[0m |2020-01-01|  34|         68|
[36malgo-1    |[0m |2020-01-01|  39|         78|
[36malgo-1    |[0m |2020-01-01|  40|         80|
[36malgo-1    |[0m |2020-01-01|  43|         86|
[36malgo-1    |[0m |2020-01-01|  48|         96|
[36malgo-1    |[0m |2020-01-01|  59|        118|
[36malgo-1    |[0m |2020-01-01|  64|        128|
[36malgo-1    |[0m |2020-01-01|  79|        158|
[36malgo-1    |[0m |2020-01-01|  82|        164|
[36malgo-1    |[0m |2020-01-01|  85|        170|
[36malgo-1    |[0m |2020-01-01| 103|        206|
[36malgo-1    |[0m |2020-01-01| 105|        210|
[36malgo-1    |[0m |2020-01-01| 105|        210|
[36malgo-1    |[0m |2020-01-01| 122|        244|
[36malgo-1    |[0m +----------+----+-----------+
[36malgo-1    |[0m only showing top 20 rows
[36malgo-1    |[0m 
[36malgo-1    |[0m Writing output to: file:///opt/ml/processing/output/data
[36malgo-1    |[0m 21/11/14 16:57:51 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.22.0.3:41209 in memory (size: 7.6 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:41109 in memory (size: 7.6 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 16:57:51 INFO FileSourceStrategy: Pushed Filters: 
[36malgo-1    |[0m 21/11/14 16:57:51 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.22.0.3:41209 in memory (size: 15.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO FileSourceStrategy: Post-Scan Filters: 
[36malgo-1    |[0m 21/11/14 16:57:51 INFO FileSourceStrategy: Output Data Schema: struct<date: string, sale: bigint>
[36malgo-1    |[0m 21/11/14 16:57:51 INFO BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:41109 in memory (size: 15.2 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO BlockManagerInfo: Removed broadcast_7_piece0 on algo-2:38733 in memory (size: 15.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.22.0.3:41209 in memory (size: 14.8 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:41109 in memory (size: 14.8 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO BlockManagerInfo: Removed broadcast_6_piece0 on algo-2:38733 in memory (size: 14.8 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.22.0.3:41209 in memory (size: 13.4 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:41109 in memory (size: 13.4 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO BlockManagerInfo: Removed broadcast_8_piece0 on algo-2:38733 in memory (size: 13.4 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
[36malgo-1    |[0m 21/11/14 16:57:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[36malgo-1    |[0m 21/11/14 16:57:51 INFO DirectFileOutputCommitter: Direct Write: DISABLED
[36malgo-1    |[0m 21/11/14 16:57:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter
[36malgo-1    |[0m 21/11/14 16:57:51 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 317.1 KiB, free 1006.9 MiB)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.22.0.3:41209 (size: 29.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO SparkContext: Created broadcast 11 from json at HelloJavaSparkApp.java:66
[36malgo-1    |[0m 21/11/14 16:57:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 16:57:51 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 16:57:51 INFO SparkContext: Starting job: json at HelloJavaSparkApp.java:66
[36malgo-1    |[0m 21/11/14 16:57:51 INFO DAGScheduler: Got job 7 (json at HelloJavaSparkApp.java:66) with 1 output partitions
[36malgo-1    |[0m 21/11/14 16:57:51 INFO DAGScheduler: Final stage: ResultStage 11 (json at HelloJavaSparkApp.java:66)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:57:51 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:57:51 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[31] at json at HelloJavaSparkApp.java:66), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:57:51 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 15.1 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.22.0.3:41209 (size: 7.5 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:57:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[31] at json at HelloJavaSparkApp.java:66) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 16:57:51 INFO YarnScheduler: Adding task set 11.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 16:57:51 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 41, algo-2, executor 1, partition 0, PROCESS_LOCAL, 7755 bytes)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-2:38733 (size: 7.5 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-2:38733 (size: 29.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 9_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Finished task 2.0 in stage 9.0 (TID 36). 4645 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 37
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Running task 4.0 in stage 9.0 (TID 37)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Finished task 4.0 in stage 9.0 (TID 37). 4646 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 38
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Running task 5.0 in stage 9.0 (TID 38)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Finished task 5.0 in stage 9.0 (TID 38). 4646 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 39
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Running task 6.0 in stage 9.0 (TID 39)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Finished task 6.0 in stage 9.0 (TID 39). 4646 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 WARN YarnCoarseGrainedExecutorBackend: eagerFSInit: Unable to eagerly init filesystem s3://does/not/exist
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] java.nio.file.AccessDeniedException: does: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@29de54b4: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@5480d6b0: Failed to connect to service endpoint: ]
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:187)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:111)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:380)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:314)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3358)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:123)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3407)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3375)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:486)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at org.apache.spark.executor.CoarseGrainedExecutorBackend.$anonfun$eagerFSInit$1(CoarseGrainedExecutorBackend.scala:274)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at scala.collection.parallel.mutable.ParArray$ParArrayIterator.flatmap2combiner(ParArray.scala:419)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at scala.collection.parallel.ParIterableLike$FlatMap.leaf(ParIterableLike.scala:1074)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at scala.collection.parallel.ParIterableLike$FlatMap.tryLeaf(ParIterableLike.scala:1070)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at scala.collection.parallel.FutureTasks.$anonfun$exec$5(Tasks.scala:499)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at scala.util.Success.$anonfun$map$1(Try.scala:255)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at scala.util.Success.map(Try.scala:213)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at java.lang.Thread.run(Thread.java:748)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] Caused by: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@29de54b4: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@5480d6b0: Failed to connect to service endpoint: ]
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:159)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1257)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:833)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:783)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5062)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:5850)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:5823)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5046)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5008)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1416)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1352)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$verifyBucketExists$1(S3AFileSystem.java:381)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	... 32 more
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] Caused by: com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@29de54b4: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@5480d6b0: Failed to connect to service endpoint: ]
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/app21/11/14 16:57:51 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 41) in 424 ms on algo-2 (executor 1) (1/1)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:57:51 INFO DAGScheduler: ResultStage 11 (json at HelloJavaSparkApp.java:66) finished in 0.428 s
[36malgo-1    |[0m 21/11/14 16:57:51 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:57:51 INFO YarnScheduler: Killing all running tasks in stage 11: Stage finished
[36malgo-1    |[0m 21/11/14 16:57:51 INFO DAGScheduler: Job 7 finished: json at HelloJavaSparkApp.java:66, took 0.431598 s
[36malgo-1    |[0m 21/11/14 16:57:51 INFO DAGScheduler: Registering RDD 32 (json at HelloJavaSparkApp.java:66) as input to shuffle 2
[36malgo-1    |[0m 21/11/14 16:57:51 INFO DAGScheduler: Got map stage job 8 (json at HelloJavaSparkApp.java:66) with 1 output partitions
[36malgo-1    |[0m 21/11/14 16:57:51 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (json at HelloJavaSparkApp.java:66)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 16:57:51 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:57:51 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[32] at json at HelloJavaSparkApp.java:66), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:57:51 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 70.4 KiB, free 1006.7 MiB)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 15.1 KiB, free 1006.7 MiB)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.22.0.3:41209 (size: 15.1 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:57:51 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[32] at json at HelloJavaSparkApp.java:66) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 16:57:51 INFO YarnScheduler: Adding task set 12.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 16:57:51 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 42, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7744 bytes)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:41109 (size: 15.1 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:57:51 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:41109 (size: 29.9 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 16:57:52 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 42) in 104 ms on algo-1 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 16:57:52 INFO YarnScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:57:52 INFO DAGScheduler: ShuffleMapStage 12 (json at HelloJavaSparkApp.java:66) finished in 0.111 s
[36malgo-1    |[0m 21/11/14 16:57:52 INFO DAGScheduler: looking for newly runnable stages
[36malgo-1    |[0m 21/11/14 16:57:52 INFO DAGScheduler: running: Set()
[36malgo-1    |[0m 21/11/14 16:57:52 INFO DAGScheduler: waiting: Set()
[36malgo-1    |[0m 21/11/14 16:57:52 INFO DAGScheduler: failed: Set()
[36malgo-1    |[0m 21/11/14 16:57:52 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 2155.
[36malgo-1    |[0m 21/11/14 16:57:52 INFO CodeGenerator: Code generated in 12.995797 ms
[36malgo-1    |[0m 21/11/14 16:57:52 INFO SparkContext: Starting job: json at HelloJavaSparkApp.java:66
[36malgo-1    |[0m 21/11/14 16:57:52 INFO DAGScheduler: Got job 9 (json at HelloJavaSparkApp.java:66) with 1 output partitions
[36malgo-1    |[0m 21/11/14 16:57:52 INFO DAGScheduler: Final stage: ResultStage 14 (json at HelloJavaSparkApp.java:66)
[36malgo-1    |[0m 21/11/14 16:57:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)
[36malgo-1    |[0m 21/11/14 16:57:52 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 16:57:52 INFO DAGScheduler: Submitting ResultStage 14 (CoalescedRDD[36] at json at HelloJavaSparkApp.java:66), which has no missing parents
[36malgo-1    |[0m 21/11/14 16:57:52 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 193.4 KiB, free 1006.5 MiB)
[36malgo-1    |[0m 21/11/14 16:57:52 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 71.6 KiB, free 1006.5 MiB)
[36malgo-1    |[0m 21/11/14 16:57:52 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.22.0.3:41209 (size: 71.6 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 16:57:52 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 16:57:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (CoalescedRDD[36] at json at HelloJavaSparkApp.java:66) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 16:57:52 INFO YarnScheduler: Adding task set 14.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 16:57:52 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 43, algo-1, executor 2, partition 0, NODE_LOCAL, 8312 bytes)
[36malgo-1    |[0m 21/11/14 16:57:52 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:41109 (size: 71.6 KiB, free: 912.1 MiB)
[36malgo-1    |[0m 21/11/14 16:57:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.22.0.3:35260
[36malgo-1    |[0m 21/11/14 16:57:52 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 43) in 277 ms on algo-1 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 16:57:52 INFO YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 16:57:52 INFO DAGScheduler: ResultStage 14 (json at HelloJavaSparkApp.java:66) finished in 0.299 s
[36malgo-1    |[0m 21/11/14 16:57:52 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 16:57:52 INFO YarnScheduler: Killing all running tasks in stage 14: Stage finished
[36malgo-1    |[0m 21/11/14 16:57:52 INFO DAGScheduler: Job 9 finished: json at HelloJavaSparkApp.java:66, took 0.303278 s
[36malgo-1    |[0m 21/11/14 16:57:52 INFO FileFormatWriter: Write Job c7a0764a-4282-4bfe-87cc-02013eff0bc4 committed.
[36malgo-1    |[0m 21/11/14 16:57:52 INFO FileFormatWriter: Finished processing stats for write job c7a0764a-4282-4bfe-87cc-02013eff0bc4.
[36malgo-1    |[0m 21/11/14 16:57:52 INFO SparkUI: Stopped Spark web UI at http://172.22.0.3:4040
[36malgo-1    |[0m 21/11/14 16:57:52 INFO YarnClientSchedulerBackend: Interrupting monitor thread
[36malgo-1    |[0m 21/11/14 16:57:52 INFO YarnClientSchedulerBackend: Shutting down all executors
[36malgo-1    |[0m 21/11/14 16:57:52 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
[36malgo-1    |[0m 21/11/14 16:57:52 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
[36malgo-1    |[0m 21/11/14 16:57:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[36malgo-1    |[0m 21/11/14 16:57:52 INFO MemoryStore: MemoryStore cleared
[36malgo-1    |[0m 21/11/14 16:57:52 INFO BlockManager: BlockManager stopped
[36malgo-1    |[0m 21/11/14 16:57:52 INFO BlockManagerMaster: BlockManagerMaster stopped
[36malgo-1    |[0m 21/11/14 16:57:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[36malgo-1    |[0m 21/11/14 16:57:52 INFO SparkContext: Successfully stopped SparkContext
[36malgo-1    |[0m 21/11/14 16:57:52 INFO ShutdownHookManager: Shutdown hook called
[36malgo-1    |[0m 21/11/14 16:57:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-8a8db27a-a8d4-4f99-91b5-59856e4140cd
[36malgo-1    |[0m 21/11/14 16:57:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-6817a287-21dc-4c38-b7fe-1986b773fd10
[36malgo-1    |[0m 2021-11-14 16:57:52,537 INFO attempt.RMAppAttemptImpl: Updating application attempt appattempt_1636909040339_0001_000001 with final state: FINISHING, and exit status: -1000
[36malgo-1    |[0m 2021-11-14 16:57:52,538 INFO attempt.RMAppAttemptImpl: appattempt_1636909040339_0001_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
[36malgo-1    |[0m 2021-11-14 16:57:52,538 INFO rmapp.RMAppImpl: Updating application application_1636909040339_0001 with final state: FINISHING
[36malgo-1    |[0m 2021-11-14 16:57:52,538 INFO recovery.RMStateStore: Updating info for app: application_1636909040339_0001
[36malgo-1    |[0m 2021-11-14 16:57:52,538 INFO rmapp.RMAppImpl: application_1636909040339_0001 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
[36malgo-1    |[0m 2021-11-14 16:57:52,538 INFO attempt.RMAppAttemptImpl: appattempt_1636909040339_0001_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
[36malgo-1    |[0m 2021-11-14 16:57:52,539 INFO rmapp.RMAppImpl: application_1636909040339_0001 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
[36malgo-1    |[0m 2021-11-14 16:57:52,548 INFO launcher.ContainerLaunch: Container container_1636909040339_0001_01_000003 succeeded 
[36malgo-1    |[0m 2021-11-14 16:57:52,556 INFO container.ContainerImpl: Container container_1636909040339_0001_01_000003 transitioned from RUNNING to EXITED_WITH_SUCCESS
[36malgo-1    |[0m 2021-11-14 16:57:52,557 INFO launcher.ContainerCleanup: Cleaning up container container_1636909040339_0001_01_000003
[36malgo-1    |[0m 2021-11-14 16:57:52,558 INFO nodemanager.NMAuditLogger: USER=root	OPERATION=Container Finished - Succeeded	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1636909040339_0001	CONTAINERID=container_1636909040339_0001_01_000003
[36malgo-1    |[0m 2021-11-14 16:57:52,559 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/container_1636909040339_0001_01_000003
[36malgo-1    |[0m 2021-11-14 16:57:52,560 INFO container.ContainerImpl: Container container_1636909040339_0001_01_000003 transitioned from EXITED_WITH_SUCCESS to DONE
[36malgo-1    |[0m 2021-11-14 16:57:52,561 INFO application.ApplicationImpl: Removing container_1636909040339_0001_01_000003 from application application_1636909040339_0001
[36malgo-1    |[0m 2021-11-14 16:57:52,561 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1636909040339_0001_01_000003
[36malgo-1    |[0m 2021-11-14 16:57:52,561 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1636909040339_0001
[36malgo-1    |[0m 2021-11-14 16:57:52,563 INFO rmcontainer.RMContainerImpl: container_1636909040339_0001_01_000003 Container Transitioned from RUNNING to COMPLETED
[36malgo-1    |[0m 2021-11-14 16:57:52,564 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636909040339_0001	CONTAINERID=container_1636909040339_0001_01_000003	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 16:57:52,573 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/container_1636909040339_0001_01_000003/launch_container.sh
[36malgo-1    |[0m 2021-11-14 16:57:52,574 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/container_1636909040339_0001_01_000003/launch_container.sh]
[36malgo-1    |[0m 2021-11-14 16:57:52,574 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/container_1636909040339_0001_01_000003/container_tokens
[36malgo-1    |[0m 2021-11-14 16:57:52,574 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/container_1636909040339_0001_01_000003/container_tokens]
[36malgo-1    |[0m 2021-11-14 16:57:52,574 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/container_1636909040339_0001_01_000003/sysfs
[36malgo-1    |[0m 2021-11-14 16:57:52,574 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/container_1636909040339_0001_01_000003/sysfs]
[36malgo-1    |[0m 2021-11-14 16:57:52,639 INFO resourcemanager.ApplicationMasterService: application_1636909040339_0001 unregistered successfully. 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/[/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stdout] 2021-11-14T16:57:45.925+0000: [GC (Allocation Failure) [PSYoungGen: 120320K->9045K(140288K)] 120320K->9061K(461312K), 0.0075112 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stdout] 2021-11-14T16:57:46.425+0000: [GC (Allocation Failure) [PSYoungGen: 129365K->7618K(140288K)] 129381K->7642K(461312K), 0.0076735 secs] [Times: user=0.01 sys=0.01, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stdout] 2021-11-14T16:57:46.769+0000: [GC (Metadata GC Threshold) [PSYoungGen: 109892K->8661K(140288K)] 109916K->8693K(461312K), 0.0051857 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stdout] 2021-11-14T16:57:46.774+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 8661K->0K(140288K)] [ParOldGen: 32K->8434K(191488K)] 8693K->8434K(331776K), [Metaspace: 20370K->20370K(1067008K)], 0.0267525 secs] [Times: user=0.06 sys=0.00, real=0.03 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stdout] 2021-11-14T16:57:47.294+0000: [GC (Allocation Failure) [PSYoungGen: 120320K->4575K(182272K)] 128754K->13018K(373760K), 0.0054297 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stdout] 2021-11-14T16:57:47.786+0000: [GC (Allocation Failure) [PSYoungGen: 182239K->7461K(245248K)] 190682K->15904K(436736K), 0.0069591 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stdout] 2021-11-14T16:57:47.897+0000: [GC (Metadata GC Threshold) [PSYoungGen: 47122K->3883K(274432K)] 55565K->12333K(465920K), 0.0045803 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stdout] 2021-11-14T16:57:47.902+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 3883K->0K(274432K)] [ParOldGen: 8450K->10068K(284160K)] 12333K->10068K(558592K), [Metaspace: 33949K->33947K(1079296K)], 0.0282093 secs] [Times: user=0.06 sys=0.00, real=0.03 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stdout] 2021-11-14T16:57:48.865+0000: [GC (Allocation Failure) [PSYoungGen: 264192K->10738K(274944K)] 274260K->21476K(559104K), 0.0125101 secs] [Times: user=0.03 sys=0.00, real=0.02 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stdout] 2021-11-14T16:57:49.787+0000: [GC (Metadata GC Threshold) [PSYoungGen: 266550K->12273K(399872K)] 277288K->26548K(684032K), 0.0120357 secs] [Times: user=0.02 sys=0.01, real=0.02 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stdout] 2021-11-14T16:57:49.799+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 12273K->0K(399872K)] [ParOldGen: 14275K->24139K(402432K)] 26548K->24139K(802304K), [Metaspace: 54395K->54395K(1099776K)], 0.0851454 secs] [Times: user=0.20 sys=0.00, real=0.08 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stdout] Heap
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stdout]  PSYoungGen      total 399872K, used 335170K [0x00000000d5580000, 0x00000000eee00000, 0x0000000100000000)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stdout]   eden space 387584K, 86% used [0x00000000d5580000,0x00000000e9cd0a78,0x00000000ed000000)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stdout]   from space 12288K, 0% used [0x00000000ee000011/14 16:57:50 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO CodeGenerator: Code generated in 19.896297 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO CodeGenerator: Code generated in 20.857213 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:50 INFO Executor: Finished task 3.0 in stage 9.0 (TID 34). 4646 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:51 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 41
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:51 INFO Executor: Running task 0.0 in stage 11.0 (TID 41)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:51 INFO TorrentBroadcast: Started reading broadcast variable 12 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:51 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:51 INFO TorrentBroadcast: Reading broadcast variable 12 took 6 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:51 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 15.1 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:51 INFO CodeGenerator: Code generated in 13.336409 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:51 INFO CodeGenerator: Code generated in 7.187481 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:51 INFO FileScanRDD: TID: 41 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:51 INFO TorrentBroadcast: Started reading broadcast variable 11 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:51 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:51 INFO TorrentBroadcast: Reading broadcast variable 11 took 7 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:51 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 445.1 KiB, free 911.8 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:51 INFO Executor: Finished task 0.0 in stage 11.0 (TID 41). 58966 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stdout] 2021-11-14T16:57:42.005+0000: [GC (Allocation Failure) [PSYoungGen: 120320K->9051K(140288K)] 120320K->9067K(461312K), 0.0076344 secs] [Times: user=0.02 sys=0.01, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stdout] 2021-11-14T16:57:42.489+0000: [GC (Allocation Failure) [PSYoungGen: 129371K->7641K(140288K)] 129387K->7665K(461312K), 0.0058379 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stdout] 2021-11-14T16:57:42.820+0000: [GC (Metadata GC Threshold) [PSYoungGen: 109246K->8654K(140288K)] 109270K->8686K(461312K), 0.0047594 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stdout] 2021-11-14T16:57:42.825+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 8654K->0K(140288K)] [ParOldGen: 32K->8398K(193024K)] 8686K->8398K(333312K), [Metaspace: 20377K->20377K(1067008K)], 0.0246584 secs] [Times: user=0.06 sys=0.00, real=0.03 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stdout] 2021-11-14T16:57:43.402+0000: [GC (Allocation Failure) [PSYoungGen: 120320K->4594K(179712K)] 128718K->13001K(372736K), 0.0045937 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stdout] 2021-11-14T16:57:43.881+0000: [GC (Allocation Failure) [PSYoungGen: 179698K->7420K(245248K)] 188105K->15826K(438272K), 0.0079195 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stdout] 2021-11-14T16:57:43.994+0000: [GC (Metadata GC Threshold) [PSYoungGen: 50513K->4070K(270848K)] 58919K->12485K(463872K), 0.0050763 secs] [Times: user=0.01 sys=0.01, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stdout] 2021-11-14T16:57:43.999+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 4070K->0K(270848K)] [ParOldGen: 8414K->10172K(281088K)] 12485K->10172K(551936K), [Metaspace: 33960K->33958K(1079296K)], 0.0264860 secs] [Times: user=0.05 sys=0.00, real=0.02 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stdout] 2021-11-14T16:57:44.724+0000: [GC (Allocation Failure) [PSYoungGen: 260608K->10744K(271360K)] 270780K->21921K(552448K), 0.0100437 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stdout] 2021-11-14T16:57:45.524+0000: [GC (Allocation Failure) [PSYoungGen: 271352K->12272K(403968K)] 282529K->25947K(685056K), 0.0121748 secs] [Times: user=0.03 sys=0.00, real=0.02 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stdout] 2021-11-14T16:57:45.554+0000: [GC (Metadata GC Threshold) [PSYoungGen: 19006K->10125K(406528K)] 32682K->23801K(687616K), 0.0080068 secs] [Times: user=0.02 sys=0.01, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stdout] 2021-11-14T16:57:45.562+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 10125K->0K(406528K)] [ParOldGen: 13675K->18727K(329216K)] 23801K->18727K(735744K), [Metaspace: 54281K->54281K(1099776K)], 0.0775351 secs] [Times: user=0.17 sys=0.00, real=0.08 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stdout] 2021-11-14T16:57:51.085+0000: [GC (Allocation Failure) [PSYoungGen: 391680K->10309K(570368K)] 410407K->29045K(899584K), 0.0106086 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stdolication_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:136)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:137)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 	... 50 more
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 40
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO Executor: Running task 0.0 in stage 10.0 (TID 40)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO TorrentBroadcast: Started reading broadcast variable 10 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 911.7 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO TorrentBroadcast: Reading broadcast variable 10 took 6 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:50 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 15.8 KiB, free 911.7 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:51 INFO CodeGenerator: Code generated in 6.85148 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:51 INFO CodeGenerator: Code generated in 11.230516 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:51 INFO FileScanRDD: TID: 40 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:51 INFO TorrentBroadcast: Started reading broadcast variable 9 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:51 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 911.7 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:51 INFO TorrentBroadcast: Reading broadcast variable 9 took 5 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:51 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 445.1 KiB, free 911.2 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:51 INFO Executor: Finished task 0.0 in stage 10.0 (TID 40). 5548 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:51 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 42
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:51 INFO Executor: Running task 0.0 in stage 12.0 (TID 42)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:51 INFO TorrentBroadcast: Started reading broadcast variable 13 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:51 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 15.1 KiB, free 911.4 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:51 INFO TorrentBroadcast: Reading broadcast variable 13 took 4 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:51 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 70.4 KiB, free 911.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:51 INFO FileScanRDD: TID: 42 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:51 INFO TorrentBroadcast: Started reading broadcast variable 11 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:51 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 911.2 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:51 INFO TorrentBroadcast: Reading broadcast variable 11 took 5 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:51 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 445.1 KiB, free 910.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO Executor: Finished task 0.0 in stage 12.0 (TID 42). 3699 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 43
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO Executor: Running task 0.0 in stage 14.0 (TID 43)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO MapOutputTrackerWorker: Updating epoch to 3 and clearing cache
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO TorrentBroadcast: Started reading broadcast variable 14 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 71.6 KiB, free 910.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO TorrentBroadcast: Reading broadcast variable 14 took 5 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 193.4 KiB, free 910.6 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO DirectFileOutputCommitter: Direct Write: DISABLED
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@172.22.0.3:46583)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO MapOutputTrackerWorker: Got the output locations
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO CodeGenerator: Code generated in 11.664711 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.1 KiB) non-empty blocks including 1 (3.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.1 KiB) non-empty blocks including 1 (3.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.0 KiB) non-empty blocks including 1 (3.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.1 KiB) non-empty blocks including 1 (3.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.0 KiB) non-empty blocks including 1 (3.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.0 KiB) non-empty blocks including 1 (3.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.0 KiB) non-empty blocks including 1 (3.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000002/stderr] 21/11/14 16:57:52 INFO Y2021-11-14 16:57:52,785 INFO launcher.ContainerLaunch: Container container_1636909040339_0001_01_000002 succeeded 
[33malgo-2    |[0m 2021-11-14 16:57:52,793 INFO container.ContainerImpl: Container container_1636909040339_0001_01_000002 transitioned from RUNNING to EXITED_WITH_SUCCESS
[33malgo-2    |[0m 2021-11-14 16:57:52,794 INFO launcher.ContainerCleanup: Cleaning up container container_1636909040339_0001_01_000002
[33malgo-2    |[0m 2021-11-14 16:57:52,803 INFO nodemanager.NMAuditLogger: USER=root	OPERATION=Container Finished - Succeeded	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1636909040339_0001	CONTAINERID=container_1636909040339_0001_01_000002
[33malgo-2    |[0m 2021-11-14 16:57:52,806 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/container_1636909040339_0001_01_000002
[33malgo-2    |[0m 2021-11-14 16:57:52,807 INFO container.ContainerImpl: Container container_1636909040339_0001_01_000002 transitioned from EXITED_WITH_SUCCESS to DONE
[33malgo-2    |[0m 2021-11-14 16:57:52,807 INFO application.ApplicationImpl: Removing container_1636909040339_0001_01_000002 from application application_1636909040339_0001
[33malgo-2    |[0m 2021-11-14 16:57:52,808 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1636909040339_0001_01_000002
[33malgo-2    |[0m 2021-11-14 16:57:52,808 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1636909040339_0001
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636909040339_0001/container_1636909040339_0001_01_000003/stderr] 21/11/14 16:57:52 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blo2021-11-14 16:57:52,810 INFO rmcontainer.RMContainerImpl: container_1636909040339_0001_01_000002 Container Transitioned from RUNNING to COMPLETED
[36malgo-1    |[0m 2021-11-14 16:57:52,810 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636909040339_0001	CONTAINERID=container_1636909040339_0001_01_000002	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[33malgo-2    |[0m 2021-11-14 16:57:52,820 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/container_1636909040339_0001_01_000002/launch_container.sh
[33malgo-2    |[0m 2021-11-14 16:57:52,820 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/container_1636909040339_0001_01_000002/launch_container.sh]
[33malgo-2    |[0m 2021-11-14 16:57:52,820 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/container_1636909040339_0001_01_000002/container_tokens
[33malgo-2    |[0m 2021-11-14 16:57:52,820 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/container_1636909040339_0001_01_000002/container_tokens]
[33malgo-2    |[0m 2021-11-14 16:57:52,820 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/container_1636909040339_0001_01_000002/sysfs
[33malgo-2    |[0m 2021-11-14 16:57:52,820 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636909040339_0001/container_1636909040339_0001_01_000002/sysfs]
[36malgo-1    |[0m 11-14 16:57 smspark-submit INFO     spark submit was successful. primary node exiting.
[33malgo-2    |[0m 11-14 16:57 urllib3.connectionpool WARNING  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f22c616a210>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m 2021-11-14 16:57:53,595 WARN datanode.DataNode: IOException in offerService
[33malgo-2    |[0m java.io.EOFException: End of File Exception between local host is: "algo-2/172.22.0.2"; destination host is: "algo-1.spark-network":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
[33malgo-2    |[0m 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[33malgo-2    |[0m 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[33malgo-2    |[0m 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[33malgo-2    |[0m 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
[33malgo-2    |[0m 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
[33malgo-2    |[0m 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:791)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
[33malgo-2    |[0m 	at com.sun.proxy.$Proxy16.sendHeartbeat(Unknown Source)
[33malgo-2    |[0m 	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:168)
[33malgo-2    |[0m 	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:517)
[33malgo-2    |[0m 	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
[33malgo-2    |[0m 	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
[33malgo-2    |[0m 	at java.lang.Thread.run(Thread.java:748)
[33malgo-2    |[0m Caused by: java.io.EOFException
[33malgo-2    |[0m 	at java.io.DataInputStream.readInt(DataInputStream.java:392)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)
[33malgo-2    |[0m 2021-11-14 16:57:53,814 INFO retry.RetryInvocationHandler: java.io.EOFException: End of File Exception between local host is: "algo-2/172.22.0.2"; destination host is: "algo-1.spark-network":8031; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException, while invoking ResourceTrackerPBClientImpl.nodeHeartbeat over null. Retrying after sleeping for 30000ms.
[33malgo-2    |[0m 11-14 16:57 urllib3.connectionpool WARNING  Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f22c616a4d0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m 11-14 16:57 urllib3.connectionpool WARNING  Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f22c616a810>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m 11-14 16:58 urllib3.connectionpool WARNING  Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f22c616ae50>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[36malgo-1 exited with code 0
[0m[33malgo-2    |[0m 11-14 16:58 urllib3.connectionpool WARNING  Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f22c6163290>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m 11-14 16:58 smspark-submit INFO     primary is down, worker now exiting
[33malgo-2 exited with code 0
[0mThe CMD variable is not set. Defaulting to a blank string.
Removing algo-1 ... 
Removing algo-2 ... 
[1A[2KRemoving algo-2 ... [32mdone[0m[1B[2A[2KRemoving algo-1 ... [32mdone[0m[2BRemoving network spark-network


Running docker-compose down ...
PASSED

=================================== FAILURES ===================================
____________________________ test_pyspark_multinode ____________________________

input_data = 'file:///opt/ml/processing/input/data/data.jsonl'
output_data = 'file:///opt/ml/processing/output/data', verbose_opt = '--verbose'

    def test_pyspark_multinode(input_data: str, output_data: str, verbose_opt: str) -> None:
        input = "--input {}".format(input_data)
        output = "--output {}".format(output_data)
        py_files_arg = "--py-files /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py"
        app_py = "/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py"
        docker_compose_cmd = (
            f"CMD='{py_files_arg} {verbose_opt} {app_py} {input} {output}' docker-compose up --force-recreate"
        )
    
        print(docker_compose_cmd)
        docker_compose_proc = subprocess.run(
            docker_compose_cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True, encoding="utf-8",
        )
    
        stdout = docker_compose_proc.stdout
        print(stdout)
    
        # assert exit code of docker compose command.
        assert 0 == docker_compose_proc.returncode
    
        # assert the expected Python script executed
        assert "Hello World, this is PySpark" in stdout
    
        # Application code does "df.show()"
        assert "only showing top 20 rows" in stdout
    
        # from spark-defaults configuration in configuration.json
        assert "(spark.executor.memory,2g)" in stdout
        assert "(spark.executor.cores,1)" in stdout
    
        # assert on stdout from docker compose to get exit code of each container.
>       assert 2 == stdout.count("exited with code 0")
E       assert 2 == 1
E         +2
E         -1

test/integration/local/test_multinode_container.py:64: AssertionError
============================ slowest test durations ============================
140.71s call     test/integration/local/test_multinode_container.py::test_pyspark_multinode
66.20s call     test/integration/local/test_multinode_container.py::test_scala_spark_multinode
56.27s call     test/integration/local/test_multinode_container.py::test_java_spark_multinode
0.00s setup    test/integration/local/test_multinode_container.py::test_pyspark_multinode
0.00s setup    test/integration/local/test_multinode_container.py::test_java_spark_multinode
0.00s teardown test/integration/local/test_multinode_container.py::test_scala_spark_multinode
0.00s setup    test/integration/local/test_multinode_container.py::test_scala_spark_multinode
0.00s teardown test/integration/local/test_multinode_container.py::test_java_spark_multinode
0.00s teardown test/integration/local/test_multinode_container.py::test_pyspark_multinode
=========================== short test summary info ============================
FAILED test/integration/local/test_multinode_container.py::test_pyspark_multinode
=================== 1 failed, 2 passed in 263.26s (0:04:23) ====================
make: *** [Makefile:81: test-local] Error 1
