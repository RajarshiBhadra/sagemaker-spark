nohup: ignoring input
pip install pipenv --upgrade
Requirement already satisfied: pipenv in /home/rajarshi/.venv/lib/python3.8/site-packages (2021.11.9)
Requirement already satisfied: pip>=18.0 in /home/rajarshi/.venv/lib/python3.8/site-packages (from pipenv) (21.3.1)
Requirement already satisfied: certifi in /home/rajarshi/.venv/lib/python3.8/site-packages (from pipenv) (2021.10.8)
Requirement already satisfied: virtualenv-clone>=0.2.5 in /home/rajarshi/.venv/lib/python3.8/site-packages (from pipenv) (0.5.7)
Requirement already satisfied: setuptools>=36.2.1 in /home/rajarshi/.venv/lib/python3.8/site-packages (from pipenv) (58.5.3)
Requirement already satisfied: virtualenv in /home/rajarshi/.venv/lib/python3.8/site-packages (from pipenv) (20.10.0)
Requirement already satisfied: filelock<4,>=3.2 in /home/rajarshi/.venv/lib/python3.8/site-packages (from virtualenv->pipenv) (3.3.2)
Requirement already satisfied: platformdirs<3,>=2 in /home/rajarshi/.venv/lib/python3.8/site-packages (from virtualenv->pipenv) (2.4.0)
Requirement already satisfied: backports.entry-points-selectable>=1.0.4 in /home/rajarshi/.venv/lib/python3.8/site-packages (from virtualenv->pipenv) (1.1.1)
Requirement already satisfied: six<2,>=1.9.0 in /home/rajarshi/.venv/lib/python3.8/site-packages (from virtualenv->pipenv) (1.16.0)
Requirement already satisfied: distlib<1,>=0.3.1 in /home/rajarshi/.venv/lib/python3.8/site-packages (from virtualenv->pipenv) (0.3.3)
pipenv run pip install --upgrade pip
Courtesy Notice: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set PIPENV_IGNORE_VIRTUALENVS=1 to force pipenv to ignore that environment and create its own instead. You can set PIPENV_VERBOSITY=-1 to suppress this warning.
Requirement already satisfied: pip in /home/rajarshi/.venv/lib/python3.8/site-packages (21.3.1)
pipenv install
Courtesy Notice: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set PIPENV_IGNORE_VIRTUALENVS=1 to force pipenv to ignore that environment and create its own instead. You can set PIPENV_VERBOSITY=-1 to suppress this warning.
Installing dependencies from Pipfile.lock (f724bd)...
cp Pipfile ./spark/processing/3.0/py3
cp Pipfile.lock ./spark/processing/3.0/py3
cp setup.py ./spark/processing/3.0/py3
pipenv run safety check  # https://github.com/pyupio/safety
Courtesy Notice: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set PIPENV_IGNORE_VIRTUALENVS=1 to force pipenv to ignore that environment and create its own instead. You can set PIPENV_VERBOSITY=-1 to suppress this warning.
+==============================================================================+
|                                                                              |
|                               /$$$$$$            /$$                         |
|                              /$$__  $$          | $$                         |
|           /$$$$$$$  /$$$$$$ | $$  \__//$$$$$$  /$$$$$$   /$$   /$$           |
|          /$$_____/ |____  $$| $$$$   /$$__  $$|_  $$_/  | $$  | $$           |
|         |  $$$$$$   /$$$$$$$| $$_/  | $$$$$$$$  | $$    | $$  | $$           |
|          \____  $$ /$$__  $$| $$    | $$_____/  | $$ /$$| $$  | $$           |
|          /$$$$$$$/|  $$$$$$$| $$    |  $$$$$$$  |  $$$$/|  $$$$$$$           |
|         |_______/  \_______/|__/     \_______/   \___/   \____  $$           |
|                                                          /$$  | $$           |
|                                                         |  $$$$$$/           |
|  by pyup.io                                              \______/            |
|                                                                              |
+==============================================================================+
| REPORT                                                                       |
| checked 93 packages, using default DB                                        |
+==============================================================================+
| No known security vulnerabilities found.                                     |
+==============================================================================+
cd test/resources/code/scala/hello-scala-spark; sbt package
[info] welcome to sbt 1.5.5 (Ubuntu Java 11.0.11)
[info] loading project definition from /home/rajarshi/sagemaker-spark/test/resources/code/scala/hello-scala-spark/project
[info] loading settings for project hello-scala-spark from hello-scala-spark.sbt ...
[info] set current project to hello-scala-spark (in build file:/home/rajarshi/sagemaker-spark/test/resources/code/scala/hello-scala-spark/)
[success] Total time: 1 s, completed Nov 14, 2021, 3:18:59 PM
cd test/resources/code/java/hello-java-spark; mvn package
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$1 (file:/usr/share/maven/lib/guice.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$1
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[[1;34mINFO[m] Scanning for projects...
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--------< [0;36mcom.amazonaws.sagemaker.spark.test:hello-java-spark[0;1m >---------[m
[[1;34mINFO[m] [1mBuilding hello-java-spark 1.0-SNAPSHOT[m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mhello-java-spark[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/rajarshi/sagemaker-spark/test/resources/code/java/hello-java-spark/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.1:compile[m [1m(default-compile)[m @ [36mhello-java-spark[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mhello-java-spark[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/rajarshi/sagemaker-spark/test/resources/code/java/hello-java-spark/src/test/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.1:testCompile[m [1m(default-testCompile)[m @ [36mhello-java-spark[0;1m ---[m
[[1;34mINFO[m] No sources to compile
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.12.4:test[m [1m(default-test)[m @ [36mhello-java-spark[0;1m ---[m
[[1;34mINFO[m] No tests to run.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:2.4:jar[m [1m(default-jar)[m @ [36mhello-java-spark[0;1m ---[m
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1;32mBUILD SUCCESS[m
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] Total time:  1.330 s
[[1;34mINFO[m] Finished at: 2021-11-14T15:19:02Z
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
pipenv run python -m pytest -s -vv test/integration/local --repo=sagemaker-spark --tag=latest --role=arn:aws:iam::533753950585:role/arn:aws:iam::533753950585:role/service-role/AmazonSageMaker-ExecutionRole-20210715T004780 --durations=0
Courtesy Notice: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set PIPENV_IGNORE_VIRTUALENVS=1 to force pipenv to ignore that environment and create its own instead. You can set PIPENV_VERBOSITY=-1 to suppress this warning.
============================= test session starts ==============================
platform linux -- Python 3.8.10, pytest-5.4.3, py-1.11.0, pluggy-0.13.1 -- /home/rajarshi/.venv/bin/python
cachedir: .pytest_cache
rootdir: /home/rajarshi/sagemaker-spark
plugins: rerunfailures-10.2, forked-1.3.0, xdist-1.32.0, parallel-0.1.0, cov-2.10.0
collecting ... collected 3 items

test/integration/local/test_multinode_container.py::test_pyspark_multinode CMD='--py-files /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py --verbose /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data' docker-compose up --force-recreate
Creating network "spark-network" with the default driver
Creating algo-1 ... 
Creating algo-2 ... 
[1A[2K
Creating algo-2 ... [32mdone[0m
[1B[2A[2K
Creating algo-1 ... [32mdone[0m
[2BAttaching to algo-2, algo-1
[33malgo-2    |[0m 11-14 15:19 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--py-files', '/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py', '--verbose', '/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py', '--input', 'file:///opt/ml/processing/input/data/data.jsonl', '--output', 'file:///opt/ml/processing/output/data']
[33malgo-2    |[0m 11-14 15:19 smspark.cli  INFO     Raw spark options before processing: {'py_files': '/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py', 'verbose': True, 'class_': None, 'jars': None, 'files': None}
[33malgo-2    |[0m 11-14 15:19 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py', '--input', 'file:///opt/ml/processing/input/data/data.jsonl', '--output', 'file:///opt/ml/processing/output/data']
[33malgo-2    |[0m 11-14 15:19 smspark.cli  INFO     Rendered spark options: {'py_files': '/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py', 'verbose': True, 'class_': None, 'jars': None, 'files': None}
[33malgo-2    |[0m 11-14 15:19 smspark.cli  INFO     Initializing processing job.
[33malgo-2    |[0m 11-14 15:19 smspark-submit INFO     {'current_host': 'algo-2', 'hosts': ['algo-1', 'algo-2']}
[33malgo-2    |[0m 11-14 15:19 smspark-submit INFO     {'ProcessingJobArn': 'processing_job_arn', 'ProcessingJobName': 'processing_job_name', 'AppSpecification': {'ImageUri': 'image_uri'}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30}}, 'RoleArn': 'iam_role'}
[33malgo-2    |[0m 11-14 15:19 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client --py-files /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py --verbose /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data
[33malgo-2    |[0m 11-14 15:19 smspark-submit INFO     waiting for hosts
[33malgo-2    |[0m 11-14 15:19 smspark-submit INFO     starting status server
[33malgo-2    |[0m 11-14 15:19 smspark-submit INFO     Status server listening on algo-2:5555
[33malgo-2    |[0m 11-14 15:19 smspark-submit INFO     bootstrapping cluster
[33malgo-2    |[0m 11-14 15:19 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING
[36malgo-1    |[0m 11-14 15:19 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--py-files', '/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py', '--verbose', '/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py', '--input', 'file:///opt/ml/processing/input/data/data.jsonl', '--output', 'file:///opt/ml/processing/output/data']
[33malgo-2    |[0m 11-14 15:19 smspark-submit INFO     copying aws jars
[36malgo-1    |[0m 11-14 15:19 smspark.cli  INFO     Raw spark options before processing: {'py_files': '/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py', 'verbose': True, 'class_': None, 'jars': None, 'files': None}
[36malgo-1    |[0m 11-14 15:19 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py', '--input', 'file:///opt/ml/processing/input/data/data.jsonl', '--output', 'file:///opt/ml/processing/output/data']
[36malgo-1    |[0m 11-14 15:19 smspark.cli  INFO     Rendered spark options: {'py_files': '/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py', 'verbose': True, 'class_': None, 'jars': None, 'files': None}
[36malgo-1    |[0m 11-14 15:19 smspark.cli  INFO     Initializing processing job.
[36malgo-1    |[0m 11-14 15:19 smspark-submit INFO     {'current_host': 'algo-1', 'hosts': ['algo-1', 'algo-2']}
[36malgo-1    |[0m 11-14 15:19 smspark-submit INFO     {'ProcessingJobArn': 'processing_job_arn', 'ProcessingJobName': 'processing_job_name', 'AppSpecification': {'ImageUri': 'image_uri'}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30}}, 'RoleArn': 'iam_role'}
[36malgo-1    |[0m 11-14 15:19 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client --py-files /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py --verbose /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data
[36malgo-1    |[0m 11-14 15:19 smspark-submit INFO     waiting for hosts
[36malgo-1    |[0m 11-14 15:19 smspark-submit INFO     starting status server
[36malgo-1    |[0m 11-14 15:19 smspark-submit INFO     Status server listening on algo-1:5555
[36malgo-1    |[0m 11-14 15:19 smspark-submit INFO     bootstrapping cluster
[36malgo-1    |[0m 11-14 15:19 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING
[33malgo-2    |[0m Serving on http://algo-2:5555
[36malgo-1    |[0m 11-14 15:19 smspark-submit INFO     copying aws jars
[36malgo-1    |[0m Serving on http://algo-1:5555
[33malgo-2    |[0m 11-14 15:19 smspark-submit INFO     Found hadoop jar hadoop-aws.jar
[33malgo-2    |[0m 11-14 15:19 smspark-submit INFO     Optional jar jets3t-0.9.0.jar in /usr/lib/hadoop/lib does not exist
[33malgo-2    |[0m 11-14 15:19 smspark-submit INFO     copying cluster config
[33malgo-2    |[0m 11-14 15:19 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml
[33malgo-2    |[0m 11-14 15:19 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml
[33malgo-2    |[0m 11-14 15:19 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[33malgo-2    |[0m 11-14 15:19 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 15:19 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh
[36malgo-1    |[0m 11-14 15:19 smspark-submit INFO     Found hadoop jar hadoop-aws.jar
[36malgo-1    |[0m 11-14 15:19 smspark-submit INFO     Optional jar jets3t-0.9.0.jar in /usr/lib/hadoop/lib does not exist
[33malgo-2    |[0m 11-14 15:19 root         INFO     Detected instance type: m5.xlarge with total memory: 16384M and total cores: 4
[36malgo-1    |[0m 11-14 15:19 smspark-submit INFO     copying cluster config
[33malgo-2    |[0m 11-14 15:19 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[36malgo-1    |[0m 11-14 15:19 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml
[36malgo-1    |[0m 11-14 15:19 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml
[33malgo-2    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0"?>
[33malgo-2    |[0m <!-- Site specific YARN configuration properties -->
[33malgo-2    |[0m  <configuration>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.resourcemanager.hostname</name>
[33malgo-2    |[0m          <value>192.168.240.3</value>
[33malgo-2    |[0m          <description>The hostname of the RM.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.hostname</name>
[33malgo-2    |[0m          <value>algo-2</value>
[33malgo-2    |[0m          <description>The hostname of the NM.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.webapp.address</name>
[33malgo-2    |[0m          <value>algo-2:8042</value>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.vmem-pmem-ratio</name>
[33malgo-2    |[0m          <value>5</value>
[33malgo-2    |[0m          <description>Ratio between virtual memory to physical memory.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.resourcemanager.am.max-attempts</name>
[33malgo-2    |[0m          <value>1</value>
[33malgo-2    |[0m          <description>The maximum number of application attempts.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.env-whitelist</name>
[33malgo-2    |[0m          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>
[33malgo-2    |[0m          <description>Environment variable whitelist</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m  
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.minimum-allocation-mb</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.maximum-allocation-mb</name>
[33malgo-2    |[0m     <value>15892</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.minimum-allocation-vcores</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.maximum-allocation-vcores</name>
[33malgo-2    |[0m     <value>4</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.nodemanager.resource.memory-mb</name>
[33malgo-2    |[0m     <value>15892</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.nodemanager.resource.cpu-vcores</name>
[33malgo-2    |[0m     <value>4</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:19 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m 11-14 15:19 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[36malgo-1    |[0m 11-14 15:19 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[33malgo-2    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.driver.host=192.168.240.3
[33malgo-2    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[33malgo-2    |[0m 
[33malgo-2    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[33malgo-2    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[33malgo-2    |[0m spark.rpc.askTimeout=300s
[33malgo-2    |[0m spark.driver.memory 2048m
[33malgo-2    |[0m spark.driver.memoryOverhead 204m
[33malgo-2    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[33malgo-2    |[0m spark.executor.memory 12399m
[33malgo-2    |[0m spark.executor.memoryOverhead 1239m
[33malgo-2    |[0m spark.executor.cores 4
[33malgo-2    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[33malgo-2    |[0m spark.executor.instances 2
[33malgo-2    |[0m spark.default.parallelism 16
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:19 root         INFO     Finished Yarn configuration files setup.
[36malgo-1    |[0m 11-14 15:19 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh
[33malgo-2    |[0m 11-14 15:19 root         INFO     reading user configuration from /opt/ml/processing/input/conf/configuration.json
[33malgo-2    |[0m 11-14 15:19 root         INFO     User configuration list or dict: [{'Classification': 'spark-defaults', 'Properties': {'spark.executor.memory': '2g', 'spark.executor.cores': '1'}}, {'Classification': 'hadoop-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'core-site', 'Properties': {'spark.executor.memory': '2g', 'spark.executor.cores': '1'}}, {'Classification': 'hadoop-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'hive-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-exec-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-site', 'Properties': {'key': 'value'}}, {'Classification': 'spark-defaults', 'Properties': {'key': 'value'}}, {'Classification': 'spark-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'spark-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'spark-hive-site', 'Properties': {'key': 'value'}}, {'Classification': 'spark-metrics', 'Properties': {'key': 'value'}}, {'Classification': 'yarn-site', 'Properties': {'key': 'value'}}, {'Classification': 'yarn-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}] , type <class 'list'>
[33malgo-2    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[33malgo-2    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.driver.host=192.168.240.3
[33malgo-2    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[33malgo-2    |[0m 
[33malgo-2    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[33malgo-2    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[33malgo-2    |[0m spark.rpc.askTimeout=300s
[33malgo-2    |[0m spark.driver.memory 2048m
[33malgo-2    |[0m spark.driver.memoryOverhead 204m
[33malgo-2    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[33malgo-2    |[0m spark.executor.memory 12399m
[33malgo-2    |[0m spark.executor.memoryOverhead 1239m
[33malgo-2    |[0m spark.executor.cores 4
[33malgo-2    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[33malgo-2    |[0m spark.executor.instances 2
[33malgo-2    |[0m spark.default.parallelism 16
[33malgo-2    |[0m spark.executor.memory 2g
[33malgo-2    |[0m spark.executor.cores 1
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/hadoop-env.sh
[33malgo-2    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/hadoop-env.sh is: 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[33malgo-2    |[0m # or more contributor license agreements.  See the NOTICE file
[33malgo-2    |[0m # distributed with this work for additional information
[33malgo-2    |[0m # regarding copyright ownership.  The ASF licenses this file
[33malgo-2    |[0m # to you under the Apache License, Version 2.0 (the
[33malgo-2    |[0m # "License"); you may not use this file except in compliance
[33malgo-2    |[0m # with the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m # Set Hadoop-specific environment variables here.
[33malgo-2    |[0m 
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## THIS FILE ACTS AS THE MASTER FILE FOR ALL HADOOP PROJECTS.
[33malgo-2    |[0m ## SETTINGS HERE WILL BE READ BY ALL HADOOP COMMANDS.  THEREFORE,
[33malgo-2    |[0m ## ONE CAN USE THIS FILE TO SET YARN, HDFS, AND MAPREDUCE
[33malgo-2    |[0m ## CONFIGURATION OPTIONS INSTEAD OF xxx-env.sh.
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## Precedence rules:
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## {yarn-env.sh|hdfs-env.sh} > hadoop-env.sh > hard-coded defaults
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## {YARN_xyz|HDFS_xyz} > HADOOP_xyz > hard-coded defaults
[33malgo-2    |[0m ##
[33malgo-2    |[0m 
[33malgo-2    |[0m # Many of the options here are built from the perspective that users
[33malgo-2    |[0m # may want to provide OVERWRITING values on the command line.
[33malgo-2    |[0m # For example:
[33malgo-2    |[0m #
[33malgo-2    |[0m #  JAVA_HOME=/usr/java/testing hdfs dfs -ls
[33malgo-2    |[0m #
[33malgo-2    |[0m # Therefore, the vast majority (BUT NOT ALL!) of these defaults
[33malgo-2    |[0m # are configured for substitution and not append.  If append
[33malgo-2    |[0m # is preferable, modify this file accordingly.
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Generic settings for HADOOP
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Technically, the only required environment variable is JAVA_HOME.
[33malgo-2    |[0m # All others are optional.  However, the defaults are probably not
[33malgo-2    |[0m # preferred.  Many sites configure these options outside of Hadoop,
[33malgo-2    |[0m # such as in /etc/profile.d
[33malgo-2    |[0m 
[33malgo-2    |[0m # The java implementation to use. By default, this environment
[33malgo-2    |[0m # variable is REQUIRED on ALL platforms except OS X!
[33malgo-2    |[0m # export JAVA_HOME=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Location of Hadoop.  By default, Hadoop will attempt to determine
[33malgo-2    |[0m # this location based upon its execution path.
[33malgo-2    |[0m # export HADOOP_HOME=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Location of Hadoop's configuration information.  i.e., where this
[33malgo-2    |[0m # file is living. If this is not defined, Hadoop will attempt to
[33malgo-2    |[0m # locate it based upon its execution path.
[33malgo-2    |[0m #
[33malgo-2    |[0m # NOTE: It is recommend that this variable not be set here but in
[33malgo-2    |[0m # /etc/profile.d or equivalent.  Some options (such as
[33malgo-2    |[0m # --config) may react strangely otherwise.
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
[33malgo-2    |[0m 
[33malgo-2    |[0m # The maximum amount of heap to use (Java -Xmx).  If no unit
[33malgo-2    |[0m # is provided, it will be converted to MB.  Daemons will
[33malgo-2    |[0m # prefer any Xmx setting in their respective _OPT variable.
[33malgo-2    |[0m # There is no default; the JVM will autoscale based upon machine
[33malgo-2    |[0m # memory size.
[33malgo-2    |[0m # export HADOOP_HEAPSIZE_MAX=
[33malgo-2    |[0m 
[33malgo-2    |[0m # The minimum amount of heap to use (Java -Xms).  If no unit
[33malgo-2    |[0m # is provided, it will be converted to MB.  Daemons will
[33malgo-2    |[0m # prefer any Xms setting in their respective _OPT variable.
[33malgo-2    |[0m # There is no default; the JVM will autoscale based upon machine
[33malgo-2    |[0m # memory size.
[33malgo-2    |[0m # export HADOOP_HEAPSIZE_MIN=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Enable extra debugging of Hadoop's JAAS binding, used to set up
[33malgo-2    |[0m # Kerberos security.
[33malgo-2    |[0m # export HADOOP_JAAS_DEBUG=true
[33malgo-2    |[0m 
[33malgo-2    |[0m # Extra Java runtime options for all Hadoop commands. We don't support
[33malgo-2    |[0m # IPv6 yet/still, so by default the preference is set to IPv4.
[33malgo-2    |[0m # export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true"
[33malgo-2    |[0m # For Kerberos debugging, an extended option set logs more information
[33malgo-2    |[0m # export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug"
[33malgo-2    |[0m 
[33malgo-2    |[0m # Some parts of the shell code may do special things dependent upon
[33malgo-2    |[0m # the operating system.  We have to set this here. See the next
[33malgo-2    |[0m # section as to why....
[33malgo-2    |[0m #export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}
[33malgo-2    |[0m 
[33malgo-2    |[0m # Extra Java runtime options for some Hadoop commands
[33malgo-2    |[0m # and clients (i.e., hdfs dfs -blah).  These get appended to HADOOP_OPTS for
[33malgo-2    |[0m # such commands.  In most cases, # this should be left empty and
[33malgo-2    |[0m # let users supply it on the command line.
[33malgo-2    |[0m # export HADOOP_CLIENT_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # A note about classpaths.
[33malgo-2    |[0m #
[33malgo-2    |[0m # By default, Apache Hadoop overrides Java's CLASSPATH
[33malgo-2    |[0m # environment variable.  It is configured such
[33malgo-2    |[0m # that it starts out blank with new entries added after passing
[33malgo-2    |[0m # a series of checks (file/dir exists, not already listed aka
[33malgo-2    |[0m # de-deduplication).  During de-deduplication, wildcards and/or
[33malgo-2    |[0m # directories are *NOT* expanded to keep it simple. Therefore,
[33malgo-2    |[0m # if the computed classpath has two specific mentions of
[33malgo-2    |[0m # awesome-methods-1.0.jar, only the first one added will be seen.
[33malgo-2    |[0m # If two directories are in the classpath that both contain
[33malgo-2    |[0m # awesome-methods-1.0.jar, then Java will pick up both versions.
[33malgo-2    |[0m 
[33malgo-2    |[0m # An additional, custom CLASSPATH. Site-wide configs should be
[33malgo-2    |[0m # handled via the shellprofile functionality, utilizing the
[33malgo-2    |[0m # hadoop_add_classpath function for greater control and much
[33malgo-2    |[0m # harder for apps/end-users to accidentally override.
[33malgo-2    |[0m # Similarly, end users should utilize ${HOME}/.hadooprc .
[33malgo-2    |[0m # This variable should ideally only be used as a short-cut,
[33malgo-2    |[0m # interactive way for temporary additions on the command line.
[33malgo-2    |[0m # export HADOOP_CLASSPATH="/some/cool/path/on/your/machine"
[33malgo-2    |[0m 
[33malgo-2    |[0m # Should HADOOP_CLASSPATH be first in the official CLASSPATH?
[33malgo-2    |[0m # export HADOOP_USER_CLASSPATH_FIRST="yes"
[33malgo-2    |[0m 
[33malgo-2    |[0m # If HADOOP_USE_CLIENT_CLASSLOADER is set, the classpath along
[33malgo-2    |[0m # with the main jar are handled by a separate isolated
[33malgo-2    |[0m # client classloader when 'hadoop jar', 'yarn jar', or 'mapred job'
[33malgo-2    |[0m # is utilized. If it is set, HADOOP_CLASSPATH and
[33malgo-2    |[0m # HADOOP_USER_CLASSPATH_FIRST are ignored.
[33malgo-2    |[0m # export HADOOP_USE_CLIENT_CLASSLOADER=true
[33malgo-2    |[0m 
[33malgo-2    |[0m # HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES overrides the default definition of
[33malgo-2    |[0m # system classes for the client classloader when HADOOP_USE_CLIENT_CLASSLOADER
[33malgo-2    |[0m # is enabled. Names ending in '.' (period) are treated as package names, and
[33malgo-2    |[0m # names starting with a '-' are treated as negative matches. For example,
[33malgo-2    |[0m # export HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES="-org.apache.hadoop.UserClass,java.,javax.,org.apache.hadoop."
[33malgo-2    |[0m 
[33malgo-2    |[0m # Enable optional, bundled Hadoop features
[33malgo-2    |[0m # This is a comma delimited list.  It may NOT be overridden via .hadooprc
[33malgo-2    |[0m # Entries may be added/removed as needed.
[33malgo-2    |[0m # export HADOOP_OPTIONAL_TOOLS="hadoop-azure,hadoop-azure-datalake,hadoop-aws,hadoop-openstack,hadoop-aliyun,hadoop-kafka"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Options for remote shell connectivity
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # There are some optional components of hadoop that allow for
[33malgo-2    |[0m # command and control of remote hosts.  For example,
[33malgo-2    |[0m # start-dfs.sh will attempt to bring up all NNs, DNS, etc.
[33malgo-2    |[0m 
[33malgo-2    |[0m # Options to pass to SSH when one of the "log into a host and
[33malgo-2    |[0m # start/stop daemons" scripts is executed
[33malgo-2    |[0m # export HADOOP_SSH_OPTS="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10s"
[33malgo-2    |[0m 
[33malgo-2    |[0m # The built-in ssh handler will limit itself to 10 simultaneous connections.
[33malgo-2    |[0m # For pdsh users, this sets the fanout size ( -f )
[33malgo-2    |[0m # Change this to increase/decrease as necessary.
[33malgo-2    |[0m # export HADOOP_SSH_PARALLEL=10
[33malgo-2    |[0m 
[33malgo-2    |[0m # Filename which contains all of the hosts for any remote execution
[33malgo-2    |[0m # helper scripts # such as workers.sh, start-dfs.sh, etc.
[33malgo-2    |[0m # export HADOOP_WORKERS="${HADOOP_CONF_DIR}/workers"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Options for all daemons
[33malgo-2    |[0m ###
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Many options may also be specified as Java properties.  It is
[33malgo-2    |[0m # very common, and in many cases, desirable, to hard-set these
[33malgo-2    |[0m # in daemon _OPTS variables.  Where applicable, the appropriate
[33malgo-2    |[0m # Java property is also identified.  Note that many are re-used
[33malgo-2    |[0m # or set differently in certain contexts (e.g., secure vs
[33malgo-2    |[0m # non-secure)
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m # Where (primarily) daemon log files are stored.
[33malgo-2    |[0m # ${HADOOP_HOME}/logs by default.
[33malgo-2    |[0m # Java property: hadoop.log.dir
[33malgo-2    |[0m # export HADOOP_LOG_DIR=${HADOOP_HOME}/logs
[33malgo-2    |[0m 
[33malgo-2    |[0m # A string representing this instance of hadoop. $USER by default.
[33malgo-2    |[0m # This is used in writing log and pid files, so keep that in mind!
[33malgo-2    |[0m # Java property: hadoop.id.str
[33malgo-2    |[0m # export HADOOP_IDENT_STRING=$USER
[33malgo-2    |[0m 
[33malgo-2    |[0m # How many seconds to pause after stopping a daemon
[33malgo-2    |[0m # export HADOOP_STOP_TIMEOUT=5
[33malgo-2    |[0m 
[33malgo-2    |[0m # Where pid files are stored.  /tmp by default.
[33malgo-2    |[0m # export HADOOP_PID_DIR=/tmp
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default log4j setting for interactive commands
[33malgo-2    |[0m # Java property: hadoop.root.logger
[33malgo-2    |[0m # export HADOOP_ROOT_LOGGER=INFO,console
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default log4j setting for daemons spawned explicitly by
[33malgo-2    |[0m # --daemon option of hadoop, hdfs, mapred and yarn command.
[33malgo-2    |[0m # Java property: hadoop.root.logger
[33malgo-2    |[0m # export HADOOP_DAEMON_ROOT_LOGGER=INFO,RFA
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default log level and output location for security-related messages.
[33malgo-2    |[0m # You will almost certainly want to change this on a per-daemon basis via
[33malgo-2    |[0m # the Java property (i.e., -Dhadoop.security.logger=foo). (Note that the
[33malgo-2    |[0m # defaults for the NN and 2NN override this by default.)
[33malgo-2    |[0m # Java property: hadoop.security.logger
[33malgo-2    |[0m # export HADOOP_SECURITY_LOGGER=INFO,NullAppender
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default process priority level
[33malgo-2    |[0m # Note that sub-processes will also run at this level!
[33malgo-2    |[0m # export HADOOP_NICENESS=0
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default name for the service level authorization file
[33malgo-2    |[0m # Java property: hadoop.policy.file
[33malgo-2    |[0m # export HADOOP_POLICYFILE="hadoop-policy.xml"
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # NOTE: this is not used by default!  <-----
[33malgo-2    |[0m # You can define variables right here and then re-use them later on.
[33malgo-2    |[0m # For example, it is common to use the same garbage collection settings
[33malgo-2    |[0m # for all the daemons.  So one could define:
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HADOOP_GC_SETTINGS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps"
[33malgo-2    |[0m #
[33malgo-2    |[0m # .. and then use it as per the b option under the namenode.
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Secure/privileged execution
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Out of the box, Hadoop uses jsvc from Apache Commons to launch daemons
[33malgo-2    |[0m # on privileged ports.  This functionality can be replaced by providing
[33malgo-2    |[0m # custom functions.  See hadoop-functions.sh for more information.
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m # The jsvc implementation to use. Jsvc is required to run secure datanodes
[33malgo-2    |[0m # that bind to privileged ports to provide authentication of data transfer
[33malgo-2    |[0m # protocol.  Jsvc is not required if SASL is configured for authentication of
[33malgo-2    |[0m # data transfer protocol using non-privileged ports.
[33malgo-2    |[0m # export JSVC_HOME=/usr/bin
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # This directory contains pids for secure and privileged processes.
[33malgo-2    |[0m #export HADOOP_SECURE_PID_DIR=${HADOOP_PID_DIR}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # This directory contains the logs for secure and privileged processes.
[33malgo-2    |[0m # Java property: hadoop.log.dir
[33malgo-2    |[0m # export HADOOP_SECURE_LOG=${HADOOP_LOG_DIR}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # When running a secure daemon, the default value of HADOOP_IDENT_STRING
[33malgo-2    |[0m # ends up being a bit bogus.  Therefore, by default, the code will
[33malgo-2    |[0m # replace HADOOP_IDENT_STRING with HADOOP_xx_SECURE_USER.  If one wants
[33malgo-2    |[0m # to keep HADOOP_IDENT_STRING untouched, then uncomment this line.
[33malgo-2    |[0m # export HADOOP_SECURE_IDENT_PRESERVE="true"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # NameNode specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default log level and output location for file system related change
[33malgo-2    |[0m # messages. For non-namenode daemons, the Java property must be set in
[33malgo-2    |[0m # the appropriate _OPTS if one wants something other than INFO,NullAppender
[33malgo-2    |[0m # Java property: hdfs.audit.logger
[33malgo-2    |[0m # export HDFS_AUDIT_LOGGER=INFO,NullAppender
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the NameNode.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # a) Set JMX options
[33malgo-2    |[0m # export HDFS_NAMENODE_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
[33malgo-2    |[0m #
[33malgo-2    |[0m # b) Set garbage collection logs
[33malgo-2    |[0m # export HDFS_NAMENODE_OPTS="${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[33malgo-2    |[0m #
[33malgo-2    |[0m # c) ... or set them directly
[33malgo-2    |[0m # export HDFS_NAMENODE_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[33malgo-2    |[0m 
[33malgo-2    |[0m # this is the default:
[33malgo-2    |[0m # export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # SecondaryNameNode specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the SecondaryNameNode.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # This is the default:
[33malgo-2    |[0m # export HDFS_SECONDARYNAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # DataNode specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the DataNode.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # This is the default:
[33malgo-2    |[0m # export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS"
[33malgo-2    |[0m 
[33malgo-2    |[0m # On secure datanodes, user to run the datanode as after dropping privileges.
[33malgo-2    |[0m # This **MUST** be uncommented to enable secure HDFS if using privileged ports
[33malgo-2    |[0m # to provide authentication of data transfer protocol.  This **MUST NOT** be
[33malgo-2    |[0m # defined if SASL is configured for authentication of data transfer protocol
[33malgo-2    |[0m # using non-privileged ports.
[33malgo-2    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[33malgo-2    |[0m # export HDFS_DATANODE_SECURE_USER=hdfs
[33malgo-2    |[0m 
[33malgo-2    |[0m # Supplemental options for secure datanodes
[33malgo-2    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[33malgo-2    |[0m # server jvm.
[33malgo-2    |[0m # export HDFS_DATANODE_SECURE_EXTRA_OPTS="-jvm server"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # NFS3 Gateway specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the NFS3 Gateway.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_NFS3_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the Hadoop portmapper.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_PORTMAP_OPTS="-Xmx512m"
[33malgo-2    |[0m 
[33malgo-2    |[0m # Supplemental options for priviliged gateways
[33malgo-2    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[33malgo-2    |[0m # server jvm.
[33malgo-2    |[0m # export HDFS_NFS3_SECURE_EXTRA_OPTS="-jvm server"
[33malgo-2    |[0m 
[33malgo-2    |[0m # On privileged gateways, user to run the gateway as after dropping privileges
[33malgo-2    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[33malgo-2    |[0m # export HDFS_NFS3_SECURE_USER=nfsserver
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # ZKFailoverController specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the ZKFailoverController.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_ZKFC_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # QuorumJournalNode specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the QuorumJournalNode.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_JOURNALNODE_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # HDFS Balancer specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the HDFS Balancer.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_BALANCER_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # HDFS Mover specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the HDFS Mover.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_MOVER_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Router-based HDFS Federation specific parameters
[33malgo-2    |[0m # Specify the JVM options to be used when starting the RBF Routers.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_DFSROUTER_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # HDFS StorageContainerManager specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the HDFS Storage Container Manager.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_STORAGECONTAINERMANAGER_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Advanced Users Only!
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # When building Hadoop, one can add the class paths to the commands
[33malgo-2    |[0m # via this special env var:
[33malgo-2    |[0m # export HADOOP_ENABLE_BUILD_PATHS="true"
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # To prevent accidents, shell commands be (superficially) locked
[33malgo-2    |[0m # to only allow certain users to execute certain subcommands.
[33malgo-2    |[0m # It uses the format of (command)_(subcommand)_USER.
[33malgo-2    |[0m #
[33malgo-2    |[0m # For example, to limit who can execute the namenode command,
[33malgo-2    |[0m # export HDFS_NAMENODE_USER=hdfs
[33malgo-2    |[0m export SPARK_MASTER_HOST=192.168.240.3
[33malgo-2    |[0m export AWS_CONTAINER_CREDENTIALS_RELATIVE_URI=
[33malgo-2    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[33malgo-2    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/core-site.xml
[33malgo-2    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/core-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0" encoding="UTF-8"?>
[33malgo-2    |[0m  <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[33malgo-2    |[0m  <!-- Put site-specific property overrides in this file. -->
[33malgo-2    |[0m 
[33malgo-2    |[0m  <configuration>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>fs.defaultFS</name>
[33malgo-2    |[0m          <value>hdfs://192.168.240.3/</value>
[33malgo-2    |[0m          <description>NameNode URI</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>fs.s3a.aws.credentials.provider</name>
[33malgo-2    |[0m          <value>com.amazonaws.auth.DefaultAWSCredentialsProviderChain</value>
[33malgo-2    |[0m          <description>AWS S3 credential provider</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>fs.s3.impl</name>
[33malgo-2    |[0m          <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
[33malgo-2    |[0m          <description>s3a filesystem implementation</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>fs.AbstractFileSystem.s3a.imp</name>
[33malgo-2    |[0m          <value>org.apache.hadoop.fs.s3a.S3A</value>
[33malgo-2    |[0m          <description>s3a filesystem implementation</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m  
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>spark.executor.memory</name>
[33malgo-2    |[0m     <value>2g</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>spark.executor.cores</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/log4j.properties
[33malgo-2    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/log4j.properties is: 
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[33malgo-2    |[0m # or more contributor license agreements.  See the NOTICE file
[33malgo-2    |[0m # distributed with this work for additional information
[33malgo-2    |[0m # regarding copyright ownership.  The ASF licenses this file
[33malgo-2    |[0m # to you under the Apache License, Version 2.0 (the
[33malgo-2    |[0m # "License"); you may not use this file except in compliance
[33malgo-2    |[0m # with the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m # Define some default values that can be overridden by system properties
[33malgo-2    |[0m hadoop.root.logger=INFO,console
[33malgo-2    |[0m hadoop.log.dir=.
[33malgo-2    |[0m hadoop.log.file=hadoop.log
[33malgo-2    |[0m 
[33malgo-2    |[0m # Define the root logger to the system property "hadoop.root.logger".
[33malgo-2    |[0m log4j.rootLogger=${hadoop.root.logger}, EventCounter
[33malgo-2    |[0m 
[33malgo-2    |[0m # Logging Threshold
[33malgo-2    |[0m log4j.threshold=ALL
[33malgo-2    |[0m 
[33malgo-2    |[0m # Null Appender
[33malgo-2    |[0m log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Rolling File Appender - cap space usage at 5gb.
[33malgo-2    |[0m #
[33malgo-2    |[0m hadoop.log.maxfilesize=256MB
[33malgo-2    |[0m hadoop.log.maxbackupindex=20
[33malgo-2    |[0m log4j.appender.RFA=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.RFA.MaxFileSize=${hadoop.log.maxfilesize}
[33malgo-2    |[0m log4j.appender.RFA.MaxBackupIndex=${hadoop.log.maxbackupindex}
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.RFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m 
[33malgo-2    |[0m # Pattern format: Date LogLevel LoggerName LogMessage
[33malgo-2    |[0m log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[33malgo-2    |[0m # Debugging Pattern format
[33malgo-2    |[0m #log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Daily Rolling File Appender
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.DRFA.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m 
[33malgo-2    |[0m # Rollver at midnight
[33malgo-2    |[0m #log4j.appender.DRFA.DatePattern=.yyyy-MM-dd
[33malgo-2    |[0m # Rollver at every hour
[33malgo-2    |[0m log4j.appender.DRFA.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m 
[33malgo-2    |[0m # Pattern format: Date LogLevel LoggerName LogMessage
[33malgo-2    |[0m log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m # Debugging Pattern format
[33malgo-2    |[0m #log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # console
[33malgo-2    |[0m # Add "console" to rootlogger above if you want to use this
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.console=org.apache.log4j.ConsoleAppender
[33malgo-2    |[0m log4j.appender.console.target=System.err
[33malgo-2    |[0m log4j.appender.console.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # TaskLog Appender
[33malgo-2    |[0m #
[33malgo-2    |[0m log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.TLA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # HDFS block state change log from block manager
[33malgo-2    |[0m #
[33malgo-2    |[0m # Uncomment the following to log normal block state change
[33malgo-2    |[0m # messages from BlockManager in NameNode.
[33malgo-2    |[0m #log4j.logger.BlockStateChange=DEBUG
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m #Security appender
[33malgo-2    |[0m #
[33malgo-2    |[0m hadoop.security.logger=INFO,NullAppender
[33malgo-2    |[0m hadoop.security.log.maxfilesize=256MB
[33malgo-2    |[0m hadoop.security.log.maxbackupindex=20
[33malgo-2    |[0m log4j.category.SecurityLogger=${hadoop.security.logger}
[33malgo-2    |[0m hadoop.security.log.file=SecurityAuth-${user.name}.audit
[33malgo-2    |[0m log4j.appender.RFAS=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.RFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
[33malgo-2    |[0m log4j.appender.RFAS.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[33malgo-2    |[0m log4j.appender.RFAS.MaxFileSize=${hadoop.security.log.maxfilesize}
[33malgo-2    |[0m log4j.appender.RFAS.MaxBackupIndex=${hadoop.security.log.maxbackupindex}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Daily Rolling Security appender
[33malgo-2    |[0m #
[33malgo-2    |[0m log4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.DRFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
[33malgo-2    |[0m log4j.appender.DRFAS.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.DRFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[33malgo-2    |[0m log4j.appender.DRFAS.DatePattern=.yyyy-MM-dd
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # hadoop configuration logging
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m # Uncomment the following line to turn off configuration deprecation warnings.
[33malgo-2    |[0m # log4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # hdfs audit logging
[33malgo-2    |[0m #
[33malgo-2    |[0m hdfs.audit.logger=INFO,NullAppender
[33malgo-2    |[0m hdfs.audit.log.maxfilesize=256MB
[33malgo-2    |[0m hdfs.audit.log.maxbackupindex=20
[33malgo-2    |[0m log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}
[33malgo-2    |[0m log4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false
[33malgo-2    |[0m log4j.appender.RFAAUDIT=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.RFAAUDIT.File=${hadoop.log.dir}/hdfs-audit.log
[33malgo-2    |[0m log4j.appender.RFAAUDIT.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.RFAAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[33malgo-2    |[0m log4j.appender.RFAAUDIT.MaxFileSize=${hdfs.audit.log.maxfilesize}
[33malgo-2    |[0m log4j.appender.RFAAUDIT.MaxBackupIndex=${hdfs.audit.log.maxbackupindex}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # NameNode metrics logging.
[33malgo-2    |[0m # The default is to retain two namenode-metrics.log files up to 64MB each.
[33malgo-2    |[0m #
[33malgo-2    |[0m namenode.metrics.logger=INFO,NullAppender
[33malgo-2    |[0m log4j.logger.NameNodeMetricsLog=${namenode.metrics.logger}
[33malgo-2    |[0m log4j.additivity.NameNodeMetricsLog=false
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.File=${hadoop.log.dir}/namenode-metrics.log
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.MaxBackupIndex=1
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.MaxFileSize=64MB
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # DataNode metrics logging.
[33malgo-2    |[0m # The default is to retain two datanode-metrics.log files up to 64MB each.
[33malgo-2    |[0m #
[33malgo-2    |[0m datanode.metrics.logger=INFO,NullAppender
[33malgo-2    |[0m log4j.logger.DataNodeMetricsLog=${datanode.metrics.logger}
[33malgo-2    |[0m log4j.additivity.DataNodeMetricsLog=false
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.File=${hadoop.log.dir}/datanode-metrics.log
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.MaxBackupIndex=1
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.MaxFileSize=64MB
[33malgo-2    |[0m 
[33malgo-2    |[0m # Custom Logging levels
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=DEBUG
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m # AWS SDK & S3A FileSystem
[33malgo-2    |[0m #log4j.logger.com.amazonaws=ERROR
[33malgo-2    |[0m log4j.logger.com.amazonaws.http.AmazonHttpClient=ERROR
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.fs.s3a.S3AFileSystem=WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Event Counter Appender
[33malgo-2    |[0m # Sends counts of logging messages at different severity levels to Hadoop Metrics.
[33malgo-2    |[0m #
[33malgo-2    |[0m log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Job Summary Appender
[33malgo-2    |[0m #
[33malgo-2    |[0m # Use following logger to send summary to separate file defined by
[33malgo-2    |[0m # hadoop.mapreduce.jobsummary.log.file :
[33malgo-2    |[0m # hadoop.mapreduce.jobsummary.logger=INFO,JSA
[33malgo-2    |[0m # 
[33malgo-2    |[0m hadoop.mapreduce.jobsummary.logger=${hadoop.root.logger}
[33malgo-2    |[0m hadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log
[33malgo-2    |[0m hadoop.mapreduce.jobsummary.log.maxfilesize=256MB
[33malgo-2    |[0m hadoop.mapreduce.jobsummary.log.maxbackupindex=20
[33malgo-2    |[0m log4j.appender.JSA=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.JSA.File=${hadoop.log.dir}/${hadoop.mapreduce.jobsummary.log.file}
[33malgo-2    |[0m log4j.appender.JSA.MaxFileSize=${hadoop.mapreduce.jobsummary.log.maxfilesize}
[33malgo-2    |[0m log4j.appender.JSA.MaxBackupIndex=${hadoop.mapreduce.jobsummary.log.maxbackupindex}
[33malgo-2    |[0m log4j.appender.JSA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.JSA.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
[33malgo-2    |[0m log4j.appender.JSA.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.logger.org.apache.hadoop.mapred.JobInProgress$JobSummary=${hadoop.mapreduce.jobsummary.logger}
[33malgo-2    |[0m log4j.additivity.org.apache.hadoop.mapred.JobInProgress$JobSummary=false
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # shuffle connection log from shuffleHandler
[33malgo-2    |[0m # Uncomment the following line to enable logging of shuffle connections
[33malgo-2    |[0m # log4j.logger.org.apache.hadoop.mapred.ShuffleHandler.audit=DEBUG
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Yarn ResourceManager Application Summary Log
[33malgo-2    |[0m #
[33malgo-2    |[0m # Set the ResourceManager summary log filename
[33malgo-2    |[0m yarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log
[33malgo-2    |[0m # Set the ResourceManager summary log level and appender
[33malgo-2    |[0m yarn.server.resourcemanager.appsummary.logger=${hadoop.root.logger}
[33malgo-2    |[0m #yarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY
[33malgo-2    |[0m 
[33malgo-2    |[0m # To enable AppSummaryLogging for the RM,
[33malgo-2    |[0m # set yarn.server.resourcemanager.appsummary.logger to
[33malgo-2    |[0m # <LEVEL>,RMSUMMARY in hadoop-env.sh
[33malgo-2    |[0m 
[33malgo-2    |[0m # Appender for ResourceManager Application Summary Log
[33malgo-2    |[0m # Requires the following properties to be set
[33malgo-2    |[0m #    - hadoop.log.dir (Hadoop Log directory)
[33malgo-2    |[0m #    - yarn.server.resourcemanager.appsummary.log.file (resource manager app summary log filename)
[33malgo-2    |[0m #    - yarn.server.resourcemanager.appsummary.logger (resource manager app summary log level and appender)
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=${yarn.server.resourcemanager.appsummary.logger}
[33malgo-2    |[0m log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=false
[33malgo-2    |[0m log4j.appender.RMSUMMARY=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.RMSUMMARY.File=${hadoop.log.dir}/${yarn.server.resourcemanager.appsummary.log.file}
[33malgo-2    |[0m log4j.appender.RMSUMMARY.MaxFileSize=256MB
[33malgo-2    |[0m log4j.appender.RMSUMMARY.MaxBackupIndex=20
[33malgo-2    |[0m log4j.appender.RMSUMMARY.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.RMSUMMARY.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m # HS audit log configs
[33malgo-2    |[0m #mapreduce.hs.audit.logger=INFO,HSAUDIT
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=${mapreduce.hs.audit.logger}
[33malgo-2    |[0m #log4j.additivity.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=false
[33malgo-2    |[0m #log4j.appender.HSAUDIT=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m #log4j.appender.HSAUDIT.File=${hadoop.log.dir}/hs-audit.log
[33malgo-2    |[0m #log4j.appender.HSAUDIT.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m #log4j.appender.HSAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[33malgo-2    |[0m #log4j.appender.HSAUDIT.DatePattern=.yyyy-MM-dd
[33malgo-2    |[0m 
[33malgo-2    |[0m # Http Server Request Logs
[33malgo-2    |[0m #log4j.logger.http.requests.namenode=INFO,namenoderequestlog
[33malgo-2    |[0m #log4j.appender.namenoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.namenoderequestlog.Filename=${hadoop.log.dir}/jetty-namenode-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.namenoderequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.http.requests.datanode=INFO,datanoderequestlog
[33malgo-2    |[0m #log4j.appender.datanoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.datanoderequestlog.Filename=${hadoop.log.dir}/jetty-datanode-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.datanoderequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.http.requests.resourcemanager=INFO,resourcemanagerrequestlog
[33malgo-2    |[0m #log4j.appender.resourcemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.resourcemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-resourcemanager-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.resourcemanagerrequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.http.requests.jobhistory=INFO,jobhistoryrequestlog
[33malgo-2    |[0m #log4j.appender.jobhistoryrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.jobhistoryrequestlog.Filename=${hadoop.log.dir}/jetty-jobhistory-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.jobhistoryrequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.http.requests.nodemanager=INFO,nodemanagerrequestlog
[33malgo-2    |[0m #log4j.appender.nodemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.nodemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-nodemanager-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.nodemanagerrequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m # WebHdfs request log on datanodes
[33malgo-2    |[0m # Specify -Ddatanode.webhdfs.logger=INFO,HTTPDRFA on datanode startup to
[33malgo-2    |[0m # direct the log to a separate file.
[33malgo-2    |[0m #datanode.webhdfs.logger=INFO,console
[33malgo-2    |[0m #log4j.logger.datanode.webhdfs=${datanode.webhdfs.logger}
[33malgo-2    |[0m #log4j.appender.HTTPDRFA=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m #log4j.appender.HTTPDRFA.File=${hadoop.log.dir}/hadoop-datanode-webhdfs.log
[33malgo-2    |[0m #log4j.appender.HTTPDRFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m #log4j.appender.HTTPDRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[33malgo-2    |[0m #log4j.appender.HTTPDRFA.DatePattern=.yyyy-MM-dd
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m # Appender for viewing information for errors and warnings
[33malgo-2    |[0m yarn.ewma.cleanupInterval=300
[33malgo-2    |[0m yarn.ewma.messageAgeLimitSeconds=86400
[33malgo-2    |[0m yarn.ewma.maxUniqueMessages=250
[33malgo-2    |[0m log4j.appender.EWMA=org.apache.hadoop.yarn.util.Log4jWarningErrorMetricsAppender
[33malgo-2    |[0m log4j.appender.EWMA.cleanupInterval=${yarn.ewma.cleanupInterval}
[33malgo-2    |[0m log4j.appender.EWMA.messageAgeLimitSeconds=${yarn.ewma.messageAgeLimitSeconds}
[33malgo-2    |[0m log4j.appender.EWMA.maxUniqueMessages=${yarn.ewma.maxUniqueMessages}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Fair scheduler state dump
[33malgo-2    |[0m #
[33malgo-2    |[0m # Use following logger to dump the state to a separate file
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=DEBUG,FSSTATEDUMP
[33malgo-2    |[0m #log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=false
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.File=${hadoop.log.dir}/fairscheduler-statedump.log
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.MaxFileSize=${hadoop.log.maxfilesize}
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.MaxBackupIndex=${hadoop.log.maxbackupindex}
[33malgo-2    |[0m 
[33malgo-2    |[0m # Log levels of third-party libraries
[33malgo-2    |[0m log4j.logger.org.apache.commons.beanutils=WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m #AWS SDK Logging
[33malgo-2    |[0m log4j.logger.com.amazonaws=WARN
[33malgo-2    |[0m log4j.logger.org.apache.zookeeper=ERROR
[33malgo-2    |[0m log4j.logger.org.apache.hadoop.hbase=WARN
[33malgo-2    |[0m log4j.logger.amazon.emr.metrics=WARN
[33malgo-2    |[0m log4j.logger.emr=INFO
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.HADOOP=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.HADOOP.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m log4j.appender.HADOOP.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.appender.HADOOP.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.HADOOP.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.JHS=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.JHS.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m log4j.appender.JHS.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.appender.JHS.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.JHS.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.MAPRED=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.MAPRED.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m log4j.appender.MAPRED.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.appender.MAPRED.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.MAPRED.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.YARN=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.YARN.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m log4j.appender.YARN.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.appender.YARN.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.YARN.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/hive/conf/hive-env.sh
[33malgo-2    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/hive/conf/hive-env.sh is: 
[33malgo-2    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[33malgo-2    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[33malgo-2    |[0m 
[36malgo-1    |[0m 11-14 15:19 root         INFO     Detected instance type: m5.xlarge with total memory: 16384M and total cores: 4
[33malgo-2    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/hive/conf/hive-log4j2.properties
[36malgo-1    |[0m 11-14 15:19 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[33malgo-2    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/hive/conf/hive-log4j2.properties is: 
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[33malgo-2    |[0m # or more contributor license agreements.  See the NOTICE file
[33malgo-2    |[0m # distributed with this work for additional information
[33malgo-2    |[0m # regarding copyright ownership.  The ASF licenses this file
[33malgo-2    |[0m # to you under the Apache License, Version 2.0 (the
[33malgo-2    |[0m # "License"); you may not use this file except in compliance
[33malgo-2    |[0m # with the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m status = INFO
[33malgo-2    |[0m name = HiveLog4j2
[33malgo-2    |[0m packages = org.apache.hadoop.hive.ql.log
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of properties
[33malgo-2    |[0m property.hive.log.level = INFO
[33malgo-2    |[0m property.hive.root.logger = DRFA
[33malgo-2    |[0m property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
[33malgo-2    |[0m property.hive.log.file = hive.log
[33malgo-2    |[0m property.hive.perflogger.log.level = INFO
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of all appenders
[33malgo-2    |[0m appenders = console, DRFA
[33malgo-2    |[0m 
[33malgo-2    |[0m # console appender
[33malgo-2    |[0m appender.console.type = Console
[33malgo-2    |[0m appender.console.name = console
[33malgo-2    |[0m appender.console.target = SYSTEM_ERR
[33malgo-2    |[0m appender.console.layout.type = PatternLayout
[33malgo-2    |[0m appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m # daily rolling file appender
[33malgo-2    |[0m appender.DRFA.type = RollingRandomAccessFile
[33malgo-2    |[0m appender.DRFA.name = DRFA
[33malgo-2    |[0m appender.DRFA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
[33malgo-2    |[0m # Use %pid in the filePattern to append <process-id>@<host-name> to the filename if you want separate log files for different CLI session
[33malgo-2    |[0m appender.DRFA.filePattern = ${sys:hive.log.dir}/${sys:hive.log.file}.%d{yyyy-MM-dd}
[33malgo-2    |[0m appender.DRFA.layout.type = PatternLayout
[33malgo-2    |[0m appender.DRFA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[33malgo-2    |[0m appender.DRFA.policies.type = Policies
[33malgo-2    |[0m appender.DRFA.policies.time.type = TimeBasedTriggeringPolicy
[33malgo-2    |[0m appender.DRFA.policies.time.interval = 1
[33malgo-2    |[0m appender.DRFA.policies.time.modulate = true
[33malgo-2    |[0m appender.DRFA.strategy.type = DefaultRolloverStrategy
[33malgo-2    |[0m appender.DRFA.strategy.max = 30
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of all loggers
[33malgo-2    |[0m loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, PerfLogger, AmazonAws, ApacheHttp
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
[33malgo-2    |[0m logger.NIOServerCnxn.level = WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
[33malgo-2    |[0m logger.ClientCnxnSocketNIO.level = WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.DataNucleus.name = DataNucleus
[33malgo-2    |[0m logger.DataNucleus.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.Datastore.name = Datastore
[33malgo-2    |[0m logger.Datastore.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.JPOX.name = JPOX
[33malgo-2    |[0m logger.JPOX.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.AmazonAws.name=com.amazonaws
[33malgo-2    |[0m logger.AmazonAws.level = INFO
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.ApacheHttp.name=org.apache.http
[33malgo-2    |[0m logger.ApacheHttp.level = INFO
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.PerfLogger.name = org.apache.hadoop.hive.ql.log.PerfLogger
[33malgo-2    |[0m logger.PerfLogger.level = ${sys:hive.perflogger.log.level}
[33malgo-2    |[0m 
[33malgo-2    |[0m # root logger
[33malgo-2    |[0m rootLogger.level = ${sys:hive.log.level}
[33malgo-2    |[0m rootLogger.appenderRefs = root
[33malgo-2    |[0m rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/hive/conf/hive-exec-log4j2.properties
[36malgo-1    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0"?>
[36malgo-1    |[0m <!-- Site specific YARN configuration properties -->
[36malgo-1    |[0m  <configuration>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.resourcemanager.hostname</name>
[36malgo-1    |[0m          <value>192.168.240.3</value>
[36malgo-1    |[0m          <description>The hostname of the RM.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.hostname</name>
[36malgo-1    |[0m          <value>algo-1</value>
[36malgo-1    |[0m          <description>The hostname of the NM.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.webapp.address</name>
[36malgo-1    |[0m          <value>algo-1:8042</value>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.vmem-pmem-ratio</name>
[36malgo-1    |[0m          <value>5</value>
[36malgo-1    |[0m          <description>Ratio between virtual memory to physical memory.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.resourcemanager.am.max-attempts</name>
[36malgo-1    |[0m          <value>1</value>
[36malgo-1    |[0m          <description>The maximum number of application attempts.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.env-whitelist</name>
[36malgo-1    |[0m          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>
[36malgo-1    |[0m          <description>Environment variable whitelist</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m  
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.minimum-allocation-mb</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.maximum-allocation-mb</name>
[36malgo-1    |[0m     <value>15892</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.minimum-allocation-vcores</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.maximum-allocation-vcores</name>
[36malgo-1    |[0m     <value>4</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.nodemanager.resource.memory-mb</name>
[36malgo-1    |[0m     <value>15892</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.nodemanager.resource.cpu-vcores</name>
[36malgo-1    |[0m     <value>4</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 
[33malgo-2    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/hive/conf/hive-exec-log4j2.properties is: 
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[33malgo-2    |[0m # or more contributor license agreements.  See the NOTICE file
[33malgo-2    |[0m # distributed with this work for additional information
[33malgo-2    |[0m # regarding copyright ownership.  The ASF licenses this file
[33malgo-2    |[0m # to you under the Apache License, Version 2.0 (the
[33malgo-2    |[0m # "License"); you may not use this file except in compliance
[33malgo-2    |[0m # with the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m status = INFO
[33malgo-2    |[0m name = HiveExecLog4j2
[33malgo-2    |[0m packages = org.apache.hadoop.hive.ql.log
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of properties
[33malgo-2    |[0m property.hive.log.level = INFO
[33malgo-2    |[0m property.hive.root.logger = FA
[33malgo-2    |[0m property.hive.query.id = hadoop
[33malgo-2    |[0m property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
[33malgo-2    |[0m property.hive.log.file = ${sys:hive.query.id}.log
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of all appenders
[33malgo-2    |[0m appenders = console, FA
[33malgo-2    |[0m 
[33malgo-2    |[0m # console appender
[33malgo-2    |[0m appender.console.type = Console
[33malgo-2    |[0m appender.console.name = console
[33malgo-2    |[0m appender.console.target = SYSTEM_ERR
[33malgo-2    |[0m appender.console.layout.type = PatternLayout
[33malgo-2    |[0m appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m # simple file appender
[33malgo-2    |[0m appender.FA.type = RandomAccessFile
[33malgo-2    |[0m appender.FA.name = FA
[33malgo-2    |[0m appender.FA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
[33malgo-2    |[0m appender.FA.layout.type = PatternLayout
[33malgo-2    |[0m appender.FA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of all loggers
[33malgo-2    |[0m loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
[33malgo-2    |[0m logger.NIOServerCnxn.level = WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
[33malgo-2    |[0m logger.ClientCnxnSocketNIO.level = WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.DataNucleus.name = DataNucleus
[33malgo-2    |[0m logger.DataNucleus.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.Datastore.name = Datastore
[33malgo-2    |[0m logger.Datastore.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.JPOX.name = JPOX
[33malgo-2    |[0m logger.JPOX.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m # root logger
[33malgo-2    |[0m rootLogger.level = ${sys:hive.log.level}
[33malgo-2    |[0m rootLogger.appenderRefs = root
[33malgo-2    |[0m rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/hive/conf/hive-site.xml
[33malgo-2    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/hive/conf/hive-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0"?>
[33malgo-2    |[0m <!--
[33malgo-2    |[0m   Licensed to the Apache Software Foundation (ASF) under one or more
[33malgo-2    |[0m   contributor license agreements.  See the NOTICE file distributed with
[33malgo-2    |[0m   this work for additional information regarding copyright ownership.
[33malgo-2    |[0m   The ASF licenses this file to You under the Apache License, Version 2.0
[33malgo-2    |[0m   (the "License"); you may not use this file except in compliance with
[33malgo-2    |[0m   the License.  You may obtain a copy of the License at
[33malgo-2    |[0m 
[33malgo-2    |[0m       http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m 
[33malgo-2    |[0m   Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m   distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m   See the License for the specific language governing permissions and
[33malgo-2    |[0m   limitations under the License.
[33malgo-2    |[0m -->
[33malgo-2    |[0m <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[33malgo-2    |[0m 
[33malgo-2    |[0m <configuration>
[33malgo-2    |[0m 
[33malgo-2    |[0m <!-- Hive Configuration can either be stored in this file or in the hadoop configuration files  -->
[33malgo-2    |[0m <!-- that are implied by Hadoop setup variables.                                                -->
[33malgo-2    |[0m <!-- Aside from Hadoop setup variables - this file is provided as a convenience so that Hive    -->
[33malgo-2    |[0m <!-- users do not have to edit hadoop configuration files (that may be managed as a centralized -->
[33malgo-2    |[0m <!-- resource).                                                                                 -->
[33malgo-2    |[0m 
[33malgo-2    |[0m <!-- Hive Execution Parameters -->
[33malgo-2    |[0m 
[33malgo-2    |[0m <property>
[33malgo-2    |[0m   <name>javax.jdo.option.ConnectionURL</name>
[33malgo-2    |[0m   <value>jdbc:derby:;databaseName=/var/lib/hive/metastore/metastore_db;create=true</value>
[33malgo-2    |[0m   <description>JDBC connect string for a JDBC metastore</description>
[33malgo-2    |[0m </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m <property>
[33malgo-2    |[0m   <name>javax.jdo.option.ConnectionDriverName</name>
[33malgo-2    |[0m   <value>org.apache.derby.jdbc.EmbeddedDriver</value>
[33malgo-2    |[0m   <description>Driver class name for a JDBC metastore</description>
[33malgo-2    |[0m </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>key</name>
[33malgo-2    |[0m     <value>value</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[33malgo-2    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.driver.host=192.168.240.3
[33malgo-2    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[33malgo-2    |[0m 
[33malgo-2    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[33malgo-2    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[33malgo-2    |[0m spark.rpc.askTimeout=300s
[33malgo-2    |[0m spark.driver.memory 2048m
[33malgo-2    |[0m spark.driver.memoryOverhead 204m
[33malgo-2    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[33malgo-2    |[0m spark.executor.memory 12399m
[33malgo-2    |[0m spark.executor.memoryOverhead 1239m
[33malgo-2    |[0m spark.executor.cores 4
[33malgo-2    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[33malgo-2    |[0m spark.executor.instances 2
[33malgo-2    |[0m spark.default.parallelism 16
[33malgo-2    |[0m spark.executor.memory 2g
[33malgo-2    |[0m spark.executor.cores 1
[33malgo-2    |[0m key value
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/spark/conf/spark-env.sh
[33malgo-2    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/spark/conf/spark-env.sh is: 
[33malgo-2    |[0m #EMPTY FILE AVOID OVERRIDDING ENV VARS
[33malgo-2    |[0m # Specifically, without copying the empty file, SPARK_HISTORY_OPTS will be overriden, 
[33malgo-2    |[0m # spark.history.ui.port defaults to 18082, and spark.eventLog.dir defaults to local fs
[33malgo-2    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[33malgo-2    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/spark/conf/log4j.properties
[36malgo-1    |[0m 11-14 15:19 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/spark/conf/log4j.properties is: 
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[36malgo-1    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[36malgo-1    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.driver.host=192.168.240.3
[36malgo-1    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[36malgo-1    |[0m 
[36malgo-1    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[36malgo-1    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[36malgo-1    |[0m spark.rpc.askTimeout=300s
[36malgo-1    |[0m spark.driver.memory 2048m
[36malgo-1    |[0m spark.driver.memoryOverhead 204m
[36malgo-1    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[36malgo-1    |[0m spark.executor.memory 12399m
[36malgo-1    |[0m spark.executor.memoryOverhead 1239m
[36malgo-1    |[0m spark.executor.cores 4
[36malgo-1    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[36malgo-1    |[0m spark.executor.instances 2
[36malgo-1    |[0m spark.default.parallelism 16
[36malgo-1    |[0m 
[33malgo-2    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/spark/conf/hive-site.xml
[33malgo-2    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/spark/conf/hive-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0"?>
[33malgo-2    |[0m <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[33malgo-2    |[0m <configuration>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>key</name>
[33malgo-2    |[0m     <value>value</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m </configuration>
[36malgo-1    |[0m 11-14 15:19 root         INFO     Finished Yarn configuration files setup.
[33malgo-2    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/spark/conf/metrics.properties
[36malgo-1    |[0m 11-14 15:19 root         INFO     reading user configuration from /opt/ml/processing/input/conf/configuration.json
[33malgo-2    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/spark/conf/metrics.properties is: 
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[36malgo-1    |[0m 11-14 15:19 root         INFO     User configuration list or dict: [{'Classification': 'spark-defaults', 'Properties': {'spark.executor.memory': '2g', 'spark.executor.cores': '1'}}, {'Classification': 'hadoop-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'core-site', 'Properties': {'spark.executor.memory': '2g', 'spark.executor.cores': '1'}}, {'Classification': 'hadoop-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'hive-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-exec-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-site', 'Properties': {'key': 'value'}}, {'Classification': 'spark-defaults', 'Properties': {'key': 'value'}}, {'Classification': 'spark-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'spark-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'spark-hive-site', 'Properties': {'key': 'value'}}, {'Classification': 'spark-metrics', 'Properties': {'key': 'value'}}, {'Classification': 'yarn-site', 'Properties': {'key': 'value'}}, {'Classification': 'yarn-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}] , type <class 'list'>
[36malgo-1    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0"?>
[33malgo-2    |[0m <!-- Site specific YARN configuration properties -->
[33malgo-2    |[0m  <configuration>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.resourcemanager.hostname</name>
[33malgo-2    |[0m          <value>192.168.240.3</value>
[33malgo-2    |[0m          <description>The hostname of the RM.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.hostname</name>
[33malgo-2    |[0m          <value>algo-2</value>
[33malgo-2    |[0m          <description>The hostname of the NM.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.webapp.address</name>
[33malgo-2    |[0m          <value>algo-2:8042</value>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.vmem-pmem-ratio</name>
[33malgo-2    |[0m          <value>5</value>
[33malgo-2    |[0m          <description>Ratio between virtual memory to physical memory.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.resourcemanager.am.max-attempts</name>
[33malgo-2    |[0m          <value>1</value>
[33malgo-2    |[0m          <description>The maximum number of application attempts.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.env-whitelist</name>
[33malgo-2    |[0m          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>
[33malgo-2    |[0m          <description>Environment variable whitelist</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m  
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.minimum-allocation-mb</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.maximum-allocation-mb</name>
[33malgo-2    |[0m     <value>15892</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.minimum-allocation-vcores</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.maximum-allocation-vcores</name>
[33malgo-2    |[0m     <value>4</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.nodemanager.resource.memory-mb</name>
[33malgo-2    |[0m     <value>15892</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.nodemanager.resource.cpu-vcores</name>
[33malgo-2    |[0m     <value>4</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>key</name>
[33malgo-2    |[0m     <value>value</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 
[36malgo-1    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[36malgo-1    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.driver.host=192.168.240.3
[36malgo-1    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[36malgo-1    |[0m 
[36malgo-1    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[36malgo-1    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[36malgo-1    |[0m spark.rpc.askTimeout=300s
[36malgo-1    |[0m spark.driver.memory 2048m
[36malgo-1    |[0m spark.driver.memoryOverhead 204m
[36malgo-1    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[36malgo-1    |[0m spark.executor.memory 12399m
[36malgo-1    |[0m spark.executor.memoryOverhead 1239m
[36malgo-1    |[0m spark.executor.cores 4
[36malgo-1    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[36malgo-1    |[0m spark.executor.instances 2
[36malgo-1    |[0m spark.default.parallelism 16
[36malgo-1    |[0m spark.executor.memory 2g
[36malgo-1    |[0m spark.executor.cores 1
[36malgo-1    |[0m 
[33malgo-2    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/yarn-env.sh
[33malgo-2    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-env.sh is: 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one or more
[33malgo-2    |[0m # contributor license agreements.  See the NOTICE file distributed with
[33malgo-2    |[0m # this work for additional information regarding copyright ownership.
[33malgo-2    |[0m # The ASF licenses this file to You under the Apache License, Version 2.0
[33malgo-2    |[0m # (the "License"); you may not use this file except in compliance with
[33malgo-2    |[0m # the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## THIS FILE ACTS AS AN OVERRIDE FOR hadoop-env.sh FOR ALL
[33malgo-2    |[0m ## WORK DONE BY THE yarn AND RELATED COMMANDS.
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## Precedence rules:
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## yarn-env.sh > hadoop-env.sh > hard-coded defaults
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## YARN_xyz > HADOOP_xyz > hard-coded defaults
[33malgo-2    |[0m ##
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Resource Manager specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the max heapsize for the ResourceManager.  If no units are
[33malgo-2    |[0m # given, it will be assumed to be in MB.
[33malgo-2    |[0m # This value will be overridden by an Xmx setting specified in either
[33malgo-2    |[0m # HADOOP_OPTS and/or YARN_RESOURCEMANAGER_OPTS.
[33malgo-2    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX
[33malgo-2    |[0m #export YARN_RESOURCEMANAGER_HEAPSIZE=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the ResourceManager.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # Examples for a Sun/Oracle JDK:
[33malgo-2    |[0m # a) override the appsummary log file:
[33malgo-2    |[0m # export YARN_RESOURCEMANAGER_OPTS="-Dyarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log -Dyarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY"
[33malgo-2    |[0m #
[33malgo-2    |[0m # b) Set JMX options
[33malgo-2    |[0m # export YARN_RESOURCEMANAGER_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
[33malgo-2    |[0m #
[33malgo-2    |[0m # c) Set garbage collection logs from hadoop-env.sh
[33malgo-2    |[0m # export YARN_RESOURCE_MANAGER_OPTS="${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[33malgo-2    |[0m #
[33malgo-2    |[0m # d) ... or set them directly
[33malgo-2    |[0m # export YARN_RESOURCEMANAGER_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[33malgo-2    |[0m #
[33malgo-2    |[0m #
[33malgo-2    |[0m # export YARN_RESOURCEMANAGER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Node Manager specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the max heapsize for the NodeManager.  If no units are
[33malgo-2    |[0m # given, it will be assumed to be in MB.
[33malgo-2    |[0m # This value will be overridden by an Xmx setting specified in either
[33malgo-2    |[0m # HADOOP_OPTS and/or YARN_NODEMANAGER_OPTS.
[33malgo-2    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[33malgo-2    |[0m #export YARN_NODEMANAGER_HEAPSIZE=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the NodeManager.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_NODEMANAGER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # TimeLineServer specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the max heapsize for the timelineserver.  If no units are
[33malgo-2    |[0m # given, it will be assumed to be in MB.
[33malgo-2    |[0m # This value will be overridden by an Xmx setting specified in either
[33malgo-2    |[0m # HADOOP_OPTS and/or YARN_TIMELINESERVER_OPTS.
[33malgo-2    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[33malgo-2    |[0m #export YARN_TIMELINE_HEAPSIZE=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the TimeLineServer.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_TIMELINESERVER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # TimeLineReader specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the TimeLineReader.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_TIMELINEREADER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Web App Proxy Server specifc parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the max heapsize for the web app proxy server.  If no units are
[33malgo-2    |[0m # given, it will be assumed to be in MB.
[33malgo-2    |[0m # This value will be overridden by an Xmx setting specified in either
[33malgo-2    |[0m # HADOOP_OPTS and/or YARN_PROXYSERVER_OPTS.
[33malgo-2    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[33malgo-2    |[0m #export YARN_PROXYSERVER_HEAPSIZE=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the proxy server.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_PROXYSERVER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Shared Cache Manager specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the
[33malgo-2    |[0m # shared cache manager server.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_SHAREDCACHEMANAGER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Router specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the Router.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_ROUTER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Registry DNS specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # For privileged registry DNS, user to run as after dropping privileges
[33malgo-2    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[33malgo-2    |[0m # export YARN_REGISTRYDNS_SECURE_USER=yarn
[33malgo-2    |[0m 
[33malgo-2    |[0m # Supplemental options for privileged registry DNS
[33malgo-2    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[33malgo-2    |[0m # server jvm.
[33malgo-2    |[0m # export YARN_REGISTRYDNS_SECURE_EXTRA_OPTS="-jvm server"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # YARN Services parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Directory containing service examples
[33malgo-2    |[0m # export YARN_SERVICE_EXAMPLES_DIR = $HADOOP_YARN_HOME/share/hadoop/yarn/yarn-service-examples
[33malgo-2    |[0m # export YARN_CONTAINER_RUNTIME_DOCKER_RUN_OVERRIDE_DISABLE=true
[33malgo-2    |[0m export YARN_LOG_DIR=/var/log/yarn/export HADOOP_DATANODE_HEAPSIZE=2048
[33malgo-2    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[33malgo-2    |[0m 
[36malgo-1    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/hadoop-env.sh
[33malgo-2    |[0m 11-14 15:19 smspark-submit INFO     waiting for cluster to be up
[36malgo-1    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/hadoop-env.sh is: 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[36malgo-1    |[0m # or more contributor license agreements.  See the NOTICE file
[36malgo-1    |[0m # distributed with this work for additional information
[36malgo-1    |[0m # regarding copyright ownership.  The ASF licenses this file
[36malgo-1    |[0m # to you under the Apache License, Version 2.0 (the
[36malgo-1    |[0m # "License"); you may not use this file except in compliance
[36malgo-1    |[0m # with the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m # Set Hadoop-specific environment variables here.
[36malgo-1    |[0m 
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## THIS FILE ACTS AS THE MASTER FILE FOR ALL HADOOP PROJECTS.
[36malgo-1    |[0m ## SETTINGS HERE WILL BE READ BY ALL HADOOP COMMANDS.  THEREFORE,
[36malgo-1    |[0m ## ONE CAN USE THIS FILE TO SET YARN, HDFS, AND MAPREDUCE
[36malgo-1    |[0m ## CONFIGURATION OPTIONS INSTEAD OF xxx-env.sh.
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## Precedence rules:
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## {yarn-env.sh|hdfs-env.sh} > hadoop-env.sh > hard-coded defaults
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## {YARN_xyz|HDFS_xyz} > HADOOP_xyz > hard-coded defaults
[36malgo-1    |[0m ##
[36malgo-1    |[0m 
[36malgo-1    |[0m # Many of the options here are built from the perspective that users
[36malgo-1    |[0m # may want to provide OVERWRITING values on the command line.
[36malgo-1    |[0m # For example:
[36malgo-1    |[0m #
[36malgo-1    |[0m #  JAVA_HOME=/usr/java/testing hdfs dfs -ls
[36malgo-1    |[0m #
[36malgo-1    |[0m # Therefore, the vast majority (BUT NOT ALL!) of these defaults
[36malgo-1    |[0m # are configured for substitution and not append.  If append
[36malgo-1    |[0m # is preferable, modify this file accordingly.
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Generic settings for HADOOP
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Technically, the only required environment variable is JAVA_HOME.
[36malgo-1    |[0m # All others are optional.  However, the defaults are probably not
[36malgo-1    |[0m # preferred.  Many sites configure these options outside of Hadoop,
[36malgo-1    |[0m # such as in /etc/profile.d
[36malgo-1    |[0m 
[36malgo-1    |[0m # The java implementation to use. By default, this environment
[36malgo-1    |[0m # variable is REQUIRED on ALL platforms except OS X!
[36malgo-1    |[0m # export JAVA_HOME=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Location of Hadoop.  By default, Hadoop will attempt to determine
[36malgo-1    |[0m # this location based upon its execution path.
[36malgo-1    |[0m # export HADOOP_HOME=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Location of Hadoop's configuration information.  i.e., where this
[36malgo-1    |[0m # file is living. If this is not defined, Hadoop will attempt to
[36malgo-1    |[0m # locate it based upon its execution path.
[36malgo-1    |[0m #
[36malgo-1    |[0m # NOTE: It is recommend that this variable not be set here but in
[36malgo-1    |[0m # /etc/profile.d or equivalent.  Some options (such as
[36malgo-1    |[0m # --config) may react strangely otherwise.
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
[36malgo-1    |[0m 
[36malgo-1    |[0m # The maximum amount of heap to use (Java -Xmx).  If no unit
[36malgo-1    |[0m # is provided, it will be converted to MB.  Daemons will
[36malgo-1    |[0m # prefer any Xmx setting in their respective _OPT variable.
[36malgo-1    |[0m # There is no default; the JVM will autoscale based upon machine
[36malgo-1    |[0m # memory size.
[36malgo-1    |[0m # export HADOOP_HEAPSIZE_MAX=
[36malgo-1    |[0m 
[36malgo-1    |[0m # The minimum amount of heap to use (Java -Xms).  If no unit
[36malgo-1    |[0m # is provided, it will be converted to MB.  Daemons will
[36malgo-1    |[0m # prefer any Xms setting in their respective _OPT variable.
[36malgo-1    |[0m # There is no default; the JVM will autoscale based upon machine
[36malgo-1    |[0m # memory size.
[36malgo-1    |[0m # export HADOOP_HEAPSIZE_MIN=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Enable extra debugging of Hadoop's JAAS binding, used to set up
[36malgo-1    |[0m # Kerberos security.
[36malgo-1    |[0m # export HADOOP_JAAS_DEBUG=true
[36malgo-1    |[0m 
[36malgo-1    |[0m # Extra Java runtime options for all Hadoop commands. We don't support
[36malgo-1    |[0m # IPv6 yet/still, so by default the preference is set to IPv4.
[36malgo-1    |[0m # export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true"
[36malgo-1    |[0m # For Kerberos debugging, an extended option set logs more information
[36malgo-1    |[0m # export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug"
[36malgo-1    |[0m 
[36malgo-1    |[0m # Some parts of the shell code may do special things dependent upon
[36malgo-1    |[0m # the operating system.  We have to set this here. See the next
[36malgo-1    |[0m # section as to why....
[36malgo-1    |[0m #export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}
[36malgo-1    |[0m 
[36malgo-1    |[0m # Extra Java runtime options for some Hadoop commands
[36malgo-1    |[0m # and clients (i.e., hdfs dfs -blah).  These get appended to HADOOP_OPTS for
[36malgo-1    |[0m # such commands.  In most cases, # this should be left empty and
[36malgo-1    |[0m # let users supply it on the command line.
[36malgo-1    |[0m # export HADOOP_CLIENT_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # A note about classpaths.
[36malgo-1    |[0m #
[36malgo-1    |[0m # By default, Apache Hadoop overrides Java's CLASSPATH
[36malgo-1    |[0m # environment variable.  It is configured such
[36malgo-1    |[0m # that it starts out blank with new entries added after passing
[36malgo-1    |[0m # a series of checks (file/dir exists, not already listed aka
[36malgo-1    |[0m # de-deduplication).  During de-deduplication, wildcards and/or
[36malgo-1    |[0m # directories are *NOT* expanded to keep it simple. Therefore,
[36malgo-1    |[0m # if the computed classpath has two specific mentions of
[36malgo-1    |[0m # awesome-methods-1.0.jar, only the first one added will be seen.
[36malgo-1    |[0m # If two directories are in the classpath that both contain
[36malgo-1    |[0m # awesome-methods-1.0.jar, then Java will pick up both versions.
[36malgo-1    |[0m 
[36malgo-1    |[0m # An additional, custom CLASSPATH. Site-wide configs should be
[36malgo-1    |[0m # handled via the shellprofile functionality, utilizing the
[36malgo-1    |[0m # hadoop_add_classpath function for greater control and much
[36malgo-1    |[0m # harder for apps/end-users to accidentally override.
[36malgo-1    |[0m # Similarly, end users should utilize ${HOME}/.hadooprc .
[36malgo-1    |[0m # This variable should ideally only be used as a short-cut,
[36malgo-1    |[0m # interactive way for temporary additions on the command line.
[36malgo-1    |[0m # export HADOOP_CLASSPATH="/some/cool/path/on/your/machine"
[36malgo-1    |[0m 
[36malgo-1    |[0m # Should HADOOP_CLASSPATH be first in the official CLASSPATH?
[36malgo-1    |[0m # export HADOOP_USER_CLASSPATH_FIRST="yes"
[36malgo-1    |[0m 
[36malgo-1    |[0m # If HADOOP_USE_CLIENT_CLASSLOADER is set, the classpath along
[36malgo-1    |[0m # with the main jar are handled by a separate isolated
[36malgo-1    |[0m # client classloader when 'hadoop jar', 'yarn jar', or 'mapred job'
[36malgo-1    |[0m # is utilized. If it is set, HADOOP_CLASSPATH and
[36malgo-1    |[0m # HADOOP_USER_CLASSPATH_FIRST are ignored.
[36malgo-1    |[0m # export HADOOP_USE_CLIENT_CLASSLOADER=true
[36malgo-1    |[0m 
[36malgo-1    |[0m # HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES overrides the default definition of
[36malgo-1    |[0m # system classes for the client classloader when HADOOP_USE_CLIENT_CLASSLOADER
[36malgo-1    |[0m # is enabled. Names ending in '.' (period) are treated as package names, and
[36malgo-1    |[0m # names starting with a '-' are treated as negative matches. For example,
[36malgo-1    |[0m # export HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES="-org.apache.hadoop.UserClass,java.,javax.,org.apache.hadoop."
[36malgo-1    |[0m 
[36malgo-1    |[0m # Enable optional, bundled Hadoop features
[36malgo-1    |[0m # This is a comma delimited list.  It may NOT be overridden via .hadooprc
[36malgo-1    |[0m # Entries may be added/removed as needed.
[36malgo-1    |[0m # export HADOOP_OPTIONAL_TOOLS="hadoop-azure,hadoop-azure-datalake,hadoop-aws,hadoop-openstack,hadoop-aliyun,hadoop-kafka"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Options for remote shell connectivity
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # There are some optional components of hadoop that allow for
[36malgo-1    |[0m # command and control of remote hosts.  For example,
[36malgo-1    |[0m # start-dfs.sh will attempt to bring up all NNs, DNS, etc.
[36malgo-1    |[0m 
[36malgo-1    |[0m # Options to pass to SSH when one of the "log into a host and
[36malgo-1    |[0m # start/stop daemons" scripts is executed
[36malgo-1    |[0m # export HADOOP_SSH_OPTS="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10s"
[36malgo-1    |[0m 
[36malgo-1    |[0m # The built-in ssh handler will limit itself to 10 simultaneous connections.
[36malgo-1    |[0m # For pdsh users, this sets the fanout size ( -f )
[36malgo-1    |[0m # Change this to increase/decrease as necessary.
[36malgo-1    |[0m # export HADOOP_SSH_PARALLEL=10
[36malgo-1    |[0m 
[36malgo-1    |[0m # Filename which contains all of the hosts for any remote execution
[36malgo-1    |[0m # helper scripts # such as workers.sh, start-dfs.sh, etc.
[36malgo-1    |[0m # export HADOOP_WORKERS="${HADOOP_CONF_DIR}/workers"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Options for all daemons
[36malgo-1    |[0m ###
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Many options may also be specified as Java properties.  It is
[36malgo-1    |[0m # very common, and in many cases, desirable, to hard-set these
[36malgo-1    |[0m # in daemon _OPTS variables.  Where applicable, the appropriate
[36malgo-1    |[0m # Java property is also identified.  Note that many are re-used
[36malgo-1    |[0m # or set differently in certain contexts (e.g., secure vs
[36malgo-1    |[0m # non-secure)
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m # Where (primarily) daemon log files are stored.
[36malgo-1    |[0m # ${HADOOP_HOME}/logs by default.
[36malgo-1    |[0m # Java property: hadoop.log.dir
[36malgo-1    |[0m # export HADOOP_LOG_DIR=${HADOOP_HOME}/logs
[36malgo-1    |[0m 
[36malgo-1    |[0m # A string representing this instance of hadoop. $USER by default.
[36malgo-1    |[0m # This is used in writing log and pid files, so keep that in mind!
[36malgo-1    |[0m # Java property: hadoop.id.str
[36malgo-1    |[0m # export HADOOP_IDENT_STRING=$USER
[36malgo-1    |[0m 
[36malgo-1    |[0m # How many seconds to pause after stopping a daemon
[36malgo-1    |[0m # export HADOOP_STOP_TIMEOUT=5
[36malgo-1    |[0m 
[36malgo-1    |[0m # Where pid files are stored.  /tmp by default.
[36malgo-1    |[0m # export HADOOP_PID_DIR=/tmp
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default log4j setting for interactive commands
[36malgo-1    |[0m # Java property: hadoop.root.logger
[36malgo-1    |[0m # export HADOOP_ROOT_LOGGER=INFO,console
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default log4j setting for daemons spawned explicitly by
[36malgo-1    |[0m # --daemon option of hadoop, hdfs, mapred and yarn command.
[36malgo-1    |[0m # Java property: hadoop.root.logger
[36malgo-1    |[0m # export HADOOP_DAEMON_ROOT_LOGGER=INFO,RFA
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default log level and output location for security-related messages.
[36malgo-1    |[0m # You will almost certainly want to change this on a per-daemon basis via
[36malgo-1    |[0m # the Java property (i.e., -Dhadoop.security.logger=foo). (Note that the
[36malgo-1    |[0m # defaults for the NN and 2NN override this by default.)
[36malgo-1    |[0m # Java property: hadoop.security.logger
[36malgo-1    |[0m # export HADOOP_SECURITY_LOGGER=INFO,NullAppender
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default process priority level
[36malgo-1    |[0m # Note that sub-processes will also run at this level!
[36malgo-1    |[0m # export HADOOP_NICENESS=0
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default name for the service level authorization file
[36malgo-1    |[0m # Java property: hadoop.policy.file
[36malgo-1    |[0m # export HADOOP_POLICYFILE="hadoop-policy.xml"
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # NOTE: this is not used by default!  <-----
[36malgo-1    |[0m # You can define variables right here and then re-use them later on.
[36malgo-1    |[0m # For example, it is common to use the same garbage collection settings
[36malgo-1    |[0m # for all the daemons.  So one could define:
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HADOOP_GC_SETTINGS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps"
[36malgo-1    |[0m #
[36malgo-1    |[0m # .. and then use it as per the b option under the namenode.
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Secure/privileged execution
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Out of the box, Hadoop uses jsvc from Apache Commons to launch daemons
[36malgo-1    |[0m # on privileged ports.  This functionality can be replaced by providing
[36malgo-1    |[0m # custom functions.  See hadoop-functions.sh for more information.
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m # The jsvc implementation to use. Jsvc is required to run secure datanodes
[36malgo-1    |[0m # that bind to privileged ports to provide authentication of data transfer
[36malgo-1    |[0m # protocol.  Jsvc is not required if SASL is configured for authentication of
[36malgo-1    |[0m # data transfer protocol using non-privileged ports.
[36malgo-1    |[0m # export JSVC_HOME=/usr/bin
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # This directory contains pids for secure and privileged processes.
[36malgo-1    |[0m #export HADOOP_SECURE_PID_DIR=${HADOOP_PID_DIR}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # This directory contains the logs for secure and privileged processes.
[36malgo-1    |[0m # Java property: hadoop.log.dir
[36malgo-1    |[0m # export HADOOP_SECURE_LOG=${HADOOP_LOG_DIR}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # When running a secure daemon, the default value of HADOOP_IDENT_STRING
[36malgo-1    |[0m # ends up being a bit bogus.  Therefore, by default, the code will
[36malgo-1    |[0m # replace HADOOP_IDENT_STRING with HADOOP_xx_SECURE_USER.  If one wants
[36malgo-1    |[0m # to keep HADOOP_IDENT_STRING untouched, then uncomment this line.
[36malgo-1    |[0m # export HADOOP_SECURE_IDENT_PRESERVE="true"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # NameNode specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default log level and output location for file system related change
[36malgo-1    |[0m # messages. For non-namenode daemons, the Java property must be set in
[36malgo-1    |[0m # the appropriate _OPTS if one wants something other than INFO,NullAppender
[36malgo-1    |[0m # Java property: hdfs.audit.logger
[36malgo-1    |[0m # export HDFS_AUDIT_LOGGER=INFO,NullAppender
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the NameNode.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # a) Set JMX options
[36malgo-1    |[0m # export HDFS_NAMENODE_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
[36malgo-1    |[0m #
[36malgo-1    |[0m # b) Set garbage collection logs
[36malgo-1    |[0m # export HDFS_NAMENODE_OPTS="${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[36malgo-1    |[0m #
[36malgo-1    |[0m # c) ... or set them directly
[36malgo-1    |[0m # export HDFS_NAMENODE_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[36malgo-1    |[0m 
[36malgo-1    |[0m # this is the default:
[36malgo-1    |[0m # export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # SecondaryNameNode specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the SecondaryNameNode.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # This is the default:
[36malgo-1    |[0m # export HDFS_SECONDARYNAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # DataNode specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the DataNode.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # This is the default:
[36malgo-1    |[0m # export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS"
[36malgo-1    |[0m 
[36malgo-1    |[0m # On secure datanodes, user to run the datanode as after dropping privileges.
[36malgo-1    |[0m # This **MUST** be uncommented to enable secure HDFS if using privileged ports
[36malgo-1    |[0m # to provide authentication of data transfer protocol.  This **MUST NOT** be
[36malgo-1    |[0m # defined if SASL is configured for authentication of data transfer protocol
[36malgo-1    |[0m # using non-privileged ports.
[36malgo-1    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[36malgo-1    |[0m # export HDFS_DATANODE_SECURE_USER=hdfs
[36malgo-1    |[0m 
[36malgo-1    |[0m # Supplemental options for secure datanodes
[36malgo-1    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[36malgo-1    |[0m # server jvm.
[36malgo-1    |[0m # export HDFS_DATANODE_SECURE_EXTRA_OPTS="-jvm server"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # NFS3 Gateway specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the NFS3 Gateway.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_NFS3_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the Hadoop portmapper.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_PORTMAP_OPTS="-Xmx512m"
[36malgo-1    |[0m 
[36malgo-1    |[0m # Supplemental options for priviliged gateways
[36malgo-1    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[36malgo-1    |[0m # server jvm.
[36malgo-1    |[0m # export HDFS_NFS3_SECURE_EXTRA_OPTS="-jvm server"
[36malgo-1    |[0m 
[36malgo-1    |[0m # On privileged gateways, user to run the gateway as after dropping privileges
[36malgo-1    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[36malgo-1    |[0m # export HDFS_NFS3_SECURE_USER=nfsserver
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # ZKFailoverController specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the ZKFailoverController.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_ZKFC_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # QuorumJournalNode specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the QuorumJournalNode.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_JOURNALNODE_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # HDFS Balancer specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the HDFS Balancer.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_BALANCER_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # HDFS Mover specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the HDFS Mover.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_MOVER_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Router-based HDFS Federation specific parameters
[36malgo-1    |[0m # Specify the JVM options to be used when starting the RBF Routers.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_DFSROUTER_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # HDFS StorageContainerManager specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the HDFS Storage Container Manager.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_STORAGECONTAINERMANAGER_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Advanced Users Only!
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # When building Hadoop, one can add the class paths to the commands
[36malgo-1    |[0m # via this special env var:
[36malgo-1    |[0m # export HADOOP_ENABLE_BUILD_PATHS="true"
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # To prevent accidents, shell commands be (superficially) locked
[36malgo-1    |[0m # to only allow certain users to execute certain subcommands.
[36malgo-1    |[0m # It uses the format of (command)_(subcommand)_USER.
[36malgo-1    |[0m #
[36malgo-1    |[0m # For example, to limit who can execute the namenode command,
[36malgo-1    |[0m # export HDFS_NAMENODE_USER=hdfs
[36malgo-1    |[0m export SPARK_MASTER_HOST=192.168.240.3
[36malgo-1    |[0m export AWS_CONTAINER_CREDENTIALS_RELATIVE_URI=
[36malgo-1    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[36malgo-1    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/core-site.xml
[36malgo-1    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/core-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0" encoding="UTF-8"?>
[36malgo-1    |[0m  <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[36malgo-1    |[0m  <!-- Put site-specific property overrides in this file. -->
[36malgo-1    |[0m 
[36malgo-1    |[0m  <configuration>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>fs.defaultFS</name>
[36malgo-1    |[0m          <value>hdfs://192.168.240.3/</value>
[36malgo-1    |[0m          <description>NameNode URI</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>fs.s3a.aws.credentials.provider</name>
[36malgo-1    |[0m          <value>com.amazonaws.auth.DefaultAWSCredentialsProviderChain</value>
[36malgo-1    |[0m          <description>AWS S3 credential provider</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>fs.s3.impl</name>
[36malgo-1    |[0m          <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
[36malgo-1    |[0m          <description>s3a filesystem implementation</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>fs.AbstractFileSystem.s3a.imp</name>
[36malgo-1    |[0m          <value>org.apache.hadoop.fs.s3a.S3A</value>
[36malgo-1    |[0m          <description>s3a filesystem implementation</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m  
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>spark.executor.memory</name>
[36malgo-1    |[0m     <value>2g</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>spark.executor.cores</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/log4j.properties
[36malgo-1    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/log4j.properties is: 
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[36malgo-1    |[0m # or more contributor license agreements.  See the NOTICE file
[36malgo-1    |[0m # distributed with this work for additional information
[36malgo-1    |[0m # regarding copyright ownership.  The ASF licenses this file
[36malgo-1    |[0m # to you under the Apache License, Version 2.0 (the
[36malgo-1    |[0m # "License"); you may not use this file except in compliance
[36malgo-1    |[0m # with the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m # Define some default values that can be overridden by system properties
[36malgo-1    |[0m hadoop.root.logger=INFO,console
[36malgo-1    |[0m hadoop.log.dir=.
[36malgo-1    |[0m hadoop.log.file=hadoop.log
[36malgo-1    |[0m 
[36malgo-1    |[0m # Define the root logger to the system property "hadoop.root.logger".
[36malgo-1    |[0m log4j.rootLogger=${hadoop.root.logger}, EventCounter
[36malgo-1    |[0m 
[36malgo-1    |[0m # Logging Threshold
[36malgo-1    |[0m log4j.threshold=ALL
[36malgo-1    |[0m 
[36malgo-1    |[0m # Null Appender
[36malgo-1    |[0m log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Rolling File Appender - cap space usage at 5gb.
[36malgo-1    |[0m #
[36malgo-1    |[0m hadoop.log.maxfilesize=256MB
[36malgo-1    |[0m hadoop.log.maxbackupindex=20
[36malgo-1    |[0m log4j.appender.RFA=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.RFA.MaxFileSize=${hadoop.log.maxfilesize}
[36malgo-1    |[0m log4j.appender.RFA.MaxBackupIndex=${hadoop.log.maxbackupindex}
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.RFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m 
[36malgo-1    |[0m # Pattern format: Date LogLevel LoggerName LogMessage
[36malgo-1    |[0m log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[36malgo-1    |[0m # Debugging Pattern format
[36malgo-1    |[0m #log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Daily Rolling File Appender
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.DRFA.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m 
[36malgo-1    |[0m # Rollver at midnight
[36malgo-1    |[0m #log4j.appender.DRFA.DatePattern=.yyyy-MM-dd
[36malgo-1    |[0m # Rollver at every hour
[36malgo-1    |[0m log4j.appender.DRFA.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m 
[36malgo-1    |[0m # Pattern format: Date LogLevel LoggerName LogMessage
[36malgo-1    |[0m log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m # Debugging Pattern format
[36malgo-1    |[0m #log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # console
[36malgo-1    |[0m # Add "console" to rootlogger above if you want to use this
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.console=org.apache.log4j.ConsoleAppender
[36malgo-1    |[0m log4j.appender.console.target=System.err
[36malgo-1    |[0m log4j.appender.console.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # TaskLog Appender
[36malgo-1    |[0m #
[36malgo-1    |[0m log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.TLA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # HDFS block state change log from block manager
[36malgo-1    |[0m #
[36malgo-1    |[0m # Uncomment the following to log normal block state change
[36malgo-1    |[0m # messages from BlockManager in NameNode.
[36malgo-1    |[0m #log4j.logger.BlockStateChange=DEBUG
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m #Security appender
[36malgo-1    |[0m #
[36malgo-1    |[0m hadoop.security.logger=INFO,NullAppender
[36malgo-1    |[0m hadoop.security.log.maxfilesize=256MB
[36malgo-1    |[0m hadoop.security.log.maxbackupindex=20
[36malgo-1    |[0m log4j.category.SecurityLogger=${hadoop.security.logger}
[36malgo-1    |[0m hadoop.security.log.file=SecurityAuth-${user.name}.audit
[36malgo-1    |[0m log4j.appender.RFAS=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.RFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
[36malgo-1    |[0m log4j.appender.RFAS.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[36malgo-1    |[0m log4j.appender.RFAS.MaxFileSize=${hadoop.security.log.maxfilesize}
[36malgo-1    |[0m log4j.appender.RFAS.MaxBackupIndex=${hadoop.security.log.maxbackupindex}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Daily Rolling Security appender
[36malgo-1    |[0m #
[36malgo-1    |[0m log4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.DRFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
[36malgo-1    |[0m log4j.appender.DRFAS.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.DRFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[36malgo-1    |[0m log4j.appender.DRFAS.DatePattern=.yyyy-MM-dd
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # hadoop configuration logging
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m # Uncomment the following line to turn off configuration deprecation warnings.
[36malgo-1    |[0m # log4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # hdfs audit logging
[36malgo-1    |[0m #
[36malgo-1    |[0m hdfs.audit.logger=INFO,NullAppender
[36malgo-1    |[0m hdfs.audit.log.maxfilesize=256MB
[36malgo-1    |[0m hdfs.audit.log.maxbackupindex=20
[36malgo-1    |[0m log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}
[36malgo-1    |[0m log4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false
[36malgo-1    |[0m log4j.appender.RFAAUDIT=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.RFAAUDIT.File=${hadoop.log.dir}/hdfs-audit.log
[36malgo-1    |[0m log4j.appender.RFAAUDIT.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.RFAAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[36malgo-1    |[0m log4j.appender.RFAAUDIT.MaxFileSize=${hdfs.audit.log.maxfilesize}
[36malgo-1    |[0m log4j.appender.RFAAUDIT.MaxBackupIndex=${hdfs.audit.log.maxbackupindex}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # NameNode metrics logging.
[36malgo-1    |[0m # The default is to retain two namenode-metrics.log files up to 64MB each.
[36malgo-1    |[0m #
[36malgo-1    |[0m namenode.metrics.logger=INFO,NullAppender
[36malgo-1    |[0m log4j.logger.NameNodeMetricsLog=${namenode.metrics.logger}
[36malgo-1    |[0m log4j.additivity.NameNodeMetricsLog=false
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.File=${hadoop.log.dir}/namenode-metrics.log
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.MaxBackupIndex=1
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.MaxFileSize=64MB
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # DataNode metrics logging.
[36malgo-1    |[0m # The default is to retain two datanode-metrics.log files up to 64MB each.
[36malgo-1    |[0m #
[36malgo-1    |[0m datanode.metrics.logger=INFO,NullAppender
[36malgo-1    |[0m log4j.logger.DataNodeMetricsLog=${datanode.metrics.logger}
[36malgo-1    |[0m log4j.additivity.DataNodeMetricsLog=false
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.File=${hadoop.log.dir}/datanode-metrics.log
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.MaxBackupIndex=1
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.MaxFileSize=64MB
[36malgo-1    |[0m 
[36malgo-1    |[0m # Custom Logging levels
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=DEBUG
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m # AWS SDK & S3A FileSystem
[36malgo-1    |[0m #log4j.logger.com.amazonaws=ERROR
[36malgo-1    |[0m log4j.logger.com.amazonaws.http.AmazonHttpClient=ERROR
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.fs.s3a.S3AFileSystem=WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Event Counter Appender
[36malgo-1    |[0m # Sends counts of logging messages at different severity levels to Hadoop Metrics.
[36malgo-1    |[0m #
[36malgo-1    |[0m log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Job Summary Appender
[36malgo-1    |[0m #
[36malgo-1    |[0m # Use following logger to send summary to separate file defined by
[36malgo-1    |[0m # hadoop.mapreduce.jobsummary.log.file :
[36malgo-1    |[0m # hadoop.mapreduce.jobsummary.logger=INFO,JSA
[36malgo-1    |[0m # 
[36malgo-1    |[0m hadoop.mapreduce.jobsummary.logger=${hadoop.root.logger}
[36malgo-1    |[0m hadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log
[36malgo-1    |[0m hadoop.mapreduce.jobsummary.log.maxfilesize=256MB
[36malgo-1    |[0m hadoop.mapreduce.jobsummary.log.maxbackupindex=20
[36malgo-1    |[0m log4j.appender.JSA=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.JSA.File=${hadoop.log.dir}/${hadoop.mapreduce.jobsummary.log.file}
[36malgo-1    |[0m log4j.appender.JSA.MaxFileSize=${hadoop.mapreduce.jobsummary.log.maxfilesize}
[36malgo-1    |[0m log4j.appender.JSA.MaxBackupIndex=${hadoop.mapreduce.jobsummary.log.maxbackupindex}
[36malgo-1    |[0m log4j.appender.JSA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.JSA.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
[36malgo-1    |[0m log4j.appender.JSA.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.logger.org.apache.hadoop.mapred.JobInProgress$JobSummary=${hadoop.mapreduce.jobsummary.logger}
[36malgo-1    |[0m log4j.additivity.org.apache.hadoop.mapred.JobInProgress$JobSummary=false
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # shuffle connection log from shuffleHandler
[36malgo-1    |[0m # Uncomment the following line to enable logging of shuffle connections
[36malgo-1    |[0m # log4j.logger.org.apache.hadoop.mapred.ShuffleHandler.audit=DEBUG
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Yarn ResourceManager Application Summary Log
[36malgo-1    |[0m #
[36malgo-1    |[0m # Set the ResourceManager summary log filename
[36malgo-1    |[0m yarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log
[36malgo-1    |[0m # Set the ResourceManager summary log level and appender
[36malgo-1    |[0m yarn.server.resourcemanager.appsummary.logger=${hadoop.root.logger}
[36malgo-1    |[0m #yarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY
[36malgo-1    |[0m 
[36malgo-1    |[0m # To enable AppSummaryLogging for the RM,
[36malgo-1    |[0m # set yarn.server.resourcemanager.appsummary.logger to
[36malgo-1    |[0m # <LEVEL>,RMSUMMARY in hadoop-env.sh
[36malgo-1    |[0m 
[36malgo-1    |[0m # Appender for ResourceManager Application Summary Log
[36malgo-1    |[0m # Requires the following properties to be set
[36malgo-1    |[0m #    - hadoop.log.dir (Hadoop Log directory)
[36malgo-1    |[0m #    - yarn.server.resourcemanager.appsummary.log.file (resource manager app summary log filename)
[36malgo-1    |[0m #    - yarn.server.resourcemanager.appsummary.logger (resource manager app summary log level and appender)
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=${yarn.server.resourcemanager.appsummary.logger}
[36malgo-1    |[0m log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=false
[36malgo-1    |[0m log4j.appender.RMSUMMARY=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.RMSUMMARY.File=${hadoop.log.dir}/${yarn.server.resourcemanager.appsummary.log.file}
[36malgo-1    |[0m log4j.appender.RMSUMMARY.MaxFileSize=256MB
[36malgo-1    |[0m log4j.appender.RMSUMMARY.MaxBackupIndex=20
[36malgo-1    |[0m log4j.appender.RMSUMMARY.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.RMSUMMARY.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m # HS audit log configs
[36malgo-1    |[0m #mapreduce.hs.audit.logger=INFO,HSAUDIT
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=${mapreduce.hs.audit.logger}
[36malgo-1    |[0m #log4j.additivity.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=false
[36malgo-1    |[0m #log4j.appender.HSAUDIT=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m #log4j.appender.HSAUDIT.File=${hadoop.log.dir}/hs-audit.log
[36malgo-1    |[0m #log4j.appender.HSAUDIT.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m #log4j.appender.HSAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[36malgo-1    |[0m #log4j.appender.HSAUDIT.DatePattern=.yyyy-MM-dd
[36malgo-1    |[0m 
[36malgo-1    |[0m # Http Server Request Logs
[36malgo-1    |[0m #log4j.logger.http.requests.namenode=INFO,namenoderequestlog
[36malgo-1    |[0m #log4j.appender.namenoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.namenoderequestlog.Filename=${hadoop.log.dir}/jetty-namenode-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.namenoderequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.http.requests.datanode=INFO,datanoderequestlog
[36malgo-1    |[0m #log4j.appender.datanoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.datanoderequestlog.Filename=${hadoop.log.dir}/jetty-datanode-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.datanoderequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.http.requests.resourcemanager=INFO,resourcemanagerrequestlog
[36malgo-1    |[0m #log4j.appender.resourcemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.resourcemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-resourcemanager-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.resourcemanagerrequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.http.requests.jobhistory=INFO,jobhistoryrequestlog
[36malgo-1    |[0m #log4j.appender.jobhistoryrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.jobhistoryrequestlog.Filename=${hadoop.log.dir}/jetty-jobhistory-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.jobhistoryrequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.http.requests.nodemanager=INFO,nodemanagerrequestlog
[36malgo-1    |[0m #log4j.appender.nodemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.nodemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-nodemanager-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.nodemanagerrequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m # WebHdfs request log on datanodes
[36malgo-1    |[0m # Specify -Ddatanode.webhdfs.logger=INFO,HTTPDRFA on datanode startup to
[36malgo-1    |[0m # direct the log to a separate file.
[36malgo-1    |[0m #datanode.webhdfs.logger=INFO,console
[36malgo-1    |[0m #log4j.logger.datanode.webhdfs=${datanode.webhdfs.logger}
[36malgo-1    |[0m #log4j.appender.HTTPDRFA=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m #log4j.appender.HTTPDRFA.File=${hadoop.log.dir}/hadoop-datanode-webhdfs.log
[36malgo-1    |[0m #log4j.appender.HTTPDRFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m #log4j.appender.HTTPDRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[36malgo-1    |[0m #log4j.appender.HTTPDRFA.DatePattern=.yyyy-MM-dd
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m # Appender for viewing information for errors and warnings
[36malgo-1    |[0m yarn.ewma.cleanupInterval=300
[36malgo-1    |[0m yarn.ewma.messageAgeLimitSeconds=86400
[36malgo-1    |[0m yarn.ewma.maxUniqueMessages=250
[36malgo-1    |[0m log4j.appender.EWMA=org.apache.hadoop.yarn.util.Log4jWarningErrorMetricsAppender
[36malgo-1    |[0m log4j.appender.EWMA.cleanupInterval=${yarn.ewma.cleanupInterval}
[36malgo-1    |[0m log4j.appender.EWMA.messageAgeLimitSeconds=${yarn.ewma.messageAgeLimitSeconds}
[36malgo-1    |[0m log4j.appender.EWMA.maxUniqueMessages=${yarn.ewma.maxUniqueMessages}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Fair scheduler state dump
[36malgo-1    |[0m #
[36malgo-1    |[0m # Use following logger to dump the state to a separate file
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=DEBUG,FSSTATEDUMP
[36malgo-1    |[0m #log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=false
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.File=${hadoop.log.dir}/fairscheduler-statedump.log
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.MaxFileSize=${hadoop.log.maxfilesize}
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.MaxBackupIndex=${hadoop.log.maxbackupindex}
[36malgo-1    |[0m 
[36malgo-1    |[0m # Log levels of third-party libraries
[36malgo-1    |[0m log4j.logger.org.apache.commons.beanutils=WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m #AWS SDK Logging
[36malgo-1    |[0m log4j.logger.com.amazonaws=WARN
[36malgo-1    |[0m log4j.logger.org.apache.zookeeper=ERROR
[36malgo-1    |[0m log4j.logger.org.apache.hadoop.hbase=WARN
[36malgo-1    |[0m log4j.logger.amazon.emr.metrics=WARN
[36malgo-1    |[0m log4j.logger.emr=INFO
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.HADOOP=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.HADOOP.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m log4j.appender.HADOOP.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.appender.HADOOP.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.HADOOP.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.JHS=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.JHS.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m log4j.appender.JHS.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.appender.JHS.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.JHS.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.MAPRED=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.MAPRED.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m log4j.appender.MAPRED.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.appender.MAPRED.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.MAPRED.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.YARN=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.YARN.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m log4j.appender.YARN.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.appender.YARN.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.YARN.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/hive/conf/hive-env.sh
[36malgo-1    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/hive/conf/hive-env.sh is: 
[36malgo-1    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[36malgo-1    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/hive/conf/hive-log4j2.properties
[36malgo-1    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/hive/conf/hive-log4j2.properties is: 
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[36malgo-1    |[0m # or more contributor license agreements.  See the NOTICE file
[36malgo-1    |[0m # distributed with this work for additional information
[36malgo-1    |[0m # regarding copyright ownership.  The ASF licenses this file
[36malgo-1    |[0m # to you under the Apache License, Version 2.0 (the
[36malgo-1    |[0m # "License"); you may not use this file except in compliance
[36malgo-1    |[0m # with the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m status = INFO
[36malgo-1    |[0m name = HiveLog4j2
[36malgo-1    |[0m packages = org.apache.hadoop.hive.ql.log
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of properties
[36malgo-1    |[0m property.hive.log.level = INFO
[36malgo-1    |[0m property.hive.root.logger = DRFA
[36malgo-1    |[0m property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
[36malgo-1    |[0m property.hive.log.file = hive.log
[36malgo-1    |[0m property.hive.perflogger.log.level = INFO
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of all appenders
[36malgo-1    |[0m appenders = console, DRFA
[36malgo-1    |[0m 
[36malgo-1    |[0m # console appender
[36malgo-1    |[0m appender.console.type = Console
[36malgo-1    |[0m appender.console.name = console
[36malgo-1    |[0m appender.console.target = SYSTEM_ERR
[36malgo-1    |[0m appender.console.layout.type = PatternLayout
[36malgo-1    |[0m appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m # daily rolling file appender
[36malgo-1    |[0m appender.DRFA.type = RollingRandomAccessFile
[36malgo-1    |[0m appender.DRFA.name = DRFA
[36malgo-1    |[0m appender.DRFA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
[36malgo-1    |[0m # Use %pid in the filePattern to append <process-id>@<host-name> to the filename if you want separate log files for different CLI session
[36malgo-1    |[0m appender.DRFA.filePattern = ${sys:hive.log.dir}/${sys:hive.log.file}.%d{yyyy-MM-dd}
[36malgo-1    |[0m appender.DRFA.layout.type = PatternLayout
[36malgo-1    |[0m appender.DRFA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[36malgo-1    |[0m appender.DRFA.policies.type = Policies
[36malgo-1    |[0m appender.DRFA.policies.time.type = TimeBasedTriggeringPolicy
[36malgo-1    |[0m appender.DRFA.policies.time.interval = 1
[36malgo-1    |[0m appender.DRFA.policies.time.modulate = true
[36malgo-1    |[0m appender.DRFA.strategy.type = DefaultRolloverStrategy
[36malgo-1    |[0m appender.DRFA.strategy.max = 30
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of all loggers
[36malgo-1    |[0m loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, PerfLogger, AmazonAws, ApacheHttp
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
[36malgo-1    |[0m logger.NIOServerCnxn.level = WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
[36malgo-1    |[0m logger.ClientCnxnSocketNIO.level = WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.DataNucleus.name = DataNucleus
[36malgo-1    |[0m logger.DataNucleus.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.Datastore.name = Datastore
[36malgo-1    |[0m logger.Datastore.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.JPOX.name = JPOX
[36malgo-1    |[0m logger.JPOX.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.AmazonAws.name=com.amazonaws
[36malgo-1    |[0m logger.AmazonAws.level = INFO
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.ApacheHttp.name=org.apache.http
[36malgo-1    |[0m logger.ApacheHttp.level = INFO
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.PerfLogger.name = org.apache.hadoop.hive.ql.log.PerfLogger
[36malgo-1    |[0m logger.PerfLogger.level = ${sys:hive.perflogger.log.level}
[36malgo-1    |[0m 
[36malgo-1    |[0m # root logger
[36malgo-1    |[0m rootLogger.level = ${sys:hive.log.level}
[36malgo-1    |[0m rootLogger.appenderRefs = root
[36malgo-1    |[0m rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/hive/conf/hive-exec-log4j2.properties
[36malgo-1    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/hive/conf/hive-exec-log4j2.properties is: 
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[36malgo-1    |[0m # or more contributor license agreements.  See the NOTICE file
[36malgo-1    |[0m # distributed with this work for additional information
[36malgo-1    |[0m # regarding copyright ownership.  The ASF licenses this file
[36malgo-1    |[0m # to you under the Apache License, Version 2.0 (the
[36malgo-1    |[0m # "License"); you may not use this file except in compliance
[36malgo-1    |[0m # with the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m status = INFO
[36malgo-1    |[0m name = HiveExecLog4j2
[36malgo-1    |[0m packages = org.apache.hadoop.hive.ql.log
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of properties
[36malgo-1    |[0m property.hive.log.level = INFO
[36malgo-1    |[0m property.hive.root.logger = FA
[36malgo-1    |[0m property.hive.query.id = hadoop
[36malgo-1    |[0m property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
[36malgo-1    |[0m property.hive.log.file = ${sys:hive.query.id}.log
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of all appenders
[36malgo-1    |[0m appenders = console, FA
[36malgo-1    |[0m 
[36malgo-1    |[0m # console appender
[36malgo-1    |[0m appender.console.type = Console
[36malgo-1    |[0m appender.console.name = console
[36malgo-1    |[0m appender.console.target = SYSTEM_ERR
[36malgo-1    |[0m appender.console.layout.type = PatternLayout
[36malgo-1    |[0m appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m # simple file appender
[36malgo-1    |[0m appender.FA.type = RandomAccessFile
[36malgo-1    |[0m appender.FA.name = FA
[36malgo-1    |[0m appender.FA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
[36malgo-1    |[0m appender.FA.layout.type = PatternLayout
[36malgo-1    |[0m appender.FA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of all loggers
[36malgo-1    |[0m loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
[36malgo-1    |[0m logger.NIOServerCnxn.level = WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
[36malgo-1    |[0m logger.ClientCnxnSocketNIO.level = WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.DataNucleus.name = DataNucleus
[36malgo-1    |[0m logger.DataNucleus.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.Datastore.name = Datastore
[36malgo-1    |[0m logger.Datastore.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.JPOX.name = JPOX
[36malgo-1    |[0m logger.JPOX.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m # root logger
[36malgo-1    |[0m rootLogger.level = ${sys:hive.log.level}
[36malgo-1    |[0m rootLogger.appenderRefs = root
[36malgo-1    |[0m rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/hive/conf/hive-site.xml
[36malgo-1    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/hive/conf/hive-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0"?>
[36malgo-1    |[0m <!--
[36malgo-1    |[0m   Licensed to the Apache Software Foundation (ASF) under one or more
[36malgo-1    |[0m   contributor license agreements.  See the NOTICE file distributed with
[36malgo-1    |[0m   this work for additional information regarding copyright ownership.
[36malgo-1    |[0m   The ASF licenses this file to You under the Apache License, Version 2.0
[36malgo-1    |[0m   (the "License"); you may not use this file except in compliance with
[36malgo-1    |[0m   the License.  You may obtain a copy of the License at
[36malgo-1    |[0m 
[36malgo-1    |[0m       http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m 
[36malgo-1    |[0m   Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m   distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m   See the License for the specific language governing permissions and
[36malgo-1    |[0m   limitations under the License.
[36malgo-1    |[0m -->
[36malgo-1    |[0m <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[36malgo-1    |[0m 
[36malgo-1    |[0m <configuration>
[36malgo-1    |[0m 
[36malgo-1    |[0m <!-- Hive Configuration can either be stored in this file or in the hadoop configuration files  -->
[36malgo-1    |[0m <!-- that are implied by Hadoop setup variables.                                                -->
[36malgo-1    |[0m <!-- Aside from Hadoop setup variables - this file is provided as a convenience so that Hive    -->
[36malgo-1    |[0m <!-- users do not have to edit hadoop configuration files (that may be managed as a centralized -->
[36malgo-1    |[0m <!-- resource).                                                                                 -->
[36malgo-1    |[0m 
[36malgo-1    |[0m <!-- Hive Execution Parameters -->
[36malgo-1    |[0m 
[36malgo-1    |[0m <property>
[36malgo-1    |[0m   <name>javax.jdo.option.ConnectionURL</name>
[36malgo-1    |[0m   <value>jdbc:derby:;databaseName=/var/lib/hive/metastore/metastore_db;create=true</value>
[36malgo-1    |[0m   <description>JDBC connect string for a JDBC metastore</description>
[36malgo-1    |[0m </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m <property>
[36malgo-1    |[0m   <name>javax.jdo.option.ConnectionDriverName</name>
[36malgo-1    |[0m   <value>org.apache.derby.jdbc.EmbeddedDriver</value>
[36malgo-1    |[0m   <description>Driver class name for a JDBC metastore</description>
[36malgo-1    |[0m </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>key</name>
[36malgo-1    |[0m     <value>value</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[36malgo-1    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.driver.host=192.168.240.3
[36malgo-1    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[36malgo-1    |[0m 
[36malgo-1    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[36malgo-1    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[36malgo-1    |[0m spark.rpc.askTimeout=300s
[36malgo-1    |[0m spark.driver.memory 2048m
[36malgo-1    |[0m spark.driver.memoryOverhead 204m
[36malgo-1    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[36malgo-1    |[0m spark.executor.memory 12399m
[36malgo-1    |[0m spark.executor.memoryOverhead 1239m
[36malgo-1    |[0m spark.executor.cores 4
[36malgo-1    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[36malgo-1    |[0m spark.executor.instances 2
[36malgo-1    |[0m spark.default.parallelism 16
[36malgo-1    |[0m spark.executor.memory 2g
[36malgo-1    |[0m spark.executor.cores 1
[36malgo-1    |[0m key value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/spark/conf/spark-env.sh
[36malgo-1    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/spark/conf/spark-env.sh is: 
[36malgo-1    |[0m #EMPTY FILE AVOID OVERRIDDING ENV VARS
[36malgo-1    |[0m # Specifically, without copying the empty file, SPARK_HISTORY_OPTS will be overriden, 
[36malgo-1    |[0m # spark.history.ui.port defaults to 18082, and spark.eventLog.dir defaults to local fs
[36malgo-1    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[36malgo-1    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/spark/conf/log4j.properties
[36malgo-1    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/spark/conf/log4j.properties is: 
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/spark/conf/hive-site.xml
[36malgo-1    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/spark/conf/hive-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0"?>
[36malgo-1    |[0m <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[36malgo-1    |[0m <configuration>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>key</name>
[36malgo-1    |[0m     <value>value</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/spark/conf/metrics.properties
[36malgo-1    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/spark/conf/metrics.properties is: 
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[36malgo-1    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0"?>
[36malgo-1    |[0m <!-- Site specific YARN configuration properties -->
[36malgo-1    |[0m  <configuration>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.resourcemanager.hostname</name>
[36malgo-1    |[0m          <value>192.168.240.3</value>
[36malgo-1    |[0m          <description>The hostname of the RM.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.hostname</name>
[36malgo-1    |[0m          <value>algo-1</value>
[36malgo-1    |[0m          <description>The hostname of the NM.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.webapp.address</name>
[36malgo-1    |[0m          <value>algo-1:8042</value>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.vmem-pmem-ratio</name>
[36malgo-1    |[0m          <value>5</value>
[36malgo-1    |[0m          <description>Ratio between virtual memory to physical memory.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.resourcemanager.am.max-attempts</name>
[36malgo-1    |[0m          <value>1</value>
[36malgo-1    |[0m          <description>The maximum number of application attempts.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.env-whitelist</name>
[36malgo-1    |[0m          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>
[36malgo-1    |[0m          <description>Environment variable whitelist</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m  
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.minimum-allocation-mb</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.maximum-allocation-mb</name>
[36malgo-1    |[0m     <value>15892</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.minimum-allocation-vcores</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.maximum-allocation-vcores</name>
[36malgo-1    |[0m     <value>4</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.nodemanager.resource.memory-mb</name>
[36malgo-1    |[0m     <value>15892</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.nodemanager.resource.cpu-vcores</name>
[36malgo-1    |[0m     <value>4</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>key</name>
[36malgo-1    |[0m     <value>value</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:19 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/yarn-env.sh
[36malgo-1    |[0m 11-14 15:19 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-env.sh is: 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one or more
[36malgo-1    |[0m # contributor license agreements.  See the NOTICE file distributed with
[36malgo-1    |[0m # this work for additional information regarding copyright ownership.
[36malgo-1    |[0m # The ASF licenses this file to You under the Apache License, Version 2.0
[36malgo-1    |[0m # (the "License"); you may not use this file except in compliance with
[36malgo-1    |[0m # the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## THIS FILE ACTS AS AN OVERRIDE FOR hadoop-env.sh FOR ALL
[36malgo-1    |[0m ## WORK DONE BY THE yarn AND RELATED COMMANDS.
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## Precedence rules:
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## yarn-env.sh > hadoop-env.sh > hard-coded defaults
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## YARN_xyz > HADOOP_xyz > hard-coded defaults
[36malgo-1    |[0m ##
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Resource Manager specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the max heapsize for the ResourceManager.  If no units are
[36malgo-1    |[0m # given, it will be assumed to be in MB.
[36malgo-1    |[0m # This value will be overridden by an Xmx setting specified in either
[36malgo-1    |[0m # HADOOP_OPTS and/or YARN_RESOURCEMANAGER_OPTS.
[36malgo-1    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX
[36malgo-1    |[0m #export YARN_RESOURCEMANAGER_HEAPSIZE=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the ResourceManager.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # Examples for a Sun/Oracle JDK:
[36malgo-1    |[0m # a) override the appsummary log file:
[36malgo-1    |[0m # export YARN_RESOURCEMANAGER_OPTS="-Dyarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log -Dyarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY"
[36malgo-1    |[0m #
[36malgo-1    |[0m # b) Set JMX options
[36malgo-1    |[0m # export YARN_RESOURCEMANAGER_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
[36malgo-1    |[0m #
[36malgo-1    |[0m # c) Set garbage collection logs from hadoop-env.sh
[36malgo-1    |[0m # export YARN_RESOURCE_MANAGER_OPTS="${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[36malgo-1    |[0m #
[36malgo-1    |[0m # d) ... or set them directly
[36malgo-1    |[0m # export YARN_RESOURCEMANAGER_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[36malgo-1    |[0m #
[36malgo-1    |[0m #
[36malgo-1    |[0m # export YARN_RESOURCEMANAGER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Node Manager specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the max heapsize for the NodeManager.  If no units are
[36malgo-1    |[0m # given, it will be assumed to be in MB.
[36malgo-1    |[0m # This value will be overridden by an Xmx setting specified in either
[36malgo-1    |[0m # HADOOP_OPTS and/or YARN_NODEMANAGER_OPTS.
[36malgo-1    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[36malgo-1    |[0m #export YARN_NODEMANAGER_HEAPSIZE=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the NodeManager.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_NODEMANAGER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # TimeLineServer specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the max heapsize for the timelineserver.  If no units are
[36malgo-1    |[0m # given, it will be assumed to be in MB.
[36malgo-1    |[0m # This value will be overridden by an Xmx setting specified in either
[36malgo-1    |[0m # HADOOP_OPTS and/or YARN_TIMELINESERVER_OPTS.
[36malgo-1    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[36malgo-1    |[0m #export YARN_TIMELINE_HEAPSIZE=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the TimeLineServer.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_TIMELINESERVER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # TimeLineReader specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the TimeLineReader.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_TIMELINEREADER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Web App Proxy Server specifc parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the max heapsize for the web app proxy server.  If no units are
[36malgo-1    |[0m # given, it will be assumed to be in MB.
[36malgo-1    |[0m # This value will be overridden by an Xmx setting specified in either
[36malgo-1    |[0m # HADOOP_OPTS and/or YARN_PROXYSERVER_OPTS.
[36malgo-1    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[36malgo-1    |[0m #export YARN_PROXYSERVER_HEAPSIZE=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the proxy server.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_PROXYSERVER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Shared Cache Manager specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the
[36malgo-1    |[0m # shared cache manager server.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_SHAREDCACHEMANAGER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Router specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the Router.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_ROUTER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Registry DNS specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # For privileged registry DNS, user to run as after dropping privileges
[36malgo-1    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[36malgo-1    |[0m # export YARN_REGISTRYDNS_SECURE_USER=yarn
[36malgo-1    |[0m 
[36malgo-1    |[0m # Supplemental options for privileged registry DNS
[36malgo-1    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[36malgo-1    |[0m # server jvm.
[36malgo-1    |[0m # export YARN_REGISTRYDNS_SECURE_EXTRA_OPTS="-jvm server"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # YARN Services parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Directory containing service examples
[36malgo-1    |[0m # export YARN_SERVICE_EXAMPLES_DIR = $HADOOP_YARN_HOME/share/hadoop/yarn/yarn-service-examples
[36malgo-1    |[0m # export YARN_CONTAINER_RUNTIME_DOCKER_RUN_OVERRIDE_DISABLE=true
[36malgo-1    |[0m export YARN_LOG_DIR=/var/log/yarn/export HADOOP_DATANODE_HEAPSIZE=2048
[36malgo-1    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[36malgo-1    |[0m 
[33malgo-2    |[0m WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.
[36malgo-1    |[0m WARNING: HADOOP_NAMENODE_OPTS has been replaced by HDFS_NAMENODE_OPTS. Using value of HADOOP_NAMENODE_OPTS.
[33malgo-2    |[0m WARNING: /usr/lib/hadoop/logs does not exist. Creating.
[33malgo-2    |[0m WARNING: /var/log/yarn/export does not exist. Creating.
[36malgo-1    |[0m WARNING: /usr/lib/hadoop/logs does not exist. Creating.
[33malgo-2    |[0m 2021-11-14 15:19:05,883 INFO nodemanager.NodeManager: STARTUP_MSG: 
[33malgo-2    |[0m /************************************************************
[33malgo-2    |[0m STARTUP_MSG: Starting NodeManager
[33malgo-2    |[0m STARTUP_MSG:   host = algo-2/192.168.240.2
[33malgo-2    |[0m STARTUP_MSG:   args = []
[33malgo-2    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[33malgo-2    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar
[33malgo-2    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[33malgo-2    |[0m STARTUP_MSG:   java = 1.8.0_302
[33malgo-2    |[0m ************************************************************/
[33malgo-2    |[0m 2021-11-14 15:19:05,892 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 15:19:05,936 INFO namenode.NameNode: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting NameNode
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/192.168.240.3
[36malgo-1    |[0m STARTUP_MSG:   args = [-format, -force]
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[33malgo-2    |[0m 2021-11-14 15:19:05,937 INFO datanode.DataNode: STARTUP_MSG: 
[33malgo-2    |[0m /************************************************************
[33malgo-2    |[0m STARTUP_MSG: Starting DataNode
[33malgo-2    |[0m STARTUP_MSG:   host = algo-2/192.168.240.2
[33malgo-2    |[0m STARTUP_MSG:   args = []
[33malgo-2    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[33malgo-2    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar
[33malgo-2    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[33malgo-2    |[0m STARTUP_MSG:   java = 1.8.0_302
[33malgo-2    |[0m ************************************************************/
[33malgo-2    |[0m 2021-11-14 15:19:05,944 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 15:19:05,949 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 15:19:06,031 INFO namenode.NameNode: createNameNode [-format, -force]
[33malgo-2    |[0m 2021-11-14 15:19:06,279 INFO resourceplugin.ResourcePluginManager: No Resource plugins found from configuration!
[33malgo-2    |[0m 2021-11-14 15:19:06,279 INFO resourceplugin.ResourcePluginManager: Found Resource plugins from configuration: null
[33malgo-2    |[0m 2021-11-14 15:19:06,282 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode
[33malgo-2    |[0m 2021-11-14 15:19:06,353 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.
[33malgo-2    |[0m 2021-11-14 15:19:06,374 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[33malgo-2    |[0m 2021-11-14 15:19:06,400 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher
[33malgo-2    |[0m 2021-11-14 15:19:06,401 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher
[33malgo-2    |[0m 2021-11-14 15:19:06,402 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper
[33malgo-2    |[0m 2021-11-14 15:19:06,403 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices
[33malgo-2    |[0m 2021-11-14 15:19:06,403 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl
[33malgo-2    |[0m 2021-11-14 15:19:06,404 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher
[33malgo-2    |[0m 2021-11-14 15:19:06,404 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler
[33malgo-2    |[0m 2021-11-14 15:19:06,406 INFO tracker.NMLogAggregationStatusTracker: the rolling interval seconds for the NodeManager Cached Log aggregation status is 600
[33malgo-2    |[0m 2021-11-14 15:19:06,424 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl
[33malgo-2    |[0m 2021-11-14 15:19:06,424 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager
[36malgo-1    |[0m Formatting using clusterid: CID-4f4b0c56-adf8-4bd0-90e7-41a740098162
[33malgo-2    |[0m 2021-11-14 15:19:06,451 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[33malgo-2    |[0m 2021-11-14 15:19:06,451 INFO impl.MetricsSystemImpl: DataNode metrics system started
[36malgo-1    |[0m 2021-11-14 15:19:06,464 INFO namenode.FSEditLog: Edit logging is async:true
[33malgo-2    |[0m 2021-11-14 15:19:06,475 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m 2021-11-14 15:19:06,480 INFO namenode.FSNamesystem: KeyProvider: null
[36malgo-1    |[0m 2021-11-14 15:19:06,481 INFO namenode.FSNamesystem: fsLock is fair: true
[36malgo-1    |[0m 2021-11-14 15:19:06,482 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
[36malgo-1    |[0m 2021-11-14 15:19:06,488 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 15:19:06,488 INFO namenode.FSNamesystem: supergroup          = supergroup
[36malgo-1    |[0m 2021-11-14 15:19:06,488 INFO namenode.FSNamesystem: isPermissionEnabled = true
[36malgo-1    |[0m 2021-11-14 15:19:06,488 INFO namenode.FSNamesystem: HA Enabled: false
[36malgo-1    |[0m 2021-11-14 15:19:06,537 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[33malgo-2    |[0m 2021-11-14 15:19:06,549 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[33malgo-2    |[0m 2021-11-14 15:19:06,549 INFO impl.MetricsSystemImpl: NodeManager metrics system started
[36malgo-1    |[0m 2021-11-14 15:19:06,550 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
[36malgo-1    |[0m 2021-11-14 15:19:06,550 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
[36malgo-1    |[0m 2021-11-14 15:19:06,555 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
[36malgo-1    |[0m 2021-11-14 15:19:06,555 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Nov 14 15:19:06
[36malgo-1    |[0m 2021-11-14 15:19:06,557 INFO util.GSet: Computing capacity for map BlocksMap
[36malgo-1    |[0m 2021-11-14 15:19:06,557 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 15:19:06,558 INFO util.GSet: 2.0% max memory 6.5 GB = 133.6 MB
[36malgo-1    |[0m 2021-11-14 15:19:06,558 INFO util.GSet: capacity      = 2^24 = 16777216 entries
[33malgo-2    |[0m 2021-11-14 15:19:06,567 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.
[33malgo-2    |[0m 2021-11-14 15:19:06,576 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.
[36malgo-1    |[0m 2021-11-14 15:19:06,601 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
[36malgo-1    |[0m 2021-11-14 15:19:06,601 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
[33malgo-2    |[0m 2021-11-14 15:19:06,606 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@4c60d6e9
[36malgo-1    |[0m 2021-11-14 15:19:06,607 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
[36malgo-1    |[0m 2021-11-14 15:19:06,608 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
[36malgo-1    |[0m 2021-11-14 15:19:06,608 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
[36malgo-1    |[0m 2021-11-14 15:19:06,608 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
[36malgo-1    |[0m 2021-11-14 15:19:06,608 INFO blockmanagement.BlockManager: defaultReplication         = 3
[36malgo-1    |[0m 2021-11-14 15:19:06,608 INFO blockmanagement.BlockManager: maxReplication             = 512
[36malgo-1    |[0m 2021-11-14 15:19:06,608 INFO blockmanagement.BlockManager: minReplication             = 1
[36malgo-1    |[0m 2021-11-14 15:19:06,609 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
[36malgo-1    |[0m 2021-11-14 15:19:06,609 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
[36malgo-1    |[0m 2021-11-14 15:19:06,609 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
[36malgo-1    |[0m 2021-11-14 15:19:06,609 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
[33malgo-2    |[0m 2021-11-14 15:19:06,609 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler
[33malgo-2    |[0m 2021-11-14 15:19:06,610 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService
[33malgo-2    |[0m 2021-11-14 15:19:06,610 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled
[33malgo-2    |[0m 2021-11-14 15:19:06,610 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[33malgo-2    |[0m 2021-11-14 15:19:06,611 INFO localizer.ResourceLocalizationService: per directory file limit = 8192
[33malgo-2    |[0m 2021-11-14 15:19:06,614 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
[33malgo-2    |[0m 2021-11-14 15:19:06,618 INFO datanode.DataNode: Configured hostname is algo-2
[33malgo-2    |[0m 2021-11-14 15:19:06,619 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[33malgo-2    |[0m 2021-11-14 15:19:06,623 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0
[33malgo-2    |[0m 2021-11-14 15:19:06,632 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker
[36malgo-1    |[0m 2021-11-14 15:19:06,632 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
[36malgo-1    |[0m 2021-11-14 15:19:06,633 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 15:19:06,633 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 15:19:06,633 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
[33malgo-2    |[0m 2021-11-14 15:19:06,639 INFO resources.ResourceHandlerModule: Using traffic control bandwidth handler
[33malgo-2    |[0m 2021-11-14 15:19:06,642 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@221a3fa4
[33malgo-2    |[0m 2021-11-14 15:19:06,643 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null
[33malgo-2    |[0m 2021-11-14 15:19:06,644 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true
[33malgo-2    |[0m 2021-11-14 15:19:06,644 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true
[33malgo-2    |[0m 2021-11-14 15:19:06,644 INFO monitor.ContainersMonitorImpl: Elastic memory control enabled: false
[33malgo-2    |[0m 2021-11-14 15:19:06,644 INFO monitor.ContainersMonitorImpl: Strict memory control enabled: true
[33malgo-2    |[0m 2021-11-14 15:19:06,644 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true
[36malgo-1    |[0m 2021-11-14 15:19:06,646 INFO util.GSet: Computing capacity for map INodeMap
[36malgo-1    |[0m 2021-11-14 15:19:06,646 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 15:19:06,647 INFO util.GSet: 1.0% max memory 6.5 GB = 66.8 MB
[36malgo-1    |[0m 2021-11-14 15:19:06,647 INFO util.GSet: capacity      = 2^23 = 8388608 entries
[33malgo-2    |[0m 2021-11-14 15:19:06,647 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.
[33malgo-2    |[0m 2021-11-14 15:19:06,648 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866
[33malgo-2    |[0m 2021-11-14 15:19:06,651 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s
[33malgo-2    |[0m 2021-11-14 15:19:06,651 INFO datanode.DataNode: Number threads for balancing is 50
[36malgo-1    |[0m 2021-11-14 15:19:06,668 INFO namenode.FSDirectory: ACLs enabled? false
[36malgo-1    |[0m 2021-11-14 15:19:06,668 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
[36malgo-1    |[0m 2021-11-14 15:19:06,668 INFO namenode.FSDirectory: XAttrs enabled? true
[36malgo-1    |[0m 2021-11-14 15:19:06,668 INFO namenode.NameNode: Caching file names occurring more than 10 times
[36malgo-1    |[0m 2021-11-14 15:19:06,673 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
[36malgo-1    |[0m 2021-11-14 15:19:06,675 INFO snapshot.SnapshotManager: SkipList is disabled
[36malgo-1    |[0m 2021-11-14 15:19:06,679 INFO util.GSet: Computing capacity for map cachedBlocks
[36malgo-1    |[0m 2021-11-14 15:19:06,679 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 15:19:06,679 INFO util.GSet: 0.25% max memory 6.5 GB = 16.7 MB
[36malgo-1    |[0m 2021-11-14 15:19:06,679 INFO util.GSet: capacity      = 2^21 = 2097152 entries
[33malgo-2    |[0m 2021-11-14 15:19:06,684 INFO conf.Configuration: resource-types.xml not found
[33malgo-2    |[0m 2021-11-14 15:19:06,684 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
[36malgo-1    |[0m 2021-11-14 15:19:06,687 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
[36malgo-1    |[0m 2021-11-14 15:19:06,687 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
[36malgo-1    |[0m 2021-11-14 15:19:06,687 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
[33malgo-2    |[0m 2021-11-14 15:19:06,689 INFO conf.Configuration: node-resources.xml not found
[33malgo-2    |[0m 2021-11-14 15:19:06,689 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.
[36malgo-1    |[0m 2021-11-14 15:19:06,690 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
[36malgo-1    |[0m 2021-11-14 15:19:06,691 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
[33malgo-2    |[0m 2021-11-14 15:19:06,691 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 15:19:06,692 INFO util.GSet: Computing capacity for map NameNodeRetryCache
[36malgo-1    |[0m 2021-11-14 15:19:06,692 INFO util.GSet: VM type       = 64-bit
[33malgo-2    |[0m 2021-11-14 15:19:06,692 INFO util.log: Logging initialized @1240ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m 2021-11-14 15:19:06,692 INFO util.GSet: 0.029999999329447746% max memory 6.5 GB = 2.0 MB
[36malgo-1    |[0m 2021-11-14 15:19:06,692 INFO util.GSet: capacity      = 2^18 = 262144 entries
[33malgo-2    |[0m 2021-11-14 15:19:06,695 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=15892 virtual-memory=79460 virtual-cores=4
[36malgo-1    |[0m 2021-11-14 15:19:06,719 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1419144703-192.168.240.3-1636903146711
[33malgo-2    |[0m 2021-11-14 15:19:06,732 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 15:19:06,736 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.
[33malgo-2    |[0m 2021-11-14 15:19:06,753 INFO ipc.Server: Starting Socket Reader #1 for port 0
[36malgo-1    |[0m 2021-11-14 15:19:06,760 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression
[33malgo-2    |[0m 2021-11-14 15:19:06,825 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[33malgo-2    |[0m 2021-11-14 15:19:06,832 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined
[33malgo-2    |[0m 2021-11-14 15:19:06,837 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[33malgo-2    |[0m 2021-11-14 15:19:06,839 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
[33malgo-2    |[0m 2021-11-14 15:19:06,839 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[33malgo-2    |[0m 2021-11-14 15:19:06,839 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 2021-11-14 15:19:06,851 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 396 bytes saved in 0 seconds .
[36malgo-1    |[0m 2021-11-14 15:19:06,859 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
[33malgo-2    |[0m 2021-11-14 15:19:06,863 INFO http.HttpServer2: Jetty bound to port 43393
[36malgo-1    |[0m 2021-11-14 15:19:06,863 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
[36malgo-1    |[0m 2021-11-14 15:19:06,864 INFO namenode.NameNode: SHUTDOWN_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m SHUTDOWN_MSG: Shutting down NameNode at algo-1/192.168.240.3
[36malgo-1    |[0m ************************************************************/
[33malgo-2    |[0m 2021-11-14 15:19:06,864 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[33malgo-2    |[0m 2021-11-14 15:19:06,886 INFO server.session: DefaultSessionIdManager workerName=node0
[33malgo-2    |[0m 2021-11-14 15:19:06,886 INFO server.session: No SessionScavenger set, using defaults
[33malgo-2    |[0m 2021-11-14 15:19:06,887 INFO server.session: node0 Scavenging every 660000ms
[33malgo-2    |[0m 2021-11-14 15:19:06,897 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a3793c7{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}
[33malgo-2    |[0m 2021-11-14 15:19:06,897 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@bb9e6dc{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}
[36malgo-1    |[0m 11-14 15:19 smspark-submit INFO     waiting for cluster to be up
[33malgo-2    |[0m 2021-11-14 15:19:06,937 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server
[33malgo-2    |[0m 2021-11-14 15:19:06,937 INFO ipc.Server: IPC Server Responder: starting
[33malgo-2    |[0m 2021-11-14 15:19:06,938 INFO ipc.Server: IPC Server listener on 0: starting
[33malgo-2    |[0m 2021-11-14 15:19:06,944 INFO security.NMContainerTokenSecretManager: Updating node address : algo-2:42603
[33malgo-2    |[0m 2021-11-14 15:19:06,950 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 500, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[33malgo-2    |[0m 2021-11-14 15:19:06,950 INFO ipc.Server: Starting Socket Reader #1 for port 8040
[33malgo-2    |[0m 2021-11-14 15:19:06,953 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server
[33malgo-2    |[0m 2021-11-14 15:19:06,953 INFO ipc.Server: IPC Server Responder: starting
[33malgo-2    |[0m 2021-11-14 15:19:06,953 INFO ipc.Server: IPC Server listener on 8040: starting
[33malgo-2    |[0m 2021-11-14 15:19:06,954 INFO localizer.ResourceLocalizationService: Localizer started on port 8040
[33malgo-2    |[0m 2021-11-14 15:19:06,955 INFO util.TypeUtil: JVM Runtime does not support Modules
[33malgo-2    |[0m 2021-11-14 15:19:06,956 INFO containermanager.ContainerManagerImpl: ContainerManager started at /192.168.240.2:42603
[33malgo-2    |[0m 2021-11-14 15:19:06,957 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-2/192.168.240.2:0
[33malgo-2    |[0m 2021-11-14 15:19:06,957 WARN tracker.NMLogAggregationStatusTracker: Log Aggregation is disabled.So is the LogAggregationStatusTracker.
[33malgo-2    |[0m 2021-11-14 15:19:06,961 INFO webapp.WebServer: Instantiating NMWebApp at algo-2:8042
[33malgo-2    |[0m 2021-11-14 15:19:06,965 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@338c99c8{datanode,/,file:///usr/lib/hadoop-hdfs/webapps/datanode/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/datanode}
[36malgo-1    |[0m WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.
[36malgo-1    |[0m WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.
[33malgo-2    |[0m 2021-11-14 15:19:06,973 INFO server.AbstractConnector: Started ServerConnector@6c372fe6{HTTP/1.1,[http/1.1]}{localhost:43393}
[33malgo-2    |[0m 2021-11-14 15:19:06,973 INFO server.Server: Started @1521ms
[36malgo-1    |[0m WARNING: HADOOP_NAMENODE_OPTS has been replaced by HDFS_NAMENODE_OPTS. Using value of HADOOP_NAMENODE_OPTS.
[33malgo-2    |[0m 2021-11-14 15:19:06,985 INFO util.log: Logging initialized @1532ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m WARNING: /var/log/yarn/export does not exist. Creating.
[33malgo-2    |[0m 2021-11-14 15:19:07,073 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[33malgo-2    |[0m 2021-11-14 15:19:07,076 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined
[33malgo-2    |[0m 2021-11-14 15:19:07,082 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[33malgo-2    |[0m 2021-11-14 15:19:07,083 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node
[33malgo-2    |[0m 2021-11-14 15:19:07,083 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[33malgo-2    |[0m 2021-11-14 15:19:07,083 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[33malgo-2    |[0m 2021-11-14 15:19:07,083 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context node
[33malgo-2    |[0m 2021-11-14 15:19:07,083 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
[33malgo-2    |[0m 2021-11-14 15:19:07,083 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
[33malgo-2    |[0m 2021-11-14 15:19:07,085 INFO http.HttpServer2: adding path spec: /node/*
[33malgo-2    |[0m 2021-11-14 15:19:07,085 INFO http.HttpServer2: adding path spec: /ws/*
[33malgo-2    |[0m 2021-11-14 15:19:07,108 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864
[33malgo-2    |[0m 2021-11-14 15:19:07,115 INFO datanode.DataNode: dnUserName = root
[33malgo-2    |[0m 2021-11-14 15:19:07,115 INFO datanode.DataNode: supergroup = supergroup
[33malgo-2    |[0m 2021-11-14 15:19:07,117 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[33malgo-2    |[0m 2021-11-14 15:19:07,164 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[33malgo-2    |[0m 2021-11-14 15:19:07,181 INFO ipc.Server: Starting Socket Reader #1 for port 9867
[33malgo-2    |[0m 2021-11-14 15:19:07,419 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:9867
[33malgo-2    |[0m 2021-11-14 15:19:07,435 INFO datanode.DataNode: Refresh request received for nameservices: null
[33malgo-2    |[0m 2021-11-14 15:19:07,448 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>
[33malgo-2    |[0m 2021-11-14 15:19:07,461 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1.spark-network/192.168.240.3:8020 starting to offer service
[33malgo-2    |[0m 2021-11-14 15:19:07,467 INFO ipc.Server: IPC Server Responder: starting
[33malgo-2    |[0m 2021-11-14 15:19:07,468 INFO ipc.Server: IPC Server listener on 9867: starting
[33malgo-2    |[0m 2021-11-14 15:19:07,468 INFO webapp.WebApps: Registered webapp guice modules
[33malgo-2    |[0m 2021-11-14 15:19:07,472 INFO http.HttpServer2: Jetty bound to port 8042
[33malgo-2    |[0m 2021-11-14 15:19:07,473 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[33malgo-2    |[0m 2021-11-14 15:19:07,508 INFO server.session: DefaultSessionIdManager workerName=node0
[33malgo-2    |[0m 2021-11-14 15:19:07,508 INFO server.session: No SessionScavenger set, using defaults
[33malgo-2    |[0m 2021-11-14 15:19:07,510 INFO server.session: node0 Scavenging every 600000ms
[36malgo-1    |[0m 2021-11-14 15:19:07,521 INFO resourcemanager.ResourceManager: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting ResourceManager
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/192.168.240.3
[36malgo-1    |[0m STARTUP_MSG:   args = []
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[33malgo-2    |[0m 2021-11-14 15:19:07,527 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[33malgo-2    |[0m 2021-11-14 15:19:07,530 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4bff64c2{logs,/logs,file:///var/log/yarn/export/,AVAILABLE}
[33malgo-2    |[0m 2021-11-14 15:19:07,531 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3cc41abc{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/static,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 15:19:07,531 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]
[33malgo-2    |[0m 2021-11-14 15:19:07,545 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException
[36malgo-1    |[0m 2021-11-14 15:19:07,552 INFO nodemanager.NodeManager: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting NodeManager
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/192.168.240.3
[36malgo-1    |[0m STARTUP_MSG:   args = []
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 15:19:07,561 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 15:19:07,577 INFO namenode.NameNode: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting NameNode
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/192.168.240.3
[36malgo-1    |[0m STARTUP_MSG:   args = []
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 15:19:07,589 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 15:19:07,637 INFO datanode.DataNode: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting DataNode
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/192.168.240.3
[36malgo-1    |[0m STARTUP_MSG:   args = []
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 15:19:07,646 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
[33malgo-2    |[0m 2021-11-14 15:19:07,670 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m 2021-11-14 15:19:07,680 INFO namenode.NameNode: createNameNode []
[33malgo-2    |[0m Nov 14, 2021 3:19:07 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[33malgo-2    |[0m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class
[33malgo-2    |[0m Nov 14, 2021 3:19:07 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[33malgo-2    |[0m INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
[33malgo-2    |[0m Nov 14, 2021 3:19:07 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[33malgo-2    |[0m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class
[33malgo-2    |[0m Nov 14, 2021 3:19:07 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
[33malgo-2    |[0m INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'
[36malgo-1    |[0m 2021-11-14 15:19:07,841 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[33malgo-2    |[0m Nov 14, 2021 3:19:07 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[33malgo-2    |[0m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 15:19:07,923 INFO conf.Configuration: found resource core-site.xml at file:/etc/hadoop/conf.empty/core-site.xml
[36malgo-1    |[0m 2021-11-14 15:19:07,963 INFO conf.Configuration: resource-types.xml not found
[36malgo-1    |[0m 2021-11-14 15:19:07,963 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
[36malgo-1    |[0m 2021-11-14 15:19:08,003 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m 2021-11-14 15:19:08,004 INFO impl.MetricsSystemImpl: NameNode metrics system started
[36malgo-1    |[0m 2021-11-14 15:19:08,011 INFO conf.Configuration: found resource yarn-site.xml at file:/etc/hadoop/conf.empty/yarn-site.xml
[36malgo-1    |[0m 2021-11-14 15:19:08,021 INFO resourceplugin.ResourcePluginManager: No Resource plugins found from configuration!
[36malgo-1    |[0m 2021-11-14 15:19:08,021 INFO resourceplugin.ResourcePluginManager: Found Resource plugins from configuration: null
[36malgo-1    |[0m 2021-11-14 15:19:08,029 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher
[36malgo-1    |[0m 2021-11-14 15:19:08,041 INFO namenode.NameNodeUtils: fs.defaultFS is hdfs://192.168.240.3/
[33malgo-2    |[0m Nov 14, 2021 3:19:08 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[33malgo-2    |[0m INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 15:19:08,075 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms
[36malgo-1    |[0m 2021-11-14 15:19:08,080 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms
[36malgo-1    |[0m 2021-11-14 15:19:08,085 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms
[36malgo-1    |[0m 2021-11-14 15:19:08,086 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode
[36malgo-1    |[0m 2021-11-14 15:19:08,101 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.
[36malgo-1    |[0m 2021-11-14 15:19:08,134 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler
[36malgo-1    |[0m 2021-11-14 15:19:08,138 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager
[36malgo-1    |[0m 2021-11-14 15:19:08,138 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
[36malgo-1    |[0m 2021-11-14 15:19:08,163 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher
[36malgo-1    |[0m 2021-11-14 15:19:08,164 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher
[36malgo-1    |[0m 2021-11-14 15:19:08,165 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper
[36malgo-1    |[0m 2021-11-14 15:19:08,166 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices
[36malgo-1    |[0m 2021-11-14 15:19:08,166 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl
[36malgo-1    |[0m 2021-11-14 15:19:08,167 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher
[36malgo-1    |[0m 2021-11-14 15:19:08,167 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler
[36malgo-1    |[0m 2021-11-14 15:19:08,170 INFO tracker.NMLogAggregationStatusTracker: the rolling interval seconds for the NodeManager Cached Log aggregation status is 600
[36malgo-1    |[0m 2021-11-14 15:19:08,175 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher
[36malgo-1    |[0m 2021-11-14 15:19:08,176 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher
[36malgo-1    |[0m 2021-11-14 15:19:08,177 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher
[36malgo-1    |[0m 2021-11-14 15:19:08,178 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher
[36malgo-1    |[0m 2021-11-14 15:19:08,191 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl
[36malgo-1    |[0m 2021-11-14 15:19:08,191 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager
[36malgo-1    |[0m 2021-11-14 15:19:08,213 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m 2021-11-14 15:19:08,229 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 15:19:08,255 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m 2021-11-14 15:19:08,256 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m 2021-11-14 15:19:08,266 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:9870
[36malgo-1    |[0m 2021-11-14 15:19:08,284 INFO util.log: Logging initialized @1283ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m 2021-11-14 15:19:08,348 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m 2021-11-14 15:19:08,348 INFO impl.MetricsSystemImpl: DataNode metrics system started
[36malgo-1    |[0m 2021-11-14 15:19:08,358 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m 2021-11-14 15:19:08,359 INFO impl.MetricsSystemImpl: NodeManager metrics system started
[36malgo-1    |[0m 2021-11-14 15:19:08,368 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m 2021-11-14 15:19:08,368 INFO impl.MetricsSystemImpl: ResourceManager metrics system started
[36malgo-1    |[0m 2021-11-14 15:19:08,384 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.
[36malgo-1    |[0m 2021-11-14 15:19:08,396 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.
[36malgo-1    |[0m 2021-11-14 15:19:08,429 INFO security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.
[36malgo-1    |[0m 2021-11-14 15:19:08,433 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager
[36malgo-1    |[0m 2021-11-14 15:19:08,434 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 15:19:08,441 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@4c60d6e9
[36malgo-1    |[0m 2021-11-14 15:19:08,442 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher
[36malgo-1    |[0m 2021-11-14 15:19:08,443 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler
[36malgo-1    |[0m 2021-11-14 15:19:08,445 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean
[36malgo-1    |[0m 2021-11-14 15:19:08,445 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.
[36malgo-1    |[0m 2021-11-14 15:19:08,445 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService
[36malgo-1    |[0m 2021-11-14 15:19:08,445 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled
[36malgo-1    |[0m 2021-11-14 15:19:08,446 INFO localizer.ResourceLocalizationService: per directory file limit = 8192
[33malgo-2    |[0m Nov 14, 2021 3:19:08 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[33malgo-2    |[0m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 15:19:08,453 INFO placement.MultiNodeSortingManager: Initializing NodeSortingService=MultiNodeSortingManager
[36malgo-1    |[0m 2021-11-14 15:19:08,455 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list
[36malgo-1    |[0m 2021-11-14 15:19:08,456 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined
[36malgo-1    |[0m 2021-11-14 15:19:08,464 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/etc/hadoop/conf.empty/capacity-scheduler.xml
[36malgo-1    |[0m 2021-11-14 15:19:08,466 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[36malgo-1    |[0m 2021-11-14 15:19:08,469 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
[36malgo-1    |[0m 2021-11-14 15:19:08,469 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[36malgo-1    |[0m 2021-11-14 15:19:08,469 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 2021-11-14 15:19:08,475 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker
[36malgo-1    |[0m 2021-11-14 15:19:08,482 INFO resources.ResourceHandlerModule: Using traffic control bandwidth handler
[36malgo-1    |[0m 2021-11-14 15:19:08,483 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1, vCores:1>
[36malgo-1    |[0m 2021-11-14 15:19:08,483 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 15:19:08,486 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@221a3fa4
[36malgo-1    |[0m 2021-11-14 15:19:08,486 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null
[36malgo-1    |[0m 2021-11-14 15:19:08,487 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true
[36malgo-1    |[0m 2021-11-14 15:19:08,487 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true
[36malgo-1    |[0m 2021-11-14 15:19:08,487 INFO monitor.ContainersMonitorImpl: Elastic memory control enabled: false
[36malgo-1    |[0m 2021-11-14 15:19:08,488 INFO monitor.ContainersMonitorImpl: Strict memory control enabled: true
[36malgo-1    |[0m 2021-11-14 15:19:08,488 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true
[33malgo-2    |[0m 2021-11-14 15:19:08,492 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5d5b5fa7{node,/,file:///tmp/jetty-algo-2-8042-_-any-7867694013106055877.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/node}
[36malgo-1    |[0m 2021-11-14 15:19:08,492 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.
[36malgo-1    |[0m 2021-11-14 15:19:08,510 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
[36malgo-1    |[0m 2021-11-14 15:19:08,510 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
[33malgo-2    |[0m 2021-11-14 15:19:08,510 INFO server.AbstractConnector: Started ServerConnector@3f390d63{HTTP/1.1,[http/1.1]}{algo-2:8042}
[33malgo-2    |[0m 2021-11-14 15:19:08,510 INFO server.Server: Started @3057ms
[33malgo-2    |[0m 2021-11-14 15:19:08,510 INFO webapp.WebApps: Web app node started at 8042
[33malgo-2    |[0m 2021-11-14 15:19:08,521 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-2:42603
[36malgo-1    |[0m 2021-11-14 15:19:08,522 INFO http.HttpServer2: Jetty bound to port 9870
[33malgo-2    |[0m 2021-11-14 15:19:08,522 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 15:19:08,523 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[33malgo-2    |[0m 2021-11-14 15:19:08,528 INFO client.RMProxy: Connecting to ResourceManager at /192.168.240.3:8031
[36malgo-1    |[0m 2021-11-14 15:19:08,534 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined
[36malgo-1    |[0m 2021-11-14 15:19:08,534 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined
[33malgo-2    |[0m 2021-11-14 15:19:08,538 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/192.168.240.3:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 15:19:08,548 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=ADMINISTER_QUEUE:*SUBMIT_APP:*, labels=*,
[36malgo-1    |[0m , reservationsContinueLooking=true, orderingPolicy=utilization, priority=0
[36malgo-1    |[0m 2021-11-14 15:19:08,548 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root
[36malgo-1    |[0m 2021-11-14 15:19:08,557 INFO conf.Configuration: resource-types.xml not found
[36malgo-1    |[0m 2021-11-14 15:19:08,557 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
[36malgo-1    |[0m 2021-11-14 15:19:08,565 INFO conf.Configuration: node-resources.xml not found
[36malgo-1    |[0m 2021-11-14 15:19:08,566 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.
[36malgo-1    |[0m 2021-11-14 15:19:08,571 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined
[36malgo-1    |[0m 2021-11-14 15:19:08,571 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined
[36malgo-1    |[0m 2021-11-14 15:19:08,573 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 15:19:08,577 INFO capacity.LeafQueue: Initializing default
[36malgo-1    |[0m capacity = 1.0 [= (float) configuredCapacity / 100 ]
[36malgo-1    |[0m absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]
[36malgo-1    |[0m maxCapacity = 1.0 [= configuredMaxCapacity ]
[36malgo-1    |[0m absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]
[36malgo-1    |[0m effectiveMinResource=<memory:0, vCores:0>
[36malgo-1    |[0m  , effectiveMaxResource=<memory:0, vCores:0>
[36malgo-1    |[0m userLimit = 100 [= configuredUserLimit ]
[36malgo-1    |[0m userLimitFactor = 1.0 [= configuredUserLimitFactor ]
[36malgo-1    |[0m maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]
[36malgo-1    |[0m maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]
[36malgo-1    |[0m usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]
[36malgo-1    |[0m absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]
[36malgo-1    |[0m maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]
[36malgo-1    |[0m minimumAllocationFactor = 0.99993706 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]
[36malgo-1    |[0m maximumAllocation = <memory:15892, vCores:4> [= configuredMaxAllocation ]
[36malgo-1    |[0m numContainers = 0 [= currentNumContainers ]
[36malgo-1    |[0m state = RUNNING [= configuredState ]
[36malgo-1    |[0m acls = ADMINISTER_QUEUE:*SUBMIT_APP:* [= configuredAcls ]
[36malgo-1    |[0m nodeLocalityDelay = 40
[36malgo-1    |[0m rackLocalityAdditionalDelay = -1
[36malgo-1    |[0m labels=*,
[36malgo-1    |[0m reservationsContinueLooking = true
[36malgo-1    |[0m preemptionDisabled = true
[36malgo-1    |[0m defaultAppPriorityPerQueue = 0
[36malgo-1    |[0m priority = 0
[36malgo-1    |[0m maxLifetime = -1 seconds
[36malgo-1    |[0m defaultLifetime = -1 seconds
[36malgo-1    |[0m 2021-11-14 15:19:08,577 INFO server.session: DefaultSessionIdManager workerName=node0
[36malgo-1    |[0m 2021-11-14 15:19:08,577 INFO server.session: No SessionScavenger set, using defaults
[36malgo-1    |[0m 2021-11-14 15:19:08,578 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>
[36malgo-1    |[0m 2021-11-14 15:19:08,579 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
[36malgo-1    |[0m 2021-11-14 15:19:08,580 INFO server.session: node0 Scavenging every 660000ms
[36malgo-1    |[0m 2021-11-14 15:19:08,581 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=15892 virtual-memory=79460 virtual-cores=4
[36malgo-1    |[0m 2021-11-14 15:19:08,582 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
[36malgo-1    |[0m 2021-11-14 15:19:08,582 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false
[33malgo-2    |[0m 2021-11-14 15:19:08,582 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []
[36malgo-1    |[0m 2021-11-14 15:19:08,584 INFO placement.MultiNodeSortingManager: MultiNode scheduling is 'false', and configured policies are 
[36malgo-1    |[0m 2021-11-14 15:19:08,584 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1, vCores:1>>, maximumAllocation=<<memory:15892, vCores:4>>, asynchronousScheduling=false, asyncScheduleInterval=5ms,multiNodePlacementEnabled=false
[36malgo-1    |[0m 2021-11-14 15:19:08,591 INFO conf.Configuration: dynamic-resources.xml not found
[33malgo-2    |[0m 2021-11-14 15:19:08,593 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]
[36malgo-1    |[0m 2021-11-14 15:19:08,593 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].
[36malgo-1    |[0m 2021-11-14 15:19:08,594 INFO resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.
[36malgo-1    |[0m 2021-11-14 15:19:08,594 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6d60fe40{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 15:19:08,595 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@73eb439a{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 15:19:08,596 INFO resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. 
[36malgo-1    |[0m 2021-11-14 15:19:08,605 INFO resourcemanager.ResourceManager: TimelineServicePublisher is not configured
[36malgo-1    |[0m 2021-11-14 15:19:08,638 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[36malgo-1    |[0m 2021-11-14 15:19:08,639 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 15:19:08,641 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
[36malgo-1    |[0m 2021-11-14 15:19:08,646 INFO datanode.DataNode: Configured hostname is algo-1
[36malgo-1    |[0m 2021-11-14 15:19:08,646 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[36malgo-1    |[0m 2021-11-14 15:19:08,651 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0
[36malgo-1    |[0m 2021-11-14 15:19:08,656 INFO util.log: Logging initialized @1652ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m 2021-11-14 15:19:08,661 INFO ipc.Server: Starting Socket Reader #1 for port 0
[36malgo-1    |[0m 2021-11-14 15:19:08,666 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m 2021-11-14 15:19:08,676 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866
[36malgo-1    |[0m 2021-11-14 15:19:08,679 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s
[36malgo-1    |[0m 2021-11-14 15:19:08,679 INFO datanode.DataNode: Number threads for balancing is 50
[36malgo-1    |[0m 2021-11-14 15:19:08,680 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4dd6fd0a{hdfs,/,file:///usr/lib/hadoop-hdfs/webapps/hdfs/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/hdfs}
[36malgo-1    |[0m 2021-11-14 15:19:08,713 INFO server.AbstractConnector: Started ServerConnector@4d14b6c2{HTTP/1.1,[http/1.1]}{0.0.0.0:9870}
[36malgo-1    |[0m 2021-11-14 15:19:08,713 INFO server.Server: Started @1713ms
[36malgo-1    |[0m 2021-11-14 15:19:08,727 INFO util.log: Logging initialized @1727ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m 2021-11-14 15:19:08,759 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 15:19:08,764 INFO http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined
[36malgo-1    |[0m 2021-11-14 15:19:08,771 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[36malgo-1    |[0m 2021-11-14 15:19:08,797 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster
[36malgo-1    |[0m 2021-11-14 15:19:08,797 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs
[36malgo-1    |[0m 2021-11-14 15:19:08,797 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static
[36malgo-1    |[0m 2021-11-14 15:19:08,797 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster
[36malgo-1    |[0m 2021-11-14 15:19:08,797 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[36malgo-1    |[0m 2021-11-14 15:19:08,797 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 2021-11-14 15:19:08,799 INFO http.HttpServer2: adding path spec: /cluster/*
[36malgo-1    |[0m 2021-11-14 15:19:08,799 INFO http.HttpServer2: adding path spec: /ws/*
[36malgo-1    |[0m 2021-11-14 15:19:08,799 INFO http.HttpServer2: adding path spec: /app/*
[33malgo-2    |[0m 11-14 15:19 smspark-submit INFO     cluster is up
[33malgo-2    |[0m 11-14 15:19 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING
[33malgo-2    |[0m 11-14 15:19 smspark-submit INFO     starting executor logs watcher
[33malgo-2    |[0m Starting executor logs watcher on log_dir: /var/log/yarn
[33malgo-2    |[0m 11-14 15:19 smspark-submit INFO     waiting for the primary to come up
[33malgo-2    |[0m 11-14 15:19 smspark-submit INFO     waiting for the primary to go down
[36malgo-1    |[0m 2021-11-14 15:19:08,904 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 15:19:08,914 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 15:19:08,917 INFO ipc.Server: IPC Server listener on 0: starting
[36malgo-1    |[0m 2021-11-14 15:19:08,925 INFO security.NMContainerTokenSecretManager: Updating node address : algo-1:44133
[36malgo-1    |[0m 2021-11-14 15:19:08,932 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 500, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 15:19:08,933 INFO ipc.Server: Starting Socket Reader #1 for port 8040
[36malgo-1    |[0m 2021-11-14 15:19:08,937 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 15:19:08,939 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 15:19:08,948 INFO ipc.Server: IPC Server listener on 8040: starting
[36malgo-1    |[0m 2021-11-14 15:19:08,953 INFO localizer.ResourceLocalizationService: Localizer started on port 8040
[36malgo-1    |[0m 2021-11-14 15:19:08,955 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 15:19:08,956 INFO containermanager.ContainerManagerImpl: ContainerManager started at /192.168.240.3:44133
[36malgo-1    |[0m 2021-11-14 15:19:08,956 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-1/192.168.240.3:0
[36malgo-1    |[0m 2021-11-14 15:19:08,956 WARN tracker.NMLogAggregationStatusTracker: Log Aggregation is disabled.So is the LogAggregationStatusTracker.
[36malgo-1    |[0m 2021-11-14 15:19:08,961 INFO webapp.WebServer: Instantiating NMWebApp at algo-1:8042
[36malgo-1    |[0m 2021-11-14 15:19:08,963 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined
[36malgo-1    |[0m 2021-11-14 15:19:08,972 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[36malgo-1    |[0m 2021-11-14 15:19:08,974 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
[36malgo-1    |[0m 2021-11-14 15:19:08,974 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[36malgo-1    |[0m 2021-11-14 15:19:08,974 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 2021-11-14 15:19:08,994 INFO util.log: Logging initialized @1987ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m 2021-11-14 15:19:09,010 INFO http.HttpServer2: Jetty bound to port 35589
[36malgo-1    |[0m 2021-11-14 15:19:09,012 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[36malgo-1    |[0m 2021-11-14 15:19:09,020 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
[36malgo-1    |[0m 2021-11-14 15:19:09,021 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
[36malgo-1    |[0m 2021-11-14 15:19:09,044 INFO server.session: DefaultSessionIdManager workerName=node0
[36malgo-1    |[0m 2021-11-14 15:19:09,044 INFO server.session: No SessionScavenger set, using defaults
[36malgo-1    |[0m 2021-11-14 15:19:09,046 INFO server.session: node0 Scavenging every 600000ms
[36malgo-1    |[0m 2021-11-14 15:19:09,057 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a3793c7{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 15:19:09,057 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@bb9e6dc{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 15:19:09,072 INFO namenode.FSEditLog: Edit logging is async:true
[36malgo-1    |[0m 2021-11-14 15:19:09,084 INFO namenode.FSNamesystem: KeyProvider: null
[36malgo-1    |[0m 2021-11-14 15:19:09,085 INFO namenode.FSNamesystem: fsLock is fair: true
[36malgo-1    |[0m 2021-11-14 15:19:09,086 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
[36malgo-1    |[0m 2021-11-14 15:19:09,093 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 15:19:09,093 INFO namenode.FSNamesystem: supergroup          = supergroup
[36malgo-1    |[0m 2021-11-14 15:19:09,093 INFO namenode.FSNamesystem: isPermissionEnabled = true
[36malgo-1    |[0m 2021-11-14 15:19:09,093 INFO namenode.FSNamesystem: HA Enabled: false
[36malgo-1    |[0m 2021-11-14 15:19:09,095 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 15:19:09,099 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined
[36malgo-1    |[0m 2021-11-14 15:19:09,105 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[36malgo-1    |[0m 2021-11-14 15:19:09,106 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node
[36malgo-1    |[0m 2021-11-14 15:19:09,106 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 2021-11-14 15:19:09,106 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[36malgo-1    |[0m 2021-11-14 15:19:09,107 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context node
[36malgo-1    |[0m 2021-11-14 15:19:09,107 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
[36malgo-1    |[0m 2021-11-14 15:19:09,107 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
[36malgo-1    |[0m 2021-11-14 15:19:09,109 INFO http.HttpServer2: adding path spec: /node/*
[36malgo-1    |[0m 2021-11-14 15:19:09,109 INFO http.HttpServer2: adding path spec: /ws/*
[36malgo-1    |[0m 2021-11-14 15:19:09,116 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m 2021-11-14 15:19:09,125 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@338c99c8{datanode,/,file:///usr/lib/hadoop-hdfs/webapps/datanode/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/datanode}
[36malgo-1    |[0m 2021-11-14 15:19:09,127 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[36malgo-1    |[0m 2021-11-14 15:19:09,133 INFO server.AbstractConnector: Started ServerConnector@6c372fe6{HTTP/1.1,[http/1.1]}{localhost:35589}
[36malgo-1    |[0m 2021-11-14 15:19:09,133 INFO server.Server: Started @2134ms
[36malgo-1    |[0m 2021-11-14 15:19:09,138 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
[36malgo-1    |[0m 2021-11-14 15:19:09,138 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
[36malgo-1    |[0m 2021-11-14 15:19:09,142 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
[36malgo-1    |[0m 2021-11-14 15:19:09,143 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Nov 14 15:19:09
[36malgo-1    |[0m 2021-11-14 15:19:09,144 INFO util.GSet: Computing capacity for map BlocksMap
[36malgo-1    |[0m 2021-11-14 15:19:09,144 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 15:19:09,146 INFO util.GSet: 2.0% max memory 6.5 GB = 133.6 MB
[36malgo-1    |[0m 2021-11-14 15:19:09,146 INFO util.GSet: capacity      = 2^24 = 16777216 entries
[36malgo-1    |[0m 2021-11-14 15:19:09,166 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
[36malgo-1    |[0m 2021-11-14 15:19:09,166 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
[36malgo-1    |[0m 2021-11-14 15:19:09,174 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
[36malgo-1    |[0m 2021-11-14 15:19:09,174 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
[36malgo-1    |[0m 2021-11-14 15:19:09,175 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
[36malgo-1    |[0m 2021-11-14 15:19:09,175 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
[36malgo-1    |[0m 2021-11-14 15:19:09,175 INFO blockmanagement.BlockManager: defaultReplication         = 3
[36malgo-1    |[0m 2021-11-14 15:19:09,175 INFO blockmanagement.BlockManager: maxReplication             = 512
[36malgo-1    |[0m 2021-11-14 15:19:09,175 INFO blockmanagement.BlockManager: minReplication             = 1
[36malgo-1    |[0m 2021-11-14 15:19:09,175 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
[36malgo-1    |[0m 2021-11-14 15:19:09,175 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
[36malgo-1    |[0m 2021-11-14 15:19:09,175 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
[36malgo-1    |[0m 2021-11-14 15:19:09,175 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
[36malgo-1    |[0m 2021-11-14 15:19:09,198 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
[36malgo-1    |[0m 2021-11-14 15:19:09,198 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 15:19:09,198 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 15:19:09,198 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 15:19:09,213 INFO util.GSet: Computing capacity for map INodeMap
[36malgo-1    |[0m 2021-11-14 15:19:09,213 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 15:19:09,213 INFO util.GSet: 1.0% max memory 6.5 GB = 66.8 MB
[36malgo-1    |[0m 2021-11-14 15:19:09,213 INFO util.GSet: capacity      = 2^23 = 8388608 entries
[36malgo-1    |[0m 2021-11-14 15:19:09,233 INFO webapp.WebApps: Registered webapp guice modules
[36malgo-1    |[0m 2021-11-14 15:19:09,240 INFO http.HttpServer2: Jetty bound to port 8088
[36malgo-1    |[0m 2021-11-14 15:19:09,242 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[36malgo-1    |[0m 2021-11-14 15:19:09,306 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864
[36malgo-1    |[0m 2021-11-14 15:19:09,313 INFO datanode.DataNode: dnUserName = root
[36malgo-1    |[0m 2021-11-14 15:19:09,313 INFO datanode.DataNode: supergroup = supergroup
[36malgo-1    |[0m 2021-11-14 15:19:09,329 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 15:19:09,335 INFO server.session: DefaultSessionIdManager workerName=node0
[36malgo-1    |[0m 2021-11-14 15:19:09,336 INFO server.session: No SessionScavenger set, using defaults
[36malgo-1    |[0m 2021-11-14 15:19:09,339 INFO server.session: node0 Scavenging every 660000ms
[36malgo-1    |[0m 2021-11-14 15:19:09,355 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 15:19:09,356 INFO namenode.FSDirectory: ACLs enabled? false
[36malgo-1    |[0m 2021-11-14 15:19:09,356 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
[36malgo-1    |[0m 2021-11-14 15:19:09,356 INFO namenode.FSDirectory: XAttrs enabled? true
[36malgo-1    |[0m 2021-11-14 15:19:09,357 INFO namenode.NameNode: Caching file names occurring more than 10 times
[36malgo-1    |[0m 2021-11-14 15:19:09,360 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 15:19:09,363 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
[36malgo-1    |[0m 2021-11-14 15:19:09,365 INFO snapshot.SnapshotManager: SkipList is disabled
[36malgo-1    |[0m 2021-11-14 15:19:09,366 INFO ipc.Server: Starting Socket Reader #1 for port 9867
[36malgo-1    |[0m 2021-11-14 15:19:09,368 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
[36malgo-1    |[0m 2021-11-14 15:19:09,369 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
[36malgo-1    |[0m 2021-11-14 15:19:09,369 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
[36malgo-1    |[0m 2021-11-14 15:19:09,371 INFO util.GSet: Computing capacity for map cachedBlocks
[36malgo-1    |[0m 2021-11-14 15:19:09,371 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 15:19:09,371 INFO util.GSet: 0.25% max memory 6.5 GB = 16.7 MB
[36malgo-1    |[0m 2021-11-14 15:19:09,371 INFO util.GSet: capacity      = 2^21 = 2097152 entries
[36malgo-1    |[0m 2021-11-14 15:19:09,382 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
[36malgo-1    |[0m 2021-11-14 15:19:09,382 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
[36malgo-1    |[0m 2021-11-14 15:19:09,382 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
[36malgo-1    |[0m 2021-11-14 15:19:09,385 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
[36malgo-1    |[0m 2021-11-14 15:19:09,385 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
[36malgo-1    |[0m 2021-11-14 15:19:09,387 INFO util.GSet: Computing capacity for map NameNodeRetryCache
[36malgo-1    |[0m 2021-11-14 15:19:09,387 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 15:19:09,387 INFO util.GSet: 0.029999999329447746% max memory 6.5 GB = 2.0 MB
[36malgo-1    |[0m 2021-11-14 15:19:09,387 INFO util.GSet: capacity      = 2^18 = 262144 entries
[36malgo-1    |[0m 2021-11-14 15:19:09,389 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@21129f1f{logs,/logs,file:///var/log/yarn/export/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 15:19:09,390 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@48d61b48{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/static,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 15:19:09,399 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException
[36malgo-1    |[0m 2021-11-14 15:19:09,408 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/namenode/in_use.lock acquired by nodename 134@algo-1
[36malgo-1    |[0m 2021-11-14 15:19:09,432 INFO namenode.FileJournalManager: Recovering unfinalized segments in /opt/amazon/hadoop/hdfs/namenode/current
[36malgo-1    |[0m 2021-11-14 15:19:09,432 INFO namenode.FSImage: No edit log streams selected.
[36malgo-1    |[0m 2021-11-14 15:19:09,433 INFO namenode.FSImage: Planning to load image: FSImageFile(file=/opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
[36malgo-1    |[0m 2021-11-14 15:19:09,496 INFO webapp.WebApps: Registered webapp guice modules
[36malgo-1    |[0m 2021-11-14 15:19:09,499 INFO http.HttpServer2: Jetty bound to port 8042
[36malgo-1    |[0m 2021-11-14 15:19:09,501 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[36malgo-1    |[0m 2021-11-14 15:19:09,513 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.
[36malgo-1    |[0m 2021-11-14 15:19:09,513 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m 2021-11-14 15:19:09,533 INFO server.session: DefaultSessionIdManager workerName=node0
[36malgo-1    |[0m 2021-11-14 15:19:09,533 INFO server.session: No SessionScavenger set, using defaults
[36malgo-1    |[0m 2021-11-14 15:19:09,535 INFO server.session: node0 Scavenging every 660000ms
[33malgo-2    |[0m 2021-11-14 15:19:09,539 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/192.168.240.3:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 15:19:09,545 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
[36malgo-1    |[0m 2021-11-14 15:19:09,546 INFO namenode.FSImage: Loaded image for txid 0 from /opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000
[36malgo-1    |[0m 2021-11-14 15:19:09,546 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 15:19:09,549 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4bff64c2{logs,/logs,file:///var/log/yarn/export/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 15:19:09,549 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3cc41abc{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/static,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 15:19:09,550 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
[36malgo-1    |[0m 2021-11-14 15:19:09,550 INFO namenode.FSEditLog: Starting log segment at 1
[36malgo-1    |[0m 2021-11-14 15:19:09,559 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException
[36malgo-1    |[0m 2021-11-14 15:19:09,590 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:9867
[36malgo-1    |[0m Nov 14, 2021 3:19:09 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class
[36malgo-1    |[0m Nov 14, 2021 3:19:09 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class
[36malgo-1    |[0m Nov 14, 2021 3:19:09 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
[36malgo-1    |[0m Nov 14, 2021 3:19:09 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
[36malgo-1    |[0m INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'
[36malgo-1    |[0m 2021-11-14 15:19:09,607 INFO datanode.DataNode: Refresh request received for nameservices: null
[36malgo-1    |[0m 2021-11-14 15:19:09,615 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>
[36malgo-1    |[0m 2021-11-14 15:19:09,624 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1/192.168.240.3:8020 starting to offer service
[36malgo-1    |[0m 2021-11-14 15:19:09,628 INFO ipc.Server: IPC Server listener on 9867: starting
[36malgo-1    |[0m 2021-11-14 15:19:09,628 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m Nov 14, 2021 3:19:09 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 15:19:09,655 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m 2021-11-14 15:19:09,659 INFO namenode.NameCache: initialized with 0 entries 0 lookups
[36malgo-1    |[0m 2021-11-14 15:19:09,660 INFO namenode.FSNamesystem: Finished loading FSImage in 269 msecs
[33malgo-2    |[0m 2021-11-14 15:19:09,687 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/192.168.240.3:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m Nov 14, 2021 3:19:09 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class
[36malgo-1    |[0m Nov 14, 2021 3:19:09 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
[36malgo-1    |[0m Nov 14, 2021 3:19:09 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class
[36malgo-1    |[0m Nov 14, 2021 3:19:09 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
[36malgo-1    |[0m INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'
[36malgo-1    |[0m Nov 14, 2021 3:19:09 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 15:19:10,002 INFO namenode.NameNode: RPC server is binding to algo-1:8020
[36malgo-1    |[0m Nov 14, 2021 3:19:10 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m Nov 14, 2021 3:19:10 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 15:19:10,030 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 15:19:10,039 INFO ipc.Server: Starting Socket Reader #1 for port 8020
[36malgo-1    |[0m 2021-11-14 15:19:10,239 INFO namenode.NameNode: Clients are to use algo-1:8020 to access this namenode/service.
[36malgo-1    |[0m 2021-11-14 15:19:10,241 INFO namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
[36malgo-1    |[0m 2021-11-14 15:19:10,253 INFO namenode.LeaseManager: Number of blocks under construction: 0
[36malgo-1    |[0m 2021-11-14 15:19:10,276 INFO blockmanagement.BlockManager: initializing replication queues
[36malgo-1    |[0m 2021-11-14 15:19:10,276 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs
[36malgo-1    |[0m 2021-11-14 15:19:10,276 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
[36malgo-1    |[0m 2021-11-14 15:19:10,276 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
[36malgo-1    |[0m 2021-11-14 15:19:10,300 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 15:19:10,303 INFO ipc.Server: IPC Server listener on 8020: starting
[36malgo-1    |[0m 2021-11-14 15:19:10,307 INFO blockmanagement.BlockManager: Total number of blocks            = 0
[36malgo-1    |[0m 2021-11-14 15:19:10,307 INFO blockmanagement.BlockManager: Number of invalid blocks          = 0
[36malgo-1    |[0m 2021-11-14 15:19:10,307 INFO blockmanagement.BlockManager: Number of under-replicated blocks = 0
[36malgo-1    |[0m 2021-11-14 15:19:10,307 INFO blockmanagement.BlockManager: Number of  over-replicated blocks = 0
[36malgo-1    |[0m 2021-11-14 15:19:10,307 INFO blockmanagement.BlockManager: Number of blocks being written    = 0
[36malgo-1    |[0m 2021-11-14 15:19:10,307 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 31 msec
[36malgo-1    |[0m 2021-11-14 15:19:10,349 INFO namenode.NameNode: NameNode RPC up at: algo-1/192.168.240.3:8020
[36malgo-1    |[0m 2021-11-14 15:19:10,352 INFO namenode.FSNamesystem: Starting services required for active state
[36malgo-1    |[0m 2021-11-14 15:19:10,352 INFO namenode.FSDirectory: Initializing quota with 4 thread(s)
[36malgo-1    |[0m 2021-11-14 15:19:10,360 INFO namenode.FSDirectory: Quota initialization completed in 8 milliseconds
[36malgo-1    |[0m name space=1
[36malgo-1    |[0m storage space=0
[36malgo-1    |[0m storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
[36malgo-1    |[0m 2021-11-14 15:19:10,375 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
[33malgo-2    |[0m 2021-11-14 15:19:10,539 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/192.168.240.3:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m Nov 14, 2021 3:19:10 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 15:19:10,615 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5d5b5fa7{node,/,file:///tmp/jetty-algo-1-8042-_-any-7321680287178636954.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/node}
[36malgo-1    |[0m Nov 14, 2021 3:19:10 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 15:19:10,628 INFO server.AbstractConnector: Started ServerConnector@3f390d63{HTTP/1.1,[http/1.1]}{algo-1:8042}
[36malgo-1    |[0m 2021-11-14 15:19:10,628 INFO server.Server: Started @3622ms
[36malgo-1    |[0m 2021-11-14 15:19:10,628 INFO webapp.WebApps: Web app node started at 8042
[36malgo-1    |[0m 2021-11-14 15:19:10,637 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-1:44133
[36malgo-1    |[0m 2021-11-14 15:19:10,648 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 15:19:10,649 INFO client.RMProxy: Connecting to ResourceManager at /192.168.240.3:8031
[36malgo-1    |[0m 2021-11-14 15:19:10,685 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@25da615a{cluster,/,file:///tmp/jetty-192_168_240_3-8088-_-any-7842039328088411380.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/cluster}
[33malgo-2    |[0m 2021-11-14 15:19:10,688 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/192.168.240.3:8031. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 15:19:10,692 INFO ipc.Client: Retrying connect to server: algo-1/192.168.240.3:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 15:19:10,710 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []
[36malgo-1    |[0m 2021-11-14 15:19:10,720 INFO server.AbstractConnector: Started ServerConnector@71e9ebae{HTTP/1.1,[http/1.1]}{192.168.240.3:8088}
[36malgo-1    |[0m 2021-11-14 15:19:10,720 INFO server.Server: Started @3716ms
[36malgo-1    |[0m 2021-11-14 15:19:10,720 INFO webapp.WebApps: Web app cluster started at 8088
[36malgo-1    |[0m 2021-11-14 15:19:10,722 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]
[36malgo-1    |[0m 2021-11-14 15:19:10,795 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 15:19:10,806 INFO ipc.Server: Starting Socket Reader #1 for port 8033
[36malgo-1    |[0m 2021-11-14 15:19:10,816 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1/192.168.240.3:8020
[36malgo-1    |[0m 2021-11-14 15:19:10,819 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
[36malgo-1    |[0m 2021-11-14 15:19:10,828 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 135@algo-1
[36malgo-1    |[0m 2021-11-14 15:19:10,829 INFO common.Storage: Storage directory with location [DISK]file:/opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 172224749. Formatting...
[36malgo-1    |[0m 2021-11-14 15:19:10,830 INFO common.Storage: Generated new storageID DS-9490f116-f3d2-48a6-b520-832da11b4d6d for directory /opt/amazon/hadoop/hdfs/datanode 
[33malgo-2    |[0m 2021-11-14 15:19:10,830 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1.spark-network/192.168.240.3:8020
[33malgo-2    |[0m 2021-11-14 15:19:10,833 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
[33malgo-2    |[0m 2021-11-14 15:19:10,840 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 18@algo-2
[33malgo-2    |[0m 2021-11-14 15:19:10,842 INFO common.Storage: Storage directory with location [DISK]file:/opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 172224749. Formatting...
[33malgo-2    |[0m 2021-11-14 15:19:10,843 INFO common.Storage: Generated new storageID DS-ecbd8ac6-b22e-462d-9426-d6f3d93be419 for directory /opt/amazon/hadoop/hdfs/datanode 
[36malgo-1    |[0m 2021-11-14 15:19:10,861 INFO common.Storage: Analyzing storage directories for bpid BP-1419144703-192.168.240.3-1636903146711
[36malgo-1    |[0m 2021-11-14 15:19:10,861 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-1419144703-192.168.240.3-1636903146711
[36malgo-1    |[0m 2021-11-14 15:19:10,862 INFO common.Storage: Block pool storage directory for location [DISK]file:/opt/amazon/hadoop/hdfs/datanode and block pool id BP-1419144703-192.168.240.3-1636903146711 is not formatted. Formatting ...
[36malgo-1    |[0m 2021-11-14 15:19:10,862 INFO common.Storage: Formatting block pool BP-1419144703-192.168.240.3-1636903146711 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-1419144703-192.168.240.3-1636903146711/current
[36malgo-1    |[0m 2021-11-14 15:19:10,873 INFO datanode.DataNode: Setting up storage: nsid=172224749;bpid=BP-1419144703-192.168.240.3-1636903146711;lv=-57;nsInfo=lv=-65;cid=CID-4f4b0c56-adf8-4bd0-90e7-41a740098162;nsid=172224749;c=1636903146711;bpid=BP-1419144703-192.168.240.3-1636903146711;dnuuid=null
[36malgo-1    |[0m 2021-11-14 15:19:10,876 INFO datanode.DataNode: Generated and persisted new Datanode UUID bbfd46a9-a4e1-4ce5-bb19-7c4042e64546
[33malgo-2    |[0m 2021-11-14 15:19:10,880 INFO common.Storage: Analyzing storage directories for bpid BP-1419144703-192.168.240.3-1636903146711
[33malgo-2    |[0m 2021-11-14 15:19:10,880 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-1419144703-192.168.240.3-1636903146711
[33malgo-2    |[0m 2021-11-14 15:19:10,881 INFO common.Storage: Block pool storage directory for location [DISK]file:/opt/amazon/hadoop/hdfs/datanode and block pool id BP-1419144703-192.168.240.3-1636903146711 is not formatted. Formatting ...
[33malgo-2    |[0m 2021-11-14 15:19:10,881 INFO common.Storage: Formatting block pool BP-1419144703-192.168.240.3-1636903146711 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-1419144703-192.168.240.3-1636903146711/current
[33malgo-2    |[0m 2021-11-14 15:19:10,888 INFO datanode.DataNode: Setting up storage: nsid=172224749;bpid=BP-1419144703-192.168.240.3-1636903146711;lv=-57;nsInfo=lv=-65;cid=CID-4f4b0c56-adf8-4bd0-90e7-41a740098162;nsid=172224749;c=1636903146711;bpid=BP-1419144703-192.168.240.3-1636903146711;dnuuid=null
[33malgo-2    |[0m 2021-11-14 15:19:10,891 INFO datanode.DataNode: Generated and persisted new Datanode UUID 5b651e59-2e31-4247-bd5f-d2409097a9d3
[36malgo-1    |[0m 2021-11-14 15:19:10,982 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 15:19:10,982 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 15:19:10,982 INFO ipc.Server: IPC Server listener on 8033: starting
[36malgo-1    |[0m 2021-11-14 15:19:10,983 INFO resourcemanager.ResourceManager: Transitioning to active state
[36malgo-1    |[0m 2021-11-14 15:19:10,991 INFO impl.FsDatasetImpl: Added new volume: DS-9490f116-f3d2-48a6-b520-832da11b4d6d
[36malgo-1    |[0m 2021-11-14 15:19:10,991 INFO impl.FsDatasetImpl: Added volume - [DISK]file:/opt/amazon/hadoop/hdfs/datanode, StorageType: DISK
[36malgo-1    |[0m 2021-11-14 15:19:10,996 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean
[36malgo-1    |[0m 2021-11-14 15:19:11,006 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode
[33malgo-2    |[0m 2021-11-14 15:19:11,008 INFO impl.FsDatasetImpl: Added new volume: DS-ecbd8ac6-b22e-462d-9426-d6f3d93be419
[33malgo-2    |[0m 2021-11-14 15:19:11,008 INFO impl.FsDatasetImpl: Added volume - [DISK]file:/opt/amazon/hadoop/hdfs/datanode, StorageType: DISK
[33malgo-2    |[0m 2021-11-14 15:19:11,013 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean
[36malgo-1    |[0m 2021-11-14 15:19:11,018 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode
[36malgo-1    |[0m 2021-11-14 15:19:11,021 INFO impl.FsDatasetImpl: Adding block pool BP-1419144703-192.168.240.3-1636903146711
[33malgo-2    |[0m 2021-11-14 15:19:11,021 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode
[36malgo-1    |[0m 2021-11-14 15:19:11,023 INFO impl.FsDatasetImpl: Scanning block pool BP-1419144703-192.168.240.3-1636903146711 on volume /opt/amazon/hadoop/hdfs/datanode...
[36malgo-1    |[0m 2021-11-14 15:19:11,027 INFO recovery.RMStateStore: Updating AMRMToken
[36malgo-1    |[0m 2021-11-14 15:19:11,027 INFO security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
[36malgo-1    |[0m 2021-11-14 15:19:11,028 INFO security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
[36malgo-1    |[0m 2021-11-14 15:19:11,028 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
[36malgo-1    |[0m 2021-11-14 15:19:11,028 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 1
[36malgo-1    |[0m 2021-11-14 15:19:11,028 INFO recovery.RMStateStore: Storing RMDTMasterKey.
[36malgo-1    |[0m 2021-11-14 15:19:11,029 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
[36malgo-1    |[0m 2021-11-14 15:19:11,029 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
[36malgo-1    |[0m 2021-11-14 15:19:11,029 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 2
[36malgo-1    |[0m 2021-11-14 15:19:11,029 INFO recovery.RMStateStore: Storing RMDTMasterKey.
[33malgo-2    |[0m 2021-11-14 15:19:11,034 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode
[33malgo-2    |[0m 2021-11-14 15:19:11,035 INFO impl.FsDatasetImpl: Adding block pool BP-1419144703-192.168.240.3-1636903146711
[33malgo-2    |[0m 2021-11-14 15:19:11,036 INFO impl.FsDatasetImpl: Scanning block pool BP-1419144703-192.168.240.3-1636903146711 on volume /opt/amazon/hadoop/hdfs/datanode...
[36malgo-1    |[0m 2021-11-14 15:19:11,038 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler
[36malgo-1    |[0m 2021-11-14 15:19:11,060 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-1419144703-192.168.240.3-1636903146711 on /opt/amazon/hadoop/hdfs/datanode: 38ms
[36malgo-1    |[0m 2021-11-14 15:19:11,061 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1419144703-192.168.240.3-1636903146711: 40ms
[36malgo-1    |[0m 2021-11-14 15:19:11,063 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-1419144703-192.168.240.3-1636903146711 on volume /opt/amazon/hadoop/hdfs/datanode...
[36malgo-1    |[0m 2021-11-14 15:19:11,063 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-1419144703-192.168.240.3-1636903146711/current/replicas doesn't exist 
[36malgo-1    |[0m 2021-11-14 15:19:11,064 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1419144703-192.168.240.3-1636903146711 on volume /opt/amazon/hadoop/hdfs/datanode: 2ms
[36malgo-1    |[0m 2021-11-14 15:19:11,065 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-1419144703-192.168.240.3-1636903146711: 2ms
[36malgo-1    |[0m 2021-11-14 15:19:11,067 INFO datanode.VolumeScanner: Now scanning bpid BP-1419144703-192.168.240.3-1636903146711 on volume /opt/amazon/hadoop/hdfs/datanode
[36malgo-1    |[0m 2021-11-14 15:19:11,069 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-9490f116-f3d2-48a6-b520-832da11b4d6d): finished scanning block pool BP-1419144703-192.168.240.3-1636903146711
[36malgo-1    |[0m 11-14 15:19 smspark-submit INFO     cluster is up
[36malgo-1    |[0m 11-14 15:19 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING
[36malgo-1    |[0m 11-14 15:19 smspark-submit INFO     starting executor logs watcher
[36malgo-1    |[0m 11-14 15:19 smspark-submit INFO     start log event log publisher
[36malgo-1    |[0m Starting executor logs watcher on log_dir: /var/log/yarn
[36malgo-1    |[0m 11-14 15:19 sagemaker-spark-event-logs-publisher INFO     Spark event log not enabled.
[36malgo-1    |[0m 11-14 15:19 smspark-submit INFO     Waiting for hosts to bootstrap: ['algo-1', 'algo-2']
[33malgo-2    |[0m 2021-11-14 15:19:11,076 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-1419144703-192.168.240.3-1636903146711 on /opt/amazon/hadoop/hdfs/datanode: 39ms
[33malgo-2    |[0m 2021-11-14 15:19:11,076 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1419144703-192.168.240.3-1636903146711: 41ms
[33malgo-2    |[0m 2021-11-14 15:19:11,078 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-1419144703-192.168.240.3-1636903146711 on volume /opt/amazon/hadoop/hdfs/datanode...
[33malgo-2    |[0m 2021-11-14 15:19:11,078 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-1419144703-192.168.240.3-1636903146711/current/replicas doesn't exist 
[36malgo-1    |[0m 2021-11-14 15:19:11,080 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-9490f116-f3d2-48a6-b520-832da11b4d6d): no suitable block pools found to scan.  Waiting 1814399987 ms.
[33malgo-2    |[0m 2021-11-14 15:19:11,080 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1419144703-192.168.240.3-1636903146711 on volume /opt/amazon/hadoop/hdfs/datanode: 2ms
[33malgo-2    |[0m 2021-11-14 15:19:11,083 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-1419144703-192.168.240.3-1636903146711: 5ms
[33malgo-2    |[0m 2021-11-14 15:19:11,085 INFO datanode.VolumeScanner: Now scanning bpid BP-1419144703-192.168.240.3-1636903146711 on volume /opt/amazon/hadoop/hdfs/datanode
[36malgo-1    |[0m 2021-11-14 15:19:11,085 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 11/14/21 9:03 PM with interval of 21600000ms
[36malgo-1    |[0m 11-14 15:19 smspark-submit INFO     Received host statuses: dict_items([('algo-1', StatusMessage(status='WAITING', timestamp='2021-11-14T15:19:11.079388')), ('algo-2', StatusMessage(status='WAITING', timestamp='2021-11-14T15:19:11.084686'))])
[33malgo-2    |[0m 2021-11-14 15:19:11,087 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-ecbd8ac6-b22e-462d-9426-d6f3d93be419): finished scanning block pool BP-1419144703-192.168.240.3-1636903146711
[36malgo-1    |[0m 2021-11-14 15:19:11,092 INFO datanode.DataNode: Block pool BP-1419144703-192.168.240.3-1636903146711 (Datanode Uuid bbfd46a9-a4e1-4ce5-bb19-7c4042e64546) service to algo-1/192.168.240.3:8020 beginning handshake with NN
[33malgo-2    |[0m 2021-11-14 15:19:11,099 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-ecbd8ac6-b22e-462d-9426-d6f3d93be419): no suitable block pools found to scan.  Waiting 1814399986 ms.
[33malgo-2    |[0m 2021-11-14 15:19:11,101 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 11/14/21 4:41 PM with interval of 21600000ms
[33malgo-2    |[0m 2021-11-14 15:19:11,105 INFO datanode.DataNode: Block pool BP-1419144703-192.168.240.3-1636903146711 (Datanode Uuid 5b651e59-2e31-4247-bd5f-d2409097a9d3) service to algo-1.spark-network/192.168.240.3:8020 beginning handshake with NN
[36malgo-1    |[0m 2021-11-14 15:19:11,133 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(192.168.240.2:9866, datanodeUuid=5b651e59-2e31-4247-bd5f-d2409097a9d3, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-4f4b0c56-adf8-4bd0-90e7-41a740098162;nsid=172224749;c=1636903146711) storage 5b651e59-2e31-4247-bd5f-d2409097a9d3
[36malgo-1    |[0m 2021-11-14 15:19:11,134 INFO net.NetworkTopology: Adding a new node: /default-rack/192.168.240.2:9866
[36malgo-1    |[0m 2021-11-14 15:19:11,134 INFO blockmanagement.BlockReportLeaseManager: Registered DN 5b651e59-2e31-4247-bd5f-d2409097a9d3 (192.168.240.2:9866).
[36malgo-1    |[0m 2021-11-14 15:19:11,136 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(192.168.240.3:9866, datanodeUuid=bbfd46a9-a4e1-4ce5-bb19-7c4042e64546, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-4f4b0c56-adf8-4bd0-90e7-41a740098162;nsid=172224749;c=1636903146711) storage bbfd46a9-a4e1-4ce5-bb19-7c4042e64546
[36malgo-1    |[0m 2021-11-14 15:19:11,136 INFO net.NetworkTopology: Adding a new node: /default-rack/192.168.240.3:9866
[36malgo-1    |[0m 2021-11-14 15:19:11,136 INFO blockmanagement.BlockReportLeaseManager: Registered DN bbfd46a9-a4e1-4ce5-bb19-7c4042e64546 (192.168.240.3:9866).
[33malgo-2    |[0m 2021-11-14 15:19:11,146 INFO datanode.DataNode: Block pool Block pool BP-1419144703-192.168.240.3-1636903146711 (Datanode Uuid 5b651e59-2e31-4247-bd5f-d2409097a9d3) service to algo-1.spark-network/192.168.240.3:8020 successfully registered with NN
[36malgo-1    |[0m 2021-11-14 15:19:11,147 INFO datanode.DataNode: Block pool Block pool BP-1419144703-192.168.240.3-1636903146711 (Datanode Uuid bbfd46a9-a4e1-4ce5-bb19-7c4042e64546) service to algo-1/192.168.240.3:8020 successfully registered with NN
[33malgo-2    |[0m 2021-11-14 15:19:11,147 INFO datanode.DataNode: For namenode algo-1.spark-network/192.168.240.3:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
[36malgo-1    |[0m 2021-11-14 15:19:11,147 INFO datanode.DataNode: For namenode algo-1/192.168.240.3:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
[36malgo-1    |[0m 2021-11-14 15:19:11,202 INFO store.AbstractFSNodeStore: Created store directory :file:/tmp/hadoop-yarn-root/node-attribute
[36malgo-1    |[0m 2021-11-14 15:19:11,213 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-9490f116-f3d2-48a6-b520-832da11b4d6d for DN 192.168.240.3:9866
[36malgo-1    |[0m 2021-11-14 15:19:11,215 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-ecbd8ac6-b22e-462d-9426-d6f3d93be419 for DN 192.168.240.2:9866
[36malgo-1    |[0m 2021-11-14 15:19:11,215 INFO store.AbstractFSNodeStore: Finished write mirror at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.mirror
[36malgo-1    |[0m 2021-11-14 15:19:11,215 INFO store.AbstractFSNodeStore: Finished create editlog file at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.editlog
[36malgo-1    |[0m 2021-11-14 15:19:11,226 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl$ForwardingEventHandler
[36malgo-1    |[0m 2021-11-14 15:19:11,226 INFO placement.MultiNodeSortingManager: Starting NodeSortingService=MultiNodeSortingManager
[36malgo-1    |[0m 2021-11-14 15:19:11,239 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 15:19:11,240 INFO ipc.Server: Starting Socket Reader #1 for port 8031
[36malgo-1    |[0m 2021-11-14 15:19:11,243 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server
[36malgo-1    |[0m 2021-11-14 15:19:11,243 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 15:19:11,243 INFO ipc.Server: IPC Server listener on 8031: starting
[36malgo-1    |[0m 2021-11-14 15:19:11,260 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 15:19:11,273 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 15:19:11,282 INFO ipc.Server: Starting Socket Reader #1 for port 8030
[36malgo-1    |[0m 2021-11-14 15:19:11,301 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 15:19:11,305 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 15:19:11,310 INFO ipc.Server: IPC Server listener on 8030: starting
[36malgo-1    |[0m 2021-11-14 15:19:11,322 INFO BlockStateChange: BLOCK* processReport 0x6fe4ccdbd763fe71: Processing first storage report for DS-ecbd8ac6-b22e-462d-9426-d6f3d93be419 from datanode 5b651e59-2e31-4247-bd5f-d2409097a9d3
[36malgo-1    |[0m 2021-11-14 15:19:11,323 INFO BlockStateChange: BLOCK* processReport 0x6fe4ccdbd763fe71: from storage DS-ecbd8ac6-b22e-462d-9426-d6f3d93be419 node DatanodeRegistration(192.168.240.2:9866, datanodeUuid=5b651e59-2e31-4247-bd5f-d2409097a9d3, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-4f4b0c56-adf8-4bd0-90e7-41a740098162;nsid=172224749;c=1636903146711), blocks: 0, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0
[36malgo-1    |[0m 2021-11-14 15:19:11,338 INFO BlockStateChange: BLOCK* processReport 0x68f8308d358fa69c: Processing first storage report for DS-9490f116-f3d2-48a6-b520-832da11b4d6d from datanode bbfd46a9-a4e1-4ce5-bb19-7c4042e64546
[36malgo-1    |[0m 2021-11-14 15:19:11,338 INFO BlockStateChange: BLOCK* processReport 0x68f8308d358fa69c: from storage DS-9490f116-f3d2-48a6-b520-832da11b4d6d node DatanodeRegistration(192.168.240.3:9866, datanodeUuid=bbfd46a9-a4e1-4ce5-bb19-7c4042e64546, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-4f4b0c56-adf8-4bd0-90e7-41a740098162;nsid=172224749;c=1636903146711), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
[33malgo-2    |[0m 2021-11-14 15:19:11,363 INFO datanode.DataNode: Successfully sent block report 0x6fe4ccdbd763fe71,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 127 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
[33malgo-2    |[0m 2021-11-14 15:19:11,363 INFO datanode.DataNode: Got finalize command for block pool BP-1419144703-192.168.240.3-1636903146711
[36malgo-1    |[0m 2021-11-14 15:19:11,366 INFO datanode.DataNode: Successfully sent block report 0x68f8308d358fa69c,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 5 msec to generate and 125 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
[36malgo-1    |[0m 2021-11-14 15:19:11,366 INFO datanode.DataNode: Got finalize command for block pool BP-1419144703-192.168.240.3-1636903146711
[36malgo-1    |[0m 2021-11-14 15:19:11,420 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 15:19:11,420 INFO ipc.Server: Starting Socket Reader #1 for port 8032
[36malgo-1    |[0m 2021-11-14 15:19:11,423 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 15:19:11,423 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 15:19:11,423 INFO ipc.Server: IPC Server listener on 8032: starting
[36malgo-1    |[0m 2021-11-14 15:19:11,431 INFO resourcemanager.ResourceManager: Transitioned to active state
[33malgo-2    |[0m 2021-11-14 15:19:11,688 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/192.168.240.3:8031. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 15:19:11,796 INFO ipc.Client: Retrying connect to server: algo-1/192.168.240.3:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 15:19:11,857 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-1(cmPort: 44133 httpPort: 8042) registered with capability: <memory:15892, vCores:4>, assigned nodeId algo-1:44133
[36malgo-1    |[0m 2021-11-14 15:19:11,857 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-2(cmPort: 42603 httpPort: 8042) registered with capability: <memory:15892, vCores:4>, assigned nodeId algo-2:42603
[36malgo-1    |[0m 2021-11-14 15:19:11,861 INFO rmnode.RMNodeImpl: algo-1:44133 Node Transitioned from NEW to RUNNING
[36malgo-1    |[0m 2021-11-14 15:19:11,861 INFO rmnode.RMNodeImpl: algo-2:42603 Node Transitioned from NEW to RUNNING
[33malgo-2    |[0m 2021-11-14 15:19:11,874 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id 90623136
[36malgo-1    |[0m 2021-11-14 15:19:11,874 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id 90623136
[33malgo-2    |[0m 2021-11-14 15:19:11,875 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id 607638161
[36malgo-1    |[0m 2021-11-14 15:19:11,875 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id 607638161
[33malgo-2    |[0m 2021-11-14 15:19:11,875 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-2:42603 with total resource of <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 15:19:11,875 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-1:44133 with total resource of <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 15:19:11,879 INFO capacity.CapacityScheduler: Added node algo-1:44133 clusterResource: <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 15:19:11,881 INFO capacity.CapacityScheduler: Added node algo-2:42603 clusterResource: <memory:31784, vCores:8>
[36malgo-1    |[0m Using properties file: /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m Adding default property: spark.driver.host=192.168.240.3
[36malgo-1    |[0m Adding default property: spark.executor.memoryOverhead=1239m
[36malgo-1    |[0m Adding default property: spark.driver.extraLibraryPath=/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m Adding default property: spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[36malgo-1    |[0m Adding default property: spark.rpc.askTimeout=300s
[36malgo-1    |[0m Adding default property: spark.driver.memory=2048m
[36malgo-1    |[0m Adding default property: spark.executor.instances=2
[36malgo-1    |[0m Adding default property: spark.driver.memoryOverhead=204m
[36malgo-1    |[0m Adding default property: key=value
[36malgo-1    |[0m Adding default property: spark.default.parallelism=16
[36malgo-1    |[0m Adding default property: spark.executor.defaultJavaOptions=-verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3
[36malgo-1    |[0m Adding default property: spark.driver.defaultJavaOptions=-XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[36malgo-1    |[0m Adding default property: spark.executor.extraLibraryPath=/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m Adding default property: spark.executor.memory=2g
[36malgo-1    |[0m Adding default property: spark.driver.extraClassPath=/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m Adding default property: spark.executor.extraClassPath=/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m Adding default property: spark.executor.cores=1
[36malgo-1    |[0m Warning: Ignoring non-Spark config property: key
[36malgo-1    |[0m Parsed arguments:
[36malgo-1    |[0m   master                  yarn
[36malgo-1    |[0m   deployMode              client
[36malgo-1    |[0m   executorMemory          2g
[36malgo-1    |[0m   executorCores           1
[36malgo-1    |[0m   totalExecutorCores      null
[36malgo-1    |[0m   propertiesFile          /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m   driverMemory            2048m
[36malgo-1    |[0m   driverCores             null
[36malgo-1    |[0m   driverExtraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m   driverExtraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m   driverExtraJavaOptions  null
[36malgo-1    |[0m   supervise               false
[36malgo-1    |[0m   queue                   null
[36malgo-1    |[0m   numExecutors            2
[36malgo-1    |[0m   files                   null
[36malgo-1    |[0m   pyFiles                 file:/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py
[36malgo-1    |[0m   archives                null
[36malgo-1    |[0m   mainClass               null
[36malgo-1    |[0m   primaryResource         file:/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py
[36malgo-1    |[0m   name                    hello_py_spark_app.py
[36malgo-1    |[0m   childArgs               [--input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data]
[36malgo-1    |[0m   jars                    null
[36malgo-1    |[0m   packages                null
[36malgo-1    |[0m   packagesExclusions      null
[36malgo-1    |[0m   repositories            null
[36malgo-1    |[0m   verbose                 true
[36malgo-1    |[0m 
[36malgo-1    |[0m Spark properties used, including those specified through
[36malgo-1    |[0m  --conf and those from the properties file /usr/lib/spark/conf/spark-defaults.conf:
[36malgo-1    |[0m   (spark.executor.defaultJavaOptions,-verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3)
[36malgo-1    |[0m   (spark.default.parallelism,16)
[36malgo-1    |[0m   (spark.executor.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)
[36malgo-1    |[0m   (spark.executor.memory,2g)
[36malgo-1    |[0m   (spark.driver.memory,2048m)
[36malgo-1    |[0m   (spark.driver.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)
[36malgo-1    |[0m   (spark.executor.instances,2)
[36malgo-1    |[0m   (spark.executor.memoryOverhead,1239m)
[36malgo-1    |[0m   (spark.rpc.askTimeout,300s)
[36malgo-1    |[0m   (spark.driver.memoryOverhead,204m)
[36malgo-1    |[0m   (spark.driver.host,192.168.240.3)
[36malgo-1    |[0m   (spark.driver.defaultJavaOptions,-XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled)
[36malgo-1    |[0m   (spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version,2)
[36malgo-1    |[0m   (spark.executor.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)
[36malgo-1    |[0m   (spark.executor.cores,1)
[36malgo-1    |[0m   (spark.driver.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)
[36malgo-1    |[0m 
[36malgo-1    |[0m     
[36malgo-1    |[0m Main class:
[36malgo-1    |[0m org.apache.spark.deploy.PythonRunner
[36malgo-1    |[0m Arguments:
[36malgo-1    |[0m file:/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py
[36malgo-1    |[0m file:///opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py
[36malgo-1    |[0m --input
[36malgo-1    |[0m file:///opt/ml/processing/input/data/data.jsonl
[36malgo-1    |[0m --output
[36malgo-1    |[0m file:///opt/ml/processing/output/data
[36malgo-1    |[0m Spark config:
[36malgo-1    |[0m (spark.driver.host,192.168.240.3)
[36malgo-1    |[0m (spark.executor.memoryOverhead,1239m)
[36malgo-1    |[0m (spark.driver.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)
[36malgo-1    |[0m (spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version,2)
[36malgo-1    |[0m (spark.app.name,hello_py_spark_app.py)
[36malgo-1    |[0m (spark.rpc.askTimeout,300s)
[36malgo-1    |[0m (spark.driver.memory,2048m)
[36malgo-1    |[0m (spark.executor.instances,2)
[36malgo-1    |[0m (spark.submit.pyFiles,/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py)
[36malgo-1    |[0m (spark.driver.memoryOverhead,204m)
[36malgo-1    |[0m (spark.default.parallelism,16)
[36malgo-1    |[0m (spark.executor.defaultJavaOptions,-verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3)
[36malgo-1    |[0m (spark.yarn.dist.pyFiles,file:///opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py)
[36malgo-1    |[0m (spark.submit.deployMode,client)
[36malgo-1    |[0m (spark.master,yarn)
[36malgo-1    |[0m (spark.driver.defaultJavaOptions,-XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled)
[36malgo-1    |[0m (spark.executor.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)
[36malgo-1    |[0m (spark.executor.memory,2g)
[36malgo-1    |[0m (spark.driver.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)
[36malgo-1    |[0m (spark.executor.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)
[36malgo-1    |[0m (spark.executor.cores,1)
[36malgo-1    |[0m (spark.yarn.isPython,true)
[36malgo-1    |[0m Classpath elements:
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m Hello World, this is PySpark!
[36malgo-1    |[0m Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[36malgo-1    |[0m 21/11/14 15:19:33 INFO SparkContext: Running Spark version 3.0.0-amzn-0
[36malgo-1    |[0m 21/11/14 15:19:33 INFO ResourceUtils: ==============================================================
[36malgo-1    |[0m 21/11/14 15:19:33 INFO ResourceUtils: Resources for spark.driver:
[36malgo-1    |[0m 
[36malgo-1    |[0m 21/11/14 15:19:33 INFO ResourceUtils: ==============================================================
[36malgo-1    |[0m 21/11/14 15:19:33 INFO SparkContext: Submitted application: SparkContainerTestApp
[36malgo-1    |[0m 21/11/14 15:19:33 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m 21/11/14 15:19:33 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m 21/11/14 15:19:33 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m 21/11/14 15:19:33 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m 21/11/14 15:19:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m 21/11/14 15:19:33 INFO Utils: Successfully started service 'sparkDriver' on port 43007.
[36malgo-1    |[0m 21/11/14 15:19:33 INFO SparkEnv: Registering MapOutputTracker
[36malgo-1    |[0m 21/11/14 15:19:33 INFO SparkEnv: Registering BlockManagerMaster
[36malgo-1    |[0m 21/11/14 15:19:33 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[36malgo-1    |[0m 21/11/14 15:19:33 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[36malgo-1    |[0m 21/11/14 15:19:33 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[36malgo-1    |[0m 21/11/14 15:19:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c023b817-0427-47dc-8379-0c5e0cba58bd
[36malgo-1    |[0m 21/11/14 15:19:33 INFO MemoryStore: MemoryStore started with capacity 1007.8 MiB
[36malgo-1    |[0m 21/11/14 15:19:33 INFO SparkEnv: Registering OutputCommitCoordinator
[36malgo-1    |[0m 21/11/14 15:19:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[36malgo-1    |[0m 21/11/14 15:19:34 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.240.3:4040
[36malgo-1    |[0m 21/11/14 15:19:34 INFO RMProxy: Connecting to ResourceManager at /192.168.240.3:8032
[36malgo-1    |[0m 21/11/14 15:19:34 INFO Client: Requesting a new application from cluster with 2 NodeManagers
[36malgo-1    |[0m 2021-11-14 15:19:34,623 INFO resourcemanager.ClientRMService: Allocated new applicationId: 1
[36malgo-1    |[0m 21/11/14 15:19:35 INFO Configuration: resource-types.xml not found
[36malgo-1    |[0m 21/11/14 15:19:35 INFO ResourceUtils: Unable to find 'resource-types.xml'.
[36malgo-1    |[0m 21/11/14 15:19:35 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15892 MB per container)
[36malgo-1    |[0m 21/11/14 15:19:35 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
[36malgo-1    |[0m 21/11/14 15:19:35 INFO Client: Setting up container launch context for our AM
[36malgo-1    |[0m 21/11/14 15:19:35 INFO Client: Setting up the launch environment for our AM container
[36malgo-1    |[0m 21/11/14 15:19:35 INFO Client: Preparing resources for our AM container
[36malgo-1    |[0m 21/11/14 15:19:35 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
[36malgo-1    |[0m 21/11/14 15:19:41 INFO Client: Uploading resource file:/tmp/spark-d2d32342-fa1c-45a8-8bdc-0f22350cb278/__spark_libs__1565290401838612967.zip -> hdfs://192.168.240.3/user/root/.sparkStaging/application_1636903150985_0001/__spark_libs__1565290401838612967.zip
[36malgo-1    |[0m 2021-11-14 15:19:41,259 INFO hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=192.168.240.3:9866, 192.168.240.2:9866 for /user/root/.sparkStaging/application_1636903150985_0001/__spark_libs__1565290401838612967.zip
[36malgo-1    |[0m 2021-11-14 15:19:41,386 INFO datanode.DataNode: Receiving BP-1419144703-192.168.240.3-1636903146711:blk_1073741825_1001 src: /192.168.240.3:39148 dest: /192.168.240.3:9866
[33malgo-2    |[0m 2021-11-14 15:19:41,448 INFO datanode.DataNode: Receiving BP-1419144703-192.168.240.3-1636903146711:blk_1073741825_1001 src: /192.168.240.3:58924 dest: /192.168.240.2:9866
[33malgo-2    |[0m 2021-11-14 15:19:41,844 INFO DataNode.clienttrace: src: /192.168.240.3:58924, dest: /192.168.240.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1812065440_28, offset: 0, srvID: 5b651e59-2e31-4247-bd5f-d2409097a9d3, blockid: BP-1419144703-192.168.240.3-1636903146711:blk_1073741825_1001, duration(ns): 372307659
[33malgo-2    |[0m 2021-11-14 15:19:41,844 INFO datanode.DataNode: PacketResponder: BP-1419144703-192.168.240.3-1636903146711:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 15:19:41,848 INFO DataNode.clienttrace: src: /192.168.240.3:39148, dest: /192.168.240.3:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1812065440_28, offset: 0, srvID: bbfd46a9-a4e1-4ce5-bb19-7c4042e64546, blockid: BP-1419144703-192.168.240.3-1636903146711:blk_1073741825_1001, duration(ns): 371902868
[36malgo-1    |[0m 2021-11-14 15:19:41,848 INFO datanode.DataNode: PacketResponder: BP-1419144703-192.168.240.3-1636903146711:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.240.2:9866] terminating
[36malgo-1    |[0m 2021-11-14 15:19:41,855 INFO hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=192.168.240.3:9866, 192.168.240.2:9866 for /user/root/.sparkStaging/application_1636903150985_0001/__spark_libs__1565290401838612967.zip
[36malgo-1    |[0m 2021-11-14 15:19:41,859 INFO datanode.DataNode: Receiving BP-1419144703-192.168.240.3-1636903146711:blk_1073741826_1002 src: /192.168.240.3:39152 dest: /192.168.240.3:9866
[33malgo-2    |[0m 2021-11-14 15:19:41,860 INFO datanode.DataNode: Receiving BP-1419144703-192.168.240.3-1636903146711:blk_1073741826_1002 src: /192.168.240.3:58928 dest: /192.168.240.2:9866
[33malgo-2    |[0m 2021-11-14 15:19:42,181 INFO DataNode.clienttrace: src: /192.168.240.3:58928, dest: /192.168.240.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1812065440_28, offset: 0, srvID: 5b651e59-2e31-4247-bd5f-d2409097a9d3, blockid: BP-1419144703-192.168.240.3-1636903146711:blk_1073741826_1002, duration(ns): 318970896
[33malgo-2    |[0m 2021-11-14 15:19:42,181 INFO datanode.DataNode: PacketResponder: BP-1419144703-192.168.240.3-1636903146711:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 15:19:42,182 INFO DataNode.clienttrace: src: /192.168.240.3:39152, dest: /192.168.240.3:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1812065440_28, offset: 0, srvID: bbfd46a9-a4e1-4ce5-bb19-7c4042e64546, blockid: BP-1419144703-192.168.240.3-1636903146711:blk_1073741826_1002, duration(ns): 319944023
[36malgo-1    |[0m 2021-11-14 15:19:42,182 INFO datanode.DataNode: PacketResponder: BP-1419144703-192.168.240.3-1636903146711:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.240.2:9866] terminating
[36malgo-1    |[0m 2021-11-14 15:19:42,184 INFO hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=192.168.240.3:9866, 192.168.240.2:9866 for /user/root/.sparkStaging/application_1636903150985_0001/__spark_libs__1565290401838612967.zip
[36malgo-1    |[0m 2021-11-14 15:19:42,187 INFO datanode.DataNode: Receiving BP-1419144703-192.168.240.3-1636903146711:blk_1073741827_1003 src: /192.168.240.3:39156 dest: /192.168.240.3:9866
[33malgo-2    |[0m 2021-11-14 15:19:42,189 INFO datanode.DataNode: Receiving BP-1419144703-192.168.240.3-1636903146711:blk_1073741827_1003 src: /192.168.240.3:58932 dest: /192.168.240.2:9866
[33malgo-2    |[0m 2021-11-14 15:19:42,468 INFO DataNode.clienttrace: src: /192.168.240.3:58932, dest: /192.168.240.2:9866, bytes: 128628638, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1812065440_28, offset: 0, srvID: 5b651e59-2e31-4247-bd5f-d2409097a9d3, blockid: BP-1419144703-192.168.240.3-1636903146711:blk_1073741827_1003, duration(ns): 277363178
[33malgo-2    |[0m 2021-11-14 15:19:42,468 INFO datanode.DataNode: PacketResponder: BP-1419144703-192.168.240.3-1636903146711:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 15:19:42,469 INFO DataNode.clienttrace: src: /192.168.240.3:39156, dest: /192.168.240.3:9866, bytes: 128628638, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1812065440_28, offset: 0, srvID: bbfd46a9-a4e1-4ce5-bb19-7c4042e64546, blockid: BP-1419144703-192.168.240.3-1636903146711:blk_1073741827_1003, duration(ns): 278441596
[36malgo-1    |[0m 2021-11-14 15:19:42,469 INFO datanode.DataNode: PacketResponder: BP-1419144703-192.168.240.3-1636903146711:blk_1073741827_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.240.2:9866] terminating
[36malgo-1    |[0m 2021-11-14 15:19:42,474 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1636903150985_0001/__spark_libs__1565290401838612967.zip is closed by DFSClient_NONMAPREDUCE_-1812065440_28
[36malgo-1    |[0m 21/11/14 15:19:42 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://192.168.240.3/user/root/.sparkStaging/application_1636903150985_0001/pyspark.zip
[36malgo-1    |[0m 2021-11-14 15:19:42,570 INFO hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=192.168.240.3:9866, 192.168.240.2:9866 for /user/root/.sparkStaging/application_1636903150985_0001/pyspark.zip
[36malgo-1    |[0m 2021-11-14 15:19:42,573 INFO datanode.DataNode: Receiving BP-1419144703-192.168.240.3-1636903146711:blk_1073741828_1004 src: /192.168.240.3:39160 dest: /192.168.240.3:9866
[33malgo-2    |[0m 2021-11-14 15:19:42,575 INFO datanode.DataNode: Receiving BP-1419144703-192.168.240.3-1636903146711:blk_1073741828_1004 src: /192.168.240.3:58936 dest: /192.168.240.2:9866
[33malgo-2    |[0m 2021-11-14 15:19:42,579 INFO DataNode.clienttrace: src: /192.168.240.3:58936, dest: /192.168.240.2:9866, bytes: 732492, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1812065440_28, offset: 0, srvID: 5b651e59-2e31-4247-bd5f-d2409097a9d3, blockid: BP-1419144703-192.168.240.3-1636903146711:blk_1073741828_1004, duration(ns): 2994519
[33malgo-2    |[0m 2021-11-14 15:19:42,579 INFO datanode.DataNode: PacketResponder: BP-1419144703-192.168.240.3-1636903146711:blk_1073741828_1004, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 15:19:42,580 INFO DataNode.clienttrace: src: /192.168.240.3:39160, dest: /192.168.240.3:9866, bytes: 732492, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1812065440_28, offset: 0, srvID: bbfd46a9-a4e1-4ce5-bb19-7c4042e64546, blockid: BP-1419144703-192.168.240.3-1636903146711:blk_1073741828_1004, duration(ns): 3812880
[36malgo-1    |[0m 2021-11-14 15:19:42,580 INFO datanode.DataNode: PacketResponder: BP-1419144703-192.168.240.3-1636903146711:blk_1073741828_1004, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.240.2:9866] terminating
[36malgo-1    |[0m 2021-11-14 15:19:42,581 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1636903150985_0001/pyspark.zip is closed by DFSClient_NONMAPREDUCE_-1812065440_28
[36malgo-1    |[0m 21/11/14 15:19:42 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9-src.zip -> hdfs://192.168.240.3/user/root/.sparkStaging/application_1636903150985_0001/py4j-0.10.9-src.zip
[36malgo-1    |[0m 2021-11-14 15:19:42,596 INFO hdfs.StateChange: BLOCK* allocate blk_1073741829_1005, replicas=192.168.240.3:9866, 192.168.240.2:9866 for /user/root/.sparkStaging/application_1636903150985_0001/py4j-0.10.9-src.zip
[36malgo-1    |[0m 2021-11-14 15:19:42,599 INFO datanode.DataNode: Receiving BP-1419144703-192.168.240.3-1636903146711:blk_1073741829_1005 src: /192.168.240.3:39164 dest: /192.168.240.3:9866
[33malgo-2    |[0m 2021-11-14 15:19:42,600 INFO datanode.DataNode: Receiving BP-1419144703-192.168.240.3-1636903146711:blk_1073741829_1005 src: /192.168.240.3:58940 dest: /192.168.240.2:9866
[33malgo-2    |[0m 2021-11-14 15:19:42,603 INFO DataNode.clienttrace: src: /192.168.240.3:58940, dest: /192.168.240.2:9866, bytes: 41587, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1812065440_28, offset: 0, srvID: 5b651e59-2e31-4247-bd5f-d2409097a9d3, blockid: BP-1419144703-192.168.240.3-1636903146711:blk_1073741829_1005, duration(ns): 1662011
[33malgo-2    |[0m 2021-11-14 15:19:42,603 INFO datanode.DataNode: PacketResponder: BP-1419144703-192.168.240.3-1636903146711:blk_1073741829_1005, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 15:19:42,604 INFO DataNode.clienttrace: src: /192.168.240.3:39164, dest: /192.168.240.3:9866, bytes: 41587, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1812065440_28, offset: 0, srvID: bbfd46a9-a4e1-4ce5-bb19-7c4042e64546, blockid: BP-1419144703-192.168.240.3-1636903146711:blk_1073741829_1005, duration(ns): 2456040
[36malgo-1    |[0m 2021-11-14 15:19:42,604 INFO datanode.DataNode: PacketResponder: BP-1419144703-192.168.240.3-1636903146711:blk_1073741829_1005, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.240.2:9866] terminating
[36malgo-1    |[0m 2021-11-14 15:19:42,605 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1636903150985_0001/py4j-0.10.9-src.zip is closed by DFSClient_NONMAPREDUCE_-1812065440_28
[36malgo-1    |[0m 21/11/14 15:19:42 INFO Client: Uploading resource file:/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py -> hdfs://192.168.240.3/user/root/.sparkStaging/application_1636903150985_0001/hello_py_spark_udfs.py
[36malgo-1    |[0m 2021-11-14 15:19:42,619 INFO hdfs.StateChange: BLOCK* allocate blk_1073741830_1006, replicas=192.168.240.3:9866, 192.168.240.2:9866 for /user/root/.sparkStaging/application_1636903150985_0001/hello_py_spark_udfs.py
[36malgo-1    |[0m 2021-11-14 15:19:42,622 INFO datanode.DataNode: Receiving BP-1419144703-192.168.240.3-1636903146711:blk_1073741830_1006 src: /192.168.240.3:39168 dest: /192.168.240.3:9866
[33malgo-2    |[0m 2021-11-14 15:19:42,623 INFO datanode.DataNode: Receiving BP-1419144703-192.168.240.3-1636903146711:blk_1073741830_1006 src: /192.168.240.3:58944 dest: /192.168.240.2:9866
[33malgo-2    |[0m 2021-11-14 15:19:42,626 INFO DataNode.clienttrace: src: /192.168.240.3:58944, dest: /192.168.240.2:9866, bytes: 6184, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1812065440_28, offset: 0, srvID: 5b651e59-2e31-4247-bd5f-d2409097a9d3, blockid: BP-1419144703-192.168.240.3-1636903146711:blk_1073741830_1006, duration(ns): 1616883
[33malgo-2    |[0m 2021-11-14 15:19:42,626 INFO datanode.DataNode: PacketResponder: BP-1419144703-192.168.240.3-1636903146711:blk_1073741830_1006, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 15:19:42,627 INFO DataNode.clienttrace: src: /192.168.240.3:39168, dest: /192.168.240.3:9866, bytes: 6184, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1812065440_28, offset: 0, srvID: bbfd46a9-a4e1-4ce5-bb19-7c4042e64546, blockid: BP-1419144703-192.168.240.3-1636903146711:blk_1073741830_1006, duration(ns): 2644897
[36malgo-1    |[0m 2021-11-14 15:19:42,627 INFO datanode.DataNode: PacketResponder: BP-1419144703-192.168.240.3-1636903146711:blk_1073741830_1006, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.240.2:9866] terminating
[36malgo-1    |[0m 2021-11-14 15:19:42,629 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1636903150985_0001/hello_py_spark_udfs.py is closed by DFSClient_NONMAPREDUCE_-1812065440_28
[36malgo-1    |[0m 21/11/14 15:19:42 INFO Client: Uploading resource file:/tmp/spark-d2d32342-fa1c-45a8-8bdc-0f22350cb278/__spark_conf__3308214816184211496.zip -> hdfs://192.168.240.3/user/root/.sparkStaging/application_1636903150985_0001/__spark_conf__.zip
[36malgo-1    |[0m 2021-11-14 15:19:42,748 INFO hdfs.StateChange: BLOCK* allocate blk_1073741831_1007, replicas=192.168.240.3:9866, 192.168.240.2:9866 for /user/root/.sparkStaging/application_1636903150985_0001/__spark_conf__.zip
[36malgo-1    |[0m 2021-11-14 15:19:42,751 INFO datanode.DataNode: Receiving BP-1419144703-192.168.240.3-1636903146711:blk_1073741831_1007 src: /192.168.240.3:39172 dest: /192.168.240.3:9866
[33malgo-2    |[0m 2021-11-14 15:19:42,752 INFO datanode.DataNode: Receiving BP-1419144703-192.168.240.3-1636903146711:blk_1073741831_1007 src: /192.168.240.3:58948 dest: /192.168.240.2:9866
[33malgo-2    |[0m 2021-11-14 15:19:42,755 INFO DataNode.clienttrace: src: /192.168.240.3:58948, dest: /192.168.240.2:9866, bytes: 252608, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1812065440_28, offset: 0, srvID: 5b651e59-2e31-4247-bd5f-d2409097a9d3, blockid: BP-1419144703-192.168.240.3-1636903146711:blk_1073741831_1007, duration(ns): 2001162
[33malgo-2    |[0m 2021-11-14 15:19:42,756 INFO datanode.DataNode: PacketResponder: BP-1419144703-192.168.240.3-1636903146711:blk_1073741831_1007, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 15:19:42,757 INFO DataNode.clienttrace: src: /192.168.240.3:39172, dest: /192.168.240.3:9866, bytes: 252608, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1812065440_28, offset: 0, srvID: bbfd46a9-a4e1-4ce5-bb19-7c4042e64546, blockid: BP-1419144703-192.168.240.3-1636903146711:blk_1073741831_1007, duration(ns): 3100477
[36malgo-1    |[0m 2021-11-14 15:19:42,757 INFO datanode.DataNode: PacketResponder: BP-1419144703-192.168.240.3-1636903146711:blk_1073741831_1007, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.240.2:9866] terminating
[36malgo-1    |[0m 2021-11-14 15:19:42,758 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1636903150985_0001/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_-1812065440_28
[36malgo-1    |[0m 21/11/14 15:19:42 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m 21/11/14 15:19:42 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m 21/11/14 15:19:42 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m 21/11/14 15:19:42 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m 21/11/14 15:19:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m 21/11/14 15:19:42 INFO Client: Submitting application application_1636903150985_0001 to ResourceManager
[36malgo-1    |[0m 2021-11-14 15:19:42,879 INFO capacity.CapacityScheduler: Application 'application_1636903150985_0001' is submitted without priority hence considering default queue/cluster priority: 0
[36malgo-1    |[0m 2021-11-14 15:19:42,879 INFO capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1636903150985_0001
[36malgo-1    |[0m 2021-11-14 15:19:42,895 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 1]. Use the global max attempts instead.
[36malgo-1    |[0m 2021-11-14 15:19:42,897 INFO resourcemanager.ClientRMService: Application with id 1 submitted by user root
[36malgo-1    |[0m 2021-11-14 15:19:42,897 INFO rmapp.RMAppImpl: Storing application with id application_1636903150985_0001
[36malgo-1    |[0m 2021-11-14 15:19:42,899 INFO resourcemanager.RMAuditLogger: USER=root	IP=192.168.240.3	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1636903150985_0001	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 15:19:42,904 INFO recovery.RMStateStore: Storing info for app: application_1636903150985_0001
[36malgo-1    |[0m 2021-11-14 15:19:42,904 INFO rmapp.RMAppImpl: application_1636903150985_0001 State change from NEW to NEW_SAVING on event = START
[36malgo-1    |[0m 2021-11-14 15:19:42,905 INFO rmapp.RMAppImpl: application_1636903150985_0001 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
[36malgo-1    |[0m 2021-11-14 15:19:42,907 INFO capacity.ParentQueue: Application added - appId: application_1636903150985_0001 user: root leaf-queue of parent: root #applications: 1
[36malgo-1    |[0m 2021-11-14 15:19:42,907 INFO capacity.CapacityScheduler: Accepted application application_1636903150985_0001 from user: root, in queue: default
[36malgo-1    |[0m 2021-11-14 15:19:42,919 INFO rmapp.RMAppImpl: application_1636903150985_0001 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
[36malgo-1    |[0m 2021-11-14 15:19:42,946 INFO resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1636903150985_0001_000001
[36malgo-1    |[0m 2021-11-14 15:19:42,947 INFO attempt.RMAppAttemptImpl: appattempt_1636903150985_0001_000001 State change from NEW to SUBMITTED on event = START
[36malgo-1    |[0m 21/11/14 15:19:42 INFO YarnClientImpl: Submitted application application_1636903150985_0001
[36malgo-1    |[0m 2021-11-14 15:19:42,997 INFO capacity.LeafQueue: Application application_1636903150985_0001 from user: root activated in queue: default
[36malgo-1    |[0m 2021-11-14 15:19:42,998 INFO capacity.LeafQueue: Application added - appId: application_1636903150985_0001 user: root, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
[36malgo-1    |[0m 2021-11-14 15:19:42,998 INFO capacity.CapacityScheduler: Added Application Attempt appattempt_1636903150985_0001_000001 to scheduler from user root in queue default
[36malgo-1    |[0m 2021-11-14 15:19:43,005 INFO attempt.RMAppAttemptImpl: appattempt_1636903150985_0001_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
[36malgo-1    |[0m 21/11/14 15:19:43 INFO Client: Application report for application_1636903150985_0001 (state: ACCEPTED)
[36malgo-1    |[0m 21/11/14 15:19:44 INFO Client: 
[36malgo-1    |[0m 	 client token: N/A
[36malgo-1    |[0m 	 diagnostics: [Sun Nov 14 15:19:42 +0000 2021] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:31784, vCores:8> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 0.0 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:31784, vCores:8> ; Queue's used capacity (absolute resource) = <memory:0, vCores:0> ; Queue's max capacity (absolute resource) = <memory:31784, vCores:8> ; 
[36malgo-1    |[0m 	 ApplicationMaster host: N/A
[36malgo-1    |[0m 	 ApplicationMaster RPC port: -1
[36malgo-1    |[0m 	 queue: default
[36malgo-1    |[0m 	 start time: 1636903182895
[36malgo-1    |[0m 	 final status: UNDEFINED
[36malgo-1    |[0m 	 tracking URL: http://algo-1:8088/proxy/application_1636903150985_0001/
[36malgo-1    |[0m 	 user: root
[36malgo-1    |[0m 2021-11-14 15:19:44,029 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1636903150985_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=
[36malgo-1    |[0m 2021-11-14 15:19:44,034 INFO rmcontainer.RMContainerImpl: container_1636903150985_0001_01_000001 Container Transitioned from NEW to ALLOCATED
[36malgo-1    |[0m 2021-11-14 15:19:44,035 INFO fica.FiCaSchedulerNode: Assigned container container_1636903150985_0001_01_000001 of capacity <memory:896, vCores:1> on host algo-1:44133, which has 1 containers, <memory:896, vCores:1> used and <memory:14996, vCores:3> available after allocation
[36malgo-1    |[0m 2021-11-14 15:19:44,035 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636903150985_0001	CONTAINERID=container_1636903150985_0001_01_000001	RESOURCE=<memory:896, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 15:19:44,055 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:44133 for container : container_1636903150985_0001_01_000001
[36malgo-1    |[0m 2021-11-14 15:19:44,064 INFO rmcontainer.RMContainerImpl: container_1636903150985_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
[36malgo-1    |[0m 2021-11-14 15:19:44,064 INFO security.NMTokenSecretManagerInRM: Clear node set for appattempt_1636903150985_0001_000001
[36malgo-1    |[0m 2021-11-14 15:19:44,064 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.028190285 absoluteUsedCapacity=0.028190285 used=<memory:896, vCores:1> cluster=<memory:31784, vCores:8>
[36malgo-1    |[0m 2021-11-14 15:19:44,064 INFO capacity.CapacityScheduler: Allocation proposal accepted
[36malgo-1    |[0m 2021-11-14 15:19:44,064 INFO attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1636903150985_0001 AttemptId: appattempt_1636903150985_0001_000001 MasterContainer: Container: [ContainerId: container_1636903150985_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-1:44133, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.240.3:44133 }, ExecutionType: GUARANTEED, ]
[36malgo-1    |[0m 2021-11-14 15:19:44,076 INFO attempt.RMAppAttemptImpl: appattempt_1636903150985_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
[36malgo-1    |[0m 2021-11-14 15:19:44,081 INFO attempt.RMAppAttemptImpl: appattempt_1636903150985_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
[36malgo-1    |[0m 2021-11-14 15:19:44,093 INFO amlauncher.AMLauncher: Launching masterappattempt_1636903150985_0001_000001
[36malgo-1    |[0m 2021-11-14 15:19:44,155 INFO amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1636903150985_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-1:44133, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.240.3:44133 }, ExecutionType: GUARANTEED, ] for AM appattempt_1636903150985_0001_000001
[36malgo-1    |[0m 2021-11-14 15:19:44,156 INFO security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1636903150985_0001_000001
[36malgo-1    |[0m 2021-11-14 15:19:44,159 INFO security.AMRMTokenSecretManager: Creating password for appattempt_1636903150985_0001_000001
[36malgo-1    |[0m 2021-11-14 15:19:44,290 INFO ipc.Server: Auth successful for appattempt_1636903150985_0001_000001 (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 15:19:44,374 INFO containermanager.ContainerManagerImpl: Start request for container_1636903150985_0001_01_000001 by user root
[36malgo-1    |[0m 2021-11-14 15:19:44,419 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1636903150985_0001
[36malgo-1    |[0m 2021-11-14 15:19:44,427 INFO application.ApplicationImpl: Application application_1636903150985_0001 transitioned from NEW to INITING
[36malgo-1    |[0m 2021-11-14 15:19:44,427 INFO application.ApplicationImpl: Adding container_1636903150985_0001_01_000001 to application application_1636903150985_0001
[36malgo-1    |[0m 2021-11-14 15:19:44,427 INFO nodemanager.NMAuditLogger: USER=root	IP=192.168.240.3	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1636903150985_0001	CONTAINERID=container_1636903150985_0001_01_000001
[36malgo-1    |[0m 2021-11-14 15:19:44,431 INFO application.ApplicationImpl: Application application_1636903150985_0001 transitioned from INITING to RUNNING
[36malgo-1    |[0m 2021-11-14 15:19:44,434 INFO container.ContainerImpl: Container container_1636903150985_0001_01_000001 transitioned from NEW to LOCALIZING
[36malgo-1    |[0m 2021-11-14 15:19:44,434 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1636903150985_0001
[36malgo-1    |[0m 2021-11-14 15:19:44,440 INFO amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1636903150985_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-1:44133, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.240.3:44133 }, ExecutionType: GUARANTEED, ] for AM appattempt_1636903150985_0001_000001
[36malgo-1    |[0m 2021-11-14 15:19:44,440 INFO attempt.RMAppAttemptImpl: appattempt_1636903150985_0001_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
[36malgo-1    |[0m 2021-11-14 15:19:44,440 INFO rmapp.RMAppImpl: update the launch time for applicationId: application_1636903150985_0001, attemptId: appattempt_1636903150985_0001_000001launchTime: 1636903184440
[36malgo-1    |[0m 2021-11-14 15:19:44,441 INFO recovery.RMStateStore: Updating info for app: application_1636903150985_0001
[36malgo-1    |[0m 2021-11-14 15:19:44,443 INFO localizer.ResourceLocalizationService: Created localizer for container_1636903150985_0001_01_000001
[36malgo-1    |[0m 2021-11-14 15:19:44,506 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1636903150985_0001_01_000001.tokens
[36malgo-1    |[0m 2021-11-14 15:19:44,516 INFO nodemanager.DefaultContainerExecutor: Initializing user root
[36malgo-1    |[0m 2021-11-14 15:19:44,521 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1636903150985_0001_01_000001.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000001.tokens
[36malgo-1    |[0m 2021-11-14 15:19:44,521 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001
[36malgo-1    |[0m 21/11/14 15:19:45 INFO Client: Application report for application_1636903150985_0001 (state: ACCEPTED)
[36malgo-1    |[0m 2021-11-14 15:19:45,018 INFO rmcontainer.RMContainerImpl: container_1636903150985_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING
[36malgo-1    |[0m 21/11/14 15:19:46 INFO Client: Application report for application_1636903150985_0001 (state: ACCEPTED)
[36malgo-1    |[0m 21/11/14 15:19:47 INFO Client: Application report for application_1636903150985_0001 (state: ACCEPTED)
[36malgo-1    |[0m 2021-11-14 15:19:47,808 INFO container.ContainerImpl: Container container_1636903150985_0001_01_000001 transitioned from LOCALIZING to SCHEDULED
[36malgo-1    |[0m 2021-11-14 15:19:47,809 INFO scheduler.ContainerScheduler: Starting container [container_1636903150985_0001_01_000001]
[36malgo-1    |[0m 2021-11-14 15:19:47,833 INFO container.ContainerImpl: Container container_1636903150985_0001_01_000001 transitioned from SCHEDULED to RUNNING
[36malgo-1    |[0m 2021-11-14 15:19:47,834 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1636903150985_0001_01_000001
[36malgo-1    |[0m 2021-11-14 15:19:47,838 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000001/default_container_executor.sh]
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/prelaunch.out
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/prelaunch.err
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/launch_container.sh
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stdout
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr
[36malgo-1    |[0m 2021-11-14 15:19:47,967 INFO monitor.ContainersMonitorImpl: container_1636903150985_0001_01_000001's ip = 192.168.240.3, and hostname = algo-1
[36malgo-1    |[0m 2021-11-14 15:19:47,972 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1636903150985_0001_01_000001 since CPU usage is not yet available.
[36malgo-1    |[0m 21/11/14 15:19:48 INFO Client: Application report for application_1636903150985_0001 (state: ACCEPTED)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1551170 2860 -r-x------   1 root     root      2927301 Nov 14 15:19 ./__spark_libs__/hadoop-yarn-common-3.2.1-amzn-1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1551171  100 -r-x------   1 root     root       100431 Nov 14 15:19 ./__spark_libs__/pyrolite-4.30.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1551172  224 -r-x------   1 root     root       225999 Nov 14 15:19 ./__spark_libs__/hadoop-yarn-registry-3.2.1-amzn-1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1551173  628 -r-x------   1 root     root       643043 Nov 14 15:19 ./__spark_libs__/joda-time-2.10.5.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1551174 4056 -r-x------   1 root     root      4153218 Nov 14 15:19 ./__spark_libs__/netty-all-4.1.47.Final.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1551175  316 -r-x------   1 root     root       322915 Nov 14 15:19 ./__spark_libs__/hadoop-yarn-client-3.2.1-amzn-1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553085  136 -r-x------   1 root     root       135896 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-mediatailor-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553086  668 -r-x------   1 root     root       680714 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-iotanalytics-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553087  636 -r-x------   1 root     root       649305 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-workspaces-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553088  224 -r-x------   1 root     root       227528 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-cloudhsm-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553089  620 -r-x------   1 root     root       631376 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-inspector-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553090 1068 -r-x------   1 root     root      1091741 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-cloudfront-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1551176   60 -r-x------   1 root     root        60261 Nov 14 15:19 ./__spark_libs__/hadoop-annotations-3.2.1-amzn-1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1551177   60 -r-x------   1 root     root        58684 Nov 14 15:19 ./__spark_libs__/chill-java-0.9.5.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553091  428 -r-x------   1 root     root       435660 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-datasync-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553092  512 -r-x------   1 root     root       521647 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-mechanicalturkrequester-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1551178   44 -r-x------   1 root     root        43740 Nov 14 15:19 ./__spark_libs__/jackson-module-paranamer-2.10.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1551179  232 -r-x------   1 root     root       233745 Nov 14 15:19 ./__spark_libs__/threeten-extra-1.5.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553093  448 -r-x------   1 root     root       455261 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-cloudsearch-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1551180 7384 -r-x------   1 root     root      7558789 Nov 14 15:19 ./__spark_libs__/spark-sql_2.12-3.0.0-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553094  432 -r-x------   1 root     root       441811 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-appconfig-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553095    8 -r-x------   1 root     root         7782 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-codegen-maven-plugin-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1551181 4112 -r-x------   1 root     root      4210625 Nov 14 15:19 ./__spark_libs__/zstd-jni-1.4.4-3.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553096  176 -r-x------   1 root     root       176990 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-cloud9-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553097  236 -r-x------   1 root     root       238845 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-mediastore-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1551182  384 -r-x------   1 root     root       392124 Nov 14 15:19 ./__spark_libs__/velocity-1.5.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1551183 1152 -r-x------   1 root     root      1175798 Nov 14 15:19 ./__spark_libs__/JTransforms-3.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553098   24 -r-x------   1 root     root        24085 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-workmailmessageflow-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553099  136 -r-x------   1 root     root       137101 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-lex-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553100  100 -r-x------   1 root     root        99335 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-connectparticipant-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553101  132 -r-x------   1 root     root       133434 Nov 14 15:19 ./__spark_libs__/aws-glue-datacatalog-spark-client-3.0.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1551184  436 -r-x------   1 root     root       443231 Nov 14 15:19 ./__spark_libs__/univocity-parsers-2.8.3.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1551185  164 -r-x------   1 root     root       164422 Nov 14 15:19 ./__spark_libs__/core-1.1.2.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553102  520 -r-x------   1 root     root       528775 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-connect-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1551186 1364 -r-x------   1 root     root      1393617 Nov 14 15:19 ./__spark_libs__/hadoop-yarn-server-common-3.2.1-amzn-1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553103 1192 -r-x------   1 root     root      1220227 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-servicecatalog-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553104 1068 -r-x------   1 root     root      1091650 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-opsworks-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553105  180 -r-x------   1 root     root       182585 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-autoscalingplans-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553106 1000 -r-x------   1 root     root      1023417 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-macie2-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1551187 2252 -r-x------   1 root     root      2305169 Nov 14 15:19 ./__spark_libs__/netlib-native_ref-win-x86_64-1.1-natives.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1551188  544 -r-x------   1 root     root       553993 Nov 14 15:19 ./__spark_libs__/netlib-native_system-osx-x86_64-1.1-natives.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1551189   12 -r-x------   1 root     root         9498 Nov 14 15:19 ./__spark_libs__/spark-tags_2.12-3.0.0-amzn-0-tests.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1551190   28 -r-x------   1 root     root        27006 Nov 14 15:19 ./__spark_libs__/aopalliance-repackaged-2.6.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552848  116 -r-x------   1 root     root       116120 Nov 14 15:19 ./__spark_libs__/kerb-crypto-1.0.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553107  208 -r-x------   1 root     root       208995 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-iot1clickprojects-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553108 2484 -r-x------   1 root     root      2541217 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-ssm-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552849   64 -r-x------   1 root     root        65464 Nov 14 15:19 ./__spark_libs__/kerb-common-1.0.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552850  200 -r-x------   1 root     root       204650 Nov 14 15:19 ./__spark_libs__/kerby-pkix-1.0.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553109  716 -r-x------   1 root     root       729294 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-organizations-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552851  504 -r-x------   1 root     root       512742 Nov 14 15:19 ./__spark_libs__/woodstox-core-5.0.3.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552852 2344 -r-x------   1 root     root      2397719 Nov 14 15:19 ./__spark_libs__/spark-network-common_2.12-3.0.0-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552853  360 -r-x------   1 root     root       365552 Nov 14 15:19 ./__spark_libs__/commons-compress-1.8.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553110  804 -r-x------   1 root     root       819646 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-directory-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552854  120 -r-x------   1 root     root       120527 Nov 14 15:19 ./__spark_libs__/hive-shims-common-2.3.7-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553111  344 -r-x------   1 root     root       348636 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-transcribe-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552855 10424 -r-x------   1 root     root     10672015 Nov 14 15:19 ./__spark_libs__/scala-compiler-2.12.10.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552856  656 -r-x------   1 root     root       668235 Nov 14 15:19 ./__spark_libs__/guice-4.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553112 1100 -r-x------   1 root     root      1122498 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-securityhub-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552857 1468 -r-x------   1 root     root      1502280 Nov 14 15:19 ./__spark_libs__/htrace-core4-4.1.0-incubating.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553113  176 -r-x------   1 root     root       177649 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-iot1clickdevices-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552858 3152 -r-x------   1 root     root      3226851 Nov 14 15:19 ./__spark_libs__/cats-kernel_2.12-2.0.0-M4.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552859 3596 -r-x------   1 root     root      3678534 Nov 14 15:19 ./__spark_libs__/scala-reflect-2.12.10.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552860   32 -r-x------   1 root     root        30674 Nov 14 15:19 ./__spark_libs__/kerby-config-1.0.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552861  300 -r-x------   1 root     root       305001 Nov 14 15:19 ./__spark_libs__/commons-httpclient-3.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553114  132 -r-x------   1 root     root       131735 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-ioteventsdata-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552862   56 -r-x------   1 root     root        55236 Nov 14 15:19 ./__spark_libs__/geronimo-jcache_1.0_spec-1.0-alpha-1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552863  224 -r-x------   1 root     root       226672 Nov 14 15:19 ./__spark_libs__/kerb-core-1.0.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553115  140 -r-x------   1 root     root       139799 Nov 14 15:19 ./__spark_libs__/aws-glue-datacatalog-hive3-client-3.0.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553116   48 -r-x------   1 root     root        47606 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-cloudwatchmetrics-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553117  404 -r-x------   1 root     root       411539 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-servicediscovery-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553118 1876 -r-x------   1 root     root      1918555 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-medialive-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552864 1712 -r-x------   1 root     root      1749371 Nov 14 15:19 ./__spark_libs__/netlib-native_ref-linux-x86_64-1.1-natives.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553119  252 -r-x------   1 root     root       254750 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-cognitosync-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552865 13504 -r-x------   1 root     root     13826799 Nov 14 15:19 ./__spark_libs__/breeze_2.12-1.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552866   96 -r-x------   1 root     root        96221 Nov 14 15:19 ./__spark_libs__/commons-pool-1.5.4.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552867 1688 -r-x------   1 root     root      1726527 Nov 14 15:19 ./__spark_libs__/ehcache-3.3.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553120  392 -r-x------   1 root     root       398329 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-elastictranscoder-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552868   92 -r-x------   1 root     root        91930 Nov 14 15:19 ./__spark_libs__/jakarta.validation-api-2.0.2.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552869   28 -r-x------   1 root     root        28355 Nov 14 15:19 ./__spark_libs__/spark-ganglia-lgpl_2.12-3.0.0-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553121  684 -r-x------   1 root     root       699987 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-personalize-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552870   44 -r-x------   1 root     root        44513 Nov 14 15:19 ./__spark_libs__/gmetric4j-1.0.10.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552871   20 -r-x------   1 root     root        19479 Nov 14 15:19 ./__spark_libs__/osgi-resource-locator-1.0.3.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552872  900 -r-x------   1 root     root       918300 Nov 14 15:19 ./__spark_libs__/hive-serde-2.3.7-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553122  192 -r-x------   1 root     root       194406 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-cloudhsmv2-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553123 1040 -r-x------   1 root     root      1061132 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-s3-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552873   16 -r-x------   1 root     root        14395 Nov 14 15:19 ./__spark_libs__/generex-1.0.2.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553124  728 -r-x------   1 root     root       743779 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-sesv2-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552874  908 -r-x------   1 root     root       926574 Nov 14 15:19 ./__spark_libs__/janino-3.0.16.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553125  136 -r-x------   1 root     root       138924 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-marketplacecatalog-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553126  488 -r-x------   1 root     root       496063 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-ecr-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553127  248 -r-x------   1 root     root       252611 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-computeoptimizer-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552875   76 -r-x------   1 root     root        76983 Nov 14 15:19 ./__spark_libs__/guice-servlet-4.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553128  616 -r-x------   1 root     root       629711 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-kendra-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552876   12 -r-x------   1 root     root        12211 Nov 14 15:19 ./__spark_libs__/slf4j-log4j12-1.7.30.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553129  764 -r-x------   1 root     root       779663 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-codepipeline-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552877  232 -r-x------   1 root     root       234942 Nov 14 15:19 ./__spark_libs__/hive-storage-api-2.7.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553130  948 -r-x------   1 root     root       966965 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-apigatewayv2-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552878  432 -r-x------   1 root     root       438843 Nov 14 15:19 ./__spark_libs__/hive-common-2.3.7-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552879  200 -r-x------   1 root     root       201965 Nov 14 15:19 ./__spark_libs__/curator-framework-2.13.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553131  368 -r-x------   1 root     root       374969 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-codeguruprofiler-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553132  296 -r-x------   1 root     root       300742 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-mediapackagevod-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552880   56 -r-x------   1 root     root        54509 Nov 14 15:19 ./__spark_libs__/native_system-java-1.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552881  244 -r-x------   1 root     root       249790 Nov 14 15:19 ./__spark_libs__/javax.jdo-3.2.0-m3.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553133  384 -r-x------   1 root     root       393130 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-networkmanager-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553134  104 -r-x------   1 root     root       103083 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-outposts-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552882  576 -r-x------   1 root     root       588337 Nov 14 15:19 ./__spark_libs__/commons-collections-3.2.2.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552883   28 -r-x------   1 root     root        25475 Nov 14 15:19 ./__spark_libs__/json-1.8.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553135  620 -r-x------   1 root     root       632485 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-lexmodelbuilding-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553136  228 -r-x------   1 root     root       229423 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-applicationautoscaling-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553137 1092 -r-x------   1 root     root      1117335 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-devicefarm-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553138  240 -r-x------   1 root     root       244715 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-lakeformation-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552884   24 -r-x------   1 root     root        24239 Nov 14 15:19 ./__spark_libs__/commons-daemon-1.0.13.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553139 2800 -r-x------   1 root     root      2864744 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-sagemaker-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552885   72 -r-x------   1 root     root        73349 Nov 14 15:19 ./__spark_libs__/jersey-container-servlet-core-2.30.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553140  552 -r-x------   1 root     root       563962 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-logs-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552886 1988 -r-x------   1 root     root      2035066 Nov 14 15:19 ./__spark_libs__/commons-math3-3.4.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553141  464 -r-x------   1 root     root       473688 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-kinesisvideo-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552887   68 -r-x------   1 root     root        69409 Nov 14 15:19 ./__spark_libs__/activation-1.1.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552888  416 -r-x------   1 root     root       423175 Nov 14 15:19 ./__spark_libs__/okhttp-3.12.6.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553142  776 -r-x------   1 root     root       791754 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-ses-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552889  220 -r-x------   1 root     root       222980 Nov 14 15:19 ./__spark_libs__/scala-parser-combinators_2.12-1.1.2.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553143   84 -r-x------   1 root     root        82717 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-pi-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552890  788 -r-x------   1 root     root       805848 Nov 14 15:19 ./__spark_libs__/hadoop-mapreduce-client-common-3.2.1-amzn-1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553144  288 -r-x------   1 root     root       293007 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-licensemanager-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552891   56 -r-x------   1 root     root        54391 Nov 14 15:19 ./__spark_libs__/objenesis-2.5.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553145   56 -r-x------   1 root     root        53254 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-apigatewaymanagementapi-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553146  628 -r-x------   1 root     root       641941 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-stepfunctions-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553147 1808 -r-x------   1 root     root      1848502 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-waf-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553148 1824 -r-x------   1 root     root      1866513 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-lightsail-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553149  396 -r-x------   1 root     root       403163 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-discovery-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553150   96 -r-x------   1 root     root        97822 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-augmentedairuntime-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553151  544 -r-x------   1 root     root       554655 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-elasticloadbalancingv2-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552892   28 -r-x------   1 root     root        27749 Nov 14 15:19 ./__spark_libs__/orc-shims-1.5.10.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552893  320 -r-x------   1 root     root       326874 Nov 14 15:19 ./__spark_libs__/httpcore-4.4.11.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552894  244 -r-x------   1 root     root       246918 Nov 14 15:19 ./__spark_libs__/commons-beanutils-1.9.4.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552895   36 -r-x------   1 root     root        35518 Nov 14 15:19 ./__spark_libs__/zjsonpatch-0.3.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552896  448 -r-x------   1 root     root       458605 Nov 14 15:19 ./__spark_libs__/netlib-native_system-linux-x86_64-1.1-natives.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553152  308 -r-x------   1 root     root       315095 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-s3control-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552897   84 -r-x------   1 root     root        85815 Nov 14 15:19 ./__spark_libs__/jersey-media-jaxb-2.30.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552898  604 -r-x------   1 root     root       616888 Nov 14 15:19 ./__spark_libs__/commons-configuration2-2.1.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553153 1436 -r-x------   1 root     root      1469407 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-chime-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552899   20 -r-x------   1 root     root        20046 Nov 14 15:19 ./__spark_libs__/kerb-identity-1.0.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552900   76 -r-x------   1 root     root        76733 Nov 14 15:19 ./__spark_libs__/jersey-hk2-2.30.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553154 40140 -r-x------   1 root     root     41099632 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-models-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552901  100 -r-x------   1 root     root       100636 Nov 14 15:19 ./__spark_libs__/jsp-api-2.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553155 1028 -r-x------   1 root     root      1051487 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-codedeploy-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552902   36 -r-x------   1 root     root        34654 Nov 14 15:19 ./__spark_libs__/paranamer-2.8.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553156  404 -r-x------   1 root     root       413024 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-managedblockchain-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553157  492 -r-x------   1 root     root       501832 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-amplify-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553158   44 -r-x------   1 root     root        43408 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-marketplaceentitlement-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552903   68 -r-x------   1 root     root        67897 Nov 14 15:19 ./__spark_libs__/jackson-annotations-2.10.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552904 1168 -r-x------   1 root     root      1194003 Nov 14 15:19 ./__spark_libs__/arpack_combined_all-0.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552905 5752 -r-x------   1 root     root      5886128 Nov 14 15:19 ./__spark_libs__/spark-mllib_2.12-3.0.0-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552906    8 -r-x------   1 root     root         4467 Nov 14 15:19 ./__spark_libs__/aopalliance-1.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552907  624 -r-x------   1 root     root       637428 Nov 14 15:19 ./__spark_libs__/netlib-native_system-win-x86_64-1.1-natives.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552908   28 -r-x------   1 root     root        26514 Nov 14 15:19 ./__spark_libs__/stax-api-1.0.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552909   80 -r-x------   1 root     root        79588 Nov 14 15:19 ./__spark_libs__/spire-macros_2.12-0.17.0-M1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552910 4064 -r-x------   1 root     root      4158923 Nov 14 15:19 ./__spark_libs__/hadoop-common-3.2.1-amzn-1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552911  332 -r-x------   1 root     root       336803 Nov 14 15:19 ./__spark_libs__/antlr4-runtime-4.7.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553159  896 -r-x------   1 root     root       913853 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-simpleworkflow-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552912  132 -r-x------   1 root     root       134696 Nov 14 15:19 ./__spark_libs__/breeze-macros_2.12-1.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553160  276 -r-x------   1 root     root       281444 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-codestar-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553161 1464 -r-x------   1 root     root      1498425 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-codecommit-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553162 1644 -r-x------   1 root     root      1683061 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-quicksight-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552913   28 -r-x------   1 root     root        27156 Nov 14 15:19 ./__spark_libs__/istack-commons-runtime-3.0.8.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552914 1112 -r-x------   1 root     root      1137921 Nov 14 15:19 ./__spark_libs__/spark-streaming_2.12-3.0.0-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553163  284 -r-x------   1 root     root       290178 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-datapipeline-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553164  696 -r-x------   1 root     root       711839 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-elasticbeanstalk-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552915  268 -r-x------   1 root     root       273370 Nov 14 15:19 ./__spark_libs__/commons-net-3.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553165  804 -r-x------   1 root     root       822890 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-kinesisanalyticsv2-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552916  228 -r-x------   1 root     root       232248 Nov 14 15:19 ./__spark_libs__/jackson-core-asl-1.9.13.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552917   56 -r-x------   1 root     root        56273 Nov 14 15:19 ./__spark_libs__/hive-shims-0.23-2.3.7-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552918  524 -r-x------   1 root     root       533455 Nov 14 15:19 ./__spark_libs__/protobuf-java-2.5.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552919   92 -r-x------   1 root     root        93210 Nov 14 15:19 ./__spark_libs__/super-csv-2.2.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553166 1008 -r-x------   1 root     root      1029561 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-dms-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552920   84 -r-x------   1 root     root        83632 Nov 14 15:19 ./__spark_libs__/json4s-ast_2.12-3.6.6.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552921 1864 -r-x------   1 root     root      1908681 Nov 14 15:19 ./__spark_libs__/datanucleus-rdbms-4.1.19.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552922  188 -r-x------   1 root     root       190432 Nov 14 15:19 ./__spark_libs__/gson-2.2.4.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553167  408 -r-x------   1 root     root       416480 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-cloudwatch-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552923  552 -r-x------   1 root     root       561459 Nov 14 15:19 ./__spark_libs__/netlib-native_system-win-i686-1.1-natives.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552924 1140 -r-x------   1 root     root      1166647 Nov 14 15:19 ./__spark_libs__/jersey-common-2.30.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553168  400 -r-x------   1 root     root       408589 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-eks-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552925  308 -r-x------   1 root     root       313702 Nov 14 15:19 ./__spark_libs__/libfb303-0.9.3.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553169  360 -r-x------   1 root     root       366549 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-dax-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553170  364 -r-x------   1 root     root       369379 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-worklink-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552926  320 -r-x------   1 root     root       325335 Nov 14 15:19 ./__spark_libs__/RoaringBitmap-0.7.45.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553171 1796 -r-x------   1 root     root      1835621 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-dynamodb-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552927  708 -r-x------   1 root     root       723203 Nov 14 15:19 ./__spark_libs__/parquet-format-2.4.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553172  264 -r-x------   1 root     root       268887 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-opsworkscm-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552928  184 -r-x------   1 root     root       186708 Nov 14 15:19 ./__spark_libs__/avro-mapred-1.8.2-hadoop2.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553173   72 -r-x------   1 root     root        70061 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-pricing-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553174  512 -r-x------   1 root     root       520978 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-glacier-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552929   84 -r-x------   1 root     root        85902 Nov 14 15:19 ./__spark_libs__/hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552930  336 -r-x------   1 root     root       341862 Nov 14 15:19 ./__spark_libs__/jackson-module-scala_2.12-2.10.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553175   44 -r-x------   1 root     root        44094 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-marketplacecommerceanalytics-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552931  280 -r-x------   1 root     root       283653 Nov 14 15:19 ./__spark_libs__/curator-recipes-2.13.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553176  888 -r-x------   1 root     root       905885 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-emr-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552932   16 -r-x------   1 root     root        15169 Nov 14 15:19 ./__spark_libs__/spark-tags_2.12-3.0.0-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552933  176 -r-x------   1 root     root       176285 Nov 14 15:19 ./__spark_libs__/automaton-1.11-8.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552934   40 -r-x------   1 root     root        38370 Nov 14 15:19 ./__spark_libs__/hive-vector-code-gen-2.3.7-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552935   36 -r-x------   1 root     root        36175 Nov 14 15:19 ./__spark_libs__/json4s-jackson_2.12-3.6.6.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552936 8008 -r-x------   1 root     root      8198272 Nov 14 15:19 ./__spark_libs__/hive-metastore-2.3.7-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553177  216 -r-x------   1 root     root       218500 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-codegurureviewer-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552937 9292 -r-x------   1 root     root      9511792 Nov 14 15:19 ./__spark_libs__/spark-catalyst_2.12-3.0.0-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552938   60 -r-x------   1 root     root        59890 Nov 14 15:19 ./__spark_libs__/spark-kvstore_2.12-3.0.0-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552939 1144 -r-x------   1 root     root      1168113 Nov 14 15:19 ./__spark_libs__/algebra_2.12-2.0.0-M2.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552940  680 -r-x------   1 root     root       693271 Nov 14 15:19 ./__spark_libs__/spark-hive_2.12-3.0.0-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553178  240 -r-x------   1 root     root       245684 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-servicequotas-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553179  112 -r-x------   1 root     root       113976 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-elasticinference-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552941  132 -r-x------   1 root     root       131590 Nov 14 15:19 ./__spark_libs__/hk2-utils-2.6.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552942  164 -r-x------   1 root     root       167761 Nov 14 15:19 ./__spark_libs__/antlr-runtime-3.5.2.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552943   72 -r-x------   1 root     root        71626 Nov 14 15:19 ./__spark_libs__/commons-compiler-3.0.16.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552944 1076 -r-x------   1 root     root      1098934 Nov 14 15:19 ./__spark_libs__/parquet-column-1.10.1-spark-amzn-2.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553180  700 -r-x------   1 root     root       714238 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-robomaker-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552945  280 -r-x------   1 root     root       286277 Nov 14 15:19 ./__spark_libs__/parquet-hadoop-1.10.1-spark-amzn-2.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552946 2140 -r-x------   1 root     root      2189117 Nov 14 15:19 ./__spark_libs__/guava-14.0.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552947   36 -r-x------   1 root     root        33786 Nov 14 15:19 ./__spark_libs__/machinist_2.12-0.6.8.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553181  392 -r-x------   1 root     root       398971 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-fms-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552948  112 -r-x------   1 root     root       112235 Nov 14 15:19 ./__spark_libs__/scala-collection-compat_2.12-2.1.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552949  304 -r-x------   1 root     root       310891 Nov 14 15:19 ./__spark_libs__/netlib-native_system-linux-armhf-1.1-natives.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553182  148 -r-x------   1 root     root       149482 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-codestarconnections-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552950  120 -r-x------   1 root     root       120316 Nov 14 15:19 ./__spark_libs__/json-smart-2.3.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552951   20 -r-x------   1 root     root        16537 Nov 14 15:19 ./__spark_libs__/jcl-over-slf4j-1.7.30.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553183  468 -r-x------   1 root     root       477556 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-machinelearning-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552952  908 -r-x------   1 root     root       927721 Nov 14 15:19 ./__spark_libs__/jersey-server-2.30.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552953  100 -r-x------   1 root     root       102174 Nov 14 15:19 ./__spark_libs__/kerby-asn1-1.0.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553184  728 -r-x------   1 root     root       741552 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-elasticsearch-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553185  624 -r-x------   1 root     root       637712 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-pinpointemail-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552954  344 -r-x------   1 root     root       348635 Nov 14 15:19 ./__spark_libs__/jackson-core-2.10.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552955 1404 -r-x------   1 root     root      1437215 Nov 14 15:19 ./__spark_libs__/arrow-vector-0.15.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553186  388 -r-x------   1 root     root       396097 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-elasticloadbalancing-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552956   16 -r-x------   1 root     root        15071 Nov 14 15:19 ./__spark_libs__/transaction-api-1.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553187  364 -r-x------   1 root     root       370023 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-ram-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553188  356 -r-x------   1 root     root       364262 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-globalaccelerator-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552957   48 -r-x------   1 root     root        46646 Nov 14 15:19 ./__spark_libs__/jackson-dataformat-yaml-2.10.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1552958  264 -r-x------   1 root     root       268780 Nov 14 15:19 ./__spark_libs__/jline-2.14.6.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553189  204 -r-x------   1 root     root       205056 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-codestarnotifications-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553191    4 drwx------   3 root     root         4096 Nov 14 15:19 ./__spark_conf__
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553220    4 -r-x------   1 root     root          934 Nov 14 15:19 ./__spark_conf__/__spark_dist_cache__.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553193    4 -r-x------   1 root     root           10 Nov 14 15:19 ./__spark_conf__/metrics.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553218  128 -r-x------   1 root     root       130394 Nov 14 15:19 ./__spark_conf__/__spark_hadoop_conf__.xml
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553192    4 -r-x------   1 root     root           10 Nov 14 15:19 ./__spark_conf__/log4j.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553194    4 drwx------   2 root     root         4096 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553206    4 -r-x------   1 root     root         2979 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/hdfs-site.xml
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553202    4 -r-x------   1 root     root         1162 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/core-site.xml
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553211    4 -r-x------   1 root     root         2316 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/ssl-client.xml.example
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553204    4 -r-x------   1 root     root         3593 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/container-log4j.properties.default
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553217    8 -r-x------   1 root     root         4113 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/mapred-queues.xml.template
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553210    4 -r-x------   1 root     root          188 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/hive-site.xml
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553199    4 -r-x------   1 root     root         1976 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/yarn-site.xml
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553198    4 -r-x------   1 root     root         3321 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/hadoop-metrics2.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553207    8 -r-x------   1 root     root         6174 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/yarn-env.sh
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553215    4 -r-x------   1 root     root         2697 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/ssl-server.xml.example
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553212   16 -r-x------   1 root     root        14890 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/log4j.properties.default
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553200   12 -r-x------   1 root     root         8260 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/capacity-scheduler.xml.default
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553197   20 -r-x------   1 root     root        16406 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/hadoop-env.sh
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553209    4 -r-x------   1 root     root         1764 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/mapred-env.sh
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553196    4 -r-x------   1 root     root          758 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/mapred-site.xml
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553195   16 -r-x------   1 root     root        14900 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/log4j.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553203   12 -r-x------   1 root     root         8260 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/capacity-scheduler.xml
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553208    4 -r-x------   1 root     root         1940 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/container-executor.cfg
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/directory.info] 1553216    4 -r-x------   1 root     root         121/11/14 15:19:49 INFO Client: Application report for application_1636903150985_0001 (state: ACCEPTED)
[36malgo-1    |[0m 21/11/14 15:19:50 INFO Client: Application report for application_1636903150985_0001 (state: ACCEPTED)
[36malgo-1    |[0m 2021-11-14 15:19:50,054 INFO ipc.Server: Auth successful for appattempt_1636903150985_0001_000001 (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 15:19:50,082 INFO resourcemanager.DefaultAMSProcessor: AM registration appattempt_1636903150985_0001_000001
[36malgo-1    |[0m 2021-11-14 15:19:50,083 INFO resourcemanager.RMAuditLogger: USER=root	IP=192.168.240.3	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1636903150985_0001	APPATTEMPTID=appattempt_1636903150985_0001_000001
[36malgo-1    |[0m 2021-11-14 15:19:50,083 INFO attempt.RMAppAttemptImpl: appattempt_1636903150985_0001_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
[36malgo-1    |[0m 2021-11-14 15:19:50,083 INFO rmapp.RMAppImpl: application_1636903150985_0001 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
[36malgo-1    |[0m 21/11/14 15:19:50 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1636903150985_0001), /proxy/application_1636903150985_0001
[36malgo-1    |[0m 21/11/14 15:19:51 INFO Client: Application report for application_1636903150985_0001 (state: RUNNING)
[36malgo-1    |[0m 21/11/14 15:19:51 INFO Client: 
[36malgo-1    |[0m 	 client token: N/A
[36malgo-1    |[0m 	 diagnostics: N/A
[36malgo-1    |[0m 	 ApplicationMaster host: 192.168.240.3
[36malgo-1    |[0m 	 ApplicationMaster RPC port: -1
[36malgo-1    |[0m 	 queue: default
[36malgo-1    |[0m 	 start time: 1636903182895
[36malgo-1    |[0m 	 final status: UNDEFINED
[36malgo-1    |[0m 	 tracking URL: http://algo-1:8088/proxy/application_1636903150985_0001/
[36malgo-1    |[0m 	 user: root
[36malgo-1    |[0m 21/11/14 15:19:51 INFO YarnClientSchedulerBackend: Application application_1636903150985_0001 has started running.
[36malgo-1    |[0m 21/11/14 15:19:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33331.
[36malgo-1    |[0m 21/11/14 15:19:51 INFO NettyBlockTransferService: Server created on 192.168.240.3:33331
[36malgo-1    |[0m 21/11/14 15:19:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[36malgo-1    |[0m 21/11/14 15:19:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.240.3, 33331, None)
[36malgo-1    |[0m 21/11/14 15:19:51 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.240.3:33331 with 1007.8 MiB RAM, BlockManagerId(driver, 192.168.240.3, 33331, None)
[36malgo-1    |[0m 21/11/14 15:19:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.240.3, 33331, None)
[36malgo-1    |[0m 21/11/14 15:19:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.240.3, 33331, None)
[36malgo-1    |[0m 21/11/14 15:19:51 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
[36malgo-1    |[0m 21/11/14 15:19:51 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 15:19:51 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[36malgo-1    |[0m 21/11/14 15:19:51 INFO SharedState: loading hive config file: file:/etc/spark/conf.dist/hive-site.xml
[36malgo-1    |[0m 21/11/14 15:19:51 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/lib/spark/spark-warehouse').
[36malgo-1    |[0m 21/11/14 15:19:51 INFO SharedState: Warehouse path is 'file:/usr/lib/spark/spark-warehouse'.
[36malgo-1    |[0m 21/11/14 15:19:51 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 15:19:51 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 15:19:51 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 15:19:51 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 15:19:51 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr] Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr] 21/11/14 15:19:48 INFO SignalUtils: Registered signal handler for TERM
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr] 21/11/14 15:19:48 INFO SignalUtils: Registered signal handler for HUP
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr] 21/11/14 15:19:48 INFO SignalUtils: Registered signal handler for INT
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr] 21/11/14 15:19:49 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr] 21/11/14 15:19:49 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr] 21/11/14 15:19:49 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr] 21/11/14 15:19:49 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr] 21/11/14 15:19:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr] 21/11/14 15:19:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr] 21/11/14 15:19:49 INFO ApplicationMaster: ApplicationAttemptId: appattempt_1636903150985_0001_000001
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr] 21/11/14 15:19:49 INFO RMProxy: Connecting to ResourceManager at /192.168.240.3:8030
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr] 21/11/14 15:19:49 INFO YarnRMClient: Registering the ApplicationMaster
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr] 21/11/14 15:19:50 INFO TransportClientFactory: Successfully created connection to /192.168.240.3:43007 after 82 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr] 21/11/14 15:19:50 INFO ApplicationMaster: Preparing Local resources
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr] 21/11/14 15:19:51 INFO ApplicationMaster: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr] ===============================================================================
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr] Default YARN executor launch context:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]   env:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]     CLASSPATH -> /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]     SPARK_YARN_STAGING_DIR -> hdfs://192.168.240.3/user/root/.sparkStaging/application_1636903150985_0001
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]     SPARK_USER -> root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]     PYTHONPATH -> {{PWD}}/__pyfiles__<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9-src.zip
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]   command:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]     LD_LIBRARY_PATH=\"/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:$LD_LIBRARY_PATH\" \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       {{JAVA_HOME}}/bin/java \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       -server \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       -Xmx2048m \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       '-verbose:gc' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       '-XX:OnOutOfMemoryError=kill -9 %p' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       '-XX:+PrintGCDetails' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       '-XX:+PrintGCDateStamps' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       '-XX:+UseParallelGC' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       '-XX:InitiatingHeapOccupancyPercent=70' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       '-XX:ConcGCThreads=1' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       '-XX:ParallelGCThreads=3' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       -Djava.io.tmpdir={{PWD}}/tmp \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       '-Dspark.rpc.askTimeout=300s' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       '-Dspark.driver.port=43007' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       -Dspark.yarn.app.container.log.dir=<LOG_DIR> \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       org.apache.spark.executor.YarnCoarseGrainedExecutorBackend \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       --driver-url \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       spark://CoarseGrainedScheduler@192.168.240.3:43007 \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       --executor-id \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       <executorId> \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       --hostname \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       <hostname> \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       --cores \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       1 \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       --app-id \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       application_1636903150985_0001 \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       --resourceProfileId \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       0 \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       --user-class-path \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       file:$PWD/__app__.jar \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       1><LOG_DIR>/stdout \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]       2><LOG_DIR>/stderr
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]   resources:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]     __spark_conf__ -> resource { scheme: "hdfs" host: "192.168.240.3" port: -1 file: "/user/root/.sparkStaging/application_1636903150985_0001/__spark_conf__.zip" } size: 252608 timestamp: 1636903182758 type: ARCHIVE visibility: PRIVATE
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]     pyspark.zip -> resource { scheme: "hdfs" host: "192.168.240.3" port: -1 file: "/user/root/.sparkStaging/application_1636903150985_0001/pyspark.zip" } size: 732492 timestamp: 1636903182581 type: FILE visibility: PRIVATE
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]     py4j-0.10.9-src.zip -> resource { scheme: "hdfs" host: "192.168.240.3" port: -1 file: "/user/root/.sparkStaging/application_1636903150985_0001/py4j-0.10.9-src.zip" } size: 41587 timestamp: 1636903182605 type: FILE visibility: PRIVATE
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]     __pyfiles__/hello_py_spark_udfs.py -> resource { scheme: "hdfs" host: "192.168.240.3" port: -1 file: "/user/root/.sparkStaging/application_1636903150985_0001/hello_py_spark_udfs.py" } size: 6184 timestamp: 1636903182629 type: FILE visibility: PRIVATE
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000001/stderr]     __spark_libs__ -> resource { scheme: "hdfs" host: "192.168.240.3" port: -1 file: "/user/root/.sparkStaging/application_1636903150985_0001/__spark_libs__1565290401838612967.zip" } size: 397064094 timestamp: 1636903182473 type: A2021-11-14 15:19:52,013 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1636903150985_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=
[36malgo-1    |[0m 2021-11-14 15:19:52,014 INFO rmcontainer.RMContainerImpl: container_1636903150985_0001_01_000002 Container Transitioned from NEW to ALLOCATED
[36malgo-1    |[0m 2021-11-14 15:19:52,014 INFO fica.FiCaSchedulerNode: Assigned container container_1636903150985_0001_01_000002 of capacity <memory:3287, vCores:1> on host algo-2:42603, which has 1 containers, <memory:3287, vCores:1> used and <memory:12605, vCores:3> available after allocation
[36malgo-1    |[0m 2021-11-14 15:19:52,014 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636903150985_0001	CONTAINERID=container_1636903150985_0001_01_000002	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 15:19:52,014 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.1316071 absoluteUsedCapacity=0.1316071 used=<memory:4183, vCores:2> cluster=<memory:31784, vCores:8>
[36malgo-1    |[0m 2021-11-14 15:19:52,014 INFO capacity.CapacityScheduler: Allocation proposal accepted
[36malgo-1    |[0m 2021-11-14 15:19:52,030 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1636903150985_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=
[36malgo-1    |[0m 2021-11-14 15:19:52,030 INFO rmcontainer.RMContainerImpl: container_1636903150985_0001_01_000003 Container Transitioned from NEW to ALLOCATED
[36malgo-1    |[0m 2021-11-14 15:19:52,030 INFO fica.FiCaSchedulerNode: Assigned container container_1636903150985_0001_01_000003 of capacity <memory:3287, vCores:1> on host algo-1:44133, which has 2 containers, <memory:4183, vCores:2> used and <memory:11709, vCores:2> available after allocation
[36malgo-1    |[0m 2021-11-14 15:19:52,031 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636903150985_0001	CONTAINERID=container_1636903150985_0001_01_000003	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 15:19:52,031 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.23502392 absoluteUsedCapacity=0.23502392 used=<memory:7470, vCores:3> cluster=<memory:31784, vCores:8>
[36malgo-1    |[0m 2021-11-14 15:19:52,031 INFO capacity.CapacityScheduler: Allocation proposal accepted
[36malgo-1    |[0m 2021-11-14 15:19:52,058 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-2:42603 for container : container_1636903150985_0001_01_000002
[36malgo-1    |[0m 2021-11-14 15:19:52,059 INFO rmcontainer.RMContainerImpl: container_1636903150985_0001_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
[36malgo-1    |[0m 2021-11-14 15:19:52,060 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:44133 for container : container_1636903150985_0001_01_000003
[36malgo-1    |[0m 2021-11-14 15:19:52,061 INFO rmcontainer.RMContainerImpl: container_1636903150985_0001_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
[36malgo-1    |[0m 2021-11-14 15:19:52,189 INFO ipc.Server: Auth successful for appattempt_1636903150985_0001_000001 (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 15:19:52,193 INFO containermanager.ContainerManagerImpl: Start request for container_1636903150985_0001_01_000003 by user root
[36malgo-1    |[0m 2021-11-14 15:19:52,195 INFO nodemanager.NMAuditLogger: USER=root	IP=192.168.240.3	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1636903150985_0001	CONTAINERID=container_1636903150985_0001_01_000003
[36malgo-1    |[0m 2021-11-14 15:19:52,195 INFO application.ApplicationImpl: Adding container_1636903150985_0001_01_000003 to application application_1636903150985_0001
[36malgo-1    |[0m 2021-11-14 15:19:52,196 INFO container.ContainerImpl: Container container_1636903150985_0001_01_000003 transitioned from NEW to LOCALIZING
[36malgo-1    |[0m 2021-11-14 15:19:52,196 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1636903150985_0001
[36malgo-1    |[0m 2021-11-14 15:19:52,197 INFO container.ContainerImpl: Container container_1636903150985_0001_01_000003 transitioned from LOCALIZING to SCHEDULED
[36malgo-1    |[0m 2021-11-14 15:19:52,197 INFO scheduler.ContainerScheduler: Starting container [container_1636903150985_0001_01_000003]
[36malgo-1    |[0m Created spark context
[36malgo-1    |[0m Created sql context
[36malgo-1    |[0m 2021-11-14 15:19:52,211 INFO container.ContainerImpl: Container container_1636903150985_0001_01_000003 transitioned from SCHEDULED to RUNNING
[36malgo-1    |[0m 2021-11-14 15:19:52,211 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1636903150985_0001_01_000003
[36malgo-1    |[0m 2021-11-14 15:19:52,214 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/default_container_executor.sh]
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/prelaunch.out
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/prelaunch.err
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/launch_container.sh
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stdout
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr
[33malgo-2    |[0m 2021-11-14 15:19:52,264 INFO ipc.Server: Auth successful for appattempt_1636903150985_0001_000001 (auth:SIMPLE)
[36malgo-1    |[0m 21/11/14 15:19:52 INFO InMemoryFileIndex: It took 29 ms to list leaf files for 1 paths.
[33malgo-2    |[0m 2021-11-14 15:19:52,344 INFO containermanager.ContainerManagerImpl: Start request for container_1636903150985_0001_01_000002 by user root
[33malgo-2    |[0m 2021-11-14 15:19:52,392 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1636903150985_0001
[36malgo-1    |[0m 21/11/14 15:19:52 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[33malgo-2    |[0m 2021-11-14 15:19:52,400 INFO nodemanager.NMAuditLogger: USER=root	IP=192.168.240.3	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1636903150985_0001	CONTAINERID=container_1636903150985_0001_01_000002
[33malgo-2    |[0m 2021-11-14 15:19:52,401 INFO application.ApplicationImpl: Application application_1636903150985_0001 transitioned from NEW to INITING
[33malgo-2    |[0m 2021-11-14 15:19:52,401 INFO application.ApplicationImpl: Adding container_1636903150985_0001_01_000002 to application application_1636903150985_0001
[33malgo-2    |[0m 2021-11-14 15:19:52,406 INFO application.ApplicationImpl: Application application_1636903150985_0001 transitioned from INITING to RUNNING
[33malgo-2    |[0m 2021-11-14 15:19:52,410 INFO container.ContainerImpl: Container container_1636903150985_0001_01_000002 transitioned from NEW to LOCALIZING
[33malgo-2    |[0m 2021-11-14 15:19:52,411 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1636903150985_0001
[33malgo-2    |[0m 2021-11-14 15:19:52,422 INFO localizer.ResourceLocalizationService: Created localizer for container_1636903150985_0001_01_000002
[33malgo-2    |[0m 2021-11-14 15:19:52,493 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1636903150985_0001_01_000002.tokens
[33malgo-2    |[0m 2021-11-14 15:19:52,503 INFO nodemanager.DefaultContainerExecutor: Initializing user root
[33malgo-2    |[0m 2021-11-14 15:19:52,510 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1636903150985_0001_01_000002.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002.tokens
[33malgo-2    |[0m 2021-11-14 15:19:52,510 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001
[36malgo-1    |[0m 2021-11-14 15:19:53,025 INFO rmcontainer.RMContainerImpl: container_1636903150985_0001_01_000002 Container Transitioned from ACQUIRED to RUNNING
[36malgo-1    |[0m 2021-11-14 15:19:53,031 INFO rmcontainer.RMContainerImpl: container_1636903150985_0001_01_000003 Container Transitioned from ACQUIRED to RUNNING
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1551186 1364 -r-x------   1 root     root      1393617 Nov 14 15:19 ./__spark_libs__/hadoop-yarn-server-common-3.2.1-amzn-1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553103 1192 -r-x------   1 root     root      1220227 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-servicecatalog-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553104 1068 -r-x------   1 root     root      1091650 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-opsworks-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553105  180 -r-x------   1 root     root       182585 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-autoscalingplans-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553106 1000 -r-x------   1 root     root      1023417 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-macie2-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1551187 2252 -r-x------   1 root     root      2305169 Nov 14 15:19 ./__spark_libs__/netlib-native_ref-win-x86_64-1.1-natives.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1551188  544 -r-x------   1 root     root       553993 Nov 14 15:19 ./__spark_libs__/netlib-native_system-osx-x86_64-1.1-natives.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1551189   12 -r-x------   1 root     root         9498 Nov 14 15:19 ./__spark_libs__/spark-tags_2.12-3.0.0-amzn-0-tests.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1551190   28 -r-x------   1 root     root        27006 Nov 14 15:19 ./__spark_libs__/aopalliance-repackaged-2.6.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552848  116 -r-x------   1 root     root       116120 Nov 14 15:19 ./__spark_libs__/kerb-crypto-1.0.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553107  208 -r-x------   1 root     root       208995 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-iot1clickprojects-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553108 2484 -r-x------   1 root     root      2541217 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-ssm-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552849   64 -r-x------   1 root     root        65464 Nov 14 15:19 ./__spark_libs__/kerb-common-1.0.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552850  200 -r-x------   1 root     root       204650 Nov 14 15:19 ./__spark_libs__/kerby-pkix-1.0.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553109  716 -r-x------   1 root     root       729294 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-organizations-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552851  504 -r-x------   1 root     root       512742 Nov 14 15:19 ./__spark_libs__/woodstox-core-5.0.3.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552852 2344 -r-x------   1 root     root      2397719 Nov 14 15:19 ./__spark_libs__/spark-network-common_2.12-3.0.0-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552853  360 -r-x------   1 root     root       365552 Nov 14 15:19 ./__spark_libs__/commons-compress-1.8.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553110  804 -r-x------   1 root     root       819646 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-directory-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552854  120 -r-x------   1 root     root       120527 Nov 14 15:19 ./__spark_libs__/hive-shims-common-2.3.7-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553111  344 -r-x------   1 root     root       348636 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-transcribe-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552855 10424 -r-x------   1 root     root     10672015 Nov 14 15:19 ./__spark_libs__/scala-compiler-2.12.10.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552856  656 -r-x------   1 root     root       668235 Nov 14 15:19 ./__spark_libs__/guice-4.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553112 1100 -r-x------   1 root     root      1122498 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-securityhub-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552857 1468 -r-x------   1 root     root      1502280 Nov 14 15:19 ./__spark_libs__/htrace-core4-4.1.0-incubating.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553113  176 -r-x------   1 root     root       177649 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-iot1clickdevices-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552858 3152 -r-x------   1 root     root      3226851 Nov 14 15:19 ./__spark_libs__/cats-kernel_2.12-2.0.0-M4.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552859 3596 -r-x------   1 root     root      3678534 Nov 14 15:19 ./__spark_libs__/scala-reflect-2.12.10.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552860   32 -r-x------   1 root     root        30674 Nov 14 15:19 ./__spark_libs__/kerby-config-1.0.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552861  300 -r-x------   1 root     root       305001 Nov 14 15:19 ./__spark_libs__/commons-httpclient-3.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553114  132 -r-x------   1 root     root       131735 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-ioteventsdata-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552862   56 -r-x------   1 root     root        55236 Nov 14 15:19 ./__spark_libs__/geronimo-jcache_1.0_spec-1.0-alpha-1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552863  224 -r-x------   1 root     root       226672 Nov 14 15:19 ./__spark_libs__/kerb-core-1.0.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553115  140 -r-x------   1 root     root       139799 Nov 14 15:19 ./__spark_libs__/aws-glue-datacatalog-hive3-client-3.0.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553116   48 -r-x------   1 root     root        47606 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-cloudwatchmetrics-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553117  404 -r-x------   1 root     root       411539 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-servicediscovery-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553118 1876 -r-x------   1 root     root      1918555 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-medialive-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552864 1712 -r-x------   1 root     root      1749371 Nov 14 15:19 ./__spark_libs__/netlib-native_ref-linux-x86_64-1.1-natives.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553119  252 -r-x------   1 root     root       254750 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-cognitosync-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552865 13504 -r-x------   1 root     root     13826799 Nov 14 15:19 ./__spark_libs__/breeze_2.12-1.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552866   96 -r-x------   1 root     root        96221 Nov 14 15:19 ./__spark_libs__/commons-pool-1.5.4.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552867 1688 -r-x------   1 root     root      1726527 Nov 14 15:19 ./__spark_libs__/ehcache-3.3.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553120  392 -r-x------   1 root     root       398329 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-elastictranscoder-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552868   92 -r-x------   1 root     root        91930 Nov 14 15:19 ./__spark_libs__/jakarta.validation-api-2.0.2.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552869   28 -r-x------   1 root     root        28355 Nov 14 15:19 ./__spark_libs__/spark-ganglia-lgpl_2.12-3.0.0-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553121  684 -r-x------   1 root     root       699987 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-personalize-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552870   44 -r-x------   1 root     root        44513 Nov 14 15:19 ./__spark_libs__/gmetric4j-1.0.10.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552871   20 -r-x------   1 root     root        19479 Nov 14 15:19 ./__spark_libs__/osgi-resource-locator-1.0.3.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552872  900 -r-x------   1 root     root       918300 Nov 14 15:19 ./__spark_libs__/hive-serde-2.3.7-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553122  192 -r-x------   1 root     root       194406 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-cloudhsmv2-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553123 1040 -r-x------   1 root     root      1061132 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-s3-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552873   16 -r-x------   1 root     root        14395 Nov 14 15:19 ./__spark_libs__/generex-1.0.2.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553124  728 -r-x------   1 root     root       743779 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-sesv2-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552874  908 -r-x------   1 root     root       926574 Nov 14 15:19 ./__spark_libs__/janino-3.0.16.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553125  136 -r-x------   1 root     root       138924 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-marketplacecatalog-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553126  488 -r-x------   1 root     root       496063 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-ecr-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553127  248 -r-x------   1 root     root       252611 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-computeoptimizer-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552875   76 -r-x------   1 root     root        76983 Nov 14 15:19 ./__spark_libs__/guice-servlet-4.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553128  616 -r-x------   1 root     root       629711 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-kendra-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552876   12 -r-x------   1 root     root        12211 Nov 14 15:19 ./__spark_libs__/slf4j-log4j12-1.7.30.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553129  764 -r-x------   1 root     root       779663 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-codepipeline-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552877  232 -r-x------   1 root     root       234942 Nov 14 15:19 ./__spark_libs__/hive-storage-api-2.7.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553130  948 -r-x------   1 root     root       966965 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-apigatewayv2-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552878  432 -r-x------   1 root     root       438843 Nov 14 15:19 ./__spark_libs__/hive-common-2.3.7-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552879  200 -r-x------   1 root     root       201965 Nov 14 15:19 ./__spark_libs__/curator-framework-2.13.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553131  368 -r-x------   1 root     root       374969 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-codeguruprofiler-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553132  296 -r-x------   1 root     root       300742 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-mediapackagevod-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552880   56 -r-x------   1 root     root        54509 Nov 14 15:19 ./__spark_libs__/native_system-java-1.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552881  244 -r-x------   1 root     root       249790 Nov 14 15:19 ./__spark_libs__/javax.jdo-3.2.0-m3.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553133  384 -r-x------   1 root     root       393130 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-networkmanager-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553134  104 -r-x------   1 root     root       103083 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-outposts-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552882  576 -r-x------   1 root     root       588337 Nov 14 15:19 ./__spark_libs__/commons-collections-3.2.2.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552883   28 -r-x------   1 root     root        25475 Nov 14 15:19 ./__spark_libs__/json-1.8.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553135  620 -r-x------   1 root     root       632485 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-lexmodelbuilding-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553136  228 -r-x------   1 root     root       229423 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-applicationautoscaling-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553137 1092 -r-x------   1 root     root      1117335 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-devicefarm-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553138  240 -r-x------   1 root     root       244715 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-lakeformation-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552884   24 -r-x------   1 root     root        24239 Nov 14 15:19 ./__spark_libs__/commons-daemon-1.0.13.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553139 2800 -r-x------   1 root     root      2864744 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-sagemaker-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552885   72 -r-x------   1 root     root        73349 Nov 14 15:19 ./__spark_libs__/jersey-container-servlet-core-2.30.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553140  552 -r-x------   1 root     root       563962 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-logs-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552886 1988 -r-x------   1 root     root      2035066 Nov 14 15:19 ./__spark_libs__/commons-math3-3.4.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553141  464 -r-x------   1 root     root       473688 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-kinesisvideo-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552887   68 -r-x------   1 root     root        69409 Nov 14 15:19 ./__spark_libs__/activation-1.1.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552888  416 -r-x------   1 root     root       423175 Nov 14 15:19 ./__spark_libs__/okhttp-3.12.6.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553142  776 -r-x------   1 root     root       791754 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-ses-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552889  220 -r-x------   1 root     root       222980 Nov 14 15:19 ./__spark_libs__/scala-parser-combinators_2.12-1.1.2.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553143   84 -r-x------   1 root     root        82717 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-pi-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552890  788 -r-x------   1 root     root       805848 Nov 14 15:19 ./__spark_libs__/hadoop-mapreduce-client-common-3.2.1-amzn-1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553144  288 -r-x------   1 root     root       293007 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-licensemanager-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552891   56 -r-x------   1 root     root        54391 Nov 14 15:19 ./__spark_libs__/objenesis-2.5.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553145   56 -r-x------   1 root     root        53254 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-apigatewaymanagementapi-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553146  628 -r-x------   1 root     root       641941 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-stepfunctions-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553147 1808 -r-x------   1 root     root      1848502 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-waf-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553148 1824 -r-x------   1 root     root      1866513 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-lightsail-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553149  396 -r-x------   1 root     root       403163 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-discovery-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553150   96 -r-x------   1 root     root        97822 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-augmentedairuntime-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553151  544 -r-x------   1 root     root       554655 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-elasticloadbalancingv2-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552892   28 -r-x------   1 root     root        27749 Nov 14 15:19 ./__spark_libs__/orc-shims-1.5.10.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552893  320 -r-x------   1 root     root       326874 Nov 14 15:19 ./__spark_libs__/httpcore-4.4.11.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552894  244 -r-x------   1 root     root       246918 Nov 14 15:19 ./__spark_libs__/commons-beanutils-1.9.4.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552895   36 -r-x------   1 root     root        35518 Nov 14 15:19 ./__spark_libs__/zjsonpatch-0.3.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552896  448 -r-x------   1 root     root       458605 Nov 14 15:19 ./__spark_libs__/netlib-native_system-linux-x86_64-1.1-natives.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553152  308 -r-x------   1 root     root       315095 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-s3control-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552897   84 -r-x------   1 root     root        85815 Nov 14 15:19 ./__spark_libs__/jersey-media-jaxb-2.30.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552898  604 -r-x------   1 root     root       616888 Nov 14 15:19 ./__spark_libs__/commons-configuration2-2.1.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553153 1436 -r-x------   1 root     root      1469407 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-chime-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552899   20 -r-x------   1 root     root        20046 Nov 14 15:19 ./__spark_libs__/kerb-identity-1.0.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552900   76 -r-x------   1 root     root        76733 Nov 14 15:19 ./__spark_libs__/jersey-hk2-2.30.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553154 40140 -r-x------   1 root     root     41099632 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-models-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552901  100 -r-x------   1 root     root       100636 Nov 14 15:19 ./__spark_libs__/jsp-api-2.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553155 1028 -r-x------   1 root     root      1051487 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-codedeploy-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552902   36 -r-x------   1 root     root        34654 Nov 14 15:19 ./__spark_libs__/paranamer-2.8.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553156  404 -r-x------   1 root     root       413024 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-managedblockchain-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553157  492 -r-x------   1 root     root       501832 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-amplify-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553158   44 -r-x------   1 root     root        43408 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-marketplaceentitlement-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552903   68 -r-x------   1 root     root        67897 Nov 14 15:19 ./__spark_libs__/jackson-annotations-2.10.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552904 1168 -r-x------   1 root     root      1194003 Nov 14 15:19 ./__spark_libs__/arpack_combined_all-0.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552905 5752 -r-x------   1 root     root      5886128 Nov 14 15:19 ./__spark_libs__/spark-mllib_2.12-3.0.0-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552906    8 -r-x------   1 root     root         4467 Nov 14 15:19 ./__spark_libs__/aopalliance-1.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552907  624 -r-x------   1 root     root       637428 Nov 14 15:19 ./__spark_libs__/netlib-native_system-win-x86_64-1.1-natives.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552908   28 -r-x------   1 root     root        26514 Nov 14 15:19 ./__spark_libs__/stax-api-1.0.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552909   80 -r-x------   1 root     root        79588 Nov 14 15:19 ./__spark_libs__/spire-macros_2.12-0.17.0-M1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552910 4064 -r-x------   1 root     root      4158923 Nov 14 15:19 ./__spark_libs__/hadoop-common-3.2.1-amzn-1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552911  332 -r-x------   1 root     root       336803 Nov 14 15:19 ./__spark_libs__/antlr4-runtime-4.7.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553159  896 -r-x------   1 root     root       913853 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-simpleworkflow-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552912  132 -r-x------   1 root     root       134696 Nov 14 15:19 ./__spark_libs__/breeze-macros_2.12-1.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553160  276 -r-x------   1 root     root       281444 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-codestar-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553161 1464 -r-x------   1 root     root      1498425 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-codecommit-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553162 1644 -r-x------   1 root     root      1683061 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-quicksight-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552913   28 -r-x------   1 root     root        27156 Nov 14 15:19 ./__spark_libs__/istack-commons-runtime-3.0.8.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552914 1112 -r-x------   1 root     root      1137921 Nov 14 15:19 ./__spark_libs__/spark-streaming_2.12-3.0.0-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553163  284 -r-x------   1 root     root       290178 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-datapipeline-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553164  696 -r-x------   1 root     root       711839 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-elasticbeanstalk-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552915  268 -r-x------   1 root     root       273370 Nov 14 15:19 ./__spark_libs__/commons-net-3.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553165  804 -r-x------   1 root     root       822890 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-kinesisanalyticsv2-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552916  228 -r-x------   1 root     root       232248 Nov 14 15:19 ./__spark_libs__/jackson-core-asl-1.9.13.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552917   56 -r-x------   1 root     root        56273 Nov 14 15:19 ./__spark_libs__/hive-shims-0.23-2.3.7-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552918  524 -r-x------   1 root     root       533455 Nov 14 15:19 ./__spark_libs__/protobuf-java-2.5.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552919   92 -r-x------   1 root     root        93210 Nov 14 15:19 ./__spark_libs__/super-csv-2.2.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553166 1008 -r-x------   1 root     root      1029561 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-dms-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552920   84 -r-x------   1 root     root        83632 Nov 14 15:19 ./__spark_libs__/json4s-ast_2.12-3.6.6.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552921 1864 -r-x------   1 root     root      1908681 Nov 14 15:19 ./__spark_libs__/datanucleus-rdbms-4.1.19.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552922  188 -r-x------   1 root     root       190432 Nov 14 15:19 ./__spark_libs__/gson-2.2.4.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553167  408 -r-x------   1 root     root       416480 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-cloudwatch-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552923  552 -r-x------   1 root     root       561459 Nov 14 15:19 ./__spark_libs__/netlib-native_system-win-i686-1.1-natives.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552924 1140 -r-x------   1 root     root      1166647 Nov 14 15:19 ./__spark_libs__/jersey-common-2.30.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553168  400 -r-x------   1 root     root       408589 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-eks-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552925  308 -r-x------   1 root     root       313702 Nov 14 15:19 ./__spark_libs__/libfb303-0.9.3.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553169  360 -r-x------   1 root     root       366549 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-dax-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553170  364 -r-x------   1 root     root       369379 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-worklink-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552926  320 -r-x------   1 root     root       325335 Nov 14 15:19 ./__spark_libs__/RoaringBitmap-0.7.45.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553171 1796 -r-x------   1 root     root      1835621 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-dynamodb-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552927  708 -r-x------   1 root     root       723203 Nov 14 15:19 ./__spark_libs__/parquet-format-2.4.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553172  264 -r-x------   1 root     root       268887 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-opsworkscm-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552928  184 -r-x------   1 root     root       186708 Nov 14 15:19 ./__spark_libs__/avro-mapred-1.8.2-hadoop2.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553173   72 -r-x------   1 root     root        70061 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-pricing-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553174  512 -r-x------   1 root     root       520978 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-glacier-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552929   84 -r-x------   1 root     root        85902 Nov 14 15:19 ./__spark_libs__/hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552930  336 -r-x------   1 root     root       341862 Nov 14 15:19 ./__spark_libs__/jackson-module-scala_2.12-2.10.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553175   44 -r-x------   1 root     root        44094 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-marketplacecommerceanalytics-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552931  280 -r-x------   1 root     root       283653 Nov 14 15:19 ./__spark_libs__/curator-recipes-2.13.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553176  888 -r-x------   1 root     root       905885 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-emr-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552932   16 -r-x------   1 root     root        15169 Nov 14 15:19 ./__spark_libs__/spark-tags_2.12-3.0.0-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552933  176 -r-x------   1 root     root       176285 Nov 14 15:19 ./__spark_libs__/automaton-1.11-8.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552934   40 -r-x------   1 root     root        38370 Nov 14 15:19 ./__spark_libs__/hive-vector-code-gen-2.3.7-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552935   36 -r-x------   1 root     root        36175 Nov 14 15:19 ./__spark_libs__/json4s-jackson_2.12-3.6.6.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552936 8008 -r-x------   1 root     root      8198272 Nov 14 15:19 ./__spark_libs__/hive-metastore-2.3.7-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553177  216 -r-x------   1 root     root       218500 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-codegurureviewer-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552937 9292 -r-x------   1 root     root      9511792 Nov 14 15:19 ./__spark_libs__/spark-catalyst_2.12-3.0.0-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552938   60 -r-x------   1 root     root        59890 Nov 14 15:19 ./__spark_libs__/spark-kvstore_2.12-3.0.0-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552939 1144 -r-x------   1 root     root      1168113 Nov 14 15:19 ./__spark_libs__/algebra_2.12-2.0.0-M2.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552940  680 -r-x------   1 root     root       693271 Nov 14 15:19 ./__spark_libs__/spark-hive_2.12-3.0.0-amzn-0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553178  240 -r-x------   1 root     root       245684 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-servicequotas-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553179  112 -r-x------   1 root     root       113976 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-elasticinference-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552941  132 -r-x------   1 root     root       131590 Nov 14 15:19 ./__spark_libs__/hk2-utils-2.6.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552942  164 -r-x------   1 root     root       167761 Nov 14 15:19 ./__spark_libs__/antlr-runtime-3.5.2.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552943   72 -r-x------   1 root     root        71626 Nov 14 15:19 ./__spark_libs__/commons-compiler-3.0.16.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552944 1076 -r-x------   1 root     root      1098934 Nov 14 15:19 ./__spark_libs__/parquet-column-1.10.1-spark-amzn-2.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553180  700 -r-x------   1 root     root       714238 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-robomaker-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552945  280 -r-x------   1 root     root       286277 Nov 14 15:19 ./__spark_libs__/parquet-hadoop-1.10.1-spark-amzn-2.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552946 2140 -r-x------   1 root     root      2189117 Nov 14 15:19 ./__spark_libs__/guava-14.0.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552947   36 -r-x------   1 root     root        33786 Nov 14 15:19 ./__spark_libs__/machinist_2.12-0.6.8.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553181  392 -r-x------   1 root     root       398971 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-fms-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552948  112 -r-x------   1 root     root       112235 Nov 14 15:19 ./__spark_libs__/scala-collection-compat_2.12-2.1.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552949  304 -r-x------   1 root     root       310891 Nov 14 15:19 ./__spark_libs__/netlib-native_system-linux-armhf-1.1-natives.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553182  148 -r-x------   1 root     root       149482 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-codestarconnections-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552950  120 -r-x------   1 root     root       120316 Nov 14 15:19 ./__spark_libs__/json-smart-2.3.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552951   20 -r-x------   1 root     root        16537 Nov 14 15:19 ./__spark_libs__/jcl-over-slf4j-1.7.30.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553183  468 -r-x------   1 root     root       477556 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-machinelearning-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552952  908 -r-x------   1 root     root       927721 Nov 14 15:19 ./__spark_libs__/jersey-server-2.30.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552953  100 -r-x------   1 root     root       102174 Nov 14 15:19 ./__spark_libs__/kerby-asn1-1.0.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553184  728 -r-x------   1 root     root       741552 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-elasticsearch-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553185  624 -r-x------   1 root     root       637712 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-pinpointemail-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552954  344 -r-x------   1 root     root       348635 Nov 14 15:19 ./__spark_libs__/jackson-core-2.10.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552955 1404 -r-x------   1 root     root      1437215 Nov 14 15:19 ./__spark_libs__/arrow-vector-0.15.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553186  388 -r-x------   1 root     root       396097 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-elasticloadbalancing-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552956   16 -r-x------   1 root     root        15071 Nov 14 15:19 ./__spark_libs__/transaction-api-1.1.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553187  364 -r-x------   1 root     root       370023 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-ram-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553188  356 -r-x------   1 root     root       364262 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-globalaccelerator-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552957   48 -r-x------   1 root     root        46646 Nov 14 15:19 ./__spark_libs__/jackson-dataformat-yaml-2.10.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1552958  264 -r-x------   1 root     root       268780 Nov 14 15:19 ./__spark_libs__/jline-2.14.6.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553189  204 -r-x------   1 root     root       205056 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-codestarnotifications-1.11.828.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553191    4 drwx------   3 root     root         4096 Nov 14 15:19 ./__spark_conf__
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553220    4 -r-x------   1 root     root          934 Nov 14 15:19 ./__spark_conf__/__spark_dist_cache__.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553193    4 -r-x------   1 root     root           10 Nov 14 15:19 ./__spark_conf__/metrics.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553218  128 -r-x------   1 root     root       130394 Nov 14 15:19 ./__spark_conf__/__spark_hadoop_conf__.xml
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553192    4 -r-x------   1 root     root           10 Nov 14 15:19 ./__spark_conf__/log4j.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553194    4 drwx------   2 root     root         4096 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553206    4 -r-x------   1 root     root         2979 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/hdfs-site.xml
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553202    4 -r-x------   1 root     root         1162 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/core-site.xml
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553211    4 -r-x------   1 root     root         2316 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/ssl-client.xml.example
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553204    4 -r-x------   1 root     root         3593 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/container-log4j.properties.default
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553217    8 -r-x------   1 root     root         4113 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/mapred-queues.xml.template
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553210    4 -r-x------   1 root     root          188 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/hive-site.xml
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553199    4 -r-x------   1 root     root         1976 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/yarn-site.xml
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553198    4 -r-x------   1 root     root         3321 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/hadoop-metrics2.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553207    8 -r-x------   1 root     root         6174 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/yarn-env.sh
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553215    4 -r-x------   1 root     root         2697 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/ssl-server.xml.example
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553212   16 -r-x------   1 root     root        14890 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/log4j.properties.default
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553200   12 -r-x------   1 root     root         8260 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/capacity-scheduler.xml.default
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553197   20 -r-x------   1 root     root        16406 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/hadoop-env.sh
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553209    4 -r-x------   1 root     root         1764 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/mapred-env.sh
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553196    4 -r-x------   1 root     root          758 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/mapred-site.xml
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553195   16 -r-x------   1 root     root        14900 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/log4j.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553203   12 -r-x------   1 root     root         8260 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/capacity-scheduler.xml
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/directory.info] 1553208    4 -r-x------   1 root     root         1940 Nov 14 15:19 ./__spark_conf__/__hadoo2021-11-14 15:19:53,983 INFO monitor.ContainersMonitorImpl: container_1636903150985_0001_01_000003's ip = 192.168.240.3, and hostname = algo-1
[36malgo-1    |[0m 2021-11-14 15:19:53,987 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1636903150985_0001_01_000003 since CPU usage is not yet available.
[36malgo-1    |[0m 21/11/14 15:19:54 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 1239, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[36malgo-1    |[0m 21/11/14 15:19:54 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 15:19:54 INFO FileSourceStrategy: Pushed Filters: 
[36malgo-1    |[0m 21/11/14 15:19:54 INFO FileSourceStrategy: Post-Scan Filters: 
[36malgo-1    |[0m 21/11/14 15:19:54 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[36malgo-1    |[0m 21/11/14 15:19:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 318.0 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 2021-11-14 15:19:55,091 INFO scheduler.AppSchedulingInfo: checking for deactivate of application :application_1636903150985_0001
[36malgo-1    |[0m 21/11/14 15:19:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:19:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.240.3:33331 (size: 30.3 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:19:55 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 15:19:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 15:19:55 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:53 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 1352@algo-1
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:53 INFO SignalUtils: Registered signal handler for TERM
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:53 INFO SignalUtils: Registered signal handler for HUP
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:53 INFO SignalUtils: Registered signal handler for INT
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:54 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:54 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:54 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:54 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:54 INFO TransportClientFactory: Successfully created connection to /192.168.240.3:43007 after 95 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:54 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:54 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:54 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:54 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:54 INFO TransportClientFactory: Successfully created connection to /192.168.240.3:43007 after 1 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:54 INFO DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/blockmgr-96dc2092-1109-45d9-a45e-8e77aa1f2092
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:54 INFO MemoryStore: MemoryStore started with capacity 921/11/14 15:19:55 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 15:19:55 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[36malgo-1    |[0m 21/11/14 15:19:55 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[36malgo-1    |[0m 21/11/14 15:19:55 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:19:55 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:19:55 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:19:55 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.240.3:40396) with ID 2
[36malgo-1    |[0m 21/11/14 15:19:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.6 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:19:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:19:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.240.3:33331 (size: 7.4 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:19:55 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:19:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 15:19:55 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 15:19:55 INFO BlockManagerMasterEndpoint: Registering block manager algo-1:33379 with 912.3 MiB RAM, BlockManagerId(2, algo-1, 33379, None)
[36malgo-1    |[0m 21/11/14 15:19:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7755 bytes)
[36malgo-1    |[0m 21/11/14 15:19:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:33379 (size: 7.4 KiB, free: 912.3 MiB)
[33malgo-2    |[0m 2021-11-14 15:19:55,999 INFO container.ContainerImpl: Container container_1636903150985_0001_01_000002 transitioned from LOCALIZING to SCHEDULED
[33malgo-2    |[0m 2021-11-14 15:19:56,000 INFO scheduler.ContainerScheduler: Starting container [container_1636903150985_0001_01_000002]
[33malgo-2    |[0m 2021-11-14 15:19:56,037 INFO container.ContainerImpl: Container container_1636903150985_0001_01_000002 transitioned from SCHEDULED to RUNNING
[33malgo-2    |[0m 2021-11-14 15:19:56,038 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1636903150985_0001_01_000002
[33malgo-2    |[0m 2021-11-14 15:19:56,041 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/default_container_executor.sh]
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/prelaunch.out
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/prelaunch.err
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/launch_container.sh
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stdout
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr
[36malgo-1    |[0m 12.3 MiB
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:55 INFO YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@192.168.240.3:43007
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:55 INFO ResourceUtils: ==============================================================
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:55 INFO ResourceUtils: Resources for spark.executor:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:55 INFO ResourceUtils: ==============================================================
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:55 INFO YarnCoarseGrainedExecutorBackend: Successfully registered with driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:55 INFO Executor: Starting executor ID 2 on host algo-1
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33379.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:55 INFO NettyBlockTransferService: Server created on algo-1:33379
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(2, algo-1, 33379, None)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(2, algo-1, 33379, None)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(2, algo-1, 33379, None)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:55 INFO MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:55 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:55 INFO MetricsSystemImpl: s3a-file-system metrics system started
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:55 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:55 INFO TorrentBroadcast: Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:55 INFO TransportClientFactory: Successfully created connection to /192.168.240.3:33331 after 1 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m [/var/log/yarn21/11/14 15:19:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:33379 (size: 30.3 KiB, free: 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553786  104 -r-x------   1 root     root       103083 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-outposts-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553529  576 -r-x------   1 root     root       588337 Nov 14 15:19 ./__spark_libs__/commons-collections-3.2.2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553530   28 -r-x------   1 root     root        25475 Nov 14 15:19 ./__spark_libs__/json-1.8.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553787  620 -r-x------   1 root     root       632485 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-lexmodelbuilding-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553788  228 -r-x------   1 root     root       229423 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-applicationautoscaling-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553789 1092 -r-x------   1 root     root      1117335 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-devicefarm-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553790  240 -r-x------   1 root     root       244715 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-lakeformation-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553531   24 -r-x------   1 root     root        24239 Nov 14 15:19 ./__spark_libs__/commons-daemon-1.0.13.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553791 2800 -r-x------   1 root     root      2864744 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-sagemaker-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553532   72 -r-x------   1 root     root        73349 Nov 14 15:19 ./__spark_libs__/jersey-container-servlet-core-2.30.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553792  552 -r-x------   1 root     root       563962 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-logs-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553533 1988 -r-x------   1 root     root      2035066 Nov 14 15:19 ./__spark_libs__/commons-math3-3.4.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553793  464 -r-x------   1 root     root       473688 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-kinesisvideo-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553534   68 -r-x------   1 root     root        69409 Nov 14 15:19 ./__spark_libs__/activation-1.1.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553535  416 -r-x------   1 root     root       423175 Nov 14 15:19 ./__spark_libs__/okhttp-3.12.6.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553794  776 -r-x------   1 root     root       791754 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-ses-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553536  220 -r-x------   1 root     root       222980 Nov 14 15:19 ./__spark_libs__/scala-parser-combinators_2.12-1.1.2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553795   84 -r-x------   1 root     root        82717 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-pi-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553537  788 -r-x------   1 root     root       805848 Nov 14 15:19 ./__spark_libs__/hadoop-mapreduce-client-common-3.2.1-amzn-1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553796  288 -r-x------   1 root     root       293007 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-licensemanager-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553538   56 -r-x------   1 root     root        54391 Nov 14 15:19 ./__spark_libs__/objenesis-2.5.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553798   56 -r-x------   1 root     root        53254 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-apigatewaymanagementapi-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553799  628 -r-x------   1 root     root       641941 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-stepfunctions-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553800 1808 -r-x------   1 root     root      1848502 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-waf-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553801 1824 -r-x------   1 root     root      1866513 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-lightsail-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553802  396 -r-x------   1 root     root       403163 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-discovery-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553803   96 -r-x------   1 root     root        97822 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-augmentedairuntime-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553804  544 -r-x------   1 root     root       554655 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-elasticloadbalancingv2-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553539   28 -r-x------   1 root     root        27749 Nov 14 15:19 ./__spark_libs__/orc-shims-1.5.10.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553540  320 -r-x------   1 root     root       326874 Nov 14 15:19 ./__spark_libs__/httpcore-4.4.11.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553541  244 -r-x------   1 root     root       246918 Nov 14 15:19 ./__spark_libs__/commons-beanutils-1.9.4.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553542   36 -r-x------   1 root     root        35518 Nov 14 15:19 ./__spark_libs__/zjsonpatch-0.3.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553543  448 -r-x------   1 root     root       458605 Nov 14 15:19 ./__spark_libs__/netlib-native_system-linux-x86_64-1.1-natives.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553805  308 -r-x------   1 root     root       315095 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-s3control-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553544   84 -r-x------   1 root     root        85815 Nov 14 15:19 ./__spark_libs__/jersey-media-jaxb-2.30.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553545  604 -r-x------   1 root     root       616888 Nov 14 15:19 ./__spark_libs__/commons-configuration2-2.1.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553806 1436 -r-x------   1 root     root      1469407 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-chime-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553546   20 -r-x------   1 root     root        20046 Nov 14 15:19 ./__spark_libs__/kerb-identity-1.0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553547   76 -r-x------   1 root     root        76733 Nov 14 15:19 ./__spark_libs__/jersey-hk2-2.30.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553807 40140 -r-x------   1 root     root     41099632 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-models-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553548  100 -r-x------   1 root     root       100636 Nov 14 15:19 ./__spark_libs__/jsp-api-2.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553808 1028 -r-x------   1 root     root      1051487 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-codedeploy-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553549   36 -r-x------   1 root     root        34654 Nov 14 15:19 ./__spark_libs__/paranamer-2.8.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553809  404 -r-x------   1 root     root       413024 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-managedblockchain-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553810  492 -r-x------   1 root     root       501832 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-amplify-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553811   44 -r-x------   1 root     root        43408 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-marketplaceentitlement-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553550   68 -r-x------   1 root     root        67897 Nov 14 15:19 ./__spark_libs__/jackson-annotations-2.10.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553551 1168 -r-x------   1 root     root      1194003 Nov 14 15:19 ./__spark_libs__/arpack_combined_all-0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553552 5752 -r-x------   1 root     root      5886128 Nov 14 15:19 ./__spark_libs__/spark-mllib_2.12-3.0.0-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553553    8 -r-x------   1 root     root         4467 Nov 14 15:19 ./__spark_libs__/aopalliance-1.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553554  624 -r-x------   1 root     root       637428 Nov 14 15:19 ./__spark_libs__/netlib-native_system-win-x86_64-1.1-natives.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553555   28 -r-x------   1 root     root        26514 Nov 14 15:19 ./__spark_libs__/stax-api-1.0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553556   80 -r-x------   1 root     root        79588 Nov 14 15:19 ./__spark_libs__/spire-macros_2.12-0.17.0-M1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553557 4064 -r-x------   1 root     root      4158923 Nov 14 15:19 ./__spark_libs__/hadoop-common-3.2.1-amzn-1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553558  332 -r-x------   1 root     root       336803 Nov 14 15:19 ./__spark_libs__/antlr4-runtime-4.7.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553812  896 -r-x------   1 root     root       913853 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-simpleworkflow-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553559  132 -r-x------   1 root     root       134696 Nov 14 15:19 ./__spark_libs__/breeze-macros_2.12-1.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553813  276 -r-x------   1 root     root       281444 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-codestar-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553814 1464 -r-x------   1 root     root      1498425 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-codecommit-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553815 1644 -r-x------   1 root     root      1683061 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-quicksight-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553560   28 -r-x------   1 root     root        27156 Nov 14 15:19 ./__spark_libs__/istack-commons-runtime-3.0.8.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553561 1112 -r-x------   1 root     root      1137921 Nov 14 15:19 ./__spark_libs__/spark-streaming_2.12-3.0.0-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553816  284 -r-x------   1 root     root       290178 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-datapipeline-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553817  696 -r-x------   1 root     root       711839 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-elasticbeanstalk-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553562  268 -r-x------   1 root     root       273370 Nov 14 15:19 ./__spark_libs__/commons-net-3.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553818  804 -r-x------   1 root     root       822890 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-kinesisanalyticsv2-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553563  228 -r-x------   1 root     root       232248 Nov 14 15:19 ./__spark_libs__/jackson-core-asl-1.9.13.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553564   56 -r-x------   1 root     root        56273 Nov 14 15:19 ./__spark_libs__/hive-shims-0.23-2.3.7-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553565  524 -r-x------   1 root     root       533455 Nov 14 15:19 ./__spark_libs__/protobuf-java-2.5.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553566   92 -r-x------   1 root     root        93210 Nov 14 15:19 ./__spark_libs__/super-csv-2.2.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553819 1008 -r-x------   1 root     root      1029561 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-dms-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553567   84 -r-x------   1 root     root        83632 Nov 14 15:19 ./__spark_libs__/json4s-ast_2.12-3.6.6.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553568 1864 -r-x------   1 root     root      1908681 Nov 14 15:19 ./__spark_libs__/datanucleus-rdbms-4.1.19.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553569  188 -r-x------   1 root     root       190432 Nov 14 15:19 ./__spark_libs__/gson-2.2.4.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553820  408 -r-x------   1 root     root       416480 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-cloudwatch-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553570  552 -r-x------   1 root     root       561459 Nov 14 15:19 ./__spark_libs__/netlib-native_system-win-i686-1.1-natives.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553571 1140 -r-x------   1 root     root      1166647 Nov 14 15:19 ./__spark_libs__/jersey-common-2.30.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553821  400 -r-x------   1 root     root       408589 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-eks-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553572  308 -r-x------   1 root     root       313702 Nov 14 15:19 ./__spark_libs__/libfb303-0.9.3.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553822  360 -r-x------   1 root     root       366549 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-dax-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553823  364 -r-x------   1 root     root       369379 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-worklink-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553573  320 -r-x------   1 root     root       325335 Nov 14 15:19 ./__spark_libs__/RoaringBitmap-0.7.45.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553824 1796 -r-x------   1 root     root      1835621 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-dynamodb-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553574  708 -r-x------   1 root     root       723203 Nov 14 15:19 ./__spark_libs__/parquet-format-2.4.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553825  264 -r-x------   1 root     root       268887 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-opsworkscm-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553575  184 -r-x------   1 root     root       186708 Nov 14 15:19 ./__spark_libs__/avro-mapred-1.8.2-hadoop2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553826   72 -r-x------   1 root     root        70061 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-pricing-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553827  512 -r-x------   1 root     root       520978 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-glacier-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553576   84 -r-x------   1 root     root        85902 Nov 14 15:19 ./__spark_libs__/hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553577  336 -r-x------   1 root     root       341862 Nov 14 15:19 ./__spark_libs__/jackson-module-scala_2.12-2.10.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553828   44 -r-x------   1 root     root        44094 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-marketplacecommerceanalytics-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553578  280 -r-x------   1 root     root       283653 Nov 14 15:19 ./__spark_libs__/curator-recipes-2.13.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553829  888 -r-x------   1 root     root       905885 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-emr-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553579   16 -r-x------   1 root     root        15169 Nov 14 15:19 ./__spark_libs__/spark-tags_2.12-3.0.0-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553580  176 -r-x------   1 root     root       176285 Nov 14 15:19 ./__spark_libs__/automaton-1.11-8.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553581   40 -r-x------   1 root     root        38370 Nov 14 15:19 ./__spark_libs__/hive-vector-code-gen-2.3.7-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553582   36 -r-x------   1 root     root        36175 Nov 14 15:19 ./__spark_libs__/json4s-jackson_2.12-3.6.6.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553583 8008 -r-x------   1 root     root      8198272 Nov 14 15:19 ./__spark_libs__/hive-metastore-2.3.7-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553831  216 -r-x------   1 root     root       218500 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-codegurureviewer-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553584 9292 -r-x------   1 root     root      9511792 Nov 14 15:19 ./__spark_libs__/spark-catalyst_2.12-3.0.0-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553585   60 -r-x------   1 root     root        59890 Nov 14 15:19 ./__spark_libs__/spark-kvstore_2.12-3.0.0-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553586 1144 -r-x------   1 root     root      1168113 Nov 14 15:19 ./__spark_libs__/algebra_2.12-2.0.0-M2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553587  680 -r-x------   1 root     root       693271 Nov 14 15:19 ./__spark_libs__/spark-hive_2.12-3.0.0-amzn-0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553832  240 -r-x------   1 root     root       245684 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-servicequotas-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553833  112 -r-x------   1 root     root       113976 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-elasticinference-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553588  132 -r-x------   1 root     root       131590 Nov 14 15:19 ./__spark_libs__/hk2-utils-2.6.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553589  164 -r-x------   1 root     root       167761 Nov 14 15:19 ./__spark_libs__/antlr-runtime-3.5.2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553590   72 -r-x------   1 root     root        71626 Nov 14 15:19 ./__spark_libs__/commons-compiler-3.0.16.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553591 1076 -r-x------   1 root     root      1098934 Nov 14 15:19 ./__spark_libs__/parquet-column-1.10.1-spark-amzn-2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553834  700 -r-x------   1 root     root       714238 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-robomaker-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553592  280 -r-x------   1 root     root       286277 Nov 14 15:19 ./__spark_libs__/parquet-hadoop-1.10.1-spark-amzn-2.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553593 2140 -r-x------   1 root     root      2189117 Nov 14 15:19 ./__spark_libs__/guava-14.0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553594   36 -r-x------   1 root     root        33786 Nov 14 15:19 ./__spark_libs__/machinist_2.12-0.6.8.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553835  392 -r-x------   1 root     root       398971 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-fms-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553595  112 -r-x------   1 root     root       112235 Nov 14 15:19 ./__spark_libs__/scala-collection-compat_2.12-2.1.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553596  304 -r-x------   1 root     root       310891 Nov 14 15:19 ./__spark_libs__/netlib-native_system-linux-armhf-1.1-natives.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553836  148 -r-x------   1 root     root       149482 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-codestarconnections-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553597  120 -r-x------   1 root     root       120316 Nov 14 15:19 ./__spark_libs__/json-smart-2.3.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553598   20 -r-x------   1 root     root        16537 Nov 14 15:19 ./__spark_libs__/jcl-over-slf4j-1.7.30.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553837  468 -r-x------   1 root     root       477556 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-machinelearning-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553599  908 -r-x------   1 root     root       927721 Nov 14 15:19 ./__spark_libs__/jersey-server-2.30.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553600  100 -r-x------   1 root     root       102174 Nov 14 15:19 ./__spark_libs__/kerby-asn1-1.0.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553838  728 -r-x------   1 root     root       741552 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-elasticsearch-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553839  624 -r-x------   1 root     root       637712 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-pinpointemail-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553601  344 -r-x------   1 root     root       348635 Nov 14 15:19 ./__spark_libs__/jackson-core-2.10.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553602 1404 -r-x------   1 root     root      1437215 Nov 14 15:19 ./__spark_libs__/arrow-vector-0.15.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553840  388 -r-x------   1 root     root       396097 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-elasticloadbalancing-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553603   16 -r-x------   1 root     root        15071 Nov 14 15:19 ./__spark_libs__/transaction-api-1.1.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553843  364 -r-x------   1 root     root       370023 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-ram-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553844  356 -r-x------   1 root     root       364262 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-globalaccelerator-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553604   48 -r-x------   1 root     root        46646 Nov 14 15:19 ./__spark_libs__/jackson-dataformat-yaml-2.10.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553605  264 -r-x------   1 root     root       268780 Nov 14 15:19 ./__spark_libs__/jline-2.14.6.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553845  204 -r-x------   1 root     root       205056 Nov 14 15:19 ./__spark_libs__/aws-java-sdk-codestarnotifications-1.11.828.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553300    4 drwx------   3 root     root         4096 Nov 14 15:19 ./__spark_conf__
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553329    4 -r-x------   1 root     root          934 Nov 14 15:19 ./__spark_conf__/__spark_dist_cache__.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553302    4 -r-x------   1 root     root           10 Nov 14 15:19 ./__spark_conf__/metrics.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553327  128 -r-x------   1 root     root       130394 Nov 14 15:19 ./__spark_conf__/__spark_hadoop_conf__.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553301    4 -r-x------   1 root     root           10 Nov 14 15:19 ./__spark_conf__/log4j.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553303    4 drwx------   2 root     root         4096 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553315    4 -r-x------   1 root     root         2979 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/hdfs-site.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553311    4 -r-x------   1 root     root         1162 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/core-site.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553320    4 -r-x------   1 root     root         2316 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/ssl-client.xml.example
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553313    4 -r-x------   1 root     root         3593 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/container-log4j.properties.default
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553326    8 -r-x------   1 root     root         4113 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/mapred-queues.xml.template
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553319    4 -r-x------   1 root     root          188 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/hive-site.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553308    4 -r-x------   1 root     root         1976 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/yarn-site.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553307    4 -r-x------   1 root     root         3321 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/hadoop-metrics2.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553316    8 -r-x------   1 root     root         6174 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/yarn-env.sh
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553324    4 -r-x------   1 root     root         2697 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/ssl-server.xml.example
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553321   16 -r-x------   1 root     root        14890 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/log4j.properties.default
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553309   12 -r-x------   1 root     root         8260 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/capacity-scheduler.xml.default
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553306   20 -r-x------   1 root     root        16406 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/hadoop-env.sh
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553318    4 -r-x------   1 root     root         1764 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/mapred-env.sh
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553305    4 -r-x------   1 root     root          758 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/mapred-site.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553304   16 -r-x------   1 root     root        14900 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/log4j.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553312   12 -r-x------   1 root     root         8260 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/capacity-scheduler.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/directory.info] 1553317    4 -r-x------   1 root     root         1940 Nov 14 15:19 ./__spark_conf__/__hadoop_conf__/container-executor.cfg
[36malgo-1    |[0m 21/11/14 15:19:57 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1763 ms on algo-1 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 15:19:57 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:19:57 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1.973 s
[36malgo-1    |[0m 21/11/14 15:19:57 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:19:57 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished
[36malgo-1    |[0m 21/11/14 15:19:57 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 2.034308 s
[36malgo-1    |[0m Loaded data
[36malgo-1    |[0m root
[36malgo-1    |[0m  |-- date: string (nullable = true)
[36malgo-1    |[0m  |-- sale: long (nullable = true)
[36malgo-1    |[0m 
[36malgo-1    |[0m 21/11/14 15:19:57 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.240.3:33331 in memory (size: 30.3 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:19:57 INFO BlockManagerInfo: Removed broadcast_0_piece0 on algo-1:33379 in memory (size: 30.3 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:19:57 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.240.3:33331 in memory (size: 7.4 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:19:57 INFO BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:33379 in memory (size: 7.4 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:19:57 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 15:19:57 INFO FileSourceStrategy: Pushed Filters: IsNotNull(sale),GreaterThan(sale,750)
[36malgo-1    |[0m 21/11/14 15:19:57 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(sale#8L),(sale#8L > 750)
[36malgo-1    |[0m 21/11/14 15:19:57 INFO FileSourceStrategy: Output Data Schema: struct<date: string, sale: bigint>
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_163690312021-11-14 15:19:57,964 INFO monitor.ContainersMonitorImpl: container_1636903150985_0001_01_000002's ip = 192.168.240.2, and hostname = algo-2
[33malgo-2    |[0m 2021-11-14 15:19:57,969 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1636903150985_0001_01_000002 since CPU usage is not yet available.
[36malgo-1    |[0m 21/11/14 15:19:58 INFO CodeGenerator: Code generated in 261.858112 ms
[36malgo-1    |[0m 21/11/14 15:19:58 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 318.0 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:19:58 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:19:58 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.240.3:33331 (size: 30.3 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:19:58 INFO SparkContext: Created broadcast 2 from showString at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 15:19:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 15:19:58 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 15:19:58 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 15:19:58 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[36malgo-1    |[0m 21/11/14 15:19:58 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
[36malgo-1    |[0m 21/11/14 15:19:58 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:19:58 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:19:58 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:19:58 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.1 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:19:58 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:19:58 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.240.3:33331 (size: 6.1 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:19:58 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:19:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 15:19:58 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 15:19:58 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7755 bytes)
[36malgo-1    |[0m 21/11/14 15:19:58 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:33379 (size: 6.1 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:19:58 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:33379 (size: 30.3 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:19:58 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 156 ms on algo-1 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 15:19:58 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:19:58 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.166 s
[36malgo-1    |[0m 21/11/14 15:19:58 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:19:58 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
[36malgo-1    |[0m 21/11/14 15:19:58 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.171206 s
[36malgo-1    |[0m 21/11/14 15:19:58 INFO CodeGenerator: Code generated in 18.601448 ms
[36malgo-1    |[0m +----------+----+
[36malgo-1    |[0m |      date|sale|
[36malgo-1    |[0m +----------+----+
[36malgo-1    |[0m |2020-01-05| 820|
[36malgo-1    |[0m |2020-01-06| 857|
[36malgo-1    |[0m |2020-01-02| 850|
[36malgo-1    |[0m |2020-01-07| 919|
[36malgo-1    |[0m |2020-01-07| 833|
[36malgo-1    |[0m |2020-01-07| 784|
[36malgo-1    |[0m |2020-01-07| 985|
[36malgo-1    |[0m |2020-01-04| 917|
[36malgo-1    |[0m |2020-01-05| 990|
[36malgo-1    |[0m |2020-01-05| 788|
[36malgo-1    |[0m |2020-01-01| 865|
[36malgo-1    |[0m |2020-01-04| 851|
[36malgo-1    |[0m |2020-01-02| 777|
[36malgo-1    |[0m |2020-01-04| 930|
[36malgo-1    |[0m |2020-01-02| 821|
[36malgo-1    |[0m |2020-01-02| 886|
[36malgo-1    |[0m |2020-01-07| 993|
[36malgo-1    |[0m |2020-01-06| 825|
[36malgo-1    |[0m |2020-01-02| 991|
[36malgo-1    |[0m |2020-01-03| 989|
[36malgo-1    |[0m +----------+----+
[36malgo-1    |[0m only showing top 20 rows
[36malgo-1    |[0m 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:57 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 454@algo-2
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:57 INFO SignalUtils: Registered signal handler for TERM
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:57 INFO SignalUtils: Registered signal handler for HUP
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:57 INFO SignalUtils: Registered signal handler for INT
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:57 INFO SecurityManager: Changing view acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:57 INFO SecurityManager: Changing modify acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:57 INFO SecurityManager: Changing view acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:57 INFO SecurityManager: Changing modify acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:58 INFO TransportClientFactory: Successfully created connection to /192.168.240.3:43007 after 97 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:58 INFO SecurityManager: Changing view acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:58 INFO SecurityManager: Changing modify acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:58 INFO SecurityManager: Changing view acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:58 INFO SecurityManager: Changing modify acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:58 INFO TransportClientFactory: Successfully created connection to /192.168.240.3:43007 after 1 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:58 INFO DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/blockmgr-9b442a57-0990-4b81-81bf-846170e4180c
[36malgo-1    |[0m 21/11/14 15:19:59 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.240.2:48682) with ID 1
[36malgo-1    |[0m /export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:55 INFO TorrentBroadcast: Reading broadcast variable 1 took 122 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.6 KiB, free 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:56 INFO FileScanRDD: TID: 0 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:56 INFO CodeGenerator: Code generated in 250.22321 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:56 INFO TorrentBroadcast: Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 912.2 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:57 INFO TorrentBroadcast: Reading broadcast variable 0 took 23 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:57 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 445.7 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:57 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2056 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:58 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 1
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:58 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:58 INFO TorrentBroadcast: Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:58 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:58 INFO TorrentBroadcast: Reading broadcast variable 3 took 10 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:58 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.1 KiB, free 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:58 INFO CodeGenerator: Code generated in 28.553459 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:58 INFO FileScanRDD: TID: 1 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:58 INFO21/11/14 15:19:59 INFO BlockManagerMasterEndpoint: Registering block manager algo-2:42513 with 912.3 MiB RAM, BlockManagerId(1, algo-2, 42513, None)
[36malgo-1    |[0m  CodeGenerator: Code generated in 12.56025 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:58 INFO TorrentBroadcast: Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:58 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:58 INFO TorrentBroadcast: Reading broadcast variable 2 took 10 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:58 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 445.7 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:19:58 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1790 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:02 WARN YarnCoarseGrainedExecutorBackend: eagerFSInit: Unable to eagerly init filesystem s3://does/not/exist
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] java.nio.file.AccessDeniedException: does: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@143fad00: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@5cdd808: Failed to connect to service endpoint: ]
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:187)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:111)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:380)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:314)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3358)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:123)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3407)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3375)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:486)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.executor.CoarseGrainedExecutorBackend.$anonfun$eagerFSInit$1(CoarseGrainedExecutorBackend.scala:274)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at scala.collection.parallel.mutable.ParArray$ParArrayIterator.flatmap2combiner(ParArray.scala:419)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at scala.collection.parallel.ParIterableLike$FlatMap.leaf(ParIterableLike.scala:1074)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at scala.collection.parallel.ParIterableLike$FlatMap.tryLeaf(ParIterableLike.scala:1070)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at scala.collection.parallel.FutureTasks.$anonfun$exec$5(Tasks.scala:499)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at scala.util.Success.$anonfun$map$1(Try.scala:255)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at scala.util.Success.map(Try.scala:213)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at java.lang.Thread.run(Thread.java:748)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] Caused by: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@143fad00: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@5cdd808: Failed to connect to service endpoint: ]
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:159)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1257)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:833)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:783)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5062)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:5850)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:5823)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5046)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:58 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:58 INFO YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@192.168.240.3:43007
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:58 INFO ResourceUtils: ==============================================================
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:58 INFO ResourceUtils: Resources for spark.executor:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:58 INFO ResourceUtils: ==============================================================
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:59 INFO YarnCoarseGrainedExecutorBackend: Successfully registered with driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:59 INFO Executor: Starting executor ID 1 on host algo-2
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42513.
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:59 INFO NettyBlockTransferService: Server created on algo-2:42513
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(1, algo-2, 42513, None)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(1, algo-2, 42513, None)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(1, algo-2, 42513, None)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:59 INFO MetricsConfig: Loaded properties from hadoop-metrics2.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:59 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:19:59 INFO MetricsSystemImpl: s3a-file-system metrics system started
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:20:05 WARN YarnCoarseGrainedExecutorBackend: eagerFSInit: Unable to eagerly init filesystem s3://does/not/exist
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] java.nio.file.AccessDeniedException: does: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@6f5edd47: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@7cc717a1: Failed to connect to service endpoint: ]
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:187)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:111)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:380)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:314)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3358)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:123)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3407)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3375)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:486)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.executor.CoarseGrainedExecutorBackend.$anonfun$eagerFSInit$1(CoarseGrainedExecutorBackend.scala:274)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at scala.collection.parallel.mutable.ParArray$ParArrayIterator.flatmap2combiner(ParArray.scala:419)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at scala.collection.parallel.ParIterableLike$FlatMap.leaf(ParIterableLike.scala:1074)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at scala.collection.parallel.ParIterableLike$FlatMap.tryLeaf(ParIterableLike.scala:1070)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at scala.collection.parallel.FutureTasks.$anonfun$exec$5(Tasks.scala:499)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at scala.util.Success.$anonfun$map$1(Try.scala:255)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at scala.util.Success.map(Try.scala:213)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at java.lang.Thread.run(Thread.java:748)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] Caused by: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@6f5edd47: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@7cc717a1: Failed to connect to service endpoint: ]
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:159)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1257)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:833)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:783)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5062)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:5850)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:5823)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5046)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5008)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1416)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1352)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$verifyBucketExists$1(S3AFileSystem.java:381)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	... 32 more
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] Caused by: com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@6f5edd47: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@7cc717a1: Failed to connect to service endpoint: ]
[36malgo-1    |[0m [/var/log/yarn21/11/14 15:20:58 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 15:20:58 INFO FileSourceStrategy: Pushed Filters: 
[36malgo-1    |[0m 21/11/14 15:20:58 INFO FileSourceStrategy: Post-Scan Filters: 
[36malgo-1    |[0m 21/11/14 15:20:58 INFO FileSourceStrategy: Output Data Schema: struct<date: string, sale: bigint>
[36malgo-1    |[0m 21/11/14 15:20:58 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.240.3:33331 in memory (size: 6.1 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:20:58 INFO BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:33379 in memory (size: 6.1 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:20:58 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.240.3:33331 in memory (size: 30.3 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:20:58 INFO BlockManagerInfo: Removed broadcast_2_piece0 on algo-1:33379 in memory (size: 30.3 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:20:58 INFO CodeGenerator: Code generated in 60.176139 ms
[36malgo-1    |[0m 21/11/14 15:20:58 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 318.0 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:20:58 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:20:58 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.240.3:33331 (size: 30.3 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:20:58 INFO SparkContext: Created broadcast 4 from collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82
[36malgo-1    |[0m 21/11/14 15:20:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 15:20:58 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 15:20:58 INFO DAGScheduler: Registering RDD 11 (collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82) as input to shuffle 0
[36malgo-1    |[0m 21/11/14 15:20:58 INFO DAGScheduler: Got map stage job 2 (collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82) with 1 output partitions
[36malgo-1    |[0m 21/11/14 15:20:58 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82)
[36malgo-1    |[0m 21/11/14 15:20:58 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:20:58 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:20:58 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:20:58 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 29.7 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:20:58 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.9 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:20:58 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.240.3:33331 (size: 13.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:20:58 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:20:58 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 15:20:58 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 15:20:58 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7744 bytes)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:33379 (size: 13.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m /export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5008)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1416)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1352)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$verifyBucketExists$1(S3AFileSystem.java:381)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	... 32 more
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] Caused by: com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@143fad00: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@5cdd808: Failed to connect to service endpoint: ]
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:136)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:137)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	... 50 more
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:58 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 2
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:58 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:58 INFO TorrentBroadcast: Started reading broadcast variable 5 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:58 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.9 KiB, free 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO TorrentBroadcast: Reading broadcast variable 5 took 6 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 29.7 KiB, free 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO CodeGenerator: Code generated in 61.600263 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO CodeGenerator: Code generated in 15.461424 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_16321/11/14 15:20:59 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:33379 (size: 30.3 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 386 ms on algo-1 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:20:59 INFO DAGScheduler: ShuffleMapStage 2 (collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82) finished in 0.404 s
[36malgo-1    |[0m 21/11/14 15:20:59 INFO DAGScheduler: looking for newly runnable stages
[36malgo-1    |[0m 21/11/14 15:20:59 INFO DAGScheduler: running: Set()
[36malgo-1    |[0m 21/11/14 15:20:59 INFO DAGScheduler: waiting: Set()
[36malgo-1    |[0m 21/11/14 15:20:59 INFO DAGScheduler: failed: Set()
[36malgo-1    |[0m 21/11/14 15:20:59 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 16.
[36malgo-1    |[0m 21/11/14 15:20:59 INFO CodeGenerator: Code generated in 23.365539 ms
[36malgo-1    |[0m 21/11/14 15:20:59 INFO SparkContext: Starting job: collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82
[36malgo-1    |[0m 21/11/14 15:20:59 INFO DAGScheduler: Got job 3 (collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82) with 15 output partitions
[36malgo-1    |[0m 21/11/14 15:20:59 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:20:59 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[14] at collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:20:59 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 29.1 KiB, free 1007.4 MiB)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.0 KiB, free 1007.4 MiB)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.240.3:33331 (size: 14.0 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:20:59 INFO DAGScheduler: Submitting 15 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[36malgo-1    |[0m 21/11/14 15:20:59 INFO YarnScheduler: Adding task set 4.0 with 15 tasks
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3, algo-2, executor 1, partition 0, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 4, algo-1, executor 2, partition 1, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:33379 (size: 14.0 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.240.3:40396
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 5, algo-1, executor 2, partition 2, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 4) in 201 ms on algo-1 (executor 2) (1/15)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-2:42513 (size: 14.0 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 6, algo-1, executor 2, partition 3, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 5) in 27 ms on algo-1 (executor 2) (2/15)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 7, algo-1, executor 2, partition 4, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 6) in 21 ms on algo-1 (executor 2) (3/15)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 8, algo-1, executor 2, partition 5, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 7) in 21 ms on algo-1 (executor 2) (4/15)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 9, algo-1, executor 2, partition 6, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 8) in 21 ms on algo-1 (executor 2) (5/15)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 10, algo-1, executor 2, partition 7, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 9) in 17 ms on algo-1 (executor 2) (6/15)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 11, algo-1, executor 2, partition 8, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 10) in 23 ms on algo-1 (executor 2) (7/15)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 12, algo-1, executor 2, partition 9, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 11) in 16 ms on algo-1 (executor 2) (8/15)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 13, algo-1, executor 2, partition 10, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 12) in 21 ms on algo-1 (executor 2) (9/15)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 14, algo-1, executor 2, partition 11, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 13) in 18 ms on algo-1 (executor 2) (10/15)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 15, algo-1, executor 2, partition 12, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 14) in 20 ms on algo-1 (executor 2) (11/15)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 16, algo-1, executor 2, partition 13, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 15) in 17 ms on algo-1 (executor 2) (12/15)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 17, algo-1, executor 2, partition 14, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 16) in 17 ms on algo-1 (executor 2) (13/15)
[36malgo-1    |[0m 21/11/14 15:20:59 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 17) in 17 ms on algo-1 (executor 2) (14/15)
[36malgo-1    |[0m 6903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO CodeGenerator: Code generated in 7.349967 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO CodeGenerator: Code generated in 9.068591 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO FileScanRDD: TID: 2 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO TorrentBroadcast: Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 879.9 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO TorrentBroadcast: Reading broadcast variable 4 took 8 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 445.7 KiB, free 879.5 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 4531 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 4
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Running task 1.0 in stage 4.0 (TID 4)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO TorrentBroadcast: Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.0 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO TorrentBroadcast: Reading broadcast variable 6 took 7 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 29.1 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@192.168.240.3:43007)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO MapOutputTrackerWorker: Got the output locations
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO CodeGenerator: Code generated in 22.884133 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Finished task 1.0 in stage 4.0 (TID 4). 3705 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 5
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Running task 2.0 in stage 4.0 (TID 5)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Finished task 2.0 in stage 4.0 (TID 5). 3667 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 6
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Running task 3.0 in stage 4.0 (TID 6)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Finished task 3.0 in stage 4.0 (TID 6). 3709 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 7
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Running task 4.0 in stage 4.0 (TID 7)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Finished task 4.0 in stage 4.0 (TID 7). 3667 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 8
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Running task 5.0 in stage 4.0 (TID 8)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Finished task 5.0 in stage 4.0 (TID 8). 3709 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 9
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Running task 6.0 in stage 4.0 (TID 9)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Finished task 6.0 in stage 4.0 (TID 9). 3667 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 10
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Running task 7.0 in stage 4.0 (TID 10)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Finished task 7.0 in stage 4.0 (TID 10). 3709 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 11
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Running task 8.0 in stage 4.0 (TID 11)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Finished task 8.0 in stage 4.0 (TID 11). 3667 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 12
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Running task 9.0 in stage 4.0 (TID 12)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Finished task 9.0 in stage 4.0 (TID 12). 3709 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 13
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Running task 10.0 in stage 4.0 (TID 13)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Finished task 10.0 in stage 4.0 (TID 13). 3667 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 14
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Running task 11.0 in stage 4.0 (TID 14)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Finished task 11.0 in stage 4.0 (TID 14). 3708 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 15
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Running task 12.0 in stage 4.0 (TID 15)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Finished task 12.0 in stage 4.0 (TID 15). 3667 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:221/11/14 15:21:00 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.240.2:48682
[36malgo-1    |[0m 21/11/14 15:21:01 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 1616 ms on algo-2 (executor 1) (15/15)
[36malgo-1    |[0m 21/11/14 15:21:01 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:21:01 INFO DAGScheduler: ResultStage 4 (collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82) finished in 1.622 s
[36malgo-1    |[0m 21/11/14 15:21:01 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:21:01 INFO YarnScheduler: Killing all running tasks in stage 4: Stage finished
[36malgo-1    |[0m 21/11/14 15:21:01 INFO DAGScheduler: Job 3 finished: collect at /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py:82, took 1.629352 s
[36malgo-1    |[0m Calculated data
[36malgo-1    |[0m 21/11/14 15:21:01 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 15:21:01 INFO FileSourceStrategy: Pushed Filters: 
[36malgo-1    |[0m 21/11/14 15:21:01 INFO FileSourceStrategy: Post-Scan Filters: 
[36malgo-1    |[0m 21/11/14 15:21:01 INFO FileSourceStrategy: Output Data Schema: struct<date: string, sale: bigint>
[36malgo-1    |[0m 21/11/14 15:21:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
[36malgo-1    |[0m 21/11/14 15:21:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[36malgo-1    |[0m 21/11/14 15:21:01 INFO DirectFileOutputCommitter: Direct Write: DISABLED
[36malgo-1    |[0m 21/11/14 15:21:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter
[36malgo-1    |[0m 21/11/14 15:21:01 INFO CodeGenerator: Code generated in 9.687334 ms
[36malgo-1    |[0m 21/11/14 15:21:01 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 318.0 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 15:21:01 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 15:21:01 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.240.3:33331 (size: 30.3 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:21:01 INFO SparkContext: Created broadcast 7 from json at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 15:21:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 15:21:01 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 15:21:01 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.240.3:33331 in memory (size: 14.0 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:21:01 INFO BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:33379 in memory (size: 14.0 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:21:01 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 15:21:01 INFO DAGScheduler: Got job 4 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[36malgo-1    |[0m 21/11/14 15:21:01 INFO DAGScheduler: Final stage: ResultStage 5 (json at NativeMethodAccessorImpl.java:0)
[36malgo-1    |[0m 21/11/14 15:21:01 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:21:01 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:21:01 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[19] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:21:01 INFO BlockManagerInfo: Removed broadcast_6_piece0 on algo-2:42513 in memory (size: 14.0 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:21:01 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.240.3:33331 in memory (size: 13.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:21:01 INFO BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:33379 in memory (size: 13.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:21:01 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 188.5 KiB, free 1007.0 MiB)
[36malgo-1    |[0m 21/11/14 15:21:01 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 68.7 KiB, free 1006.9 MiB)
[36malgo-1    |[0m 21/11/14 15:21:01 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.240.3:33331 (size: 68.7 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:21:01 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:21:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[19] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 15:21:01 INFO YarnScheduler: Adding task set 5.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 15:21:01 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 18, algo-2, executor 1, partition 0, PROCESS_LOCAL, 7755 bytes)
[36malgo-1    |[0m 21/11/14 15:21:01 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-2:42513 (size: 68.7 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:21:02 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-2:42513 (size: 30.3 KiB, free: 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:136)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:137)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	... 50 more
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:20:59 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 3
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:20:59 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:20:59 INFO MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:20:59 INFO TorrentBroadcast: Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:20:59 INFO TransportClientFactory: Successfully created connection to /192.168.240.3:33331 after 2 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:20:59 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.0 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:20:59 INFO TorrentBroadcast: Reading broadcast variable 6 took 109 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:20:59 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 29.1 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:00 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:00 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@192.168.240.3:43007)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:00 INFO MapOutputTrackerWorker: Got the output locations
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:00 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:00 INFO CodeGenerator: Code generated in 373.906877 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:01 INFO CodeGenerator: Code generated in 10.350189 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:01 INFO CodeGenerator: Code generated in 7.931144 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:01 INFO CodeGenerator: Code generated in 8.85979 ms
[36malgo-1    |[0m 21/11/14 15:21:02 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 18) in 1518 ms on algo-2 (executor 1) (1/1)
[36malgo-1    |[0m 21/11/14 15:21:02 INFO YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:21:02 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 48799
[36malgo-1    |[0m 21/11/14 15:21:02 INFO DAGScheduler: ResultStage 5 (json at NativeMethodAccessorImpl.java:0) finished in 1.546 s
[36malgo-1    |[0m 21/11/14 15:21:02 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:21:02 INFO YarnScheduler: Killing all running tasks in stage 5: Stage finished
[36malgo-1    |[0m 21/11/14 15:21:02 INFO DAGScheduler: Job 4 finished: json at NativeMethodAccessorImpl.java:0, took 1.550460 s
[36malgo-1    |[0m 21/11/14 15:21:02 INFO FileFormatWriter: Write Job 23f452f1-9f2c-4b65-bb12-4d1c76d2dd9f committed.
[36malgo-1    |[0m 21/11/14 15:21:02 INFO FileFormatWriter: Finished processing stats for write job 23f452f1-9f2c-4b65-bb12-4d1c76d2dd9f.
[36malgo-1    |[0m Saved data
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:01 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 3710 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:01 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 18
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:01 INFO Executor: Running task 0.0 in stage 5.0 (TID 18)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:01 INFO TorrentBroadcast: Started reading broadcast variable 8 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:01 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 68.7 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:01 INFO TorrentBroadcast: Reading broadcast variable 8 took 9 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:01 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 188.5 KiB, free 912.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:01 INFO CodeGenerator: Code generated in 10.032866 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:02 INFO FileScanRDD: TID: 18 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:02 INFO CodeGenerator: Code generated in 15.689598 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:02 INFO CodeGenerator: Code generated in 13.16971 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:02 INFO CodeGenerator: Code generated in 10.821039 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:02 INFO TorrentBroadcast: Started reading broadcast variable 7 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:02 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 912.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:02 INFO TorrentBroadcast: Reading broadcast variable 7 took 9 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:02 INFO DirectFileOutputCommitter: Direct Write: DISABLED
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter
[36malgo-1    |[0m 21/11/14 15:21:03 INFO CodeGenerator: Code generated in 13.898276 ms
[36malgo-1    |[0m 21/11/14 15:21:03 INFO CodeGenerator: Code generated in 10.54215 ms
[36malgo-1    |[0m 21/11/14 15:21:03 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 15:21:03 INFO DAGScheduler: Got job 5 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[36malgo-1    |[0m 21/11/14 15:21:03 INFO DAGScheduler: Final stage: ResultStage 6 (showString at NativeMethodAccessorImpl.java:0)
[36malgo-1    |[0m 21/11/14 15:21:03 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:21:03 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:21:03 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[31] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:21:03 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 17.3 KiB, free 1006.9 MiB)
[36malgo-1    |[0m 21/11/14 15:21:03 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 1006.9 MiB)
[36malgo-1    |[0m 21/11/14 15:21:03 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.240.3:33331 (size: 8.0 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:21:03 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:21:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[31] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 15:21:03 INFO YarnScheduler: Adding task set 6.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 15:21:03 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 19, algo-2, executor 1, partition 0, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:03 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-2:42513 (size: 8.0 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:21:03 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 19) in 719 ms on algo-2 (executor 1) (1/1)
[36malgo-1    |[0m 21/11/14 15:21:03 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:21:03 INFO DAGScheduler: ResultStage 6 (showString at NativeMethodAccessorImpl.java:0) finished in 0.726 s
[36malgo-1    |[0m 21/11/14 15:21:03 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:21:03 INFO YarnScheduler: Killing all running tasks in stage 6: Stage finished
[36malgo-1    |[0m 21/11/14 15:21:03 INFO DAGScheduler: Job 5 finished: showString at NativeMethodAccessorImpl.java:0, took 0.730628 s
[36malgo-1    |[0m 21/11/14 15:21:03 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 15:21:03 INFO DAGScheduler: Got job 6 (showString at NativeMethodAccessorImpl.java:0) with 4 output partitions
[36malgo-1    |[0m 21/11/14 15:21:03 INFO DAGScheduler: Final stage: ResultStage 7 (showString at NativeMethodAccessorImpl.java:0)
[36malgo-1    |[0m 21/11/14 15:21:03 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:21:03 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:21:03 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[31] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:21:03 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 17.3 KiB, free 1006.9 MiB)
[36malgo-1    |[0m 21/11/14 15:21:03 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 1006.9 MiB)
[36malgo-1    |[0m 21/11/14 15:21:03 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.240.3:33331 (size: 8.0 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:21:03 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:21:03 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 7 (MapPartitionsRDD[31] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1, 2, 3, 4))
[36malgo-1    |[0m 21/11/14 15:21:03 INFO YarnScheduler: Adding task set 7.0 with 4 tasks
[36malgo-1    |[0m 21/11/14 15:21:03 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 20, algo-1, executor 2, partition 1, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:03 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 21, algo-2, executor 1, partition 2, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:03 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:33379 (size: 8.0 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:21:03 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-2:42513 (size: 8.0 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:21:03 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 22, algo-2, executor 1, partition 3, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:03 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 21) in 32 ms on algo-2 (executor 1) (1/4)
[36malgo-1    |[0m 21/11/14 15:21:03 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 23, algo-2, executor 1, partition 4, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:03 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 22) in 26 ms on algo-2 (executor 1) (2/4)
[36malgo-1    |[0m 21/11/14 15:21:03 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 23) in 56 ms on algo-2 (executor 1) (3/4)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:02 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 445.7 KiB, free 911.6 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:02 INFO PythonUDFRunner: Times: total = 1266, boot = 405, init = 859, finish = 2
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:02 INFO FileOutputCommitter: Saved output of task 'attempt_20211114152101_0005_m_000000_18' to file:/opt/ml/processing/output/data
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:02 INFO SparkHadoopMapRedUtil: attempt_20211114152101_0005_m_000000_18: Committed
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:02 INFO Executor: Finished task 0.0 in stage 5.0 (TID 18). 3249 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 19
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO Executor: Running task 0.0 in stage 6.0 (TID 19)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO TorrentBroadcast: Started reading broadcast variable 9 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 911.6 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO TorrentBroadcast: Reading broadcast variable 9 took 6 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 17.3 KiB, free 911.6 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO CodeGenerator: Code generated in 10.041772 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO CodeGenerator: Code generated in 9.347314 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO PythonRunner: Times: total = 2, boot = -339, init = 341, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO CodeGenerator: Code generated in 11.959276 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO CodeGenerator: Code generated in 14.37186 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO PythonUDFRunner: Times: total = 603, boot = 3, init = 600, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO Executor: Finished task 0.0 in stage 6.0 (TID 19). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 21
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO Executor: Running task 1.0 in stage 7.0 (TID 21)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 20) in 1258 ms on algo-1 (executor 2) (4/4)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: ResultStage 7 (showString at NativeMethodAccessorImpl.java:0) finished in 1.264 s
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:21:05 INFO YarnScheduler: Killing all running tasks in stage 7: Stage finished
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Job 6 finished: showString at NativeMethodAccessorImpl.java:0, took 1.267826 s
[36malgo-1    |[0m 21/11/14 15:21:05 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Got job 7 (showString at NativeMethodAccessorImpl.java:0) with 11 output partitions
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Final stage: ResultStage 8 (showString at NativeMethodAccessorImpl.java:0)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[31] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:21:05 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 17.3 KiB, free 1006.9 MiB)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.240.3:33331 (size: 8.0 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Submitting 11 missing tasks from ResultStage 8 (MapPartitionsRDD[31] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15))
[36malgo-1    |[0m 21/11/14 15:21:05 INFO YarnScheduler: Adding task set 8.0 with 11 tasks
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 24, algo-2, executor 1, partition 5, PROCESS_LOCAL, 7398 bytes)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 25, algo-1, executor 2, partition 6, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:33379 (size: 8.0 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-2:42513 (size: 8.0 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 26, algo-1, executor 2, partition 7, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 25) in 28 ms on algo-1 (executor 2) (1/11)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Starting task 3.0 in stage 8.0 (TID 27, algo-1, executor 2, partition 8, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 26) in 33 ms on algo-1 (executor 2) (2/11)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Starting task 4.0 in stage 8.0 (TID 28, algo-2, executor 1, partition 9, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 24) in 73 ms on algo-2 (executor 1) (3/11)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Starting task 5.0 in stage 8.0 (TID 29, algo-2, executor 1, partition 10, PROCESS_LOCAL, 7398 bytes)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Finished task 4.0 in stage 8.0 (TID 28) in 27 ms on algo-2 (executor 1) (4/11)
[36malgo-1    |[0m 0:59 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 16
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Running task 13.0 in stage 4.0 (TID 16)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Finished task 13.0 in stage 4.0 (TID 16). 3708 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 17
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Running task 14.0 in stage 4.0 (TID 17)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:20:59 INFO Executor: Finished task 14.0 in stage 4.0 (TID 17). 3667 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:03 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 20
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:03 INFO Executor: Running task 0.0 in stage 7.0 (TID 20)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:03 INFO TorrentBroadcast: Started reading broadcast variable 10 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:03 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:03 INFO TorrentBroadcast: Reading broadcast variable 10 took 6 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:03 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 17.3 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:04 INFO CodeGenerator: Code generated in 12.527391 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:04 INFO CodeGenerator: Code generated in 8.58508 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:04 INFO PythonRunner: Times: total = 418, boot = 404, init = 14, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:04 INFO CodeGenerator: Code generated in 11.102228 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/21/11/14 15:21:05 INFO TaskSetManager: Starting task 6.0 in stage 8.0 (TID 30, algo-1, executor 2, partition 11, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Finished task 3.0 in stage 8.0 (TID 27) in 57 ms on algo-1 (executor 2) (5/11)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Starting task 7.0 in stage 8.0 (TID 31, algo-2, executor 1, partition 12, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Finished task 5.0 in stage 8.0 (TID 29) in 60 ms on algo-2 (executor 1) (6/11)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Starting task 8.0 in stage 8.0 (TID 32, algo-1, executor 2, partition 13, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Finished task 6.0 in stage 8.0 (TID 30) in 56 ms on algo-1 (executor 2) (7/11)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Starting task 9.0 in stage 8.0 (TID 33, algo-2, executor 1, partition 14, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Finished task 7.0 in stage 8.0 (TID 31) in 62 ms on algo-2 (executor 1) (8/11)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Starting task 10.0 in stage 8.0 (TID 34, algo-1, executor 2, partition 15, PROCESS_LOCAL, 7398 bytes)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Finished task 8.0 in stage 8.0 (TID 32) in 62 ms on algo-1 (executor 2) (9/11)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Finished task 9.0 in stage 8.0 (TID 33) in 60 ms on algo-2 (executor 1) (10/11)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Finished task 10.0 in stage 8.0 (TID 34) in 69 ms on algo-1 (executor 2) (11/11)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: ResultStage 8 (showString at NativeMethodAccessorImpl.java:0) finished in 0.306 s
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:21:05 INFO YarnScheduler: Killing all running tasks in stage 8: Stage finished
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Job 7 finished: showString at NativeMethodAccessorImpl.java:0, took 0.311102 s
[36malgo-1    |[0m 21/11/14 15:21:05 INFO CodeGenerator: Code generated in 11.062024 ms
[36malgo-1    |[0m +--------+-----------+--------------------+--------------------+
[36malgo-1    |[0m |batch_id|primary_key|                text|               words|
[36malgo-1    |[0m +--------+-----------+--------------------+--------------------+
[36malgo-1    |[0m |       1|          1|This is a great book|This is a great book|
[36malgo-1    |[0m |       1|          1|This is a great book|This is a great book|
[36malgo-1    |[0m |       1|          1|This is a great book|This is a great book|
[36malgo-1    |[0m +--------+-----------+--------------------+--------------------+
[36malgo-1    |[0m 
[36malgo-1    |[0m 21/11/14 15:21:05 INFO CodeGenerator: Code generated in 12.960152 ms
[36malgo-1    |[0m 21/11/14 15:21:05 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Got job 8 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Final stage: ResultStage 9 (showString at NativeMethodAccessorImpl.java:0)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[36] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:21:05 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 18.2 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.240.3:33331 (size: 8.2 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[36] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 15:21:05 INFO YarnScheduler: Adding task set 9.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 35, algo-2, executor 1, partition 0, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-2:42513 (size: 8.2 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 35) in 67 ms on algo-2 (executor 1) (1/1)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: ResultStage 9 (showString at NativeMethodAccessorImpl.java:0) finished in 0.074 s
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:21:05 INFO YarnScheduler: Killing all running tasks in stage 9: Stage finished
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Job 8 finished: showString at NativeMethodAccessorImpl.java:0, took 0.078257 s
[36malgo-1    |[0m 21/11/14 15:21:05 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Got job 9 (showString at NativeMethodAccessorImpl.java:0) with 4 output partitions
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Final stage: ResultStage 10 (showString at NativeMethodAccessorImpl.java:0)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[36] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:21:05 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 18.2 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 192.168.240.3:33331 (size: 8.2 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 10 (MapPartitionsRDD[36] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1, 2, 3, 4))
[36malgo-1    |[0m 21/11/14 15:21:05 INFO YarnScheduler: Adding task set 10.0 with 4 tasks
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 36, algo-1, executor 2, partition 1, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 37, algo-2, executor 1, partition 2, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:33379 (size: 8.2 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-2:42513 (size: 8.2 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 38, algo-2, executor 1, partition 3, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 37) in 67 ms on algo-2 (executor 1) (1/4)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 39, algo-1, executor 2, partition 4, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 36) in 79 ms on algo-1 (executor 2) (2/4)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 39) in 35 ms on algo-1 (executor 2) (3/4)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 38) in 55 ms on algo-2 (executor 1) (4/4)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: ResultStage 10 (showString at NativeMethodAccessorImpl.java:0) finished in 0.128 s
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:21:05 INFO YarnScheduler: Killing all running tasks in stage 10: Stage finished
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Job 9 finished: showString at NativeMethodAccessorImpl.java:0, took 0.132501 s
[36malgo-1    |[0m 21/11/14 15:21:05 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Got job 10 (showString at NativeMethodAccessorImpl.java:0) with 11 output partitions
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Final stage: ResultStage 11 (showString at NativeMethodAccessorImpl.java:0)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[36] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:21:05 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 18.2 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 192.168.240.3:33331 (size: 8.2 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:21:05 INFO DAGScheduler: Submitting 11 missing tasks from ResultStage 11 (MapPartitionsRDD[36] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15))
[36malgo-1    |[0m 21/11/14 15:21:05 INFO YarnScheduler: Adding task set 11.0 with 11 tasks
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 40, algo-1, executor 2, partition 5, PROCESS_LOCAL, 7398 bytes)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 41, algo-2, executor 1, partition 6, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:33379 (size: 8.2 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-2:42513 (size: 8.2 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Starting task 2.0 in stage 11.0 (TID 42, algo-2, executor 1, partition 7, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 41) in 69 ms on algo-2 (executor 1) (1/11)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Starting task 3.0 in stage 11.0 (TID 43, algo-1, executor 2, partition 8, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 40) in 84 ms on algo-1 (executor 2) (2/11)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Starting task 4.0 in stage 11.0 (TID 44, algo-2, executor 1, partition 9, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Finished task 2.0 in stage 11.0 (TID 42) in 56 ms on algo-2 (executor 1) (3/11)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Starting task 5.0 in stage 11.0 (TID 45, algo-1, executor 2, partition 10, PROCESS_LOCAL, 7398 bytes)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Finished task 3.0 in stage 11.0 (TID 43) in 59 ms on algo-1 (executor 2) (4/11)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Starting task 6.0 in stage 11.0 (TID 46, algo-2, executor 1, partition 11, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:05 INFO TaskSetManager: Finished task 4.0 in stage 11.0 (TID 44) in 58 ms on algo-2 (executor 1) (5/11)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Starting task 7.0 in stage 11.0 (TID 47, algo-1, executor 2, partition 12, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Finished task 5.0 in stage 11.0 (TID 45) in 70 ms on algo-1 (executor 2) (6/11)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Starting task 8.0 in stage 11.0 (TID 48, algo-2, executor 1, partition 13, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Finished task 6.0 in stage 11.0 (TID 46) in 56 ms on algo-2 (executor 1) (7/11)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 192.168.240.3:33331 in memory (size: 8.2 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Removed broadcast_13_piece0 on algo-2:42513 in memory (size: 8.2 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:33379 in memory (size: 8.2 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Starting task 9.0 in stage 11.0 (TID 49, algo-1, executor 2, partition 14, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Finished task 7.0 in stage 11.0 (TID 47) in 56 ms on algo-1 (executor 2) (8/11)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 192.168.240.3:33331 in memory (size: 8.0 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:33379 in memory (size: 8.0 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Removed broadcast_10_piece0 on algo-2:42513 in memory (size: 8.0 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.240.3:33331 in memory (size: 30.3 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Removed broadcast_7_piece0 on algo-2:42513 in memory (size: 30.3 KiB, free: 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO TorrentBroadcast: Started reading broadcast variable 10 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 911.6 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO TorrentBroadcast: Reading broadcast variable 10 took 10 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 17.3 KiB, free 911.5 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO PythonRunner: Times: total = 2, boot = -645, init = 647, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO PythonUDFRunner: Times: total = 1, boot = -23, init = 24, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO Executor: Finished task 1.0 in stage 7.0 (TID 21). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 22
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO Executor: Running task 2.0 in stage 7.0 (TID 22)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO PythonRunner: Times: total = 14, boot = 14, init = 0, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO PythonUDFRunner: Times: total = 11, boot = 11, init = 0, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO Executor: Finished task 2.0 in stage 7.0 (TID 22). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 23
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO Executor: Running task 3.0 in stage 7.0 (TID 23)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO PythonRunner: Times: total = 45, boot = 18, init = 27, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO PythonUDFRunner: Times: total = 42, boot = 12, init = 30, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:03 INFO Executor: Finished task 3.0 in stage 7.0 (TID 23). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 24
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO Executor: Running task 0.0 in stage 8.0 (TID 24)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO TorrentBroadcast: Started reading broadcast variable 11 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 911.5 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO TorrentBroadcast: Reading broadcast variable 11 took 6 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 17.3 KiB, free 911.5 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO PythonRunner: Times: total = 2, boot = -1153, init = 1155, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO PythonUDFRunner: Times: total = 46, boot = -1159, init = 1201, finish = 4
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO Executor: Finished task 0.0 in stage 8.0 (TID 24). 2094 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 28
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO Executor: Running task 4.0 in stage 8.0 (TID 28)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO PythonRunner: Times: total = 2, boot = -29, init = 31, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO PythonUDFRunner: Times: total = 14, boot = 13, init = 1, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO Executor: Finished task 4.0 in stage 8.0 (TID 28). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 29
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO Executor: Running task 5.0 in stage 8.0 (TID 29)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO PythonRunner: Times: total = 45, boot = 2, init = 43, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO PythonUDFRunner: Times: total = 43, boot = 13, init = 30, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO Executor: Finished task 5.0 in stage 8.0 (TID 29). 2094 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 31
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO Executor: Running task 7.0 in stage 8.0 (TID 31)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO PythonRunner: Times: total = 16, boot = 16, init = 0, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO PythonUDFRunner: Times: total = 44, boot = 14, init = 30, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO Executor: Finished task 7.0 in stage 8.0 (TID 31). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 33
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO Executor: Running task 9.0 in stage 8.0 (TID 33)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO PythonRunner: Times: total = 2, boot = -22, init = 24, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO PythonUDFRunner: Times: total = 45, boot = 18, init = 27, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO Executor: Finished task 9.0 in stage 8.0 (TID 33). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 35
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO Executor: Running task 0.0 in stage 9.0 (TID 35)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO TorrentBroadcast: Started reading broadcast variable 12 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 911.5 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO TorrentBroadcast: Reading broadcast variable 12 took 6 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 18.2 KiB, free 911.5 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO CodeGenerator: Code generated in 9.932562 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO CodeGenerator: Code generated in 11.351773 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO PythonRunner: Times: total = 45, boot = -188, init = 233, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO PythonUDFRunner: Times: total = 43, boot = -147, init = 190, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO Executor: Finished task 0.0 in stage 9.0 (TID 35). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 37
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO Executor: Running task 1.0 in stage 10.0 (TID 37)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO TorrentBroadcast: Started reading broadcast variable 13 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 911.5 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO TorrentBroadcast: Reading broadcast variable 13 took 8 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 18.2 KiB, free 911.5 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO PythonRunner: Times: total = 2, boot = -7, init = 9, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO PythonUDFRunner: Times: total = 43, boot = -7, init = 50, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO Executor: Finished task 1.0 in stage 10.0 (TID 37). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 38
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO Executor: Running task 2.0 in stage 10.0 (TID 38)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO PythonRunner: Times: total = 1, boot = -30, init = 31, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO PythonUDFRunner: Times: total = 43, boot = 18, init = 25, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO Executor: Finished task 2.0 in stage 10.0 (TID 38). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 41
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO Executor: Running task 1.0 in stage 11.0 (TID 41)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO TorrentBroadcast: Started reading broadcast variable 14 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 911.5 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO TorrentBroadcast: Reading broadcast variable 14 took 7 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 18.2 KiB, free 911.4 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO PythonRunner: Times: total = 42, boot = -44, init = 86, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO PythonUDFRunner: Times: total = 46, boot = -9, init = 55, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO Executor: Finished task 1.0 in stage 11.0 (TID 41). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 42
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 192.168.240.3:33331 in memory (size: 8.2 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Removed broadcast_12_piece0 on algo-2:42513 in memory (size: 8.2 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.240.3:33331 in memory (size: 30.3 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:33379 in memory (size: 30.3 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Starting task 10.0 in stage 11.0 (TID 50, algo-2, executor 1, partition 15, PROCESS_LOCAL, 7398 bytes)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Finished task 8.0 in stage 11.0 (TID 48) in 57 ms on algo-2 (executor 1) (9/11)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 192.168.240.3:33331 in memory (size: 68.7 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Removed broadcast_8_piece0 on algo-2:42513 in memory (size: 68.7 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 192.168.240.3:33331 in memory (size: 8.0 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:33379 in memory (size: 8.0 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Removed broadcast_11_piece0 on algo-2:42513 in memory (size: 8.0 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Removed broadcast_9_piece0 on algo-2:42513 in memory (size: 8.0 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 192.168.240.3:33331 in memory (size: 8.0 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Finished task 9.0 in stage 11.0 (TID 49) in 57 ms on algo-1 (executor 2) (10/11)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Finished task 10.0 in stage 11.0 (TID 50) in 75 ms on algo-2 (executor 1) (11/11)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: ResultStage 11 (showString at NativeMethodAccessorImpl.java:0) finished in 0.373 s
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:21:06 INFO YarnScheduler: Killing all running tasks in stage 11: Stage finished
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: Job 10 finished: showString at NativeMethodAccessorImpl.java:0, took 0.377028 s
[36malgo-1    |[0m 21/11/14 15:21:06 INFO CodeGenerator: Code generated in 12.178905 ms
[36malgo-1    |[0m +--------+-----------+--------------------+--------------------+---------------+
[36malgo-1    |[0m |batch_id|primary_key|                text|               words|sentiment_score|
[36malgo-1    |[0m +--------+-----------+--------------------+--------------------+---------------+
[36malgo-1    |[0m |       1|          1|This is a great book|This is a great book|         0.6249|
[36malgo-1    |[0m |       1|          1|This is a great book|This is a great book|         0.6249|
[36malgo-1    |[0m |       1|          1|This is a great book|This is a great book|         0.6249|
[36malgo-1    |[0m +--------+-----------+--------------------+--------------------+---------------+
[36malgo-1    |[0m 
[36malgo-1    |[0m 11/14 15:21:04 INFO CodeGenerator: Code generated in 11.480213 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO PythonUDFRunner: Times: total = 654, boot = 3, init = 651, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO Executor: Finished task 0.0 in stage 7.0 (TID 20). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 25
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO Executor: Running task 1.0 in stage 8.0 (TID 25)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO TorrentBroadcast: Started reading broadcast variable 11 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO TorrentBroadcast: Reading broadcast variable 11 took 6 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 17.3 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO PythonRunner: Times: total = 2, boot = -730, init = 732, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO PythonUDFRunner: Times: total = 2, boot = -13, init = 15, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO Executor: Finished task 1.0 in stage 8.0 (TID 25). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 26
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO Executor: Running task 2.0 in stage 8.0 (TID 26)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO PythonRunner: Times: total = 2, boot = -8, init = 10, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO PythonUDFRunner: Times: total = 20, boot = 19, init = 1, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO Executor: Finished task 2.0 in stage 8.0 (TID 26). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 27
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO Executor: Running task 3.0 in stage 8.0 (TID 27)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO PythonRunner: Times: total = 45, boot = -22, init = 67, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO PythonUDFRunner: Times: total = 42, boot = 20, init = 22, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO Executor: Finished task 3.0 in stage 8.0 (TID 27). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 30
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO Executor: Running task 6.0 in stage 8.0 (TID 30)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO PythonRunner: Times: total = 42, boot = -1, init = 43, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO PythonUDFRunner: Times: total = 43, boot = 12, init = 31, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO Executor: Finished task 6.0 in stage 8.0 (TID 30). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 32
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO Executor: Running task 8.0 in stage 8.0 (TID 32)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO PythonRunner: Times: total = 42, boot = -6, init = 48, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO PythonUDFRunner: Times: total = 46, boot = 16, init = 30, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO Executor: Finished task 8.0 in stage 8.0 (TID 32). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 34
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO Executor: Running task 10.0 in stage 8.0 (TID 34)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO PythonRunner: Times: total = 45, boot = -11, init = 56, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO PythonUDFRunner: Times: total = 52, boot = 16, init = 35, finish = 1
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO Executor: Finished task 10.0 in stage 8.0 (TID 34). 2094 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 36
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO Executor: Running task 0.0 in stage 10.0 (TID 36)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO TorrentBroadcast: Started reading broadcast variable 13 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO TorrentBroadcast: Reading broadcast variable 13 took 7 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 18.2 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO PythonRunner: Times: total = 3, boot = -245, init = 248, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO CodeGenerator: Code generated in 14.495923 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO CodeGenerator: Code generated in 16.087598 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO PythonUDFRunner: Times: total = 48, boot = -221, init = 269, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO Executor: Finished task 0.0 in stage 10.0 (TID 36). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 39
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO Executor: Running task 3.0 in stage 10.0 (TID 39)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO PythonRunner: Times: total = 2, boot = -56, init = 58, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO PythonUDFRunner: Times: total = 22, boot = 22, init = 0, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO Executor: Finished task 3.0 in stage 10.0 (TID 39). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 40
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO Executor: Running task 0.0 in stage 11.0 (TID 40)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO TorrentBroadcast: Started reading broadcast variable 14 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO TorrentBroadcast: Reading broadcast variable 14 took 6 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 18.2 KiB, free 911.7 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO PythonRunner: Times: total = 48, boot = -57, init = 105, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] Performing Sentiment Analysis
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO PythonUDFRunner: Times: total = 60, boot = -16, init = 63, finish = 13
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO Executor: Finished task 0.0 in stage 11.0 (TID 40). 2109 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 43
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO Executor: Running task 3.0 in stage 11.0 (TID 43)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO PythonRunner: Times: total = 48, boot = -20, init = 68, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO PythonUDFRunner: Times: total = 45, boot = 13, init = 32, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO Executor: Finished task 3.0 in stage 11.0 (TID 43). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 45
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO Executor: Running task 5.0 in stage 11.0 (TID 45)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:05 INFO PythonRunner: Times: total = 42, boot = -2, init = 44, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] Performing Sentiment Analysis
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO PythonUDFRunner: Times: total = 57, boot = 12, init = 34, finish = 11
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO Executor: Finished task 5.0 in stage 11.0 (TID 45). 2109 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 47
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO Executor: Running task 7.0 in stage 11.0 (TID 47)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO PythonRunner: Times: total = 42, boot = -20, init = 62, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO PythonUDFRunner: Times: total = 43, boot = 12, init = 31, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO Executor: Finished task 7.0 in stage 11.0 (TID 47). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 49
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO Executor: Running task 9.0 in stage 11.0 (TID 49)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO PythonRunner: Times: total = 43, boot = -4, init = 47, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO PythonUDFRunner: Times: total = 43, boot = 23, init = 20, fini21/11/14 15:21:06 INFO CodeGenerator: Code generated in 16.919054 ms
[36malgo-1    |[0m 21/11/14 15:21:06 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: Got job 11 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: Final stage: ResultStage 12 (showString at NativeMethodAccessorImpl.java:0)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[41] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:21:06 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 20.9 KiB, free 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 192.168.240.3:33331 (size: 8.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[41] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 15:21:06 INFO YarnScheduler: Adding task set 12.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 51, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:33379 (size: 8.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 51) in 109 ms on algo-1 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO YarnScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: ResultStage 12 (showString at NativeMethodAccessorImpl.java:0) finished in 0.115 s
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:21:06 INFO YarnScheduler: Killing all running tasks in stage 12: Stage finished
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: Job 11 finished: showString at NativeMethodAccessorImpl.java:0, took 0.120374 s
[36malgo-1    |[0m 21/11/14 15:21:06 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: Got job 12 (showString at NativeMethodAccessorImpl.java:0) with 4 output partitions
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: Final stage: ResultStage 13 (showString at NativeMethodAccessorImpl.java:0)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[41] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:21:06 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 20.9 KiB, free 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 192.168.240.3:33331 (size: 8.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 13 (MapPartitionsRDD[41] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1, 2, 3, 4))
[36malgo-1    |[0m 21/11/14 15:21:06 INFO YarnScheduler: Adding task set 13.0 with 4 tasks
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 52, algo-2, executor 1, partition 1, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 53, algo-1, executor 2, partition 2, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:33379 (size: 8.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-2:42513 (size: 8.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Starting task 2.0 in stage 13.0 (TID 54, algo-1, executor 2, partition 3, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 53) in 62 ms on algo-1 (executor 2) (1/4)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Starting task 3.0 in stage 13.0 (TID 55, algo-2, executor 1, partition 4, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 52) in 85 ms on algo-2 (executor 1) (2/4)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Finished task 3.0 in stage 13.0 (TID 55) in 21 ms on algo-2 (executor 1) (3/4)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Finished task 2.0 in stage 13.0 (TID 54) in 55 ms on algo-1 (executor 2) (4/4)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: ResultStage 13 (showString at NativeMethodAccessorImpl.java:0) finished in 0.121 s
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:21:06 INFO YarnScheduler: Killing all running tasks in stage 13: Stage finished
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: Job 12 finished: showString at NativeMethodAccessorImpl.java:0, took 0.124571 s
[36malgo-1    |[0m 21/11/14 15:21:06 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: Got job 13 (showString at NativeMethodAccessorImpl.java:0) with 11 output partitions
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: Final stage: ResultStage 14 (showString at NativeMethodAccessorImpl.java:0)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[41] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:21:06 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 20.9 KiB, free 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 192.168.240.3:33331 (size: 8.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:21:06 INFO DAGScheduler: Submitting 11 missing tasks from ResultStage 14 (MapPartitionsRDD[41] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15))
[36malgo-1    |[0m 21/11/14 15:21:06 INFO YarnScheduler: Adding task set 14.0 with 11 tasks
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 56, algo-2, executor 1, partition 5, PROCESS_LOCAL, 7398 bytes)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 57, algo-1, executor 2, partition 6, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-2:42513 (size: 8.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:33379 (size: 8.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Starting task 2.0 in stage 14.0 (TID 58, algo-1, executor 2, partition 7, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 57) in 62 ms on algo-1 (executor 2) (1/11)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Starting task 3.0 in stage 14.0 (TID 59, algo-2, executor 1, partition 8, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:06 WARN TaskSetManager: Lost task 0.0 in stage 14.0 (TID 56, algo-2, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 35, in decorated
[36malgo-1    |[0m     return func(*args, **kwargs)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 57, in tokenize
[36malgo-1    |[0m     return nltk.tokenize.sent_tokenize(text)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/tokenize/__init__.py", line 106, in sent_tokenize
[36malgo-1    |[0m     tokenizer = load(f"tokenizers/punkt/{language}.pickle")
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 750, in load
[36malgo-1    |[0m     opened_resource = _open(resource_url)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 876, in _open
[36malgo-1    |[0m     return find(path_, path + [""]).open()
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 583, in find
[36malgo-1    |[0m     raise LookupError(resource_not_found)
[36malgo-1    |[0m LookupError: 
[36malgo-1    |[0m **********************************************************************
[36malgo-1    |[0m   Resource [93mpunkt[0m not found.
[36malgo-1    |[0m   Please use the NLTK Downloader to obtain the resource:
[36malgo-1    |[0m 
[36malgo-1    |[0m   [31m>>> import nltk
[36malgo-1    |[0m   >>> nltk.download('punkt')
[36malgo-1    |[0m   [0m
[36malgo-1    |[0m   For more information see: https://www.nltk.org/data.html
[36malgo-1    |[0m 
[36malgo-1    |[0m   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[36malgo-1    |[0m 
[36malgo-1    |[0m   Searched in:
[36malgo-1    |[0m     - '/home/nltk_data'
[36malgo-1    |[0m     - '/usr/nltk_data'
[36malgo-1    |[0m     - '/usr/share/nltk_data'
[36malgo-1    |[0m     - '/usr/lib/nltk_data'
[36malgo-1    |[0m     - '/usr/share/nltk_data'
[36malgo-1    |[0m     - '/usr/local/share/nltk_data'
[36malgo-1    |[0m     - '/usr/lib/nltk_data'
[36malgo-1    |[0m     - '/usr/local/lib/nltk_data'
[36malgo-1    |[0m     - ''
[36malgo-1    |[0m **********************************************************************
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m During handling of the above exception, another exception occurred:
[36malgo-1    |[0m 
[36malgo-1    |[0m Traceback (most recent call last):
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/worker.py", line 605, in main
[36malgo-1    |[0m     process()
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/worker.py", line 597, in process
[36malgo-1    |[0m     serializer.dump_stream(out_iter, outfile)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/serializers.py", line 223, in dump_stream
[36malgo-1    |[0m     self.serializer.dump_stream(self._batched(iterator), stream)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/serializers.py", line 141, in dump_stream
[36malgo-1    |[0m     for obj in iterator:
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/serializers.py", line 212, in _batched
[36malgo-1    |[0m     for item in iterator:
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/worker.py", line 450, in mapper
[36malgo-1    |[0m     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/worker.py", line 450, in <genexpr>
[36malgo-1    |[0m     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/worker.py", line 90, in <lambda>
[36malgo-1    |[0m     return lambda *a: f(*a)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/util.py", line 121, in wrapper
[36malgo-1    |[0m     return f(*args, **kwargs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/worker.py", line 82, in <lambda>
[36malgo-1    |[0m     return lambda *a: g(f(*a))
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/__pyfiles__/hello_py_spark_udfs.py", line 60, in f_emo
[36malgo-1    |[0m     out = emotion_fit(lower_s)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/__pyfiles__/hello_py_spark_udfs.py", line 157, in emotion_fit
[36malgo-1    |[0m     nrclex_out = NRCLex(str(clean_cell))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nrclex.py", line 2873, in __init__
[36malgo-1    |[0m     self.words = list(blob.words)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 24, in __get__
[36malgo-1    |[0m     value = obj.__dict__[self.func.__name__] = self.func(obj)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/blob.py", line 678, in words
[36malgo-1    |[0m     return WordList(word_tokenize(self.raw, include_punc=False))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 73, in word_tokenize
[36malgo-1    |[0m     for sentence in sent_tokenize(text))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/base.py", line 64, in itokenize
[36malgo-1    |[0m     return (t for t in self.tokenize(text, *args, **kwargs))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 38, in decorated
[36malgo-1    |[0m     raise MissingCorpusError()
[36malgo-1    |[0m textblob.exceptions.MissingCorpusError: 
[36malgo-1    |[0m Looks like you are missing some required data for this feature.
[36malgo-1    |[0m 
[36malgo-1    |[0m To download the necessary data, simply run
[36malgo-1    |[0m 
[36malgo-1    |[0m     python -m textblob.download_corpora
[36malgo-1    |[0m 
[36malgo-1    |[0m or use the NLTK downloader to download the missing data: http://nltk.org/data.html
[36malgo-1    |[0m If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)
[36malgo-1    |[0m 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
[36malgo-1    |[0m 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[36malgo-1    |[0m 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
[36malgo-1    |[0m 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[36malgo-1    |[0m 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[36malgo-1    |[0m 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
[36malgo-1    |[0m 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
[36malgo-1    |[0m 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
[36malgo-1    |[0m 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[36malgo-1    |[0m 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
[36malgo-1    |[0m 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
[36malgo-1    |[0m 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[36malgo-1    |[0m 	at org.apache.spark.scheduler.Task.run(Task.scala:127)
[36malgo-1    |[0m 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
[36malgo-1    |[0m 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
[36malgo-1    |[0m 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
[36malgo-1    |[0m 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[36malgo-1    |[0m 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[36malgo-1    |[0m 	at java.lang.Thread.run(Thread.java:748)
[36malgo-1    |[0m 
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Starting task 0.1 in stage 14.0 (TID 60, algo-1, executor 2, partition 5, PROCESS_LOCAL, 7398 bytes)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Finished task 2.0 in stage 14.0 (TID 58) in 54 ms on algo-1 (executor 2) (2/11)
[36malgo-1    |[0m 21/11/14 15:21:06 INFO TaskSetManager: Starting task 4.0 in stage 14.0 (TID 61, algo-1, executor 2, partition 9, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:06 WARN TaskSetManager: Lost task 0.1 in stage 14.0 (TID 60, algo-1, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 35, in decorated
[36malgo-1    |[0m     return func(*args, **kwargs)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 57, in tokenize
[36malgo-1    |[0m     return nltk.tokenize.sent_tokenize(text)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/tokenize/__init__.py", line 106, in sent_tokenize
[36malgo-1    |[0m     tokenizer = load(f"tokenizers/punkt/{language}.pickle")
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 750, in load
[36malgo-1    |[0m     opened_resource = _open(resource_url)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 876, in _open
[36malgo-1    |[0m     return find(path_, path + [""]).open()
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 583, in find
[36malgo-1    |[0m     raise LookupError(resource_not_found)
[36malgo-1    |[0m LookupError: 
[36malgo-1    |[0m **********************************************************************
[36malgo-1    |[0m   Resource [93mpunkt[0m not found.
[36malgo-1    |[0m   Please use the NLTK Downloader to obtain the resource:
[36malgo-1    |[0m 
[36malgo-1    |[0m   [31m>>> import nltk
[36malgo-1    |[0m   >>> nltk.download('punkt')
[36malgo-1    |[0m   [0m
[36malgo-1    |[0m   For more information see: https://www.nltk.org/data.html
[36malgo-1    |[0m 
[36malgo-1    |[0m   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[36malgo-1    |[0m 
[36malgo-1    |[0m   Searched in:
[36malgo-1    |[0m     - '/home/nltk_data'
[36malgo-1    |[0m     - '/usr/nltk_data'
[36malgo-1    |[0m     - '/usr/share/nltk_data'
[36malgo-1    |[0m     - '/usr/lib/nltk_data'
[36malgo-1    |[0m     - '/usr/share/nltk_data'
[36malgo-1    |[0m     - '/usr/local/share/nltk_data'
[36malgo-1    |[0m     - '/usr/lib/nltk_data'
[36malgo-1    |[0m     - '/usr/local/lib/nltk_data'
[36malgo-1    |[0m     - ''
[36malgo-1    |[0m **********************************************************************
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m During handling of the above exception, another exception occurred:
[36malgo-1    |[0m 
[36malgo-1    |[0m Traceback (most recent call last):
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 605, in main
[36malgo-1    |[0m     process()
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 597, in process
[36malgo-1    |[0m     serializer.dump_stream(out_iter, outfile)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 223, in dump_stream
[36malgo-1    |[0m     self.serializer.dump_stream(self._batched(iterator), stream)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 141, in dump_stream
[36malgo-1    |[0m     for obj in iterator:
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 212, in _batched
[36malgo-1    |[0m     for item in iterator:
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 450, in mapper
[36malgo-1    |[0m     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 450, in <genexpr>
[36malgo-1    |[0m     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 90, in <lambda>
[36malgo-1    |[0m     return lambda *a: f(*a)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/util.py", line 121, in wrapper
[36malgo-1    |[0m     return f(*args, **kwargs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 82, in <lambda>
[36malgo-1    |[0m     return lambda *a: g(f(*a))
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/__pyfiles__/hello_py_spark_udfs.py", line 60, in f_emo
[36malgo-1    |[0m     out = emotion_fit(lower_s)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/__pyfiles__/hello_py_spark_udfs.py", line 157, in emotion_fit
[36malgo-1    |[0m     nrclex_out = NRCLex(str(clean_cell))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nrclex.py", line 2873, in __init__
[36malgo-1    |[0m     self.words = list(blob.words)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 24, in __get__
[36malgo-1    |[0m     value = obj.__dict__[self.func.__name__] = self.func(obj)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/blob.py", line 678, in words
[36malgo-1    |[0m     return WordList(word_tokenize(self.raw, include_punc=False))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 73, in word_tokenize
[36malgo-1    |[0m     for sentence in sent_tokenize(text))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/base.py", line 64, in itokenize
[36malgo-1    |[0m     return (t for t in self.tokenize(text, *args, **kwargs))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 38, in decorated
[36malgo-1    |[0m     raise MissingCorpusError()
[36malgo-1    |[0m textblob.exceptions.MissingCorpusError: 
[36malgo-1    |[0m Looks like you are missing some required data for this feature.
[36malgo-1    |[0m 
[36malgo-1    |[0m To download the necessary data, simply run
[36malgo-1    |[0m 
[36malgo-1    |[0m     python -m textblob.download_corpora
[36malgo-1    |[0m 
[36malgo-1    |[0m or use the NLTK downloader to download the missing data: http://nltk.org/data.html
[36malgo-1    |[0m If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)
[36malgo-1    |[0m 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
[36malgo-1    |[0m 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[36malgo-1    |[0m 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
[36malgo-1    |[0m 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[36malgo-1    |[0m 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[36malgo-1    |[0m 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
[36malgo-1    |[0m 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
[36malgo-1    |[0m 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
[36malgo-1    |[0m 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[36malgo-1    |[0m 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
[36malgo-1    |[0m 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
[36malgo-1    |[0m 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[36malgo-1    |[0m 	at org.apache.spark.scheduler.Task.run(Task.scala:127)
[36malgo-1    |[0m 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
[36malgo-1    |[0m 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
[36malgo-1    |[0m 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
[36malgo-1    |[0m 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[36malgo-1    |[0m 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[36malgo-1    |[0m 	at java.lang.Thread.run(Thread.java:748)
[36malgo-1    |[0m 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO Executor: Running task 2.0 in stage 11.0 (TID 42)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO PythonRunner: Times: total = 11, boot = 11, init = 0, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO PythonUDFRunner: Times: total = 42, boot = 19, init = 23, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO Executor: Finished task 2.0 in stage 11.0 (TID 42). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 44
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO Executor: Running task 4.0 in stage 11.0 (TID 44)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO PythonRunner: Times: total = 2, boot = -20, init = 22, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO PythonUDFRunner: Times: total = 44, boot = 17, init = 27, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO Executor: Finished task 4.0 in stage 11.0 (TID 44). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 46
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:05 INFO Executor: Running task 6.0 in stage 11.0 (TID 46)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO PythonRunner: Times: total = 46, boot = -27, init = 73, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO PythonUDFRunner: Times: total = 43, boot = 10, init = 33, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO Executor: Finished task 6.0 in stage 11.0 (TID 46). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 48
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO Executor: Running task 8.0 in stage 11.0 (TID 48)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO PythonRunner: Times: total = 25, boot = 24, init = 1, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO PythonUDFRunner: Times: total = 42, boot = 20, init = 22, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO Executor: Finished task 8.0 in stage 11.0 (TID 48). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 50
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO Executor: Running task 10.0 in stage 11.0 (TID 50)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO PythonRunner: Times: total = 5, boot = 3, init = 2, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] Performing Sentiment Analysis
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO PythonUDFRunner: Times: total = 54, boot = 16, init = 26, finish = 12
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO Executor: Finished task 10.0 in stage 11.0 (TID 50). 2109 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 52
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO Executor: Running task 0.0 in stage 13.0 (TID 52)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO TorrentBroadcast: Started reading broadcast variable 16 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO TorrentBroadcast: Reading broadcast variable 16 took 5 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 20.9 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO CodeGenerator: Code generated in 16.596239 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO PythonRunner: Times: total = 45, boot = -350, init = 395, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO CodeGenerator: Code generated in 25.546509 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO PythonUDFRunner: Times: total = 41, boot = -298, init = 339, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO Executor: Finished task 0.0 in stage 13.0 (TID 52). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 55
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO Executor: Running task 3.0 in stage 13.0 (TID 55)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO PythonRunner: Times: total = 5, boot = 4, init = 1, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO PythonUDFRunner: Times: total = 8, boot = 7, init = 1, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO Executor: Finished task 3.0 in stage 13.0 (TID 55). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 56
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO Executor: Running task 0.0 in stage 14.0 (TID 56)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO TorrentBroadcast: Started reading broadcast variable 17 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO TorrentBroadcast: Reading broadcast variable 17 took 6 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 20.9 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO PythonRunner: Times: total = 2, boot = -25, init = 27, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 1st Check
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] **********************************************************************
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   Resource [93mpunkt[0m not found.
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   Please use the NLTK Downloader to obtain the resource:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   [31m>>> import nltk
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   >>> nltk.download('punkt')
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   [0m
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   For more information see: https://www.nltk.org/data.html
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   Searched in:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/home/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/share/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/lib/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/share/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/local/share/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/lib/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/local/lib/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - ''
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] **********************************************************************
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 ERROR Executor: Exception in task 0.0 in stage 14.0 (TID 56)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 35, in decorated
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     return func(*args, **kwargs)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 57, in tokenize
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     return nltk.tokenize.sent_tokenize(text)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/tokenize/__init__.py", line 106, in sent_tokenize
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     tokenizer = load(f"tokenizers/punkt/{language}.pickle")
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 750, in load
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     opened_resource = _open(resource_url)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 876, in _open
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     return find(path_, path + [""]).open()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 583, in find
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     raise LookupError(resource_not_found)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] LookupError: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] **********************************************************************
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   Resource [93mpunkt[0m not found.
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   Please use the NLTK Downloader to obtain the resource:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   [31m>>> import nltk
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   >>> nltk.download('punkt')
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   [0m
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   For more information see: https://www.nltk.org/data.html
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   Searched in:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/home/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/share/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/lib/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/share/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/local/share/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/lib/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/local/lib/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - ''
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] **********************************************************************
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] During handling of the above exception, another exception occurred:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] Traceback (most recent call last):
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/worker.py", line 605, in main
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     process()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/worker.py", line 597, in process
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     serializer.dump_stream(out_iter, outfile)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/serializers.py", line 223, in dump_stream
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     self.serializer.dump_stream(self._batched(iterator), stream)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/serializers.py", line 141, in dump_stream
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     for obj in iterator:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/serializers.py", line 212, in _batched
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     for item in iterator:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/worker.py", line 450, in mapper
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/worker.py", line 450, in <genexpr>
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/worker.py", line 90, in <lambda>
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     return lambda *a: f(*a)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/util.py", line 121, in wrapper
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     return f(*args, **kwargs)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/worker.py", line 82, in <lambda>
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     return lambda *a: g(f(*a))
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/__pyfiles__/hello_py_spark_udfs.py", line 60, in f_emo
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     out = emotion_fit(lower_s)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/__pyfiles__/hello_py_spark_udfs.py", line 157, in emotion_fit
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     nrclex_out = NRCLex(str(clean_cell))
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/nrclex.py", line 2873, in __init__
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     self.words = list(blob.words)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 24, in __get__
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     value = obj.__dict__[self.func.__name__] = self.func(obj)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/blob.py", line 678, in words
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     return WordList(word_tokenize(self.raw, include_punc=False))
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 73, in word_tokenize
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     for sentence in sent_tokenize(text))
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/base.py", line 64, in itokenize
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     return (t for t in self.tokenize(text, *args, **kwargs))
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 38, in decorated
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     raise MissingCorpusError()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] textblob.exceptions.MissingCorpusError: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] Looks like you are missing some required data for this feature.
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] To download the necessary data, simply run
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     python -m textblob.download_corpora
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] or use the NLTK downloader to download the missing data: http://nltk.org/data.html
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.scheduler.Task.run(Task.scala:127)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[36malgo-1    |[0m sh = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO Executor: Finished task 9.0 in stage 11.0 (TID 49). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 51
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO Executor: Running task 0.0 in stage 12.0 (TID 51)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO TorrentBroadcast: Started reading broadcast variable 15 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO TorrentBroadcast: Reading broadcast variable 15 took 5 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 20.9 KiB, free 912.2 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO PythonRunner: Times: total = 2, boot = -238, init = 240, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO CodeGenerator: Code generated in 15.933895 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO CodeGenerator: Code generated in 20.861256 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO PythonUDFRunner: Times: total = 2, boot = -222, init = 224, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO Executor: Finished task 0.0 in stage 12.0 (TID 51). 2065 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 53
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO Executor: Running task 1.0 in stage 13.0 (TID 53)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO TorrentBroadcast: Started reading broadcast variable 16 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 912.2 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO TorrentBroadcast: Reading broadcast variable 16 took 5 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 20.9 KiB, free 912.2 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO PythonRunner: Times: total = 42, boot = -112, init = 154, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO PythonUDFRunner: Times: total = 40, boot = -91, init = 131, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO Executor: Finished task 1.0 in stage 13.0 (TID 53). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 54
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO Executor: Running task 2.0 in stage 13.0 (TID 54)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO PythonRunner: Times: total = 1, boot = 1, init = 0, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO PythonUDFRunner: Times: total = 42, boot = 23, init = 19, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO Executor: Finished task 2.0 in stage 13.0 (TID 54). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 57
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO Executor: Running task 1.0 in stage 14.0 (TID 57)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO TorrentBroadcast: Started reading broadcast variable 17 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 912.2 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO TorrentBroadcast: Reading broadcast variable 17 took 5 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 20.9 KiB, free 912.2 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO PythonRunner: Times: total = 1, boot = -65, init = 66, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO PythonUDFRunner: Times: total = 41, boot = -5, init = 46, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO Executor: Finished task 1.0 in stage 14.0 (TID 57). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 58
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO Executor: Running task 2.0 in stage 14.0 (TID 58)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO PythonRunner: Times: total = 45, boot = -43, init = 88, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO PythonUDFRunner: Times: total = 43, boot = 17, init = 26, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO Executor: Finished task 2.0 in stage 14.0 (TID 58). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 60
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO Executor: Running task 0.1 in stage 14.0 (TID 60)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO PythonRunner: Times: total = 2, boot = -1, init = 2, finish = 1
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 1st Check
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] **********************************************************************
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   Resource [93mpunkt[0m not found.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   Please use the NLTK Downloader to obtain the resource:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   [31m>>> import nltk
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   >>> nltk.download('punkt')
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   [0m
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   For more information see: https://www.nltk.org/data.html
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   Searched in:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/home/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/share/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/lib/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/share/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/local/share/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/lib/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/local/lib/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - ''
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] **********************************************************************
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 ERROR Executor: Exception in task 0.1 in stage 14.0 (TID 60)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 35, in decorated
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     return func(*args, **kwargs)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 57, in tokenize
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     return nltk.tokenize.sent_tokenize(text)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/tokenize/__init__.py", line 106, in sent_tokenize
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     tokenizer = load(f"tokenizers/punkt/{language}.pickle")
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 750, in load
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     opened_resource = _open(resource_url)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 876, in _open
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     return find(path_, path + [""]).open()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 583, in find
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     raise LookupError(resource_not_found)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] LookupError: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] **********************************************************************
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   Resource [93mpunkt[0m not found.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   Please use the NLTK Downloader to obtain the resource:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   [31m>>> import nltk
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   >>> nltk.download('punkt')
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   [0m
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   For more information see: https://www.nltk.org/data.html
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   Searched in:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/home/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/share/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/lib/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/share/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/local/share/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/lib/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/local/lib/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - ''
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] **********************************************************************
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] During handling of the above exception, another exception occurred:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] Traceback (most recent call last):
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 605, in main
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     process()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 597, in process
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     serializer.dump_stream(out_iter, outfile)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 223, in dump_stream
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     self.serializer.dump_stream(self._batched(iterator), stream)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 141, in dump_stream
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     for obj in iterator:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 212, in _batched
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     for item in iterator:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 450, in mapper
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 450, in <genexpr>
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 90, in <lambda>
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     return lambda *a: f(*a)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/util.py", line 121, in wrapper
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     return f(*args, **kwargs)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 82, in <lambda>
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     return lambda *a: g(f(*a))
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/__pyfiles__/hello_py_spark_udfs.py", line 60, in f_emo
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     out = emotion_fit(lower_s)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/__pyfiles__/hello_py_spark_udfs.py", line 157, in emotion_fit
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     nrclex_out = NRCLex(str(clean_cell))
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/nrclex.py", line 2873, in __init__
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     self.words = list(blob.words)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 24, in __get__
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     value = obj.__dict__[self.func.__name__] = self.func(obj)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/blob.py", line 678, in words
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     return WordList(word_tokenize(self.raw, include_punc=False))
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 73, in word_tokenize
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     for sentence in sent_tokenize(text))
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/base.py", line 64, in itokenize
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     return (t for t in self.tokenize(text, *args, **kwargs))
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 38, in decorated
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     raise MissingCorpusError()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] textblob.exceptions.MissingCorpusError: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] Looks like you are missing some required data for this feature.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] To download the necessary data, simply run
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     python -m textblob.download_corpora
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] or use the NLTK downloader to download the missing data: http://nltk.org/data.html
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.scheduler.Task.run(Task.scala:127)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at java.lang.Thread.run(Thread.java:748)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 61
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO Executor: Running task 4.0 in stage 14.0 (TID 61)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_16369031521/11/14 15:21:07 INFO TaskSetManager: Starting task 0.2 in stage 14.0 (TID 62, algo-2, executor 1, partition 5, PROCESS_LOCAL, 7398 bytes)
[36malgo-1    |[0m 21/11/14 15:21:07 INFO TaskSetManager: Finished task 3.0 in stage 14.0 (TID 59) in 662 ms on algo-2 (executor 1) (3/11)
[36malgo-1    |[0m 21/11/14 15:21:07 INFO TaskSetManager: Starting task 5.0 in stage 14.0 (TID 63, algo-2, executor 1, partition 10, PROCESS_LOCAL, 7398 bytes)
[36malgo-1    |[0m 21/11/14 15:21:07 INFO TaskSetManager: Lost task 0.2 in stage 14.0 (TID 62) on algo-2, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 35, in decorated
[36malgo-1    |[0m     return func(*args, **kwargs)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 57, in tokenize
[36malgo-1    |[0m     return nltk.tokenize.sent_tokenize(text)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/tokenize/__init__.py", line 106, in sent_tokenize
[36malgo-1    |[0m     tokenizer = load(f"tokenizers/punkt/{language}.pickle")
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 750, in load
[36malgo-1    |[0m     opened_resource = _open(resource_url)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 876, in _open
[36malgo-1    |[0m     return find(path_, path + [""]).open()
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 583, in find
[36malgo-1    |[0m     raise LookupError(resource_not_found)
[36malgo-1    |[0m LookupError: 
[36malgo-1    |[0m **********************************************************************
[36malgo-1    |[0m   Resource [93mpunkt[0m not found.
[36malgo-1    |[0m   Please use the NLTK Downloader to obtain the resource:
[36malgo-1    |[0m 
[36malgo-1    |[0m   [31m>>> import nltk
[36malgo-1    |[0m   >>> nltk.download('punkt')
[36malgo-1    |[0m   [0m
[36malgo-1    |[0m   For more information see: https://www.nltk.org/data.html
[36malgo-1    |[0m 
[36malgo-1    |[0m   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[36malgo-1    |[0m 
[36malgo-1    |[0m   Searched in:
[36malgo-1    |[0m     - '/home/nltk_data'
[36malgo-1    |[0m     - '/usr/nltk_data'
[36malgo-1    |[0m     - '/usr/share/nltk_data'
[36malgo-1    |[0m     - '/usr/lib/nltk_data'
[36malgo-1    |[0m     - '/usr/share/nltk_data'
[36malgo-1    |[0m     - '/usr/local/share/nltk_data'
[36malgo-1    |[0m     - '/usr/lib/nltk_data'
[36malgo-1    |[0m     - '/usr/local/lib/nltk_data'
[36malgo-1    |[0m     - ''
[36malgo-1    |[0m **********************************************************************
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m During handling of the above exception, another exception occurred:
[36malgo-1    |[0m 
[36malgo-1    |[0m Traceback (most recent call last):
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/worker.py", line 605, in main
[36malgo-1    |[0m     process()
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/worker.py", line 597, in process
[36malgo-1    |[0m     serializer.dump_stream(out_iter, outfile)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/serializers.py", line 223, in dump_stream
[36malgo-1    |[0m     self.serializer.dump_stream(self._batched(iterator), stream)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/serializers.py", line 141, in dump_stream
[36malgo-1    |[0m     for obj in iterator:
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/serializers.py", line 212, in _batched
[36malgo-1    |[0m     for item in iterator:
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/worker.py", line 450, in mapper
[36malgo-1    |[0m     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/worker.py", line 450, in <genexpr>
[36malgo-1    |[0m     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/worker.py", line 90, in <lambda>
[36malgo-1    |[0m     return lambda *a: f(*a)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/util.py", line 121, in wrapper
[36malgo-1    |[0m     return f(*args, **kwargs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/worker.py", line 82, in <lambda>
[36malgo-1    |[0m     return lambda *a: g(f(*a))
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/__pyfiles__/hello_py_spark_udfs.py", line 60, in f_emo
[36malgo-1    |[0m     out = emotion_fit(lower_s)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/__pyfiles__/hello_py_spark_udfs.py", line 157, in emotion_fit
[36malgo-1    |[0m     nrclex_out = NRCLex(str(clean_cell))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nrclex.py", line 2873, in __init__
[36malgo-1    |[0m     self.words = list(blob.words)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 24, in __get__
[36malgo-1    |[0m     value = obj.__dict__[self.func.__name__] = self.func(obj)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/blob.py", line 678, in words
[36malgo-1    |[0m     return WordList(word_tokenize(self.raw, include_punc=False))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 73, in word_tokenize
[36malgo-1    |[0m     for sentence in sent_tokenize(text))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/base.py", line 64, in itokenize
[36malgo-1    |[0m     return (t for t in self.tokenize(text, *args, **kwargs))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 38, in decorated
[36malgo-1    |[0m     raise MissingCorpusError()
[36malgo-1    |[0m textblob.exceptions.MissingCorpusError: 
[36malgo-1    |[0m Looks like you are missing some required data for this feature.
[36malgo-1    |[0m 
[36malgo-1    |[0m To download the necessary data, simply run
[36malgo-1    |[0m 
[36malgo-1    |[0m     python -m textblob.download_corpora
[36malgo-1    |[0m 
[36malgo-1    |[0m or use the NLTK downloader to download the missing data: http://nltk.org/data.html
[36malgo-1    |[0m If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.
[36malgo-1    |[0m 
[36malgo-1    |[0m ) [duplicate 1]
[36malgo-1    |[0m 21/11/14 15:21:07 INFO TaskSetManager: Starting task 0.3 in stage 14.0 (TID 64, algo-1, executor 2, partition 5, PROCESS_LOCAL, 7398 bytes)
[36malgo-1    |[0m 21/11/14 15:21:07 INFO TaskSetManager: Finished task 4.0 in stage 14.0 (TID 61) in 634 ms on algo-1 (executor 2) (4/11)
[36malgo-1    |[0m 21/11/14 15:21:07 INFO TaskSetManager: Starting task 6.0 in stage 14.0 (TID 65, algo-1, executor 2, partition 11, PROCESS_LOCAL, 7344 bytes)
[36malgo-1    |[0m 21/11/14 15:21:07 INFO TaskSetManager: Lost task 0.3 in stage 14.0 (TID 64) on algo-1, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 35, in decorated
[36malgo-1    |[0m     return func(*args, **kwargs)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 57, in tokenize
[36malgo-1    |[0m     return nltk.tokenize.sent_tokenize(text)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/tokenize/__init__.py", line 106, in sent_tokenize
[36malgo-1    |[0m     tokenizer = load(f"tokenizers/punkt/{language}.pickle")
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 750, in load
[36malgo-1    |[0m     opened_resource = _open(resource_url)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 876, in _open
[36malgo-1    |[0m     return find(path_, path + [""]).open()
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 583, in find
[36malgo-1    |[0m     raise LookupError(resource_not_found)
[36malgo-1    |[0m LookupError: 
[36malgo-1    |[0m **********************************************************************
[36malgo-1    |[0m   Resource [93mpunkt[0m not found.
[36malgo-1    |[0m   Please use the NLTK Downloader to obtain the resource:
[36malgo-1    |[0m 
[36malgo-1    |[0m   [31m>>> import nltk
[36malgo-1    |[0m   >>> nltk.download('punkt')
[36malgo-1    |[0m   [0m
[36malgo-1    |[0m   For more information see: https://www.nltk.org/data.html
[36malgo-1    |[0m 
[36malgo-1    |[0m   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[36malgo-1    |[0m 
[36malgo-1    |[0m   Searched in:
[36malgo-1    |[0m     - '/home/nltk_data'
[36malgo-1    |[0m     - '/usr/nltk_data'
[36malgo-1    |[0m     - '/usr/share/nltk_data'
[36malgo-1    |[0m     - '/usr/lib/nltk_data'
[36malgo-1    |[0m     - '/usr/share/nltk_data'
[36malgo-1    |[0m     - '/usr/local/share/nltk_data'
[36malgo-1    |[0m     - '/usr/lib/nltk_data'
[36malgo-1    |[0m     - '/usr/local/lib/nltk_data'
[36malgo-1    |[0m     - ''
[36malgo-1    |[0m **********************************************************************
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m During handling of the above exception, another exception occurred:
[36malgo-1    |[0m 
[36malgo-1    |[0m Traceback (most recent call last):
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 605, in main
[36malgo-1    |[0m     process()
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 597, in process
[36malgo-1    |[0m     serializer.dump_stream(out_iter, outfile)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 223, in dump_stream
[36malgo-1    |[0m     self.serializer.dump_stream(self._batched(iterator), stream)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 141, in dump_stream
[36malgo-1    |[0m     for obj in iterator:
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 212, in _batched
[36malgo-1    |[0m     for item in iterator:
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 450, in mapper
[36malgo-1    |[0m     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 450, in <genexpr>
[36malgo-1    |[0m     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 90, in <lambda>
[36malgo-1    |[0m     return lambda *a: f(*a)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/util.py", line 121, in wrapper
[36malgo-1    |[0m     return f(*args, **kwargs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 82, in <lambda>
[36malgo-1    |[0m     return lambda *a: g(f(*a))
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/__pyfiles__/hello_py_spark_udfs.py", line 60, in f_emo
[36malgo-1    |[0m     out = emotion_fit(lower_s)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/__pyfiles__/hello_py_spark_udfs.py", line 157, in emotion_fit
[36malgo-1    |[0m     nrclex_out = NRCLex(str(clean_cell))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nrclex.py", line 2873, in __init__
[36malgo-1    |[0m     self.words = list(blob.words)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 24, in __get__
[36malgo-1    |[0m     value = obj.__dict__[self.func.__name__] = self.func(obj)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/blob.py", line 678, in words
[36malgo-1    |[0m     return WordList(word_tokenize(self.raw, include_punc=False))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 73, in word_tokenize
[36malgo-1    |[0m     for sentence in sent_tokenize(text))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/base.py", line 64, in itokenize
[36malgo-1    |[0m     return (t for t in self.tokenize(text, *args, **kwargs))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 38, in decorated
[36malgo-1    |[0m     raise MissingCorpusError()
[36malgo-1    |[0m textblob.exceptions.MissingCorpusError: 
[36malgo-1    |[0m Looks like you are missing some required data for this feature.
[36malgo-1    |[0m 
[36malgo-1    |[0m To download the necessary data, simply run
[36malgo-1    |[0m 
[36malgo-1    |[0m     python -m textblob.download_corpora
[36malgo-1    |[0m 
[36malgo-1    |[0m or use the NLTK downloader to download the missing data: http://nltk.org/data.html
[36malgo-1    |[0m If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.
[36malgo-1    |[0m 
[36malgo-1    |[0m ) [duplicate 1]
[36malgo-1    |[0m 21/11/14 15:21:07 ERROR TaskSetManager: Task 0 in stage 14.0 failed 4 times; aborting job
[36malgo-1    |[0m 21/11/14 15:21:07 INFO YarnScheduler: Cancelling stage 14
[36malgo-1    |[0m 21/11/14 15:21:07 INFO YarnScheduler: Killing all running tasks in stage 14: Stage cancelled
[36malgo-1    |[0m 21/11/14 15:21:07 INFO YarnScheduler: Stage 14 was cancelled
[36malgo-1    |[0m 21/11/14 15:21:07 INFO DAGScheduler: ResultStage 14 (showString at NativeMethodAccessorImpl.java:0) failed in 0.897 s due to Job aborted due to stage failure: Task 0 in stage 14.0 failed 4 times, most recent failure: Lost task 0.3 in stage 14.0 (TID 64, algo-1, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 35, in decorated
[36malgo-1    |[0m     return func(*args, **kwargs)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 57, in tokenize
[36malgo-1    |[0m     return nltk.tokenize.sent_tokenize(text)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/tokenize/__init__.py", line 106, in sent_tokenize
[36malgo-1    |[0m     tokenizer = load(f"tokenizers/punkt/{language}.pickle")
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 750, in load
[36malgo-1    |[0m     opened_resource = _open(resource_url)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 876, in _open
[36malgo-1    |[0m     return find(path_, path + [""]).open()
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 583, in find
[36malgo-1    |[0m     raise LookupError(resource_not_found)
[36malgo-1    |[0m LookupError: 
[36malgo-1    |[0m **********************************************************************
[36malgo-1    |[0m   Resource [93mpunkt[0m not found.
[36malgo-1    |[0m   Please use the NLTK Downloader to obtain the resource:
[36malgo-1    |[0m 
[36malgo-1    |[0m   [31m>>> import nltk
[36malgo-1    |[0m   >>> nltk.download('punkt')
[36malgo-1    |[0m   [0m
[36malgo-1    |[0m   For more information see: https://www.nltk.org/data.html
[36malgo-1    |[0m 
[36malgo-1    |[0m   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[36malgo-1    |[0m 
[36malgo-1    |[0m   Searched in:
[36malgo-1    |[0m     - '/home/nltk_data'
[36malgo-1    |[0m     - '/usr/nltk_data'
[36malgo-1    |[0m     - '/usr/share/nltk_data'
[36malgo-1    |[0m     - '/usr/lib/nltk_data'
[36malgo-1    |[0m     - '/usr/share/nltk_data'
[36malgo-1    |[0m     - '/usr/local/share/nltk_data'
[36malgo-1    |[0m     - '/usr/lib/nltk_data'
[36malgo-1    |[0m     - '/usr/local/lib/nltk_data'
[36malgo-1    |[0m     - ''
[36malgo-1    |[0m **********************************************************************
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m During handling of the above exception, another exception occurred:
[36malgo-1    |[0m 
[36malgo-1    |[0m Traceback (most recent call last):
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 605, in main
[36malgo-1    |[0m     process()
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 597, in process
[36malgo-1    |[0m     serializer.dump_stream(out_iter, outfile)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 223, in dump_stream
[36malgo-1    |[0m     self.serializer.dump_stream(self._batched(iterator), stream)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 141, in dump_stream
[36malgo-1    |[0m     for obj in iterator:
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 212, in _batched
[36malgo-1    |[0m     for item in iterator:
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 450, in mapper
[36malgo-1    |[0m     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 450, in <genexpr>
[36malgo-1    |[0m     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 90, in <lambda>
[36malgo-1    |[0m     return lambda *a: f(*a)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/util.py", line 121, in wrapper
[36malgo-1    |[0m     return f(*args, **kwargs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 82, in <lambda>
[36malgo-1    |[0m     return lambda *a: g(f(*a))
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/__pyfiles__/hello_py_spark_udfs.py", line 60, in f_emo
[36malgo-1    |[0m     out = emotion_fit(lower_s)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/__pyfiles__/hello_py_spark_udfs.py", line 157, in emotion_fit
[36malgo-1    |[0m     nrclex_out = NRCLex(str(clean_cell))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nrclex.py", line 2873, in __init__
[36malgo-1    |[0m     self.words = list(blob.words)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 24, in __get__
[36malgo-1    |[0m     value = obj.__dict__[self.func.__name__] = self.func(obj)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/blob.py", line 678, in words
[36malgo-1    |[0m     return WordList(word_tokenize(self.raw, include_punc=False))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 73, in word_tokenize
[36malgo-1    |[0m     for sentence in sent_tokenize(text))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/base.py", line 64, in itokenize
[36malgo-1    |[0m     return (t for t in self.tokenize(text, *args, **kwargs))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 38, in decorated
[36malgo-1    |[0m     raise MissingCorpusError()
[36malgo-1    |[0m textblob.exceptions.MissingCorpusError: 
[36malgo-1    |[0m Looks like you are missing some required data for this feature.
[36malgo-1    |[0m 
[36malgo-1    |[0m To download the necessary data, simply run
[36malgo-1    |[0m 
[36malgo-1    |[0m     python -m textblob.download_corpora
[36malgo-1    |[0m 
[36malgo-1    |[0m or use the NLTK downloader to download the missing data: http://nltk.org/data.html
[36malgo-1    |[0m If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)
[36malgo-1    |[0m 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
[36malgo-1    |[0m 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[36malgo-1    |[0m 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
[36malgo-1    |[0m 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[36malgo-1    |[0m 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[36malgo-1    |[0m 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
[36malgo-1    |[0m 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
[36malgo-1    |[0m 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
[36malgo-1    |[0m 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
[36malgo-1    |[0m 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[36malgo-1    |[0m 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
[36malgo-1    |[0m 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
[36malgo-1    |[0m 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[36malgo-1    |[0m 	at org.apache.spark.scheduler.Task.run(Task.scala:127)
[36malgo-1    |[0m 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
[36malgo-1    |[0m 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
[36malgo-1    |[0m 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
[36malgo-1    |[0m 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[36malgo-1    |[0m 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[36malgo-1    |[0m 	at java.lang.Thread.run(Thread.java:748)
[36malgo-1    |[0m 
[36malgo-1    |[0m Driver stacktrace:
[36malgo-1    |[0m 21/11/14 15:21:07 INFO DAGScheduler: Job 13 failed: showString at NativeMethodAccessorImpl.java:0, took 0.901658 s
[36malgo-1    |[0m 21/11/14 15:21:07 WARN TaskSetManager: Lost task 6.0 in stage 14.0 (TID 65, algo-1, executor 2): TaskKilled (Stage cancelled)
[36malgo-1    |[0m Traceback (most recent call last):
[36malgo-1    |[0m   File "/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py", line 121, in <module>
[36malgo-1    |[0m     output.show()
[36malgo-1    |[0m   File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 441, in show
[36malgo-1    |[0m   File "/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__
[36malgo-1    |[0m   File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 137, in deco
[36malgo-1    |[0m   File "<string>", line 3, in raise_from
[36malgo-1    |[0m pyspark.sql.utils.PythonException: 
[36malgo-1    |[0m   An exception was thrown from Python worker in the executor. The below is the Python worker stacktrace.
[36malgo-1    |[0m Traceback (most recent call last):
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 35, in decorated
[36malgo-1    |[0m     return func(*args, **kwargs)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 57, in tokenize
[36malgo-1    |[0m     return nltk.tokenize.sent_tokenize(text)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/tokenize/__init__.py", line 106, in sent_tokenize
[36malgo-1    |[0m     tokenizer = load(f"tokenizers/punkt/{language}.pickle")
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 750, in load
[36malgo-1    |[0m     opened_resource = _open(resource_url)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 876, in _open
[36malgo-1    |[0m     return find(path_, path + [""]).open()
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 583, in find
[36malgo-1    |[0m     raise LookupError(resource_not_found)
[36malgo-1    |[0m LookupError: 
[36malgo-1    |[0m **********************************************************************
[36malgo-1    |[0m   Resource [93mpunkt[0m not found.
[36malgo-1    |[0m   Please use the NLTK Downloader to obtain the resource:
[36malgo-1    |[0m 
[36malgo-1    |[0m   [31m>>> import nltk
[36malgo-1    |[0m   >>> nltk.download('punkt')
[36malgo-1    |[0m   [0m
[36malgo-1    |[0m   For more information see: https://www.nltk.org/data.html
[36malgo-1    |[0m 
[36malgo-1    |[0m   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[36malgo-1    |[0m 
[36malgo-1    |[0m   Searched in:
[36malgo-1    |[0m     - '/home/nltk_data'
[36malgo-1    |[0m     - '/usr/nltk_data'
[36malgo-1    |[0m     - '/usr/share/nltk_data'
[36malgo-1    |[0m     - '/usr/lib/nltk_data'
[36malgo-1    |[0m     - '/usr/share/nltk_data'
[36malgo-1    |[0m     - '/usr/local/share/nltk_data'
[36malgo-1    |[0m     - '/usr/lib/nltk_data'
[36malgo-1    |[0m     - '/usr/local/lib/nltk_data'
[36malgo-1    |[0m     - ''
[36malgo-1    |[0m **********************************************************************
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m During handling of the above exception, another exception occurred:
[36malgo-1    |[0m 
[36malgo-1    |[0m Traceback (most recent call last):
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 605, in main
[36malgo-1    |[0m     process()
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 597, in process
[36malgo-1    |[0m     serializer.dump_stream(out_iter, outfile)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 223, in dump_stream
[36malgo-1    |[0m     self.serializer.dump_stream(self._batched(iterator), stream)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 141, in dump_stream
[36malgo-1    |[0m     for obj in iterator:
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 212, in _batched
[36malgo-1    |[0m     for item in iterator:
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 450, in mapper
[36malgo-1    |[0m     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 450, in <genexpr>
[36malgo-1    |[0m     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 90, in <lambda>
[36malgo-1    |[0m     return lambda *a: f(*a)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/util.py", line 121, in wrapper
[36malgo-1    |[0m     return f(*args, **kwargs)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 82, in <lambda>
[36malgo-1    |[0m     return lambda *a: g(f(*a))
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/__pyfiles__/hello_py_spark_udfs.py", line 60, in f_emo
[36malgo-1    |[0m     out = emotion_fit(lower_s)
[36malgo-1    |[0m   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/__pyfiles__/hello_py_spark_udfs.py", line 157, in emotion_fit
[36malgo-1    |[0m     nrclex_out = NRCLex(str(clean_cell))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/nrclex.py", line 2873, in __init__
[36malgo-1    |[0m     self.words = list(blob.words)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 24, in __get__
[36malgo-1    |[0m     value = obj.__dict__[self.func.__name__] = self.func(obj)
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/blob.py", line 678, in words
[36malgo-1    |[0m     return WordList(word_tokenize(self.raw, include_punc=False))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 73, in word_tokenize
[36malgo-1    |[0m     for sentence in sent_tokenize(text))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/base.py", line 64, in itokenize
[36malgo-1    |[0m     return (t for t in self.tokenize(text, *args, **kwargs))
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 38, in decorated
[36malgo-1    |[0m     raise MissingCorpusError()
[36malgo-1    |[0m textblob.exceptions.MissingCorpusError: 
[36malgo-1    |[0m Looks like you are missing some required data for this feature.
[36malgo-1    |[0m 
[36malgo-1    |[0m To download the necessary data, simply run
[36malgo-1    |[0m 
[36malgo-1    |[0m     python -m textblob.download_corpora
[36malgo-1    |[0m 
[36malgo-1    |[0m or use the NLTK downloader to download the missing data: http://nltk.org/data.html
[36malgo-1    |[0m If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m 21/11/14 15:21:08 WARN TaskSetManager: Lost task 5.0 in stage 14.0 (TID 63, algo-2, executor 1): TaskKilled (Stage cancelled)
[36malgo-1    |[0m 21/11/14 15:21:08 INFO YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at java.lang.Thread.run(Thread.java:748)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 59
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO Executor: Running task 3.0 in stage 14.0 (TID 59)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:06 INFO PythonRunner: Times: total = 42, boot = -68, init = 110, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:07 INFO PythonUDFRunner: Times: total = 649, boot = 4, init = 645, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:07 INFO Executor: Finished task 3.0 in stage 14.0 (TID 59). 2022 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 62
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:07 INFO Executor: Running task 0.2 in stage 14.0 (TID 62)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:07 INFO PythonRunner: Times: total = 2, boot = -596, init = 598, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 1st Check
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] **********************************************************************
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   Resource [93mpunkt[0m not found.
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   Please use the NLTK Downloader to obtain the resource:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   [31m>>> import nltk
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   >>> nltk.download('punkt')
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   [0m
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   For more information see: https://www.nltk.org/data.html
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   Searched in:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/home/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/share/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/lib/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/share/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/local/share/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/lib/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/local/lib/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - ''
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] **********************************************************************
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:07 ERROR Executor: Exception in task 0.2 in stage 14.0 (TID 62)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 35, in decorated
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     return func(*args, **kwargs)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 57, in tokenize
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     return nltk.tokenize.sent_tokenize(text)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/tokenize/__init__.py", line 106, in sent_tokenize
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     tokenizer = load(f"tokenizers/punkt/{language}.pickle")
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 750, in load
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     opened_resource = _open(resource_url)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 876, in _open
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     return find(path_, path + [""]).open()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 583, in find
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     raise LookupError(resource_not_found)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] LookupError: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] **********************************************************************
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   Resource [93mpunkt[0m not found.
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   Please use the NLTK Downloader to obtain the resource:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   [31m>>> import nltk
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   >>> nltk.download('punkt')
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   [0m
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   For more information see: https://www.nltk.org/data.html
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   Searched in:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/home/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/share/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/lib/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/share/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/local/share/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/lib/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/local/lib/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - ''
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] **********************************************************************
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] During handling of the above exception, another exception occurred:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] Traceback (most recent call last):
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/worker.py", line 605, in main
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     process()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/worker.py", line 597, in process
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     serializer.dump_stream(out_iter, outfile)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/serializers.py", line 223, in dump_stream
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     self.serializer.dump_stream(self._batched(iterator), stream)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/serializers.py", line 141, in dump_stream
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     for obj in iterator:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/serializers.py", line 212, in _batched
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     for item in iterator:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/worker.py", line 450, in mapper
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/worker.py", line 450, in <genexpr>
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/worker.py", line 90, in <lambda>
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     return lambda *a: f(*a)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/util.py", line 121, in wrapper
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     return f(*args, **kwargs)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/pyspark.zip/pyspark/worker.py", line 82, in <lambda>
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     return lambda *a: g(f(*a))
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/__pyfiles__/hello_py_spark_udfs.py", line 60, in f_emo
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     out = emotion_fit(lower_s)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/__pyfiles__/hello_py_spark_udfs.py", line 157, in emotion_fit
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     nrclex_out = NRCLex(str(clean_cell))
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/nrclex.py", line 2873, in __init__
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     self.words = list(blob.words)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 24, in __get__
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     value = obj.__dict__[self.func.__name__] = self.func(obj)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/blob.py", line 678, in words
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     return WordList(word_tokenize(self.raw, include_punc=False))
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 73, in word_tokenize
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     for sentence in sent_tokenize(text))
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/base.py", line 64, in itokenize
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     return (t for t in self.tokenize(text, *args, **kwargs))
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 38, in decorated
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     raise MissingCorpusError()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] textblob.exceptions.MissingCorpusError: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] Looks like you are missing some required data for this feature.
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] To download the necessary data, simply run
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     python -m textblob.download_corpora
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] or use the NLTK downloader to download the missing data: http://nltk.org/data.html
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.scheduler.Task.run(Task.scala:127)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 	at java.lang.Thread.run(Thread.java:748)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 63
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:07 INFO Executor: Running task 5.0 in stage 14.0 (TID 63)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:07 INFO PythonRunner: Times: total = 2, boot = -5, init = 7, finish = 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 21/11/14 15:21:07 INFO Executor: Executor is trying to kill task 5.0 in stage 14.0 (TID 63), reason: Stage cancelled
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 1st Check
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] **********************************************************************
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   Resource [93mpunkt[0m not found.
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   Please use the NLTK Downloader to obtain the resource:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   [31m>>> import nltk
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   >>> nltk.download('punkt')
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   [0m
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   For more information see: https://www.nltk.org/data.html
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]   Searched in:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/home/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/share/nltk_data'
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '/usr/lib/nltk_data'
[36malgo-1    |[0m 21/11/14 15:21:08 INFO SparkContext: Invoking stop() from shutdown hook
[36malgo-1    |[0m 21/11/14 15:21:08 INFO SparkUI: Stopped Spark web UI at http://192.168.240.3:4040
[36malgo-1    |[0m 21/11/14 15:21:08 INFO YarnClientSchedulerBackend: Interrupting monitor thread
[36malgo-1    |[0m 21/11/14 15:21:08 INFO YarnClientSchedulerBackend: Shutting down all executors
[36malgo-1    |[0m 21/11/14 15:21:08 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
[36malgo-1    |[0m 21/11/14 15:21:08 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
[36malgo-1    |[0m 21/11/14 15:21:08 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[36malgo-1    |[0m 21/11/14 15:21:08 INFO MemoryStore: MemoryStore cleared
[36malgo-1    |[0m 21/11/14 15:21:08 INFO BlockManager: BlockManager stopped
[36malgo-1    |[0m 21/11/14 15:21:08 INFO BlockManagerMaster: BlockManagerMaster stopped
[36malgo-1    |[0m 21/11/14 15:21:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[36malgo-1    |[0m 21/11/14 15:21:08 INFO SparkContext: Successfully stopped SparkContext
[36malgo-1    |[0m 21/11/14 15:21:08 INFO ShutdownHookManager: Shutdown hook called
[36malgo-1    |[0m 21/11/14 15:21:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-8b7d3c2c-5e75-49f5-aabb-8e54a7ab5e3e
[36malgo-1    |[0m 21/11/14 15:21:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-d2d32342-fa1c-45a8-8bdc-0f22350cb278/pyspark-e51b0e3f-3d6f-40fb-b3fc-3d54fc78003c
[36malgo-1    |[0m 21/11/14 15:21:08 INFO ShutdownHookManager: Deleting directory /tmp/localPyFiles-8688029b-0876-40e4-8fd1-e567349bd3c5
[36malgo-1    |[0m 21/11/14 15:21:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-d2d32342-fa1c-45a8-8bdc-0f22350cb278
[36malgo-1    |[0m 2021-11-14 15:21:08,220 INFO attempt.RMAppAttemptImpl: Updating application attempt appattempt_1636903150985_0001_000001 with final state: FINISHING, and exit status: -1000
[36malgo-1    |[0m 2021-11-14 15:21:08,221 INFO attempt.RMAppAttemptImpl: appattempt_1636903150985_0001_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
[36malgo-1    |[0m 2021-11-14 15:21:08,221 INFO rmapp.RMAppImpl: Updating application application_1636903150985_0001 with final state: FINISHING
[36malgo-1    |[0m 2021-11-14 15:21:08,221 INFO recovery.RMStateStore: Updating info for app: application_1636903150985_0001
[36malgo-1    |[0m 2021-11-14 15:21:08,221 INFO rmapp.RMAppImpl: application_1636903150985_0001 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
[36malgo-1    |[0m 2021-11-14 15:21:08,222 INFO attempt.RMAppAttemptImpl: appattempt_1636903150985_0001_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
[36malgo-1    |[0m 2021-11-14 15:21:08,222 INFO rmapp.RMAppImpl: application_1636903150985_0001 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stderr]     - '2021-11-14 15:21:08,243 INFO launcher.ContainerLaunch: Container container_1636903150985_0001_01_000002 succeeded 
[36malgo-1    |[0m 2021-11-14 15:21:08,245 INFO launcher.ContainerLaunch: Container container_1636903150985_0001_01_000003 succeeded 
[33malgo-2    |[0m 2021-11-14 15:21:08,252 INFO container.ContainerImpl: Container container_1636903150985_0001_01_000002 transitioned from RUNNING to EXITED_WITH_SUCCESS
[36malgo-1    |[0m 2021-11-14 15:21:08,253 INFO container.ContainerImpl: Container container_1636903150985_0001_01_000003 transitioned from RUNNING to EXITED_WITH_SUCCESS
[33malgo-2    |[0m 2021-11-14 15:21:08,253 INFO launcher.ContainerCleanup: Cleaning up container container_1636903150985_0001_01_000002
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stdout] 2021-11-14T15:19:53.482+0000: [GC (Allocation Failure) [PSYoungGen: 120320K->8792K(140288K)] 120320K->8808K(461312K), 0.0084808 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stdout] 2021-11-14T15:19:53.983+0000: [GC (Allocation Failure) [PSYoungGen: 129112K->7726K(140288K)] 129128K->7750K(461312K), 0.0077261 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stdout] 2021-11-14T15:19:54.312+0000: [GC (Metadata GC Threshold) [PSYoungGen: 110129K->8718K(140288K)] 110153K->8750K(461312K), 0.0052415 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stdout] 2021-11-14T15:19:54.318+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 8718K->0K(140288K)] [ParOldGen: 32K->8430K(190976K)] 8750K->8430K(331264K), [Metaspace: 20382K->20382K(1067008K)], 0.0274907 secs] [Times: user=0.05 sys=0.00, real=0.02 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stdout] 2021-11-14T15:19:54.851+0000: [GC (Allocation Failure) [PSYoungGen: 120320K->4523K(181760K)] 128750K->12962K(372736K), 0.0051161 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stdout] 2021-11-14T15:19:55.296+0000: [GC (Allocation Failure) [PSYoungGen: 181675K->7314K(245248K)] 190114K->15753K(436224K), 0.0079559 secs] [Times: user=0.02 sys=0.00, real=0.00 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stdout] 2021-11-14T15:19:55.424+0000: [GC (Metadata GC Threshold) [PSYoungGen: 57735K->4038K(275456K)] 66173K->12485K(466432K), 0.0049397 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stdout] 2021-11-14T15:19:55.429+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 4038K->0K(275456K)] [ParOldGen: 8446K->10206K(290304K)] 12485K->10206K(565760K), [Metaspace: 33970K->33967K(1079296K)], 0.0348142 secs] [Times: user=0.06 sys=0.00, real=0.03 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stdout] 2021-11-14T15:19:56.122+0000: [GC (Allocation Failure) [PSYoungGen: 265728K->10234K(275968K)] 275934K->21963K(566272K), 0.0111402 secs] [Times: user=0.01 sys=0.01, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stdout] 2021-11-14T15:19:56.985+0000: [GC (Allocation Failure) [PSYoungGen: 275962K->12287K(395264K)] 287691K->26399K(685568K), 0.0121903 secs] [Times: user=0.03 sys=0.01, real=0.02 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stdout] 2021-11-14T15:19:57.005+0000: [GC (Metadata GC Threshold) [PSYoungGen: 16374K->9950K(397824K)] 30486K->24070K(688128K), 0.0086896 secs] [Times: user=0.03 sys=0.00, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stdout] 2021-11-14T15:19:57.014+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 9950K->0K(397824K)] [ParOldGen: 14120K->18589K(332800K)] 24070K->18589K(730624K), [Metaspace: 54360K->54360K(1099776K)], 0.0868183 secs] [Times: user=0.25 sys=0.00, real=0.09 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stdout] 2021-11-14T15:21:06.398+0000: [GC (Allocation Failure) [PSYoungGen: 382976K->10755K(569856K)] 401565K->127656K(902656K), 0.0319712 secs] [Times: user=0.08 sys=0.04, real=0.03 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stdou2021-11-14 15:21:08,254 INFO launcher.ContainerCleanup: Cleaning up container container_1636903150985_0001_01_000003
[33malgo-2    |[0m 2021-11-14 15:21:08,254 INFO nodemanager.NMAuditLogger: USER=root	OPERATION=Container Finished - Succeeded	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1636903150985_0001	CONTAINERID=container_1636903150985_0001_01_000002
[33malgo-2    |[0m 2021-11-14 15:21:08,255 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002
[36malgo-1    |[0m 2021-11-14 15:21:08,256 INFO nodemanager.NMAuditLogger: USER=root	OPERATION=Container Finished - Succeeded	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1636903150985_0001	CONTAINERID=container_1636903150985_0001_01_000003
[36malgo-1    |[0m 0985_0001_01_000003/stderr] 21/11/14 15:21:06 INFO PythonRunner: Times: total = 1, boot = -66, init = 67, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:07 INFO PythonUDFRunner: Times: total = 623, boot = 3, init = 620, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:07 INFO Executor: Finished task 4.0 in stage 14.0 (TID 61). 2022 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 64
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:07 INFO Executor: Running task 0.3 in stage 14.0 (TID 64)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:07 INFO PythonRunner: Times: total = 44, boot = -629, init = 673, finish = 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 1st Check
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] **********************************************************************
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   Resource [93mpunkt[0m not found.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   Please use the NLTK Downloader to obtain the resource:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   [31m>>> import nltk
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   >>> nltk.download('punkt')
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   [0m
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   For more information see: https://www.nltk.org/data.html
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   Searched in:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/home/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/share/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/lib/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/share/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/local/share/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/lib/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/local/lib/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - ''
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] **********************************************************************
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 21/11/14 15:21:07 ERROR Executor: Exception in task 0.3 in stage 14.0 (TID 64)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 35, in decorated
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     return func(*args, **kwargs)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 57, in tokenize
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     return nltk.tokenize.sent_tokenize(text)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/tokenize/__init__.py", line 106, in sent_tokenize
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     tokenizer = load(f"tokenizers/punkt/{language}.pickle")
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 750, in load
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     opened_resource = _open(resource_url)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 876, in _open
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     return find(path_, path + [""]).open()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/nltk/data.py", line 583, in find
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     raise LookupError(resource_not_found)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] LookupError: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] **********************************************************************
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   Resource [93mpunkt[0m not found.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   Please use the NLTK Downloader to obtain the resource:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   [31m>>> import nltk
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   >>> nltk.download('punkt')
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   [0m
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   For more information see: https://www.nltk.org/data.html
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   Searched in:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/home/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/share/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/lib/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/share/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/local/share/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/lib/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - '/usr/local/lib/nltk_data'
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     - ''
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] **********************************************************************
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] During handling of the above exception, another exception occurred:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] Traceback (most recent call last):
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 605, in main
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     process()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 597, in process
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     serializer.dump_stream(out_iter, outfile)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 223, in dump_stream
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     self.serializer.dump_stream(self._batched(iterator), stream)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 141, in dump_stream
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     for obj in iterator:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/serializers.py", line 212, in _batched
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     for item in iterator:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 450, in mapper
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 450, in <genexpr>
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 90, in <lambda>
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     return lambda *a: f(*a)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/util.py", line 121, in wrapper
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     return f(*args, **kwargs)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/pyspark.zip/pyspark/worker.py", line 82, in <lambda>
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     return lambda *a: g(f(*a))
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/__pyfiles__/hello_py_spark_udfs.py", line 60, in f_emo
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     out = emotion_fit(lower_s)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/__pyfiles__/hello_py_spark_udfs.py", line 157, in emotion_fit
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     nrclex_out = NRCLex(str(clean_cell))
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/nrclex.py", line 2873, in __init__
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     self.words = list(blob.words)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 24, in __get__
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     value = obj.__dict__[self.func.__name__] = self.func(obj)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/blob.py", line 678, in words
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     return WordList(word_tokenize(self.raw, include_punc=False))
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/tokenizers.py", line 73, in word_tokenize
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     for sentence in sent_tokenize(text))
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/base.py", line 64, in itokenize
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     return (t for t in self.tokenize(text, *args, **kwargs))
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]   File "/usr/local/lib/python3.7/site-packages/textblob/decorators.py", line 38, in decorated
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     raise MissingCorpusError()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] textblob.exceptions.MissingCorpusError: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] Looks like you are missing some required data for this feature.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] To download the necessary data, simply run
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr]     python -m textblob.download_corpora
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] or use the NLTK downloader to download the missing data: http://nltk.org/data.html
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.scheduler.Task.run(Task.scala:127)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000003/stderr] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/2021-11-14 15:21:08,257 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003
[33malgo-2    |[0m 2021-11-14 15:21:08,257 INFO container.ContainerImpl: Container container_1636903150985_0001_01_000002 transitioned from EXITED_WITH_SUCCESS to DONE
[33malgo-2    |[0m 2021-11-14 15:21:08,257 INFO application.ApplicationImpl: Removing container_1636903150985_0001_01_000002 from application application_1636903150985_0001
[33malgo-2    |[0m 2021-11-14 15:21:08,258 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1636903150985_0001_01_000002
[33malgo-2    |[0m 2021-11-14 15:21:08,258 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1636903150985_0001
[36malgo-1    |[0m 2021-11-14 15:21:08,258 INFO container.ContainerImpl: Container container_1636903150985_0001_01_000003 transitioned from EXITED_WITH_SUCCESS to DONE
[36malgo-1    |[0m 2021-11-14 15:21:08,259 INFO application.ApplicationImpl: Removing container_1636903150985_0001_01_000003 from application application_1636903150985_0001
[36malgo-1    |[0m 2021-11-14 15:21:08,259 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1636903150985_0001_01_000003
[36malgo-1    |[0m 2021-11-14 15:21:08,259 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1636903150985_0001
[36malgo-1    |[0m 2021-11-14 15:21:08,260 INFO rmcontainer.RMContainerImpl: container_1636903150985_0001_01_000002 Container Transitioned from RUNNING to COMPLETED
[36malgo-1    |[0m 2021-11-14 15:21:08,260 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636903150985_0001	CONTAINERID=container_1636903150985_0001_01_000002	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 15:21:08,261 INFO rmcontainer.RMContainerImpl: container_1636903150985_0001_01_000003 Container Transitioned from RUNNING to COMPLETED
[36malgo-1    |[0m 2021-11-14 15:21:08,261 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636903150985_0001	CONTAINERID=container_1636903150985_0001_01_000003	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[33malgo-2    |[0m 2021-11-14 15:21:08,270 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/launch_container.sh
[33malgo-2    |[0m 2021-11-14 15:21:08,271 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/launch_container.sh]
[33malgo-2    |[0m 2021-11-14 15:21:08,271 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/container_tokens
[33malgo-2    |[0m 2021-11-14 15:21:08,271 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/container_tokens]
[33malgo-2    |[0m 2021-11-14 15:21:08,271 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/sysfs
[33malgo-2    |[0m 2021-11-14 15:21:08,271 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000002/sysfs]
[36malgo-1    |[0m 2021-11-14 15:21:08,272 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/launch_container.sh
[36malgo-1    |[0m 2021-11-14 15:21:08,272 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/launch_container.sh]
[36malgo-1    |[0m 2021-11-14 15:21:08,272 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/container_tokens
[36malgo-1    |[0m 2021-11-14 15:21:08,272 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/container_tokens]
[36malgo-1    |[0m 2021-11-14 15:21:08,272 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/sysfs
[36malgo-1    |[0m 2021-11-14 15:21:08,272 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903150985_0001/container_1636903150985_0001_01_000003/sysfs]
[36malgo-1    |[0m 2021-11-14 15:21:08,322 INFO resourcemanager.ApplicationMasterService: application_1636903150985_0001 unregistered successfully. 
[36malgo-1    |[0m 2021-11-14 15:21:08,334 INFO namenode.FSEditLog: Number of transactions: 48 Total time for transactions(ms): 14 Number of transactions batched in Syncs: 9 Number of syncs: 39 SyncTimes(ms): 36 
[36malgo-1    |[0m 11-14 15:21 smspark-submit ERROR    spark-submit command failed with exit code 1: Command 'spark-submit --master yarn --deploy-mode client --py-files /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py --verbose /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data' returned non-zero exit status 1.
[36malgo-1    |[0m Traceback (most recent call last):
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/smspark/job.py", line 149, in run
[36malgo-1    |[0m     subprocess.run(spark_submit_cmd, check=True, shell=True)
[36malgo-1    |[0m   File "/usr/lib64/python3.7/subprocess.py", line 512, in run
[36malgo-1    |[0m     output=stdout, stderr=stderr)
[36malgo-1    |[0m subprocess.CalledProcessError: Command 'spark-submit --master yarn --deploy-mode client --py-files /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py --verbose /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data' returned non-zero exit status 1.
[36malgo-1    |[0m Command 'spark-submit --master yarn --deploy-mode client --py-files /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py --verbose /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data' returned non-zero exit status 1.
[36malgo-1    |[0m Traceback (most recent call last):
[36malgo-1    |[0m   File "/usr/local/lib/python3.7/site-packages/smspark/job.py", line 149, in run
[36malgo-1    |[0m     subprocess.run(spark_submit_cmd, check=True, shell=True)
[36malgo-1    |[0m   File "/usr/lib64/python3.7/subprocess.py", line 512, in run
[36malgo-1    |[0m     output=stdout, stderr=stderr)
[36malgo-1    |[0m subprocess.CalledProcessError: Command 'spark-submit --master yarn --deploy-mode client --py-files /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py --verbose /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data' returned non-zero exit status 1.
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:21 smspark-submit INFO     exiting with code 1: Algorithm Error: (caused by CalledProcessError): spark failed with a non-zero exit code: Command 'spark-submit --master yarn --deploy-mode client --py-files /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py --verbose /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data' returned non-zero exit status 1.
[36malgo-1 exited with code 1
[0m[33malgo-2    |[0m 11-14 15:21 urllib3.connectionpool WARNING  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f84aa4a2310>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stdout] 2021-11-14T15:19:57.316+0000: [GC (Allocation Failure) [PSYoungGen: 120320K->9071K(140288K)] 120320K->9087K(461312K), 0.0073578 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stdout] 2021-11-14T15:19:57.787+0000: [GC (Allocation Failure) [PSYoungGen: 129391K->7673K(140288K)] 129407K->7697K(461312K), 0.0065404 secs] [Times: user=0.01 sys=0.01, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stdout] 2021-11-14T15:19:58.162+0000: [GC (Metadata GC Threshold) [PSYoungGen: 109404K->8678K(140288K)] 109428K->8710K(461312K), 0.0051921 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stdout] 2021-11-14T15:19:58.167+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 8678K->0K(140288K)] [ParOldGen: 32K->8436K(194560K)] 8710K->8436K(334848K), [Metaspace: 20379K->20379K(1067008K)], 0.0276727 secs] [Times: user=0.06 sys=0.00, real=0.03 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stdout] 2021-11-14T15:19:58.666+0000: [GC (Allocation Failure) [PSYoungGen: 120320K->4597K(178688K)] 128756K->13041K(373248K), 0.0044755 secs] [Times: user=0.00 sys=0.01, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stdout] 2021-11-14T15:19:59.066+0000: [GC (Allocation Failure) [PSYoungGen: 178677K->7420K(245248K)] 187121K->15864K(439808K), 0.0064338 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stdout] 2021-11-14T15:19:59.177+0000: [GC (GCLocker Initiated GC) [PSYoungGen: 53600K->4015K(266240K)] 62044K->12467K(460800K), 0.0041012 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stdout] 2021-11-14T15:19:59.199+0000: [GC (Metadata GC Threshold) [PSYoungGen: 16507K->3444K(266752K)] 24959K->11904K(461312K), 0.0035735 secs] [Times: user=0.02 sys=0.00, real=0.00 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stdout] 2021-11-14T15:19:59.203+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 3444K->0K(266752K)] [ParOldGen: 8460K->9650K(243200K)] 11904K->9650K(509952K), [Metaspace: 34323K->34320K(1081344K)], 0.0243030 secs] [Times: user=0.05 sys=0.00, real=0.02 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stdout] 2021-11-14T15:20:59.868+0000: [GC (Allocation Failure) [PSYoungGen: 256000K->10726K(442880K)] 265650K->20981K(686080K), 0.0111208 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stdout] 2021-11-14T15:21:00.798+0000: [GC (Metadata GC Threshold) [PSYoungGen: 276713K->12276K(471040K)] 286968K->25348K(714240K), 0.0120343 secs] [Times: user=0.02 sys=0.01, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stdout] 2021-11-14T15:21:00.810+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 12276K->0K(471040K)] [ParOldGen: 13071K->23585K(297472K)] 25348K->23585K(768512K), [Metaspace: 54994K->54986K(1101824K)], 0.0797406 secs] [Times: user=0.17 sys=0.01, real=0.08 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stdout] Heap
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/container_1636903150985_0001_01_000002/stdout]  PSYoungGen      total 471040K, used 370896K [0x00000000d5580000, 0x00000000fbd80000, 0x0000000100000000)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903150985_0001/cont2021-11-14 15:21:09,265 INFO retry.RetryInvocationHandler: java.io.EOFException: End of File Exception between local host is: "algo-2/192.168.240.2"; destination host is: "algo-1.spark-network":8031; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException, while invoking ResourceTrackerPBClientImpl.nodeHeartbeat over null. Retrying after sleeping for 30000ms.
[33malgo-2    |[0m 11-14 15:21 urllib3.connectionpool WARNING  Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f84aa4a51d0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m 2021-11-14 15:21:11,164 WARN datanode.DataNode: IOException in offerService
[33malgo-2    |[0m java.io.EOFException: End of File Exception between local host is: "algo-2/192.168.240.2"; destination host is: "algo-1.spark-network":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
[33malgo-2    |[0m 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[33malgo-2    |[0m 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[33malgo-2    |[0m 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[33malgo-2    |[0m 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
[33malgo-2    |[0m 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
[33malgo-2    |[0m 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:791)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
[33malgo-2    |[0m 	at com.sun.proxy.$Proxy16.sendHeartbeat(Unknown Source)
[33malgo-2    |[0m 	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:168)
[33malgo-2    |[0m 	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:517)
[33malgo-2    |[0m 	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
[33malgo-2    |[0m 	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
[33malgo-2    |[0m 	at java.lang.Thread.run(Thread.java:748)
[33malgo-2    |[0m Caused by: java.io.EOFException
[33malgo-2    |[0m 	at java.io.DataInputStream.readInt(DataInputStream.java:392)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)
[33malgo-2    |[0m 11-14 15:21 urllib3.connectionpool WARNING  Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f84aa4a5ed0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m 11-14 15:21 urllib3.connectionpool WARNING  Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f84aa43f2d0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m 11-14 15:21 urllib3.connectionpool WARNING  Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f84aa43f6d0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m 11-14 15:21 smspark-submit INFO     primary is down, worker now exiting
[33malgo-2 exited with code 0
[0m
FAILED
test/integration/local/test_multinode_container.py::test_scala_spark_multinode JARS_MOUNT=./test/resources/code/scala/hello-scala-spark/lib_managed/jars/org.json4s/json4s-native_2.12:/opt/ml/processing/input/jars CMD='--jars /opt/ml/processing/input/jars --class com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp --verbose /opt/ml/processing/input/code/scala/hello-scala-spark/target/scala-2.12/hello-scala-spark_2.12-1.0.jar --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data' docker-compose up --force-recreate
Recreating algo-1 ... 
Recreating algo-2 ... 
[1A[2K
Recreating algo-2 ... [32mdone[0m
[1B[2A[2K
Recreating algo-1 ... [32mdone[0m
[2BAttaching to algo-2, algo-1
[33malgo-2    |[0m 11-14 15:21 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--jars', '/opt/ml/processing/input/jars', '--class', 'com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp', '--verbose', '/opt/ml/processing/input/code/scala/hello-scala-spark/target/scala-2.12/hello-scala-spark_2.12-1.0.jar', '--input', 'file:///opt/ml/processing/input/data/data.jsonl', '--output', 'file:///opt/ml/processing/output/data']
[33malgo-2    |[0m 11-14 15:21 smspark.cli  INFO     Raw spark options before processing: {'jars': '/opt/ml/processing/input/jars', 'class_': 'com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp', 'verbose': True, 'py_files': None, 'files': None}
[33malgo-2    |[0m 11-14 15:21 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/scala/hello-scala-spark/target/scala-2.12/hello-scala-spark_2.12-1.0.jar', '--input', 'file:///opt/ml/processing/input/data/data.jsonl', '--output', 'file:///opt/ml/processing/output/data']
[33malgo-2    |[0m 11-14 15:21 smspark.cli  INFO     Rendered spark options: {'jars': '/opt/ml/processing/input/jars/json4s-native_2.12-3.6.9.jar', 'class_': 'com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp', 'verbose': True, 'py_files': None, 'files': None}
[33malgo-2    |[0m 11-14 15:21 smspark.cli  INFO     Initializing processing job.
[33malgo-2    |[0m 11-14 15:21 smspark-submit INFO     {'current_host': 'algo-2', 'hosts': ['algo-1', 'algo-2']}
[33malgo-2    |[0m 11-14 15:21 smspark-submit INFO     {'ProcessingJobArn': 'processing_job_arn', 'ProcessingJobName': 'processing_job_name', 'AppSpecification': {'ImageUri': 'image_uri'}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30}}, 'RoleArn': 'iam_role'}
[33malgo-2    |[0m 11-14 15:21 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client --jars /opt/ml/processing/input/jars/json4s-native_2.12-3.6.9.jar --class com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp --verbose /opt/ml/processing/input/code/scala/hello-scala-spark/target/scala-2.12/hello-scala-spark_2.12-1.0.jar --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data
[33malgo-2    |[0m 11-14 15:21 smspark-submit INFO     waiting for hosts
[33malgo-2    |[0m 11-14 15:21 smspark-submit INFO     starting status server
[33malgo-2    |[0m 11-14 15:21 smspark-submit INFO     Status server listening on algo-2:5555
[33malgo-2    |[0m 11-14 15:21 smspark-submit INFO     bootstrapping cluster
[33malgo-2    |[0m 11-14 15:21 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING
[33malgo-2    |[0m 11-14 15:21 smspark-submit INFO     copying aws jars
[33malgo-2    |[0m Serving on http://algo-2:5555
[36malgo-1    |[0m 11-14 15:21 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--jars', '/opt/ml/processing/input/jars', '--class', 'com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp', '--verbose', '/opt/ml/processing/input/code/scala/hello-scala-spark/target/scala-2.12/hello-scala-spark_2.12-1.0.jar', '--input', 'file:///opt/ml/processing/input/data/data.jsonl', '--output', 'file:///opt/ml/processing/output/data']
[36malgo-1    |[0m 11-14 15:21 smspark.cli  INFO     Raw spark options before processing: {'jars': '/opt/ml/processing/input/jars', 'class_': 'com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp', 'verbose': True, 'py_files': None, 'files': None}
[36malgo-1    |[0m 11-14 15:21 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/scala/hello-scala-spark/target/scala-2.12/hello-scala-spark_2.12-1.0.jar', '--input', 'file:///opt/ml/processing/input/data/data.jsonl', '--output', 'file:///opt/ml/processing/output/data']
[36malgo-1    |[0m 11-14 15:21 smspark.cli  INFO     Rendered spark options: {'jars': '/opt/ml/processing/input/jars/json4s-native_2.12-3.6.9.jar', 'class_': 'com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp', 'verbose': True, 'py_files': None, 'files': None}
[36malgo-1    |[0m 11-14 15:21 smspark.cli  INFO     Initializing processing job.
[36malgo-1    |[0m 11-14 15:21 smspark-submit INFO     {'current_host': 'algo-1', 'hosts': ['algo-1', 'algo-2']}
[36malgo-1    |[0m 11-14 15:21 smspark-submit INFO     {'ProcessingJobArn': 'processing_job_arn', 'ProcessingJobName': 'processing_job_name', 'AppSpecification': {'ImageUri': 'image_uri'}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30}}, 'RoleArn': 'iam_role'}
[36malgo-1    |[0m 11-14 15:21 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client --jars /opt/ml/processing/input/jars/json4s-native_2.12-3.6.9.jar --class com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp --verbose /opt/ml/processing/input/code/scala/hello-scala-spark/target/scala-2.12/hello-scala-spark_2.12-1.0.jar --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data
[36malgo-1    |[0m 11-14 15:21 smspark-submit INFO     waiting for hosts
[36malgo-1    |[0m 11-14 15:21 smspark-submit INFO     starting status server
[36malgo-1    |[0m 11-14 15:21 smspark-submit INFO     Status server listening on algo-1:5555
[36malgo-1    |[0m 11-14 15:21 smspark-submit INFO     bootstrapping cluster
[36malgo-1    |[0m 11-14 15:21 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING
[36malgo-1    |[0m 11-14 15:21 smspark-submit INFO     copying aws jars
[36malgo-1    |[0m Serving on http://algo-1:5555
[33malgo-2    |[0m 11-14 15:21 smspark-submit INFO     Found hadoop jar hadoop-aws.jar
[33malgo-2    |[0m 11-14 15:21 smspark-submit INFO     Optional jar jets3t-0.9.0.jar in /usr/lib/hadoop/lib does not exist
[33malgo-2    |[0m 11-14 15:21 smspark-submit INFO     copying cluster config
[33malgo-2    |[0m 11-14 15:21 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml
[33malgo-2    |[0m 11-14 15:21 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml
[33malgo-2    |[0m 11-14 15:21 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[33malgo-2    |[0m 11-14 15:21 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 15:21 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh
[33malgo-2    |[0m 11-14 15:21 root         INFO     Detected instance type: m5.xlarge with total memory: 16384M and total cores: 4
[33malgo-2    |[0m 11-14 15:21 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[33malgo-2    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0"?>
[33malgo-2    |[0m <!-- Site specific YARN configuration properties -->
[33malgo-2    |[0m  <configuration>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.resourcemanager.hostname</name>
[33malgo-2    |[0m          <value>192.168.240.3</value>
[33malgo-2    |[0m          <description>The hostname of the RM.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.hostname</name>
[33malgo-2    |[0m          <value>algo-2</value>
[33malgo-2    |[0m          <description>The hostname of the NM.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.webapp.address</name>
[33malgo-2    |[0m          <value>algo-2:8042</value>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.vmem-pmem-ratio</name>
[33malgo-2    |[0m          <value>5</value>
[33malgo-2    |[0m          <description>Ratio between virtual memory to physical memory.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.resourcemanager.am.max-attempts</name>
[33malgo-2    |[0m          <value>1</value>
[33malgo-2    |[0m          <description>The maximum number of application attempts.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.env-whitelist</name>
[33malgo-2    |[0m          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>
[33malgo-2    |[0m          <description>Environment variable whitelist</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m  
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.minimum-allocation-mb</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.maximum-allocation-mb</name>
[33malgo-2    |[0m     <value>15892</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.minimum-allocation-vcores</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.maximum-allocation-vcores</name>
[33malgo-2    |[0m     <value>4</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.nodemanager.resource.memory-mb</name>
[33malgo-2    |[0m     <value>15892</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.nodemanager.resource.cpu-vcores</name>
[33malgo-2    |[0m     <value>4</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:21 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[33malgo-2    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.driver.host=192.168.240.3
[33malgo-2    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[33malgo-2    |[0m 
[33malgo-2    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[33malgo-2    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[33malgo-2    |[0m spark.rpc.askTimeout=300s
[33malgo-2    |[0m spark.driver.memory 2048m
[33malgo-2    |[0m spark.driver.memoryOverhead 204m
[33malgo-2    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[33malgo-2    |[0m spark.executor.memory 12399m
[33malgo-2    |[0m spark.executor.memoryOverhead 1239m
[33malgo-2    |[0m spark.executor.cores 4
[33malgo-2    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[33malgo-2    |[0m spark.executor.instances 2
[33malgo-2    |[0m spark.default.parallelism 16
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:21 root         INFO     Finished Yarn configuration files setup.
[33malgo-2    |[0m 11-14 15:21 root         INFO     reading user configuration from /opt/ml/processing/input/conf/configuration.json
[33malgo-2    |[0m 11-14 15:21 root         INFO     User configuration list or dict: [{'Classification': 'spark-defaults', 'Properties': {'spark.executor.memory': '2g', 'spark.executor.cores': '1'}}, {'Classification': 'hadoop-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'core-site', 'Properties': {'spark.executor.memory': '2g', 'spark.executor.cores': '1'}}, {'Classification': 'hadoop-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'hive-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-exec-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-site', 'Properties': {'key': 'value'}}, {'Classification': 'spark-defaults', 'Properties': {'key': 'value'}}, {'Classification': 'spark-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'spark-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'spark-hive-site', 'Properties': {'key': 'value'}}, {'Classification': 'spark-metrics', 'Properties': {'key': 'value'}}, {'Classification': 'yarn-site', 'Properties': {'key': 'value'}}, {'Classification': 'yarn-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}] , type <class 'list'>
[33malgo-2    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[33malgo-2    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.driver.host=192.168.240.3
[33malgo-2    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[33malgo-2    |[0m 
[33malgo-2    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[33malgo-2    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[33malgo-2    |[0m spark.rpc.askTimeout=300s
[33malgo-2    |[0m spark.driver.memory 2048m
[33malgo-2    |[0m spark.driver.memoryOverhead 204m
[33malgo-2    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[33malgo-2    |[0m spark.executor.memory 12399m
[33malgo-2    |[0m spark.executor.memoryOverhead 1239m
[33malgo-2    |[0m spark.executor.cores 4
[33malgo-2    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[33malgo-2    |[0m spark.executor.instances 2
[33malgo-2    |[0m spark.default.parallelism 16
[33malgo-2    |[0m spark.executor.memory 2g
[33malgo-2    |[0m spark.executor.cores 1
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/hadoop-env.sh
[33malgo-2    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/hadoop-env.sh is: 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[33malgo-2    |[0m # or more contributor license agreements.  See the NOTICE file
[33malgo-2    |[0m # distributed with this work for additional information
[33malgo-2    |[0m # regarding copyright ownership.  The ASF licenses this file
[33malgo-2    |[0m # to you under the Apache License, Version 2.0 (the
[33malgo-2    |[0m # "License"); you may not use this file except in compliance
[33malgo-2    |[0m # with the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m # Set Hadoop-specific environment variables here.
[33malgo-2    |[0m 
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## THIS FILE ACTS AS THE MASTER FILE FOR ALL HADOOP PROJECTS.
[33malgo-2    |[0m ## SETTINGS HERE WILL BE READ BY ALL HADOOP COMMANDS.  THEREFORE,
[33malgo-2    |[0m ## ONE CAN USE THIS FILE TO SET YARN, HDFS, AND MAPREDUCE
[33malgo-2    |[0m ## CONFIGURATION OPTIONS INSTEAD OF xxx-env.sh.
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## Precedence rules:
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## {yarn-env.sh|hdfs-env.sh} > hadoop-env.sh > hard-coded defaults
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## {YARN_xyz|HDFS_xyz} > HADOOP_xyz > hard-coded defaults
[33malgo-2    |[0m ##
[33malgo-2    |[0m 
[33malgo-2    |[0m # Many of the options here are built from the perspective that users
[33malgo-2    |[0m # may want to provide OVERWRITING values on the command line.
[33malgo-2    |[0m # For example:
[33malgo-2    |[0m #
[33malgo-2    |[0m #  JAVA_HOME=/usr/java/testing hdfs dfs -ls
[33malgo-2    |[0m #
[33malgo-2    |[0m # Therefore, the vast majority (BUT NOT ALL!) of these defaults
[33malgo-2    |[0m # are configured for substitution and not append.  If append
[33malgo-2    |[0m # is preferable, modify this file accordingly.
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Generic settings for HADOOP
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Technically, the only required environment variable is JAVA_HOME.
[33malgo-2    |[0m # All others are optional.  However, the defaults are probably not
[33malgo-2    |[0m # preferred.  Many sites configure these options outside of Hadoop,
[33malgo-2    |[0m # such as in /etc/profile.d
[33malgo-2    |[0m 
[33malgo-2    |[0m # The java implementation to use. By default, this environment
[33malgo-2    |[0m # variable is REQUIRED on ALL platforms except OS X!
[33malgo-2    |[0m # export JAVA_HOME=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Location of Hadoop.  By default, Hadoop will attempt to determine
[33malgo-2    |[0m # this location based upon its execution path.
[33malgo-2    |[0m # export HADOOP_HOME=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Location of Hadoop's configuration information.  i.e., where this
[33malgo-2    |[0m # file is living. If this is not defined, Hadoop will attempt to
[33malgo-2    |[0m # locate it based upon its execution path.
[33malgo-2    |[0m #
[33malgo-2    |[0m # NOTE: It is recommend that this variable not be set here but in
[33malgo-2    |[0m # /etc/profile.d or equivalent.  Some options (such as
[33malgo-2    |[0m # --config) may react strangely otherwise.
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
[33malgo-2    |[0m 
[33malgo-2    |[0m # The maximum amount of heap to use (Java -Xmx).  If no unit
[33malgo-2    |[0m # is provided, it will be converted to MB.  Daemons will
[33malgo-2    |[0m # prefer any Xmx setting in their respective _OPT variable.
[33malgo-2    |[0m # There is no default; the JVM will autoscale based upon machine
[33malgo-2    |[0m # memory size.
[33malgo-2    |[0m # export HADOOP_HEAPSIZE_MAX=
[33malgo-2    |[0m 
[33malgo-2    |[0m # The minimum amount of heap to use (Java -Xms).  If no unit
[33malgo-2    |[0m # is provided, it will be converted to MB.  Daemons will
[33malgo-2    |[0m # prefer any Xms setting in their respective _OPT variable.
[33malgo-2    |[0m # There is no default; the JVM will autoscale based upon machine
[33malgo-2    |[0m # memory size.
[33malgo-2    |[0m # export HADOOP_HEAPSIZE_MIN=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Enable extra debugging of Hadoop's JAAS binding, used to set up
[33malgo-2    |[0m # Kerberos security.
[33malgo-2    |[0m # export HADOOP_JAAS_DEBUG=true
[33malgo-2    |[0m 
[33malgo-2    |[0m # Extra Java runtime options for all Hadoop commands. We don't support
[33malgo-2    |[0m # IPv6 yet/still, so by default the preference is set to IPv4.
[33malgo-2    |[0m # export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true"
[33malgo-2    |[0m # For Kerberos debugging, an extended option set logs more information
[33malgo-2    |[0m # export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug"
[33malgo-2    |[0m 
[33malgo-2    |[0m # Some parts of the shell code may do special things dependent upon
[33malgo-2    |[0m # the operating system.  We have to set this here. See the next
[33malgo-2    |[0m # section as to why....
[33malgo-2    |[0m #export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}
[33malgo-2    |[0m 
[33malgo-2    |[0m # Extra Java runtime options for some Hadoop commands
[33malgo-2    |[0m # and clients (i.e., hdfs dfs -blah).  These get appended to HADOOP_OPTS for
[33malgo-2    |[0m # such commands.  In most cases, # this should be left empty and
[33malgo-2    |[0m # let users supply it on the command line.
[33malgo-2    |[0m # export HADOOP_CLIENT_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # A note about classpaths.
[33malgo-2    |[0m #
[33malgo-2    |[0m # By default, Apache Hadoop overrides Java's CLASSPATH
[33malgo-2    |[0m # environment variable.  It is configured such
[33malgo-2    |[0m # that it starts out blank with new entries added after passing
[33malgo-2    |[0m # a series of checks (file/dir exists, not already listed aka
[33malgo-2    |[0m # de-deduplication).  During de-deduplication, wildcards and/or
[33malgo-2    |[0m # directories are *NOT* expanded to keep it simple. Therefore,
[33malgo-2    |[0m # if the computed classpath has two specific mentions of
[33malgo-2    |[0m # awesome-methods-1.0.jar, only the first one added will be seen.
[33malgo-2    |[0m # If two directories are in the classpath that both contain
[33malgo-2    |[0m # awesome-methods-1.0.jar, then Java will pick up both versions.
[33malgo-2    |[0m 
[33malgo-2    |[0m # An additional, custom CLASSPATH. Site-wide configs should be
[33malgo-2    |[0m # handled via the shellprofile functionality, utilizing the
[33malgo-2    |[0m # hadoop_add_classpath function for greater control and much
[33malgo-2    |[0m # harder for apps/end-users to accidentally override.
[33malgo-2    |[0m # Similarly, end users should utilize ${HOME}/.hadooprc .
[33malgo-2    |[0m # This variable should ideally only be used as a short-cut,
[33malgo-2    |[0m # interactive way for temporary additions on the command line.
[33malgo-2    |[0m # export HADOOP_CLASSPATH="/some/cool/path/on/your/machine"
[33malgo-2    |[0m 
[33malgo-2    |[0m # Should HADOOP_CLASSPATH be first in the official CLASSPATH?
[33malgo-2    |[0m # export HADOOP_USER_CLASSPATH_FIRST="yes"
[33malgo-2    |[0m 
[33malgo-2    |[0m # If HADOOP_USE_CLIENT_CLASSLOADER is set, the classpath along
[33malgo-2    |[0m # with the main jar are handled by a separate isolated
[33malgo-2    |[0m # client classloader when 'hadoop jar', 'yarn jar', or 'mapred job'
[33malgo-2    |[0m # is utilized. If it is set, HADOOP_CLASSPATH and
[33malgo-2    |[0m # HADOOP_USER_CLASSPATH_FIRST are ignored.
[33malgo-2    |[0m # export HADOOP_USE_CLIENT_CLASSLOADER=true
[33malgo-2    |[0m 
[33malgo-2    |[0m # HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES overrides the default definition of
[33malgo-2    |[0m # system classes for the client classloader when HADOOP_USE_CLIENT_CLASSLOADER
[33malgo-2    |[0m # is enabled. Names ending in '.' (period) are treated as package names, and
[33malgo-2    |[0m # names starting with a '-' are treated as negative matches. For example,
[33malgo-2    |[0m # export HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES="-org.apache.hadoop.UserClass,java.,javax.,org.apache.hadoop."
[33malgo-2    |[0m 
[33malgo-2    |[0m # Enable optional, bundled Hadoop features
[33malgo-2    |[0m # This is a comma delimited list.  It may NOT be overridden via .hadooprc
[33malgo-2    |[0m # Entries may be added/removed as needed.
[33malgo-2    |[0m # export HADOOP_OPTIONAL_TOOLS="hadoop-azure,hadoop-azure-datalake,hadoop-aws,hadoop-openstack,hadoop-aliyun,hadoop-kafka"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Options for remote shell connectivity
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # There are some optional components of hadoop that allow for
[33malgo-2    |[0m # command and control of remote hosts.  For example,
[33malgo-2    |[0m # start-dfs.sh will attempt to bring up all NNs, DNS, etc.
[33malgo-2    |[0m 
[33malgo-2    |[0m # Options to pass to SSH when one of the "log into a host and
[33malgo-2    |[0m # start/stop daemons" scripts is executed
[33malgo-2    |[0m # export HADOOP_SSH_OPTS="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10s"
[33malgo-2    |[0m 
[33malgo-2    |[0m # The built-in ssh handler will limit itself to 10 simultaneous connections.
[33malgo-2    |[0m # For pdsh users, this sets the fanout size ( -f )
[33malgo-2    |[0m # Change this to increase/decrease as necessary.
[33malgo-2    |[0m # export HADOOP_SSH_PARALLEL=10
[33malgo-2    |[0m 
[33malgo-2    |[0m # Filename which contains all of the hosts for any remote execution
[33malgo-2    |[0m # helper scripts # such as workers.sh, start-dfs.sh, etc.
[33malgo-2    |[0m # export HADOOP_WORKERS="${HADOOP_CONF_DIR}/workers"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Options for all daemons
[33malgo-2    |[0m ###
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Many options may also be specified as Java properties.  It is
[33malgo-2    |[0m # very common, and in many cases, desirable, to hard-set these
[33malgo-2    |[0m # in daemon _OPTS variables.  Where applicable, the appropriate
[33malgo-2    |[0m # Java property is also identified.  Note that many are re-used
[33malgo-2    |[0m # or set differently in certain contexts (e.g., secure vs
[33malgo-2    |[0m # non-secure)
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m # Where (primarily) daemon log files are stored.
[33malgo-2    |[0m # ${HADOOP_HOME}/logs by default.
[33malgo-2    |[0m # Java property: hadoop.log.dir
[33malgo-2    |[0m # export HADOOP_LOG_DIR=${HADOOP_HOME}/logs
[33malgo-2    |[0m 
[33malgo-2    |[0m # A string representing this instance of hadoop. $USER by default.
[33malgo-2    |[0m # This is used in writing log and pid files, so keep that in mind!
[33malgo-2    |[0m # Java property: hadoop.id.str
[33malgo-2    |[0m # export HADOOP_IDENT_STRING=$USER
[33malgo-2    |[0m 
[33malgo-2    |[0m # How many seconds to pause after stopping a daemon
[33malgo-2    |[0m # export HADOOP_STOP_TIMEOUT=5
[33malgo-2    |[0m 
[33malgo-2    |[0m # Where pid files are stored.  /tmp by default.
[33malgo-2    |[0m # export HADOOP_PID_DIR=/tmp
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default log4j setting for interactive commands
[33malgo-2    |[0m # Java property: hadoop.root.logger
[33malgo-2    |[0m # export HADOOP_ROOT_LOGGER=INFO,console
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default log4j setting for daemons spawned explicitly by
[33malgo-2    |[0m # --daemon option of hadoop, hdfs, mapred and yarn command.
[33malgo-2    |[0m # Java property: hadoop.root.logger
[33malgo-2    |[0m # export HADOOP_DAEMON_ROOT_LOGGER=INFO,RFA
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default log level and output location for security-related messages.
[33malgo-2    |[0m # You will almost certainly want to change this on a per-daemon basis via
[33malgo-2    |[0m # the Java property (i.e., -Dhadoop.security.logger=foo). (Note that the
[33malgo-2    |[0m # defaults for the NN and 2NN override this by default.)
[33malgo-2    |[0m # Java property: hadoop.security.logger
[33malgo-2    |[0m # export HADOOP_SECURITY_LOGGER=INFO,NullAppender
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default process priority level
[33malgo-2    |[0m # Note that sub-processes will also run at this level!
[33malgo-2    |[0m # export HADOOP_NICENESS=0
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default name for the service level authorization file
[33malgo-2    |[0m # Java property: hadoop.policy.file
[33malgo-2    |[0m # export HADOOP_POLICYFILE="hadoop-policy.xml"
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # NOTE: this is not used by default!  <-----
[33malgo-2    |[0m # You can define variables right here and then re-use them later on.
[33malgo-2    |[0m # For example, it is common to use the same garbage collection settings
[33malgo-2    |[0m # for all the daemons.  So one could define:
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HADOOP_GC_SETTINGS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps"
[33malgo-2    |[0m #
[33malgo-2    |[0m # .. and then use it as per the b option under the namenode.
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Secure/privileged execution
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Out of the box, Hadoop uses jsvc from Apache Commons to launch daemons
[33malgo-2    |[0m # on privileged ports.  This functionality can be replaced by providing
[33malgo-2    |[0m # custom functions.  See hadoop-functions.sh for more information.
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m # The jsvc implementation to use. Jsvc is required to run secure datanodes
[33malgo-2    |[0m # that bind to privileged ports to provide authentication of data transfer
[33malgo-2    |[0m # protocol.  Jsvc is not required if SASL is configured for authentication of
[33malgo-2    |[0m # data transfer protocol using non-privileged ports.
[33malgo-2    |[0m # export JSVC_HOME=/usr/bin
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # This directory contains pids for secure and privileged processes.
[33malgo-2    |[0m #export HADOOP_SECURE_PID_DIR=${HADOOP_PID_DIR}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # This directory contains the logs for secure and privileged processes.
[33malgo-2    |[0m # Java property: hadoop.log.dir
[33malgo-2    |[0m # export HADOOP_SECURE_LOG=${HADOOP_LOG_DIR}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # When running a secure daemon, the default value of HADOOP_IDENT_STRING
[33malgo-2    |[0m # ends up being a bit bogus.  Therefore, by default, the code will
[33malgo-2    |[0m # replace HADOOP_IDENT_STRING with HADOOP_xx_SECURE_USER.  If one wants
[33malgo-2    |[0m # to keep HADOOP_IDENT_STRING untouched, then uncomment this line.
[33malgo-2    |[0m # export HADOOP_SECURE_IDENT_PRESERVE="true"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # NameNode specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default log level and output location for file system related change
[33malgo-2    |[0m # messages. For non-namenode daemons, the Java property must be set in
[33malgo-2    |[0m # the appropriate _OPTS if one wants something other than INFO,NullAppender
[33malgo-2    |[0m # Java property: hdfs.audit.logger
[33malgo-2    |[0m # export HDFS_AUDIT_LOGGER=INFO,NullAppender
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the NameNode.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # a) Set JMX options
[33malgo-2    |[0m # export HDFS_NAMENODE_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
[33malgo-2    |[0m #
[33malgo-2    |[0m # b) Set garbage collection logs
[33malgo-2    |[0m # export HDFS_NAMENODE_OPTS="${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[33malgo-2    |[0m #
[33malgo-2    |[0m # c) ... or set them directly
[33malgo-2    |[0m # export HDFS_NAMENODE_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[33malgo-2    |[0m 
[33malgo-2    |[0m # this is the default:
[33malgo-2    |[0m # export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # SecondaryNameNode specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the SecondaryNameNode.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # This is the default:
[33malgo-2    |[0m # export HDFS_SECONDARYNAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # DataNode specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the DataNode.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # This is the default:
[33malgo-2    |[0m # export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS"
[33malgo-2    |[0m 
[33malgo-2    |[0m # On secure datanodes, user to run the datanode as after dropping privileges.
[33malgo-2    |[0m # This **MUST** be uncommented to enable secure HDFS if using privileged ports
[33malgo-2    |[0m # to provide authentication of data transfer protocol.  This **MUST NOT** be
[33malgo-2    |[0m # defined if SASL is configured for authentication of data transfer protocol
[33malgo-2    |[0m # using non-privileged ports.
[33malgo-2    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[33malgo-2    |[0m # export HDFS_DATANODE_SECURE_USER=hdfs
[33malgo-2    |[0m 
[33malgo-2    |[0m # Supplemental options for secure datanodes
[33malgo-2    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[33malgo-2    |[0m # server jvm.
[33malgo-2    |[0m # export HDFS_DATANODE_SECURE_EXTRA_OPTS="-jvm server"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # NFS3 Gateway specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the NFS3 Gateway.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_NFS3_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the Hadoop portmapper.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_PORTMAP_OPTS="-Xmx512m"
[33malgo-2    |[0m 
[33malgo-2    |[0m # Supplemental options for priviliged gateways
[33malgo-2    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[33malgo-2    |[0m # server jvm.
[33malgo-2    |[0m # export HDFS_NFS3_SECURE_EXTRA_OPTS="-jvm server"
[33malgo-2    |[0m 
[33malgo-2    |[0m # On privileged gateways, user to run the gateway as after dropping privileges
[33malgo-2    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[33malgo-2    |[0m # export HDFS_NFS3_SECURE_USER=nfsserver
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # ZKFailoverController specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the ZKFailoverController.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_ZKFC_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # QuorumJournalNode specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the QuorumJournalNode.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_JOURNALNODE_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # HDFS Balancer specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the HDFS Balancer.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_BALANCER_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # HDFS Mover specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the HDFS Mover.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_MOVER_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Router-based HDFS Federation specific parameters
[33malgo-2    |[0m # Specify the JVM options to be used when starting the RBF Routers.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_DFSROUTER_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # HDFS StorageContainerManager specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the HDFS Storage Container Manager.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_STORAGECONTAINERMANAGER_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Advanced Users Only!
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # When building Hadoop, one can add the class paths to the commands
[33malgo-2    |[0m # via this special env var:
[33malgo-2    |[0m # export HADOOP_ENABLE_BUILD_PATHS="true"
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # To prevent accidents, shell commands be (superficially) locked
[33malgo-2    |[0m # to only allow certain users to execute certain subcommands.
[33malgo-2    |[0m # It uses the format of (command)_(subcommand)_USER.
[33malgo-2    |[0m #
[33malgo-2    |[0m # For example, to limit who can execute the namenode command,
[33malgo-2    |[0m # export HDFS_NAMENODE_USER=hdfs
[33malgo-2    |[0m export SPARK_MASTER_HOST=192.168.240.3
[33malgo-2    |[0m export AWS_CONTAINER_CREDENTIALS_RELATIVE_URI=
[33malgo-2    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[33malgo-2    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/core-site.xml
[33malgo-2    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/core-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0" encoding="UTF-8"?>
[33malgo-2    |[0m  <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[33malgo-2    |[0m  <!-- Put site-specific property overrides in this file. -->
[33malgo-2    |[0m 
[33malgo-2    |[0m  <configuration>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>fs.defaultFS</name>
[33malgo-2    |[0m          <value>hdfs://192.168.240.3/</value>
[33malgo-2    |[0m          <description>NameNode URI</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>fs.s3a.aws.credentials.provider</name>
[33malgo-2    |[0m          <value>com.amazonaws.auth.DefaultAWSCredentialsProviderChain</value>
[33malgo-2    |[0m          <description>AWS S3 credential provider</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>fs.s3.impl</name>
[33malgo-2    |[0m          <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
[33malgo-2    |[0m          <description>s3a filesystem implementation</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>fs.AbstractFileSystem.s3a.imp</name>
[33malgo-2    |[0m          <value>org.apache.hadoop.fs.s3a.S3A</value>
[33malgo-2    |[0m          <description>s3a filesystem implementation</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m  
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>spark.executor.memory</name>
[33malgo-2    |[0m     <value>2g</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>spark.executor.cores</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/log4j.properties
[33malgo-2    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/log4j.properties is: 
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[33malgo-2    |[0m # or more contributor license agreements.  See the NOTICE file
[33malgo-2    |[0m # distributed with this work for additional information
[33malgo-2    |[0m # regarding copyright ownership.  The ASF licenses this file
[33malgo-2    |[0m # to you under the Apache License, Version 2.0 (the
[33malgo-2    |[0m # "License"); you may not use this file except in compliance
[33malgo-2    |[0m # with the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m # Define some default values that can be overridden by system properties
[33malgo-2    |[0m hadoop.root.logger=INFO,console
[33malgo-2    |[0m hadoop.log.dir=.
[33malgo-2    |[0m hadoop.log.file=hadoop.log
[33malgo-2    |[0m 
[33malgo-2    |[0m # Define the root logger to the system property "hadoop.root.logger".
[33malgo-2    |[0m log4j.rootLogger=${hadoop.root.logger}, EventCounter
[33malgo-2    |[0m 
[33malgo-2    |[0m # Logging Threshold
[33malgo-2    |[0m log4j.threshold=ALL
[33malgo-2    |[0m 
[33malgo-2    |[0m # Null Appender
[33malgo-2    |[0m log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Rolling File Appender - cap space usage at 5gb.
[33malgo-2    |[0m #
[33malgo-2    |[0m hadoop.log.maxfilesize=256MB
[33malgo-2    |[0m hadoop.log.maxbackupindex=20
[33malgo-2    |[0m log4j.appender.RFA=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.RFA.MaxFileSize=${hadoop.log.maxfilesize}
[33malgo-2    |[0m log4j.appender.RFA.MaxBackupIndex=${hadoop.log.maxbackupindex}
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.RFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m 
[33malgo-2    |[0m # Pattern format: Date LogLevel LoggerName LogMessage
[33malgo-2    |[0m log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[33malgo-2    |[0m # Debugging Pattern format
[33malgo-2    |[0m #log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Daily Rolling File Appender
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.DRFA.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m 
[33malgo-2    |[0m # Rollver at midnight
[33malgo-2    |[0m #log4j.appender.DRFA.DatePattern=.yyyy-MM-dd
[33malgo-2    |[0m # Rollver at every hour
[33malgo-2    |[0m log4j.appender.DRFA.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m 
[33malgo-2    |[0m # Pattern format: Date LogLevel LoggerName LogMessage
[33malgo-2    |[0m log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m # Debugging Pattern format
[33malgo-2    |[0m #log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # console
[33malgo-2    |[0m # Add "console" to rootlogger above if you want to use this
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.console=org.apache.log4j.ConsoleAppender
[33malgo-2    |[0m log4j.appender.console.target=System.err
[33malgo-2    |[0m log4j.appender.console.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # TaskLog Appender
[33malgo-2    |[0m #
[33malgo-2    |[0m log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.TLA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # HDFS block state change log from block manager
[33malgo-2    |[0m #
[33malgo-2    |[0m # Uncomment the following to log normal block state change
[33malgo-2    |[0m # messages from BlockManager in NameNode.
[33malgo-2    |[0m #log4j.logger.BlockStateChange=DEBUG
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m #Security appender
[33malgo-2    |[0m #
[33malgo-2    |[0m hadoop.security.logger=INFO,NullAppender
[33malgo-2    |[0m hadoop.security.log.maxfilesize=256MB
[33malgo-2    |[0m hadoop.security.log.maxbackupindex=20
[33malgo-2    |[0m log4j.category.SecurityLogger=${hadoop.security.logger}
[33malgo-2    |[0m hadoop.security.log.file=SecurityAuth-${user.name}.audit
[33malgo-2    |[0m log4j.appender.RFAS=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.RFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
[33malgo-2    |[0m log4j.appender.RFAS.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[33malgo-2    |[0m log4j.appender.RFAS.MaxFileSize=${hadoop.security.log.maxfilesize}
[33malgo-2    |[0m log4j.appender.RFAS.MaxBackupIndex=${hadoop.security.log.maxbackupindex}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Daily Rolling Security appender
[33malgo-2    |[0m #
[33malgo-2    |[0m log4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.DRFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
[33malgo-2    |[0m log4j.appender.DRFAS.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.DRFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[33malgo-2    |[0m log4j.appender.DRFAS.DatePattern=.yyyy-MM-dd
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # hadoop configuration logging
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m # Uncomment the following line to turn off configuration deprecation warnings.
[33malgo-2    |[0m # log4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # hdfs audit logging
[33malgo-2    |[0m #
[33malgo-2    |[0m hdfs.audit.logger=INFO,NullAppender
[33malgo-2    |[0m hdfs.audit.log.maxfilesize=256MB
[33malgo-2    |[0m hdfs.audit.log.maxbackupindex=20
[33malgo-2    |[0m log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}
[33malgo-2    |[0m log4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false
[33malgo-2    |[0m log4j.appender.RFAAUDIT=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.RFAAUDIT.File=${hadoop.log.dir}/hdfs-audit.log
[33malgo-2    |[0m log4j.appender.RFAAUDIT.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.RFAAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[33malgo-2    |[0m log4j.appender.RFAAUDIT.MaxFileSize=${hdfs.audit.log.maxfilesize}
[33malgo-2    |[0m log4j.appender.RFAAUDIT.MaxBackupIndex=${hdfs.audit.log.maxbackupindex}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # NameNode metrics logging.
[33malgo-2    |[0m # The default is to retain two namenode-metrics.log files up to 64MB each.
[33malgo-2    |[0m #
[33malgo-2    |[0m namenode.metrics.logger=INFO,NullAppender
[33malgo-2    |[0m log4j.logger.NameNodeMetricsLog=${namenode.metrics.logger}
[33malgo-2    |[0m log4j.additivity.NameNodeMetricsLog=false
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.File=${hadoop.log.dir}/namenode-metrics.log
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.MaxBackupIndex=1
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.MaxFileSize=64MB
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # DataNode metrics logging.
[33malgo-2    |[0m # The default is to retain two datanode-metrics.log files up to 64MB each.
[33malgo-2    |[0m #
[33malgo-2    |[0m datanode.metrics.logger=INFO,NullAppender
[33malgo-2    |[0m log4j.logger.DataNodeMetricsLog=${datanode.metrics.logger}
[33malgo-2    |[0m log4j.additivity.DataNodeMetricsLog=false
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.File=${hadoop.log.dir}/datanode-metrics.log
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.MaxBackupIndex=1
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.MaxFileSize=64MB
[33malgo-2    |[0m 
[33malgo-2    |[0m # Custom Logging levels
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=DEBUG
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m # AWS SDK & S3A FileSystem
[33malgo-2    |[0m #log4j.logger.com.amazonaws=ERROR
[33malgo-2    |[0m log4j.logger.com.amazonaws.http.AmazonHttpClient=ERROR
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.fs.s3a.S3AFileSystem=WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Event Counter Appender
[33malgo-2    |[0m # Sends counts of logging messages at different severity levels to Hadoop Metrics.
[33malgo-2    |[0m #
[33malgo-2    |[0m log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Job Summary Appender
[33malgo-2    |[0m #
[33malgo-2    |[0m # Use following logger to send summary to separate file defined by
[33malgo-2    |[0m # hadoop.mapreduce.jobsummary.log.file :
[33malgo-2    |[0m # hadoop.mapreduce.jobsummary.logger=INFO,JSA
[33malgo-2    |[0m # 
[33malgo-2    |[0m hadoop.mapreduce.jobsummary.logger=${hadoop.root.logger}
[33malgo-2    |[0m hadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log
[33malgo-2    |[0m hadoop.mapreduce.jobsummary.log.maxfilesize=256MB
[33malgo-2    |[0m hadoop.mapreduce.jobsummary.log.maxbackupindex=20
[33malgo-2    |[0m log4j.appender.JSA=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.JSA.File=${hadoop.log.dir}/${hadoop.mapreduce.jobsummary.log.file}
[33malgo-2    |[0m log4j.appender.JSA.MaxFileSize=${hadoop.mapreduce.jobsummary.log.maxfilesize}
[33malgo-2    |[0m log4j.appender.JSA.MaxBackupIndex=${hadoop.mapreduce.jobsummary.log.maxbackupindex}
[33malgo-2    |[0m log4j.appender.JSA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.JSA.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
[33malgo-2    |[0m log4j.appender.JSA.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.logger.org.apache.hadoop.mapred.JobInProgress$JobSummary=${hadoop.mapreduce.jobsummary.logger}
[33malgo-2    |[0m log4j.additivity.org.apache.hadoop.mapred.JobInProgress$JobSummary=false
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # shuffle connection log from shuffleHandler
[33malgo-2    |[0m # Uncomment the following line to enable logging of shuffle connections
[33malgo-2    |[0m # log4j.logger.org.apache.hadoop.mapred.ShuffleHandler.audit=DEBUG
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Yarn ResourceManager Application Summary Log
[33malgo-2    |[0m #
[33malgo-2    |[0m # Set the ResourceManager summary log filename
[33malgo-2    |[0m yarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log
[33malgo-2    |[0m # Set the ResourceManager summary log level and appender
[33malgo-2    |[0m yarn.server.resourcemanager.appsummary.logger=${hadoop.root.logger}
[33malgo-2    |[0m #yarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY
[33malgo-2    |[0m 
[33malgo-2    |[0m # To enable AppSummaryLogging for the RM,
[33malgo-2    |[0m # set yarn.server.resourcemanager.appsummary.logger to
[33malgo-2    |[0m # <LEVEL>,RMSUMMARY in hadoop-env.sh
[33malgo-2    |[0m 
[33malgo-2    |[0m # Appender for ResourceManager Application Summary Log
[33malgo-2    |[0m # Requires the following properties to be set
[33malgo-2    |[0m #    - hadoop.log.dir (Hadoop Log directory)
[33malgo-2    |[0m #    - yarn.server.resourcemanager.appsummary.log.file (resource manager app summary log filename)
[33malgo-2    |[0m #    - yarn.server.resourcemanager.appsummary.logger (resource manager app summary log level and appender)
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=${yarn.server.resourcemanager.appsummary.logger}
[33malgo-2    |[0m log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=false
[33malgo-2    |[0m log4j.appender.RMSUMMARY=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.RMSUMMARY.File=${hadoop.log.dir}/${yarn.server.resourcemanager.appsummary.log.file}
[33malgo-2    |[0m log4j.appender.RMSUMMARY.MaxFileSize=256MB
[33malgo-2    |[0m log4j.appender.RMSUMMARY.MaxBackupIndex=20
[33malgo-2    |[0m log4j.appender.RMSUMMARY.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.RMSUMMARY.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m # HS audit log configs
[33malgo-2    |[0m #mapreduce.hs.audit.logger=INFO,HSAUDIT
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=${mapreduce.hs.audit.logger}
[33malgo-2    |[0m #log4j.additivity.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=false
[33malgo-2    |[0m #log4j.appender.HSAUDIT=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m #log4j.appender.HSAUDIT.File=${hadoop.log.dir}/hs-audit.log
[33malgo-2    |[0m #log4j.appender.HSAUDIT.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m #log4j.appender.HSAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[33malgo-2    |[0m #log4j.appender.HSAUDIT.DatePattern=.yyyy-MM-dd
[33malgo-2    |[0m 
[33malgo-2    |[0m # Http Server Request Logs
[33malgo-2    |[0m #log4j.logger.http.requests.namenode=INFO,namenoderequestlog
[33malgo-2    |[0m #log4j.appender.namenoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.namenoderequestlog.Filename=${hadoop.log.dir}/jetty-namenode-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.namenoderequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.http.requests.datanode=INFO,datanoderequestlog
[33malgo-2    |[0m #log4j.appender.datanoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.datanoderequestlog.Filename=${hadoop.log.dir}/jetty-datanode-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.datanoderequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.http.requests.resourcemanager=INFO,resourcemanagerrequestlog
[33malgo-2    |[0m #log4j.appender.resourcemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.resourcemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-resourcemanager-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.resourcemanagerrequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.http.requests.jobhistory=INFO,jobhistoryrequestlog
[33malgo-2    |[0m #log4j.appender.jobhistoryrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.jobhistoryrequestlog.Filename=${hadoop.log.dir}/jetty-jobhistory-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.jobhistoryrequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.http.requests.nodemanager=INFO,nodemanagerrequestlog
[33malgo-2    |[0m #log4j.appender.nodemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.nodemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-nodemanager-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.nodemanagerrequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m # WebHdfs request log on datanodes
[33malgo-2    |[0m # Specify -Ddatanode.webhdfs.logger=INFO,HTTPDRFA on datanode startup to
[33malgo-2    |[0m # direct the log to a separate file.
[33malgo-2    |[0m #datanode.webhdfs.logger=INFO,console
[33malgo-2    |[0m #log4j.logger.datanode.webhdfs=${datanode.webhdfs.logger}
[33malgo-2    |[0m #log4j.appender.HTTPDRFA=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m #log4j.appender.HTTPDRFA.File=${hadoop.log.dir}/hadoop-datanode-webhdfs.log
[33malgo-2    |[0m #log4j.appender.HTTPDRFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m #log4j.appender.HTTPDRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[33malgo-2    |[0m #log4j.appender.HTTPDRFA.DatePattern=.yyyy-MM-dd
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m # Appender for viewing information for errors and warnings
[33malgo-2    |[0m yarn.ewma.cleanupInterval=300
[33malgo-2    |[0m yarn.ewma.messageAgeLimitSeconds=86400
[33malgo-2    |[0m yarn.ewma.maxUniqueMessages=250
[33malgo-2    |[0m log4j.appender.EWMA=org.apache.hadoop.yarn.util.Log4jWarningErrorMetricsAppender
[33malgo-2    |[0m log4j.appender.EWMA.cleanupInterval=${yarn.ewma.cleanupInterval}
[33malgo-2    |[0m log4j.appender.EWMA.messageAgeLimitSeconds=${yarn.ewma.messageAgeLimitSeconds}
[33malgo-2    |[0m log4j.appender.EWMA.maxUniqueMessages=${yarn.ewma.maxUniqueMessages}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Fair scheduler state dump
[33malgo-2    |[0m #
[33malgo-2    |[0m # Use following logger to dump the state to a separate file
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=DEBUG,FSSTATEDUMP
[33malgo-2    |[0m #log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=false
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.File=${hadoop.log.dir}/fairscheduler-statedump.log
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.MaxFileSize=${hadoop.log.maxfilesize}
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.MaxBackupIndex=${hadoop.log.maxbackupindex}
[33malgo-2    |[0m 
[33malgo-2    |[0m # Log levels of third-party libraries
[33malgo-2    |[0m log4j.logger.org.apache.commons.beanutils=WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m #AWS SDK Logging
[33malgo-2    |[0m log4j.logger.com.amazonaws=WARN
[33malgo-2    |[0m log4j.logger.org.apache.zookeeper=ERROR
[33malgo-2    |[0m log4j.logger.org.apache.hadoop.hbase=WARN
[33malgo-2    |[0m log4j.logger.amazon.emr.metrics=WARN
[33malgo-2    |[0m log4j.logger.emr=INFO
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.HADOOP=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.HADOOP.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m log4j.appender.HADOOP.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.appender.HADOOP.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.HADOOP.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.JHS=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.JHS.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m log4j.appender.JHS.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.appender.JHS.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.JHS.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.MAPRED=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.MAPRED.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m log4j.appender.MAPRED.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.appender.MAPRED.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.MAPRED.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.YARN=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.YARN.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m log4j.appender.YARN.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.appender.YARN.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.YARN.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/hive/conf/hive-env.sh
[33malgo-2    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/hive/conf/hive-env.sh is: 
[33malgo-2    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[33malgo-2    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/hive/conf/hive-log4j2.properties
[33malgo-2    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/hive/conf/hive-log4j2.properties is: 
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[33malgo-2    |[0m # or more contributor license agreements.  See the NOTICE file
[33malgo-2    |[0m # distributed with this work for additional information
[33malgo-2    |[0m # regarding copyright ownership.  The ASF licenses this file
[33malgo-2    |[0m # to you under the Apache License, Version 2.0 (the
[33malgo-2    |[0m # "License"); you may not use this file except in compliance
[33malgo-2    |[0m # with the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m status = INFO
[33malgo-2    |[0m name = HiveLog4j2
[33malgo-2    |[0m packages = org.apache.hadoop.hive.ql.log
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of properties
[33malgo-2    |[0m property.hive.log.level = INFO
[33malgo-2    |[0m property.hive.root.logger = DRFA
[33malgo-2    |[0m property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
[33malgo-2    |[0m property.hive.log.file = hive.log
[33malgo-2    |[0m property.hive.perflogger.log.level = INFO
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of all appenders
[33malgo-2    |[0m appenders = console, DRFA
[33malgo-2    |[0m 
[33malgo-2    |[0m # console appender
[33malgo-2    |[0m appender.console.type = Console
[33malgo-2    |[0m appender.console.name = console
[33malgo-2    |[0m appender.console.target = SYSTEM_ERR
[33malgo-2    |[0m appender.console.layout.type = PatternLayout
[33malgo-2    |[0m appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m # daily rolling file appender
[33malgo-2    |[0m appender.DRFA.type = RollingRandomAccessFile
[33malgo-2    |[0m appender.DRFA.name = DRFA
[33malgo-2    |[0m appender.DRFA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
[33malgo-2    |[0m # Use %pid in the filePattern to append <process-id>@<host-name> to the filename if you want separate log files for different CLI session
[33malgo-2    |[0m appender.DRFA.filePattern = ${sys:hive.log.dir}/${sys:hive.log.file}.%d{yyyy-MM-dd}
[33malgo-2    |[0m appender.DRFA.layout.type = PatternLayout
[33malgo-2    |[0m appender.DRFA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[33malgo-2    |[0m appender.DRFA.policies.type = Policies
[33malgo-2    |[0m appender.DRFA.policies.time.type = TimeBasedTriggeringPolicy
[33malgo-2    |[0m appender.DRFA.policies.time.interval = 1
[33malgo-2    |[0m appender.DRFA.policies.time.modulate = true
[33malgo-2    |[0m appender.DRFA.strategy.type = DefaultRolloverStrategy
[33malgo-2    |[0m appender.DRFA.strategy.max = 30
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of all loggers
[33malgo-2    |[0m loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, PerfLogger, AmazonAws, ApacheHttp
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
[33malgo-2    |[0m logger.NIOServerCnxn.level = WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
[33malgo-2    |[0m logger.ClientCnxnSocketNIO.level = WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.DataNucleus.name = DataNucleus
[33malgo-2    |[0m logger.DataNucleus.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.Datastore.name = Datastore
[33malgo-2    |[0m logger.Datastore.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.JPOX.name = JPOX
[33malgo-2    |[0m logger.JPOX.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.AmazonAws.name=com.amazonaws
[33malgo-2    |[0m logger.AmazonAws.level = INFO
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.ApacheHttp.name=org.apache.http
[33malgo-2    |[0m logger.ApacheHttp.level = INFO
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.PerfLogger.name = org.apache.hadoop.hive.ql.log.PerfLogger
[33malgo-2    |[0m logger.PerfLogger.level = ${sys:hive.perflogger.log.level}
[33malgo-2    |[0m 
[33malgo-2    |[0m # root logger
[33malgo-2    |[0m rootLogger.level = ${sys:hive.log.level}
[33malgo-2    |[0m rootLogger.appenderRefs = root
[33malgo-2    |[0m rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/hive/conf/hive-exec-log4j2.properties
[33malgo-2    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/hive/conf/hive-exec-log4j2.properties is: 
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[33malgo-2    |[0m # or more contributor license agreements.  See the NOTICE file
[33malgo-2    |[0m # distributed with this work for additional information
[33malgo-2    |[0m # regarding copyright ownership.  The ASF licenses this file
[33malgo-2    |[0m # to you under the Apache License, Version 2.0 (the
[33malgo-2    |[0m # "License"); you may not use this file except in compliance
[33malgo-2    |[0m # with the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m status = INFO
[33malgo-2    |[0m name = HiveExecLog4j2
[33malgo-2    |[0m packages = org.apache.hadoop.hive.ql.log
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of properties
[33malgo-2    |[0m property.hive.log.level = INFO
[33malgo-2    |[0m property.hive.root.logger = FA
[33malgo-2    |[0m property.hive.query.id = hadoop
[33malgo-2    |[0m property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
[33malgo-2    |[0m property.hive.log.file = ${sys:hive.query.id}.log
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of all appenders
[33malgo-2    |[0m appenders = console, FA
[33malgo-2    |[0m 
[33malgo-2    |[0m # console appender
[33malgo-2    |[0m appender.console.type = Console
[33malgo-2    |[0m appender.console.name = console
[33malgo-2    |[0m appender.console.target = SYSTEM_ERR
[33malgo-2    |[0m appender.console.layout.type = PatternLayout
[33malgo-2    |[0m appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m # simple file appender
[33malgo-2    |[0m appender.FA.type = RandomAccessFile
[33malgo-2    |[0m appender.FA.name = FA
[33malgo-2    |[0m appender.FA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
[33malgo-2    |[0m appender.FA.layout.type = PatternLayout
[33malgo-2    |[0m appender.FA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of all loggers
[33malgo-2    |[0m loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
[33malgo-2    |[0m logger.NIOServerCnxn.level = WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
[33malgo-2    |[0m logger.ClientCnxnSocketNIO.level = WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.DataNucleus.name = DataNucleus
[33malgo-2    |[0m logger.DataNucleus.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.Datastore.name = Datastore
[33malgo-2    |[0m logger.Datastore.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.JPOX.name = JPOX
[33malgo-2    |[0m logger.JPOX.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m # root logger
[33malgo-2    |[0m rootLogger.level = ${sys:hive.log.level}
[33malgo-2    |[0m rootLogger.appenderRefs = root
[33malgo-2    |[0m rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/hive/conf/hive-site.xml
[33malgo-2    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/hive/conf/hive-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0"?>
[33malgo-2    |[0m <!--
[33malgo-2    |[0m   Licensed to the Apache Software Foundation (ASF) under one or more
[33malgo-2    |[0m   contributor license agreements.  See the NOTICE file distributed with
[33malgo-2    |[0m   this work for additional information regarding copyright ownership.
[33malgo-2    |[0m   The ASF licenses this file to You under the Apache License, Version 2.0
[33malgo-2    |[0m   (the "License"); you may not use this file except in compliance with
[33malgo-2    |[0m   the License.  You may obtain a copy of the License at
[33malgo-2    |[0m 
[33malgo-2    |[0m       http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m 
[33malgo-2    |[0m   Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m   distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m   See the License for the specific language governing permissions and
[33malgo-2    |[0m   limitations under the License.
[33malgo-2    |[0m -->
[33malgo-2    |[0m <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[33malgo-2    |[0m 
[33malgo-2    |[0m <configuration>
[33malgo-2    |[0m 
[33malgo-2    |[0m <!-- Hive Configuration can either be stored in this file or in the hadoop configuration files  -->
[33malgo-2    |[0m <!-- that are implied by Hadoop setup variables.                                                -->
[33malgo-2    |[0m <!-- Aside from Hadoop setup variables - this file is provided as a convenience so that Hive    -->
[33malgo-2    |[0m <!-- users do not have to edit hadoop configuration files (that may be managed as a centralized -->
[33malgo-2    |[0m <!-- resource).                                                                                 -->
[33malgo-2    |[0m 
[33malgo-2    |[0m <!-- Hive Execution Parameters -->
[33malgo-2    |[0m 
[33malgo-2    |[0m <property>
[33malgo-2    |[0m   <name>javax.jdo.option.ConnectionURL</name>
[33malgo-2    |[0m   <value>jdbc:derby:;databaseName=/var/lib/hive/metastore/metastore_db;create=true</value>
[33malgo-2    |[0m   <description>JDBC connect string for a JDBC metastore</description>
[33malgo-2    |[0m </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m <property>
[33malgo-2    |[0m   <name>javax.jdo.option.ConnectionDriverName</name>
[33malgo-2    |[0m   <value>org.apache.derby.jdbc.EmbeddedDriver</value>
[33malgo-2    |[0m   <description>Driver class name for a JDBC metastore</description>
[33malgo-2    |[0m </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>key</name>
[33malgo-2    |[0m     <value>value</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[33malgo-2    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.driver.host=192.168.240.3
[33malgo-2    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[33malgo-2    |[0m 
[33malgo-2    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[33malgo-2    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[33malgo-2    |[0m spark.rpc.askTimeout=300s
[33malgo-2    |[0m spark.driver.memory 2048m
[33malgo-2    |[0m spark.driver.memoryOverhead 204m
[33malgo-2    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[33malgo-2    |[0m spark.executor.memory 12399m
[33malgo-2    |[0m spark.executor.memoryOverhead 1239m
[33malgo-2    |[0m spark.executor.cores 4
[33malgo-2    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[33malgo-2    |[0m spark.executor.instances 2
[33malgo-2    |[0m spark.default.parallelism 16
[33malgo-2    |[0m spark.executor.memory 2g
[33malgo-2    |[0m spark.executor.cores 1
[33malgo-2    |[0m key value
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/spark/conf/spark-env.sh
[33malgo-2    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/spark/conf/spark-env.sh is: 
[33malgo-2    |[0m #EMPTY FILE AVOID OVERRIDDING ENV VARS
[33malgo-2    |[0m # Specifically, without copying the empty file, SPARK_HISTORY_OPTS will be overriden, 
[33malgo-2    |[0m # spark.history.ui.port defaults to 18082, and spark.eventLog.dir defaults to local fs
[33malgo-2    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[33malgo-2    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/spark/conf/log4j.properties
[33malgo-2    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/spark/conf/log4j.properties is: 
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/spark/conf/hive-site.xml
[33malgo-2    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/spark/conf/hive-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0"?>
[33malgo-2    |[0m <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[33malgo-2    |[0m <configuration>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>key</name>
[33malgo-2    |[0m     <value>value</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/spark/conf/metrics.properties
[33malgo-2    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/spark/conf/metrics.properties is: 
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[33malgo-2    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0"?>
[33malgo-2    |[0m <!-- Site specific YARN configuration properties -->
[33malgo-2    |[0m  <configuration>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.resourcemanager.hostname</name>
[33malgo-2    |[0m          <value>192.168.240.3</value>
[33malgo-2    |[0m          <description>The hostname of the RM.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.hostname</name>
[33malgo-2    |[0m          <value>algo-2</value>
[33malgo-2    |[0m          <description>The hostname of the NM.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.webapp.address</name>
[33malgo-2    |[0m          <value>algo-2:8042</value>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.vmem-pmem-ratio</name>
[33malgo-2    |[0m          <value>5</value>
[33malgo-2    |[0m          <description>Ratio between virtual memory to physical memory.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.resourcemanager.am.max-attempts</name>
[33malgo-2    |[0m          <value>1</value>
[33malgo-2    |[0m          <description>The maximum number of application attempts.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.env-whitelist</name>
[33malgo-2    |[0m          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>
[33malgo-2    |[0m          <description>Environment variable whitelist</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m  
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.minimum-allocation-mb</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.maximum-allocation-mb</name>
[33malgo-2    |[0m     <value>15892</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.minimum-allocation-vcores</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.maximum-allocation-vcores</name>
[33malgo-2    |[0m     <value>4</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.nodemanager.resource.memory-mb</name>
[33malgo-2    |[0m     <value>15892</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.nodemanager.resource.cpu-vcores</name>
[33malgo-2    |[0m     <value>4</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>key</name>
[33malgo-2    |[0m     <value>value</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/yarn-env.sh
[33malgo-2    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-env.sh is: 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one or more
[33malgo-2    |[0m # contributor license agreements.  See the NOTICE file distributed with
[33malgo-2    |[0m # this work for additional information regarding copyright ownership.
[33malgo-2    |[0m # The ASF licenses this file to You under the Apache License, Version 2.0
[33malgo-2    |[0m # (the "License"); you may not use this file except in compliance with
[33malgo-2    |[0m # the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## THIS FILE ACTS AS AN OVERRIDE FOR hadoop-env.sh FOR ALL
[33malgo-2    |[0m ## WORK DONE BY THE yarn AND RELATED COMMANDS.
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## Precedence rules:
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## yarn-env.sh > hadoop-env.sh > hard-coded defaults
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## YARN_xyz > HADOOP_xyz > hard-coded defaults
[33malgo-2    |[0m ##
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Resource Manager specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the max heapsize for the ResourceManager.  If no units are
[33malgo-2    |[0m # given, it will be assumed to be in MB.
[33malgo-2    |[0m # This value will be overridden by an Xmx setting specified in either
[33malgo-2    |[0m # HADOOP_OPTS and/or YARN_RESOURCEMANAGER_OPTS.
[33malgo-2    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX
[33malgo-2    |[0m #export YARN_RESOURCEMANAGER_HEAPSIZE=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the ResourceManager.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # Examples for a Sun/Oracle JDK:
[33malgo-2    |[0m # a) override the appsummary log file:
[33malgo-2    |[0m # export YARN_RESOURCEMANAGER_OPTS="-Dyarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log -Dyarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY"
[33malgo-2    |[0m #
[33malgo-2    |[0m # b) Set JMX options
[33malgo-2    |[0m # export YARN_RESOURCEMANAGER_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
[33malgo-2    |[0m #
[33malgo-2    |[0m # c) Set garbage collection logs from hadoop-env.sh
[33malgo-2    |[0m # export YARN_RESOURCE_MANAGER_OPTS="${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[33malgo-2    |[0m #
[33malgo-2    |[0m # d) ... or set them directly
[33malgo-2    |[0m # export YARN_RESOURCEMANAGER_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[33malgo-2    |[0m #
[33malgo-2    |[0m #
[33malgo-2    |[0m # export YARN_RESOURCEMANAGER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Node Manager specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the max heapsize for the NodeManager.  If no units are
[33malgo-2    |[0m # given, it will be assumed to be in MB.
[33malgo-2    |[0m # This value will be overridden by an Xmx setting specified in either
[33malgo-2    |[0m # HADOOP_OPTS and/or YARN_NODEMANAGER_OPTS.
[33malgo-2    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[33malgo-2    |[0m #export YARN_NODEMANAGER_HEAPSIZE=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the NodeManager.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_NODEMANAGER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # TimeLineServer specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the max heapsize for the timelineserver.  If no units are
[33malgo-2    |[0m # given, it will be assumed to be in MB.
[33malgo-2    |[0m # This value will be overridden by an Xmx setting specified in either
[33malgo-2    |[0m # HADOOP_OPTS and/or YARN_TIMELINESERVER_OPTS.
[33malgo-2    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[33malgo-2    |[0m #export YARN_TIMELINE_HEAPSIZE=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the TimeLineServer.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_TIMELINESERVER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # TimeLineReader specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the TimeLineReader.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_TIMELINEREADER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Web App Proxy Server specifc parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the max heapsize for the web app proxy server.  If no units are
[33malgo-2    |[0m # given, it will be assumed to be in MB.
[33malgo-2    |[0m # This value will be overridden by an Xmx setting specified in either
[33malgo-2    |[0m # HADOOP_OPTS and/or YARN_PROXYSERVER_OPTS.
[33malgo-2    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[33malgo-2    |[0m #export YARN_PROXYSERVER_HEAPSIZE=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the proxy server.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_PROXYSERVER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Shared Cache Manager specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the
[33malgo-2    |[0m # shared cache manager server.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_SHAREDCACHEMANAGER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Router specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the Router.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_ROUTER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Registry DNS specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # For privileged registry DNS, user to run as after dropping privileges
[33malgo-2    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[33malgo-2    |[0m # export YARN_REGISTRYDNS_SECURE_USER=yarn
[33malgo-2    |[0m 
[33malgo-2    |[0m # Supplemental options for privileged registry DNS
[33malgo-2    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[33malgo-2    |[0m # server jvm.
[33malgo-2    |[0m # export YARN_REGISTRYDNS_SECURE_EXTRA_OPTS="-jvm server"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # YARN Services parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Directory containing service examples
[33malgo-2    |[0m # export YARN_SERVICE_EXAMPLES_DIR = $HADOOP_YARN_HOME/share/hadoop/yarn/yarn-service-examples
[33malgo-2    |[0m # export YARN_CONTAINER_RUNTIME_DOCKER_RUN_OVERRIDE_DISABLE=true
[33malgo-2    |[0m export YARN_LOG_DIR=/var/log/yarn/export HADOOP_DATANODE_HEAPSIZE=2048
[33malgo-2    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:21 smspark-submit INFO     waiting for cluster to be up
[33malgo-2    |[0m WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.
[33malgo-2    |[0m WARNING: /usr/lib/hadoop/logs does not exist. Creating.
[33malgo-2    |[0m WARNING: /var/log/yarn/export does not exist. Creating.
[36malgo-1    |[0m 11-14 15:21 smspark-submit INFO     Found hadoop jar hadoop-aws.jar
[36malgo-1    |[0m 11-14 15:21 smspark-submit INFO     Optional jar jets3t-0.9.0.jar in /usr/lib/hadoop/lib does not exist
[36malgo-1    |[0m 11-14 15:21 smspark-submit INFO     copying cluster config
[36malgo-1    |[0m 11-14 15:21 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml
[36malgo-1    |[0m 11-14 15:21 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml
[36malgo-1    |[0m 11-14 15:21 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[36malgo-1    |[0m 11-14 15:21 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m 11-14 15:21 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh
[36malgo-1    |[0m 11-14 15:21 root         INFO     Detected instance type: m5.xlarge with total memory: 16384M and total cores: 4
[36malgo-1    |[0m 11-14 15:21 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[36malgo-1    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0"?>
[36malgo-1    |[0m <!-- Site specific YARN configuration properties -->
[36malgo-1    |[0m  <configuration>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.resourcemanager.hostname</name>
[36malgo-1    |[0m          <value>192.168.240.3</value>
[36malgo-1    |[0m          <description>The hostname of the RM.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.hostname</name>
[36malgo-1    |[0m          <value>algo-1</value>
[36malgo-1    |[0m          <description>The hostname of the NM.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.webapp.address</name>
[36malgo-1    |[0m          <value>algo-1:8042</value>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.vmem-pmem-ratio</name>
[36malgo-1    |[0m          <value>5</value>
[36malgo-1    |[0m          <description>Ratio between virtual memory to physical memory.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.resourcemanager.am.max-attempts</name>
[36malgo-1    |[0m          <value>1</value>
[36malgo-1    |[0m          <description>The maximum number of application attempts.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.env-whitelist</name>
[36malgo-1    |[0m          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>
[36malgo-1    |[0m          <description>Environment variable whitelist</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m  
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.minimum-allocation-mb</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.maximum-allocation-mb</name>
[36malgo-1    |[0m     <value>15892</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.minimum-allocation-vcores</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.maximum-allocation-vcores</name>
[36malgo-1    |[0m     <value>4</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.nodemanager.resource.memory-mb</name>
[36malgo-1    |[0m     <value>15892</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.nodemanager.resource.cpu-vcores</name>
[36malgo-1    |[0m     <value>4</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:21 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[36malgo-1    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.driver.host=192.168.240.3
[36malgo-1    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[36malgo-1    |[0m 
[36malgo-1    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[36malgo-1    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[36malgo-1    |[0m spark.rpc.askTimeout=300s
[36malgo-1    |[0m spark.driver.memory 2048m
[36malgo-1    |[0m spark.driver.memoryOverhead 204m
[36malgo-1    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[36malgo-1    |[0m spark.executor.memory 12399m
[36malgo-1    |[0m spark.executor.memoryOverhead 1239m
[36malgo-1    |[0m spark.executor.cores 4
[36malgo-1    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[36malgo-1    |[0m spark.executor.instances 2
[36malgo-1    |[0m spark.default.parallelism 16
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:21 root         INFO     Finished Yarn configuration files setup.
[36malgo-1    |[0m 11-14 15:21 root         INFO     reading user configuration from /opt/ml/processing/input/conf/configuration.json
[36malgo-1    |[0m 11-14 15:21 root         INFO     User configuration list or dict: [{'Classification': 'spark-defaults', 'Properties': {'spark.executor.memory': '2g', 'spark.executor.cores': '1'}}, {'Classification': 'hadoop-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'core-site', 'Properties': {'spark.executor.memory': '2g', 'spark.executor.cores': '1'}}, {'Classification': 'hadoop-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'hive-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-exec-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-site', 'Properties': {'key': 'value'}}, {'Classification': 'spark-defaults', 'Properties': {'key': 'value'}}, {'Classification': 'spark-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'spark-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'spark-hive-site', 'Properties': {'key': 'value'}}, {'Classification': 'spark-metrics', 'Properties': {'key': 'value'}}, {'Classification': 'yarn-site', 'Properties': {'key': 'value'}}, {'Classification': 'yarn-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}] , type <class 'list'>
[36malgo-1    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[36malgo-1    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.driver.host=192.168.240.3
[36malgo-1    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[36malgo-1    |[0m 
[36malgo-1    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[36malgo-1    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[36malgo-1    |[0m spark.rpc.askTimeout=300s
[36malgo-1    |[0m spark.driver.memory 2048m
[36malgo-1    |[0m spark.driver.memoryOverhead 204m
[36malgo-1    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[36malgo-1    |[0m spark.executor.memory 12399m
[36malgo-1    |[0m spark.executor.memoryOverhead 1239m
[36malgo-1    |[0m spark.executor.cores 4
[36malgo-1    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[36malgo-1    |[0m spark.executor.instances 2
[36malgo-1    |[0m spark.default.parallelism 16
[36malgo-1    |[0m spark.executor.memory 2g
[36malgo-1    |[0m spark.executor.cores 1
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/hadoop-env.sh
[36malgo-1    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/hadoop-env.sh is: 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[36malgo-1    |[0m # or more contributor license agreements.  See the NOTICE file
[36malgo-1    |[0m # distributed with this work for additional information
[36malgo-1    |[0m # regarding copyright ownership.  The ASF licenses this file
[36malgo-1    |[0m # to you under the Apache License, Version 2.0 (the
[36malgo-1    |[0m # "License"); you may not use this file except in compliance
[36malgo-1    |[0m # with the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m # Set Hadoop-specific environment variables here.
[36malgo-1    |[0m 
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## THIS FILE ACTS AS THE MASTER FILE FOR ALL HADOOP PROJECTS.
[36malgo-1    |[0m ## SETTINGS HERE WILL BE READ BY ALL HADOOP COMMANDS.  THEREFORE,
[36malgo-1    |[0m ## ONE CAN USE THIS FILE TO SET YARN, HDFS, AND MAPREDUCE
[36malgo-1    |[0m ## CONFIGURATION OPTIONS INSTEAD OF xxx-env.sh.
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## Precedence rules:
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## {yarn-env.sh|hdfs-env.sh} > hadoop-env.sh > hard-coded defaults
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## {YARN_xyz|HDFS_xyz} > HADOOP_xyz > hard-coded defaults
[36malgo-1    |[0m ##
[36malgo-1    |[0m 
[36malgo-1    |[0m # Many of the options here are built from the perspective that users
[36malgo-1    |[0m # may want to provide OVERWRITING values on the command line.
[36malgo-1    |[0m # For example:
[36malgo-1    |[0m #
[36malgo-1    |[0m #  JAVA_HOME=/usr/java/testing hdfs dfs -ls
[36malgo-1    |[0m #
[36malgo-1    |[0m # Therefore, the vast majority (BUT NOT ALL!) of these defaults
[36malgo-1    |[0m # are configured for substitution and not append.  If append
[36malgo-1    |[0m # is preferable, modify this file accordingly.
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Generic settings for HADOOP
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Technically, the only required environment variable is JAVA_HOME.
[36malgo-1    |[0m # All others are optional.  However, the defaults are probably not
[36malgo-1    |[0m # preferred.  Many sites configure these options outside of Hadoop,
[36malgo-1    |[0m # such as in /etc/profile.d
[36malgo-1    |[0m 
[36malgo-1    |[0m # The java implementation to use. By default, this environment
[36malgo-1    |[0m # variable is REQUIRED on ALL platforms except OS X!
[36malgo-1    |[0m # export JAVA_HOME=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Location of Hadoop.  By default, Hadoop will attempt to determine
[36malgo-1    |[0m # this location based upon its execution path.
[36malgo-1    |[0m # export HADOOP_HOME=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Location of Hadoop's configuration information.  i.e., where this
[36malgo-1    |[0m # file is living. If this is not defined, Hadoop will attempt to
[36malgo-1    |[0m # locate it based upon its execution path.
[36malgo-1    |[0m #
[36malgo-1    |[0m # NOTE: It is recommend that this variable not be set here but in
[36malgo-1    |[0m # /etc/profile.d or equivalent.  Some options (such as
[36malgo-1    |[0m # --config) may react strangely otherwise.
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
[36malgo-1    |[0m 
[36malgo-1    |[0m # The maximum amount of heap to use (Java -Xmx).  If no unit
[36malgo-1    |[0m # is provided, it will be converted to MB.  Daemons will
[36malgo-1    |[0m # prefer any Xmx setting in their respective _OPT variable.
[36malgo-1    |[0m # There is no default; the JVM will autoscale based upon machine
[36malgo-1    |[0m # memory size.
[36malgo-1    |[0m # export HADOOP_HEAPSIZE_MAX=
[36malgo-1    |[0m 
[36malgo-1    |[0m # The minimum amount of heap to use (Java -Xms).  If no unit
[36malgo-1    |[0m # is provided, it will be converted to MB.  Daemons will
[36malgo-1    |[0m # prefer any Xms setting in their respective _OPT variable.
[36malgo-1    |[0m # There is no default; the JVM will autoscale based upon machine
[36malgo-1    |[0m # memory size.
[36malgo-1    |[0m # export HADOOP_HEAPSIZE_MIN=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Enable extra debugging of Hadoop's JAAS binding, used to set up
[36malgo-1    |[0m # Kerberos security.
[36malgo-1    |[0m # export HADOOP_JAAS_DEBUG=true
[36malgo-1    |[0m 
[36malgo-1    |[0m # Extra Java runtime options for all Hadoop commands. We don't support
[36malgo-1    |[0m # IPv6 yet/still, so by default the preference is set to IPv4.
[36malgo-1    |[0m # export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true"
[36malgo-1    |[0m # For Kerberos debugging, an extended option set logs more information
[36malgo-1    |[0m # export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug"
[36malgo-1    |[0m 
[36malgo-1    |[0m # Some parts of the shell code may do special things dependent upon
[36malgo-1    |[0m # the operating system.  We have to set this here. See the next
[36malgo-1    |[0m # section as to why....
[36malgo-1    |[0m #export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}
[36malgo-1    |[0m 
[36malgo-1    |[0m # Extra Java runtime options for some Hadoop commands
[36malgo-1    |[0m # and clients (i.e., hdfs dfs -blah).  These get appended to HADOOP_OPTS for
[36malgo-1    |[0m # such commands.  In most cases, # this should be left empty and
[36malgo-1    |[0m # let users supply it on the command line.
[36malgo-1    |[0m # export HADOOP_CLIENT_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # A note about classpaths.
[36malgo-1    |[0m #
[36malgo-1    |[0m # By default, Apache Hadoop overrides Java's CLASSPATH
[36malgo-1    |[0m # environment variable.  It is configured such
[36malgo-1    |[0m # that it starts out blank with new entries added after passing
[36malgo-1    |[0m # a series of checks (file/dir exists, not already listed aka
[36malgo-1    |[0m # de-deduplication).  During de-deduplication, wildcards and/or
[36malgo-1    |[0m # directories are *NOT* expanded to keep it simple. Therefore,
[36malgo-1    |[0m # if the computed classpath has two specific mentions of
[36malgo-1    |[0m # awesome-methods-1.0.jar, only the first one added will be seen.
[36malgo-1    |[0m # If two directories are in the classpath that both contain
[36malgo-1    |[0m # awesome-methods-1.0.jar, then Java will pick up both versions.
[36malgo-1    |[0m 
[36malgo-1    |[0m # An additional, custom CLASSPATH. Site-wide configs should be
[36malgo-1    |[0m # handled via the shellprofile functionality, utilizing the
[36malgo-1    |[0m # hadoop_add_classpath function for greater control and much
[36malgo-1    |[0m # harder for apps/end-users to accidentally override.
[36malgo-1    |[0m # Similarly, end users should utilize ${HOME}/.hadooprc .
[36malgo-1    |[0m # This variable should ideally only be used as a short-cut,
[36malgo-1    |[0m # interactive way for temporary additions on the command line.
[36malgo-1    |[0m # export HADOOP_CLASSPATH="/some/cool/path/on/your/machine"
[36malgo-1    |[0m 
[36malgo-1    |[0m # Should HADOOP_CLASSPATH be first in the official CLASSPATH?
[36malgo-1    |[0m # export HADOOP_USER_CLASSPATH_FIRST="yes"
[36malgo-1    |[0m 
[36malgo-1    |[0m # If HADOOP_USE_CLIENT_CLASSLOADER is set, the classpath along
[36malgo-1    |[0m # with the main jar are handled by a separate isolated
[36malgo-1    |[0m # client classloader when 'hadoop jar', 'yarn jar', or 'mapred job'
[36malgo-1    |[0m # is utilized. If it is set, HADOOP_CLASSPATH and
[36malgo-1    |[0m # HADOOP_USER_CLASSPATH_FIRST are ignored.
[36malgo-1    |[0m # export HADOOP_USE_CLIENT_CLASSLOADER=true
[36malgo-1    |[0m 
[36malgo-1    |[0m # HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES overrides the default definition of
[36malgo-1    |[0m # system classes for the client classloader when HADOOP_USE_CLIENT_CLASSLOADER
[36malgo-1    |[0m # is enabled. Names ending in '.' (period) are treated as package names, and
[36malgo-1    |[0m # names starting with a '-' are treated as negative matches. For example,
[36malgo-1    |[0m # export HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES="-org.apache.hadoop.UserClass,java.,javax.,org.apache.hadoop."
[36malgo-1    |[0m 
[36malgo-1    |[0m # Enable optional, bundled Hadoop features
[36malgo-1    |[0m # This is a comma delimited list.  It may NOT be overridden via .hadooprc
[36malgo-1    |[0m # Entries may be added/removed as needed.
[36malgo-1    |[0m # export HADOOP_OPTIONAL_TOOLS="hadoop-azure,hadoop-azure-datalake,hadoop-aws,hadoop-openstack,hadoop-aliyun,hadoop-kafka"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Options for remote shell connectivity
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # There are some optional components of hadoop that allow for
[36malgo-1    |[0m # command and control of remote hosts.  For example,
[36malgo-1    |[0m # start-dfs.sh will attempt to bring up all NNs, DNS, etc.
[36malgo-1    |[0m 
[36malgo-1    |[0m # Options to pass to SSH when one of the "log into a host and
[36malgo-1    |[0m # start/stop daemons" scripts is executed
[36malgo-1    |[0m # export HADOOP_SSH_OPTS="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10s"
[36malgo-1    |[0m 
[36malgo-1    |[0m # The built-in ssh handler will limit itself to 10 simultaneous connections.
[36malgo-1    |[0m # For pdsh users, this sets the fanout size ( -f )
[36malgo-1    |[0m # Change this to increase/decrease as necessary.
[36malgo-1    |[0m # export HADOOP_SSH_PARALLEL=10
[36malgo-1    |[0m 
[36malgo-1    |[0m # Filename which contains all of the hosts for any remote execution
[36malgo-1    |[0m # helper scripts # such as workers.sh, start-dfs.sh, etc.
[36malgo-1    |[0m # export HADOOP_WORKERS="${HADOOP_CONF_DIR}/workers"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Options for all daemons
[36malgo-1    |[0m ###
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Many options may also be specified as Java properties.  It is
[36malgo-1    |[0m # very common, and in many cases, desirable, to hard-set these
[36malgo-1    |[0m # in daemon _OPTS variables.  Where applicable, the appropriate
[36malgo-1    |[0m # Java property is also identified.  Note that many are re-used
[36malgo-1    |[0m # or set differently in certain contexts (e.g., secure vs
[36malgo-1    |[0m # non-secure)
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m # Where (primarily) daemon log files are stored.
[36malgo-1    |[0m # ${HADOOP_HOME}/logs by default.
[36malgo-1    |[0m # Java property: hadoop.log.dir
[36malgo-1    |[0m # export HADOOP_LOG_DIR=${HADOOP_HOME}/logs
[36malgo-1    |[0m 
[36malgo-1    |[0m # A string representing this instance of hadoop. $USER by default.
[36malgo-1    |[0m # This is used in writing log and pid files, so keep that in mind!
[36malgo-1    |[0m # Java property: hadoop.id.str
[36malgo-1    |[0m # export HADOOP_IDENT_STRING=$USER
[36malgo-1    |[0m 
[36malgo-1    |[0m # How many seconds to pause after stopping a daemon
[36malgo-1    |[0m # export HADOOP_STOP_TIMEOUT=5
[36malgo-1    |[0m 
[36malgo-1    |[0m # Where pid files are stored.  /tmp by default.
[36malgo-1    |[0m # export HADOOP_PID_DIR=/tmp
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default log4j setting for interactive commands
[36malgo-1    |[0m # Java property: hadoop.root.logger
[36malgo-1    |[0m # export HADOOP_ROOT_LOGGER=INFO,console
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default log4j setting for daemons spawned explicitly by
[36malgo-1    |[0m # --daemon option of hadoop, hdfs, mapred and yarn command.
[36malgo-1    |[0m # Java property: hadoop.root.logger
[36malgo-1    |[0m # export HADOOP_DAEMON_ROOT_LOGGER=INFO,RFA
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default log level and output location for security-related messages.
[36malgo-1    |[0m # You will almost certainly want to change this on a per-daemon basis via
[36malgo-1    |[0m # the Java property (i.e., -Dhadoop.security.logger=foo). (Note that the
[36malgo-1    |[0m # defaults for the NN and 2NN override this by default.)
[36malgo-1    |[0m # Java property: hadoop.security.logger
[36malgo-1    |[0m # export HADOOP_SECURITY_LOGGER=INFO,NullAppender
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default process priority level
[36malgo-1    |[0m # Note that sub-processes will also run at this level!
[36malgo-1    |[0m # export HADOOP_NICENESS=0
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default name for the service level authorization file
[36malgo-1    |[0m # Java property: hadoop.policy.file
[36malgo-1    |[0m # export HADOOP_POLICYFILE="hadoop-policy.xml"
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # NOTE: this is not used by default!  <-----
[36malgo-1    |[0m # You can define variables right here and then re-use them later on.
[36malgo-1    |[0m # For example, it is common to use the same garbage collection settings
[36malgo-1    |[0m # for all the daemons.  So one could define:
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HADOOP_GC_SETTINGS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps"
[36malgo-1    |[0m #
[36malgo-1    |[0m # .. and then use it as per the b option under the namenode.
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Secure/privileged execution
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Out of the box, Hadoop uses jsvc from Apache Commons to launch daemons
[36malgo-1    |[0m # on privileged ports.  This functionality can be replaced by providing
[36malgo-1    |[0m # custom functions.  See hadoop-functions.sh for more information.
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m # The jsvc implementation to use. Jsvc is required to run secure datanodes
[36malgo-1    |[0m # that bind to privileged ports to provide authentication of data transfer
[36malgo-1    |[0m # protocol.  Jsvc is not required if SASL is configured for authentication of
[36malgo-1    |[0m # data transfer protocol using non-privileged ports.
[36malgo-1    |[0m # export JSVC_HOME=/usr/bin
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # This directory contains pids for secure and privileged processes.
[36malgo-1    |[0m #export HADOOP_SECURE_PID_DIR=${HADOOP_PID_DIR}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # This directory contains the logs for secure and privileged processes.
[36malgo-1    |[0m # Java property: hadoop.log.dir
[36malgo-1    |[0m # export HADOOP_SECURE_LOG=${HADOOP_LOG_DIR}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # When running a secure daemon, the default value of HADOOP_IDENT_STRING
[36malgo-1    |[0m # ends up being a bit bogus.  Therefore, by default, the code will
[36malgo-1    |[0m # replace HADOOP_IDENT_STRING with HADOOP_xx_SECURE_USER.  If one wants
[36malgo-1    |[0m # to keep HADOOP_IDENT_STRING untouched, then uncomment this line.
[36malgo-1    |[0m # export HADOOP_SECURE_IDENT_PRESERVE="true"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # NameNode specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default log level and output location for file system related change
[36malgo-1    |[0m # messages. For non-namenode daemons, the Java property must be set in
[36malgo-1    |[0m # the appropriate _OPTS if one wants something other than INFO,NullAppender
[36malgo-1    |[0m # Java property: hdfs.audit.logger
[36malgo-1    |[0m # export HDFS_AUDIT_LOGGER=INFO,NullAppender
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the NameNode.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # a) Set JMX options
[36malgo-1    |[0m # export HDFS_NAMENODE_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
[36malgo-1    |[0m #
[36malgo-1    |[0m # b) Set garbage collection logs
[36malgo-1    |[0m # export HDFS_NAMENODE_OPTS="${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[36malgo-1    |[0m #
[36malgo-1    |[0m # c) ... or set them directly
[36malgo-1    |[0m # export HDFS_NAMENODE_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[36malgo-1    |[0m 
[36malgo-1    |[0m # this is the default:
[36malgo-1    |[0m # export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # SecondaryNameNode specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the SecondaryNameNode.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # This is the default:
[36malgo-1    |[0m # export HDFS_SECONDARYNAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # DataNode specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the DataNode.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # This is the default:
[36malgo-1    |[0m # export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS"
[36malgo-1    |[0m 
[36malgo-1    |[0m # On secure datanodes, user to run the datanode as after dropping privileges.
[36malgo-1    |[0m # This **MUST** be uncommented to enable secure HDFS if using privileged ports
[36malgo-1    |[0m # to provide authentication of data transfer protocol.  This **MUST NOT** be
[36malgo-1    |[0m # defined if SASL is configured for authentication of data transfer protocol
[36malgo-1    |[0m # using non-privileged ports.
[36malgo-1    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[36malgo-1    |[0m # export HDFS_DATANODE_SECURE_USER=hdfs
[36malgo-1    |[0m 
[36malgo-1    |[0m # Supplemental options for secure datanodes
[36malgo-1    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[36malgo-1    |[0m # server jvm.
[36malgo-1    |[0m # export HDFS_DATANODE_SECURE_EXTRA_OPTS="-jvm server"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # NFS3 Gateway specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the NFS3 Gateway.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_NFS3_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the Hadoop portmapper.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_PORTMAP_OPTS="-Xmx512m"
[36malgo-1    |[0m 
[36malgo-1    |[0m # Supplemental options for priviliged gateways
[36malgo-1    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[36malgo-1    |[0m # server jvm.
[36malgo-1    |[0m # export HDFS_NFS3_SECURE_EXTRA_OPTS="-jvm server"
[36malgo-1    |[0m 
[36malgo-1    |[0m # On privileged gateways, user to run the gateway as after dropping privileges
[36malgo-1    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[36malgo-1    |[0m # export HDFS_NFS3_SECURE_USER=nfsserver
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # ZKFailoverController specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the ZKFailoverController.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_ZKFC_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # QuorumJournalNode specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the QuorumJournalNode.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_JOURNALNODE_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # HDFS Balancer specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the HDFS Balancer.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_BALANCER_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # HDFS Mover specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the HDFS Mover.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_MOVER_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Router-based HDFS Federation specific parameters
[36malgo-1    |[0m # Specify the JVM options to be used when starting the RBF Routers.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_DFSROUTER_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # HDFS StorageContainerManager specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the HDFS Storage Container Manager.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_STORAGECONTAINERMANAGER_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Advanced Users Only!
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # When building Hadoop, one can add the class paths to the commands
[36malgo-1    |[0m # via this special env var:
[36malgo-1    |[0m # export HADOOP_ENABLE_BUILD_PATHS="true"
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # To prevent accidents, shell commands be (superficially) locked
[36malgo-1    |[0m # to only allow certain users to execute certain subcommands.
[36malgo-1    |[0m # It uses the format of (command)_(subcommand)_USER.
[36malgo-1    |[0m #
[36malgo-1    |[0m # For example, to limit who can execute the namenode command,
[36malgo-1    |[0m # export HDFS_NAMENODE_USER=hdfs
[36malgo-1    |[0m export SPARK_MASTER_HOST=192.168.240.3
[36malgo-1    |[0m export AWS_CONTAINER_CREDENTIALS_RELATIVE_URI=
[36malgo-1    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[36malgo-1    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/core-site.xml
[36malgo-1    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/core-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0" encoding="UTF-8"?>
[36malgo-1    |[0m  <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[36malgo-1    |[0m  <!-- Put site-specific property overrides in this file. -->
[36malgo-1    |[0m 
[36malgo-1    |[0m  <configuration>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>fs.defaultFS</name>
[36malgo-1    |[0m          <value>hdfs://192.168.240.3/</value>
[36malgo-1    |[0m          <description>NameNode URI</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>fs.s3a.aws.credentials.provider</name>
[36malgo-1    |[0m          <value>com.amazonaws.auth.DefaultAWSCredentialsProviderChain</value>
[36malgo-1    |[0m          <description>AWS S3 credential provider</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>fs.s3.impl</name>
[36malgo-1    |[0m          <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
[36malgo-1    |[0m          <description>s3a filesystem implementation</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>fs.AbstractFileSystem.s3a.imp</name>
[36malgo-1    |[0m          <value>org.apache.hadoop.fs.s3a.S3A</value>
[36malgo-1    |[0m          <description>s3a filesystem implementation</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m  
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>spark.executor.memory</name>
[36malgo-1    |[0m     <value>2g</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>spark.executor.cores</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/log4j.properties
[36malgo-1    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/log4j.properties is: 
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[36malgo-1    |[0m # or more contributor license agreements.  See the NOTICE file
[36malgo-1    |[0m # distributed with this work for additional information
[36malgo-1    |[0m # regarding copyright ownership.  The ASF licenses this file
[36malgo-1    |[0m # to you under the Apache License, Version 2.0 (the
[36malgo-1    |[0m # "License"); you may not use this file except in compliance
[36malgo-1    |[0m # with the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m # Define some default values that can be overridden by system properties
[36malgo-1    |[0m hadoop.root.logger=INFO,console
[36malgo-1    |[0m hadoop.log.dir=.
[36malgo-1    |[0m hadoop.log.file=hadoop.log
[36malgo-1    |[0m 
[36malgo-1    |[0m # Define the root logger to the system property "hadoop.root.logger".
[36malgo-1    |[0m log4j.rootLogger=${hadoop.root.logger}, EventCounter
[36malgo-1    |[0m 
[36malgo-1    |[0m # Logging Threshold
[36malgo-1    |[0m log4j.threshold=ALL
[36malgo-1    |[0m 
[36malgo-1    |[0m # Null Appender
[36malgo-1    |[0m log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Rolling File Appender - cap space usage at 5gb.
[36malgo-1    |[0m #
[36malgo-1    |[0m hadoop.log.maxfilesize=256MB
[36malgo-1    |[0m hadoop.log.maxbackupindex=20
[36malgo-1    |[0m log4j.appender.RFA=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.RFA.MaxFileSize=${hadoop.log.maxfilesize}
[36malgo-1    |[0m log4j.appender.RFA.MaxBackupIndex=${hadoop.log.maxbackupindex}
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.RFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m 
[36malgo-1    |[0m # Pattern format: Date LogLevel LoggerName LogMessage
[36malgo-1    |[0m log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[36malgo-1    |[0m # Debugging Pattern format
[36malgo-1    |[0m #log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Daily Rolling File Appender
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.DRFA.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m 
[36malgo-1    |[0m # Rollver at midnight
[36malgo-1    |[0m #log4j.appender.DRFA.DatePattern=.yyyy-MM-dd
[36malgo-1    |[0m # Rollver at every hour
[36malgo-1    |[0m log4j.appender.DRFA.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m 
[36malgo-1    |[0m # Pattern format: Date LogLevel LoggerName LogMessage
[36malgo-1    |[0m log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m # Debugging Pattern format
[36malgo-1    |[0m #log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # console
[36malgo-1    |[0m # Add "console" to rootlogger above if you want to use this
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.console=org.apache.log4j.ConsoleAppender
[36malgo-1    |[0m log4j.appender.console.target=System.err
[36malgo-1    |[0m log4j.appender.console.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # TaskLog Appender
[36malgo-1    |[0m #
[36malgo-1    |[0m log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.TLA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # HDFS block state change log from block manager
[36malgo-1    |[0m #
[36malgo-1    |[0m # Uncomment the following to log normal block state change
[36malgo-1    |[0m # messages from BlockManager in NameNode.
[36malgo-1    |[0m #log4j.logger.BlockStateChange=DEBUG
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m #Security appender
[36malgo-1    |[0m #
[36malgo-1    |[0m hadoop.security.logger=INFO,NullAppender
[36malgo-1    |[0m hadoop.security.log.maxfilesize=256MB
[36malgo-1    |[0m hadoop.security.log.maxbackupindex=20
[36malgo-1    |[0m log4j.category.SecurityLogger=${hadoop.security.logger}
[36malgo-1    |[0m hadoop.security.log.file=SecurityAuth-${user.name}.audit
[36malgo-1    |[0m log4j.appender.RFAS=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.RFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
[36malgo-1    |[0m log4j.appender.RFAS.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[36malgo-1    |[0m log4j.appender.RFAS.MaxFileSize=${hadoop.security.log.maxfilesize}
[36malgo-1    |[0m log4j.appender.RFAS.MaxBackupIndex=${hadoop.security.log.maxbackupindex}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Daily Rolling Security appender
[36malgo-1    |[0m #
[36malgo-1    |[0m log4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.DRFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
[36malgo-1    |[0m log4j.appender.DRFAS.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.DRFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[36malgo-1    |[0m log4j.appender.DRFAS.DatePattern=.yyyy-MM-dd
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # hadoop configuration logging
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m # Uncomment the following line to turn off configuration deprecation warnings.
[36malgo-1    |[0m # log4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # hdfs audit logging
[36malgo-1    |[0m #
[36malgo-1    |[0m hdfs.audit.logger=INFO,NullAppender
[36malgo-1    |[0m hdfs.audit.log.maxfilesize=256MB
[36malgo-1    |[0m hdfs.audit.log.maxbackupindex=20
[36malgo-1    |[0m log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}
[36malgo-1    |[0m log4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false
[36malgo-1    |[0m log4j.appender.RFAAUDIT=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.RFAAUDIT.File=${hadoop.log.dir}/hdfs-audit.log
[36malgo-1    |[0m log4j.appender.RFAAUDIT.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.RFAAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[36malgo-1    |[0m log4j.appender.RFAAUDIT.MaxFileSize=${hdfs.audit.log.maxfilesize}
[36malgo-1    |[0m log4j.appender.RFAAUDIT.MaxBackupIndex=${hdfs.audit.log.maxbackupindex}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # NameNode metrics logging.
[36malgo-1    |[0m # The default is to retain two namenode-metrics.log files up to 64MB each.
[36malgo-1    |[0m #
[36malgo-1    |[0m namenode.metrics.logger=INFO,NullAppender
[36malgo-1    |[0m log4j.logger.NameNodeMetricsLog=${namenode.metrics.logger}
[36malgo-1    |[0m log4j.additivity.NameNodeMetricsLog=false
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.File=${hadoop.log.dir}/namenode-metrics.log
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.MaxBackupIndex=1
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.MaxFileSize=64MB
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # DataNode metrics logging.
[36malgo-1    |[0m # The default is to retain two datanode-metrics.log files up to 64MB each.
[36malgo-1    |[0m #
[36malgo-1    |[0m datanode.metrics.logger=INFO,NullAppender
[36malgo-1    |[0m log4j.logger.DataNodeMetricsLog=${datanode.metrics.logger}
[36malgo-1    |[0m log4j.additivity.DataNodeMetricsLog=false
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.File=${hadoop.log.dir}/datanode-metrics.log
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.MaxBackupIndex=1
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.MaxFileSize=64MB
[36malgo-1    |[0m 
[36malgo-1    |[0m # Custom Logging levels
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=DEBUG
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m # AWS SDK & S3A FileSystem
[36malgo-1    |[0m #log4j.logger.com.amazonaws=ERROR
[36malgo-1    |[0m log4j.logger.com.amazonaws.http.AmazonHttpClient=ERROR
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.fs.s3a.S3AFileSystem=WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Event Counter Appender
[36malgo-1    |[0m # Sends counts of logging messages at different severity levels to Hadoop Metrics.
[36malgo-1    |[0m #
[36malgo-1    |[0m log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Job Summary Appender
[36malgo-1    |[0m #
[36malgo-1    |[0m # Use following logger to send summary to separate file defined by
[36malgo-1    |[0m # hadoop.mapreduce.jobsummary.log.file :
[36malgo-1    |[0m # hadoop.mapreduce.jobsummary.logger=INFO,JSA
[36malgo-1    |[0m # 
[36malgo-1    |[0m hadoop.mapreduce.jobsummary.logger=${hadoop.root.logger}
[36malgo-1    |[0m hadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log
[36malgo-1    |[0m hadoop.mapreduce.jobsummary.log.maxfilesize=256MB
[36malgo-1    |[0m hadoop.mapreduce.jobsummary.log.maxbackupindex=20
[36malgo-1    |[0m log4j.appender.JSA=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.JSA.File=${hadoop.log.dir}/${hadoop.mapreduce.jobsummary.log.file}
[36malgo-1    |[0m log4j.appender.JSA.MaxFileSize=${hadoop.mapreduce.jobsummary.log.maxfilesize}
[36malgo-1    |[0m log4j.appender.JSA.MaxBackupIndex=${hadoop.mapreduce.jobsummary.log.maxbackupindex}
[36malgo-1    |[0m log4j.appender.JSA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.JSA.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
[36malgo-1    |[0m log4j.appender.JSA.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.logger.org.apache.hadoop.mapred.JobInProgress$JobSummary=${hadoop.mapreduce.jobsummary.logger}
[36malgo-1    |[0m log4j.additivity.org.apache.hadoop.mapred.JobInProgress$JobSummary=false
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # shuffle connection log from shuffleHandler
[36malgo-1    |[0m # Uncomment the following line to enable logging of shuffle connections
[36malgo-1    |[0m # log4j.logger.org.apache.hadoop.mapred.ShuffleHandler.audit=DEBUG
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Yarn ResourceManager Application Summary Log
[36malgo-1    |[0m #
[36malgo-1    |[0m # Set the ResourceManager summary log filename
[36malgo-1    |[0m yarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log
[36malgo-1    |[0m # Set the ResourceManager summary log level and appender
[36malgo-1    |[0m yarn.server.resourcemanager.appsummary.logger=${hadoop.root.logger}
[36malgo-1    |[0m #yarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY
[36malgo-1    |[0m 
[36malgo-1    |[0m # To enable AppSummaryLogging for the RM,
[36malgo-1    |[0m # set yarn.server.resourcemanager.appsummary.logger to
[36malgo-1    |[0m # <LEVEL>,RMSUMMARY in hadoop-env.sh
[36malgo-1    |[0m 
[36malgo-1    |[0m # Appender for ResourceManager Application Summary Log
[36malgo-1    |[0m # Requires the following properties to be set
[36malgo-1    |[0m #    - hadoop.log.dir (Hadoop Log directory)
[36malgo-1    |[0m #    - yarn.server.resourcemanager.appsummary.log.file (resource manager app summary log filename)
[36malgo-1    |[0m #    - yarn.server.resourcemanager.appsummary.logger (resource manager app summary log level and appender)
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=${yarn.server.resourcemanager.appsummary.logger}
[36malgo-1    |[0m log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=false
[36malgo-1    |[0m log4j.appender.RMSUMMARY=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.RMSUMMARY.File=${hadoop.log.dir}/${yarn.server.resourcemanager.appsummary.log.file}
[36malgo-1    |[0m log4j.appender.RMSUMMARY.MaxFileSize=256MB
[36malgo-1    |[0m log4j.appender.RMSUMMARY.MaxBackupIndex=20
[36malgo-1    |[0m log4j.appender.RMSUMMARY.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.RMSUMMARY.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m # HS audit log configs
[36malgo-1    |[0m #mapreduce.hs.audit.logger=INFO,HSAUDIT
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=${mapreduce.hs.audit.logger}
[36malgo-1    |[0m #log4j.additivity.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=false
[36malgo-1    |[0m #log4j.appender.HSAUDIT=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m #log4j.appender.HSAUDIT.File=${hadoop.log.dir}/hs-audit.log
[36malgo-1    |[0m #log4j.appender.HSAUDIT.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m #log4j.appender.HSAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[36malgo-1    |[0m #log4j.appender.HSAUDIT.DatePattern=.yyyy-MM-dd
[36malgo-1    |[0m 
[36malgo-1    |[0m # Http Server Request Logs
[36malgo-1    |[0m #log4j.logger.http.requests.namenode=INFO,namenoderequestlog
[36malgo-1    |[0m #log4j.appender.namenoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.namenoderequestlog.Filename=${hadoop.log.dir}/jetty-namenode-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.namenoderequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.http.requests.datanode=INFO,datanoderequestlog
[36malgo-1    |[0m #log4j.appender.datanoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.datanoderequestlog.Filename=${hadoop.log.dir}/jetty-datanode-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.datanoderequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.http.requests.resourcemanager=INFO,resourcemanagerrequestlog
[36malgo-1    |[0m #log4j.appender.resourcemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.resourcemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-resourcemanager-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.resourcemanagerrequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.http.requests.jobhistory=INFO,jobhistoryrequestlog
[36malgo-1    |[0m #log4j.appender.jobhistoryrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.jobhistoryrequestlog.Filename=${hadoop.log.dir}/jetty-jobhistory-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.jobhistoryrequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.http.requests.nodemanager=INFO,nodemanagerrequestlog
[36malgo-1    |[0m #log4j.appender.nodemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.nodemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-nodemanager-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.nodemanagerrequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m # WebHdfs request log on datanodes
[36malgo-1    |[0m # Specify -Ddatanode.webhdfs.logger=INFO,HTTPDRFA on datanode startup to
[36malgo-1    |[0m # direct the log to a separate file.
[36malgo-1    |[0m #datanode.webhdfs.logger=INFO,console
[36malgo-1    |[0m #log4j.logger.datanode.webhdfs=${datanode.webhdfs.logger}
[36malgo-1    |[0m #log4j.appender.HTTPDRFA=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m #log4j.appender.HTTPDRFA.File=${hadoop.log.dir}/hadoop-datanode-webhdfs.log
[36malgo-1    |[0m #log4j.appender.HTTPDRFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m #log4j.appender.HTTPDRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[36malgo-1    |[0m #log4j.appender.HTTPDRFA.DatePattern=.yyyy-MM-dd
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m # Appender for viewing information for errors and warnings
[36malgo-1    |[0m yarn.ewma.cleanupInterval=300
[36malgo-1    |[0m yarn.ewma.messageAgeLimitSeconds=86400
[36malgo-1    |[0m yarn.ewma.maxUniqueMessages=250
[36malgo-1    |[0m log4j.appender.EWMA=org.apache.hadoop.yarn.util.Log4jWarningErrorMetricsAppender
[36malgo-1    |[0m log4j.appender.EWMA.cleanupInterval=${yarn.ewma.cleanupInterval}
[36malgo-1    |[0m log4j.appender.EWMA.messageAgeLimitSeconds=${yarn.ewma.messageAgeLimitSeconds}
[36malgo-1    |[0m log4j.appender.EWMA.maxUniqueMessages=${yarn.ewma.maxUniqueMessages}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Fair scheduler state dump
[36malgo-1    |[0m #
[36malgo-1    |[0m # Use following logger to dump the state to a separate file
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=DEBUG,FSSTATEDUMP
[36malgo-1    |[0m #log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=false
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.File=${hadoop.log.dir}/fairscheduler-statedump.log
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.MaxFileSize=${hadoop.log.maxfilesize}
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.MaxBackupIndex=${hadoop.log.maxbackupindex}
[36malgo-1    |[0m 
[36malgo-1    |[0m # Log levels of third-party libraries
[36malgo-1    |[0m log4j.logger.org.apache.commons.beanutils=WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m #AWS SDK Logging
[36malgo-1    |[0m log4j.logger.com.amazonaws=WARN
[36malgo-1    |[0m log4j.logger.org.apache.zookeeper=ERROR
[36malgo-1    |[0m log4j.logger.org.apache.hadoop.hbase=WARN
[36malgo-1    |[0m log4j.logger.amazon.emr.metrics=WARN
[36malgo-1    |[0m log4j.logger.emr=INFO
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.HADOOP=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.HADOOP.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m log4j.appender.HADOOP.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.appender.HADOOP.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.HADOOP.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.JHS=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.JHS.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m log4j.appender.JHS.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.appender.JHS.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.JHS.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.MAPRED=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.MAPRED.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m log4j.appender.MAPRED.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.appender.MAPRED.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.MAPRED.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.YARN=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.YARN.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m log4j.appender.YARN.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.appender.YARN.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.YARN.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/hive/conf/hive-env.sh
[36malgo-1    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/hive/conf/hive-env.sh is: 
[36malgo-1    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[36malgo-1    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/hive/conf/hive-log4j2.properties
[36malgo-1    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/hive/conf/hive-log4j2.properties is: 
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[36malgo-1    |[0m # or more contributor license agreements.  See the NOTICE file
[36malgo-1    |[0m # distributed with this work for additional information
[36malgo-1    |[0m # regarding copyright ownership.  The ASF licenses this file
[36malgo-1    |[0m # to you under the Apache License, Version 2.0 (the
[36malgo-1    |[0m # "License"); you may not use this file except in compliance
[36malgo-1    |[0m # with the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m status = INFO
[36malgo-1    |[0m name = HiveLog4j2
[36malgo-1    |[0m packages = org.apache.hadoop.hive.ql.log
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of properties
[36malgo-1    |[0m property.hive.log.level = INFO
[36malgo-1    |[0m property.hive.root.logger = DRFA
[36malgo-1    |[0m property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
[36malgo-1    |[0m property.hive.log.file = hive.log
[36malgo-1    |[0m property.hive.perflogger.log.level = INFO
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of all appenders
[36malgo-1    |[0m appenders = console, DRFA
[36malgo-1    |[0m 
[36malgo-1    |[0m # console appender
[36malgo-1    |[0m appender.console.type = Console
[36malgo-1    |[0m appender.console.name = console
[36malgo-1    |[0m appender.console.target = SYSTEM_ERR
[36malgo-1    |[0m appender.console.layout.type = PatternLayout
[36malgo-1    |[0m appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m # daily rolling file appender
[36malgo-1    |[0m appender.DRFA.type = RollingRandomAccessFile
[36malgo-1    |[0m appender.DRFA.name = DRFA
[36malgo-1    |[0m appender.DRFA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
[36malgo-1    |[0m # Use %pid in the filePattern to append <process-id>@<host-name> to the filename if you want separate log files for different CLI session
[36malgo-1    |[0m appender.DRFA.filePattern = ${sys:hive.log.dir}/${sys:hive.log.file}.%d{yyyy-MM-dd}
[36malgo-1    |[0m appender.DRFA.layout.type = PatternLayout
[36malgo-1    |[0m appender.DRFA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[36malgo-1    |[0m appender.DRFA.policies.type = Policies
[36malgo-1    |[0m appender.DRFA.policies.time.type = TimeBasedTriggeringPolicy
[36malgo-1    |[0m appender.DRFA.policies.time.interval = 1
[36malgo-1    |[0m appender.DRFA.policies.time.modulate = true
[36malgo-1    |[0m appender.DRFA.strategy.type = DefaultRolloverStrategy
[36malgo-1    |[0m appender.DRFA.strategy.max = 30
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of all loggers
[36malgo-1    |[0m loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, PerfLogger, AmazonAws, ApacheHttp
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
[36malgo-1    |[0m logger.NIOServerCnxn.level = WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
[36malgo-1    |[0m logger.ClientCnxnSocketNIO.level = WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.DataNucleus.name = DataNucleus
[36malgo-1    |[0m logger.DataNucleus.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.Datastore.name = Datastore
[36malgo-1    |[0m logger.Datastore.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.JPOX.name = JPOX
[36malgo-1    |[0m logger.JPOX.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.AmazonAws.name=com.amazonaws
[36malgo-1    |[0m logger.AmazonAws.level = INFO
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.ApacheHttp.name=org.apache.http
[36malgo-1    |[0m logger.ApacheHttp.level = INFO
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.PerfLogger.name = org.apache.hadoop.hive.ql.log.PerfLogger
[36malgo-1    |[0m logger.PerfLogger.level = ${sys:hive.perflogger.log.level}
[36malgo-1    |[0m 
[36malgo-1    |[0m # root logger
[36malgo-1    |[0m rootLogger.level = ${sys:hive.log.level}
[36malgo-1    |[0m rootLogger.appenderRefs = root
[36malgo-1    |[0m rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/hive/conf/hive-exec-log4j2.properties
[36malgo-1    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/hive/conf/hive-exec-log4j2.properties is: 
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[36malgo-1    |[0m # or more contributor license agreements.  See the NOTICE file
[36malgo-1    |[0m # distributed with this work for additional information
[36malgo-1    |[0m # regarding copyright ownership.  The ASF licenses this file
[36malgo-1    |[0m # to you under the Apache License, Version 2.0 (the
[36malgo-1    |[0m # "License"); you may not use this file except in compliance
[36malgo-1    |[0m # with the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m status = INFO
[36malgo-1    |[0m name = HiveExecLog4j2
[36malgo-1    |[0m packages = org.apache.hadoop.hive.ql.log
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of properties
[36malgo-1    |[0m property.hive.log.level = INFO
[36malgo-1    |[0m property.hive.root.logger = FA
[36malgo-1    |[0m property.hive.query.id = hadoop
[36malgo-1    |[0m property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
[36malgo-1    |[0m property.hive.log.file = ${sys:hive.query.id}.log
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of all appenders
[36malgo-1    |[0m appenders = console, FA
[36malgo-1    |[0m 
[36malgo-1    |[0m # console appender
[36malgo-1    |[0m appender.console.type = Console
[36malgo-1    |[0m appender.console.name = console
[36malgo-1    |[0m appender.console.target = SYSTEM_ERR
[36malgo-1    |[0m appender.console.layout.type = PatternLayout
[36malgo-1    |[0m appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m # simple file appender
[36malgo-1    |[0m appender.FA.type = RandomAccessFile
[36malgo-1    |[0m appender.FA.name = FA
[36malgo-1    |[0m appender.FA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
[36malgo-1    |[0m appender.FA.layout.type = PatternLayout
[36malgo-1    |[0m appender.FA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of all loggers
[36malgo-1    |[0m loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
[36malgo-1    |[0m logger.NIOServerCnxn.level = WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
[36malgo-1    |[0m logger.ClientCnxnSocketNIO.level = WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.DataNucleus.name = DataNucleus
[36malgo-1    |[0m logger.DataNucleus.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.Datastore.name = Datastore
[36malgo-1    |[0m logger.Datastore.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.JPOX.name = JPOX
[36malgo-1    |[0m logger.JPOX.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m # root logger
[36malgo-1    |[0m rootLogger.level = ${sys:hive.log.level}
[36malgo-1    |[0m rootLogger.appenderRefs = root
[36malgo-1    |[0m rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/hive/conf/hive-site.xml
[36malgo-1    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/hive/conf/hive-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0"?>
[36malgo-1    |[0m <!--
[36malgo-1    |[0m   Licensed to the Apache Software Foundation (ASF) under one or more
[36malgo-1    |[0m   contributor license agreements.  See the NOTICE file distributed with
[36malgo-1    |[0m   this work for additional information regarding copyright ownership.
[36malgo-1    |[0m   The ASF licenses this file to You under the Apache License, Version 2.0
[36malgo-1    |[0m   (the "License"); you may not use this file except in compliance with
[36malgo-1    |[0m   the License.  You may obtain a copy of the License at
[36malgo-1    |[0m 
[36malgo-1    |[0m       http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m 
[36malgo-1    |[0m   Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m   distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m   See the License for the specific language governing permissions and
[36malgo-1    |[0m   limitations under the License.
[36malgo-1    |[0m -->
[36malgo-1    |[0m <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[36malgo-1    |[0m 
[36malgo-1    |[0m <configuration>
[36malgo-1    |[0m 
[36malgo-1    |[0m <!-- Hive Configuration can either be stored in this file or in the hadoop configuration files  -->
[36malgo-1    |[0m <!-- that are implied by Hadoop setup variables.                                                -->
[36malgo-1    |[0m <!-- Aside from Hadoop setup variables - this file is provided as a convenience so that Hive    -->
[36malgo-1    |[0m <!-- users do not have to edit hadoop configuration files (that may be managed as a centralized -->
[36malgo-1    |[0m <!-- resource).                                                                                 -->
[36malgo-1    |[0m 
[36malgo-1    |[0m <!-- Hive Execution Parameters -->
[36malgo-1    |[0m 
[36malgo-1    |[0m <property>
[36malgo-1    |[0m   <name>javax.jdo.option.ConnectionURL</name>
[36malgo-1    |[0m   <value>jdbc:derby:;databaseName=/var/lib/hive/metastore/metastore_db;create=true</value>
[36malgo-1    |[0m   <description>JDBC connect string for a JDBC metastore</description>
[36malgo-1    |[0m </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m <property>
[36malgo-1    |[0m   <name>javax.jdo.option.ConnectionDriverName</name>
[36malgo-1    |[0m   <value>org.apache.derby.jdbc.EmbeddedDriver</value>
[36malgo-1    |[0m   <description>Driver class name for a JDBC metastore</description>
[36malgo-1    |[0m </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>key</name>
[36malgo-1    |[0m     <value>value</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[36malgo-1    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.driver.host=192.168.240.3
[36malgo-1    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[36malgo-1    |[0m 
[36malgo-1    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[36malgo-1    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[36malgo-1    |[0m spark.rpc.askTimeout=300s
[36malgo-1    |[0m spark.driver.memory 2048m
[36malgo-1    |[0m spark.driver.memoryOverhead 204m
[36malgo-1    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[36malgo-1    |[0m spark.executor.memory 12399m
[36malgo-1    |[0m spark.executor.memoryOverhead 1239m
[36malgo-1    |[0m spark.executor.cores 4
[36malgo-1    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[36malgo-1    |[0m spark.executor.instances 2
[36malgo-1    |[0m spark.default.parallelism 16
[36malgo-1    |[0m spark.executor.memory 2g
[36malgo-1    |[0m spark.executor.cores 1
[36malgo-1    |[0m key value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/spark/conf/spark-env.sh
[36malgo-1    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/spark/conf/spark-env.sh is: 
[36malgo-1    |[0m #EMPTY FILE AVOID OVERRIDDING ENV VARS
[36malgo-1    |[0m # Specifically, without copying the empty file, SPARK_HISTORY_OPTS will be overriden, 
[36malgo-1    |[0m # spark.history.ui.port defaults to 18082, and spark.eventLog.dir defaults to local fs
[36malgo-1    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[36malgo-1    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/spark/conf/log4j.properties
[36malgo-1    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/spark/conf/log4j.properties is: 
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/spark/conf/hive-site.xml
[36malgo-1    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/spark/conf/hive-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0"?>
[36malgo-1    |[0m <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[36malgo-1    |[0m <configuration>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>key</name>
[36malgo-1    |[0m     <value>value</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/spark/conf/metrics.properties
[36malgo-1    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/spark/conf/metrics.properties is: 
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[36malgo-1    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0"?>
[36malgo-1    |[0m <!-- Site specific YARN configuration properties -->
[36malgo-1    |[0m  <configuration>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.resourcemanager.hostname</name>
[36malgo-1    |[0m          <value>192.168.240.3</value>
[36malgo-1    |[0m          <description>The hostname of the RM.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.hostname</name>
[36malgo-1    |[0m          <value>algo-1</value>
[36malgo-1    |[0m          <description>The hostname of the NM.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.webapp.address</name>
[36malgo-1    |[0m          <value>algo-1:8042</value>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.vmem-pmem-ratio</name>
[36malgo-1    |[0m          <value>5</value>
[36malgo-1    |[0m          <description>Ratio between virtual memory to physical memory.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.resourcemanager.am.max-attempts</name>
[36malgo-1    |[0m          <value>1</value>
[36malgo-1    |[0m          <description>The maximum number of application attempts.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.env-whitelist</name>
[36malgo-1    |[0m          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>
[36malgo-1    |[0m          <description>Environment variable whitelist</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m  
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.minimum-allocation-mb</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.maximum-allocation-mb</name>
[36malgo-1    |[0m     <value>15892</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.minimum-allocation-vcores</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.maximum-allocation-vcores</name>
[36malgo-1    |[0m     <value>4</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.nodemanager.resource.memory-mb</name>
[36malgo-1    |[0m     <value>15892</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.nodemanager.resource.cpu-vcores</name>
[36malgo-1    |[0m     <value>4</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>key</name>
[36malgo-1    |[0m     <value>value</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:21 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/yarn-env.sh
[36malgo-1    |[0m 11-14 15:21 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-env.sh is: 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one or more
[36malgo-1    |[0m # contributor license agreements.  See the NOTICE file distributed with
[36malgo-1    |[0m # this work for additional information regarding copyright ownership.
[36malgo-1    |[0m # The ASF licenses this file to You under the Apache License, Version 2.0
[36malgo-1    |[0m # (the "License"); you may not use this file except in compliance with
[36malgo-1    |[0m # the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## THIS FILE ACTS AS AN OVERRIDE FOR hadoop-env.sh FOR ALL
[36malgo-1    |[0m ## WORK DONE BY THE yarn AND RELATED COMMANDS.
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## Precedence rules:
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## yarn-env.sh > hadoop-env.sh > hard-coded defaults
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## YARN_xyz > HADOOP_xyz > hard-coded defaults
[36malgo-1    |[0m ##
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Resource Manager specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the max heapsize for the ResourceManager.  If no units are
[36malgo-1    |[0m # given, it will be assumed to be in MB.
[36malgo-1    |[0m # This value will be overridden by an Xmx setting specified in either
[36malgo-1    |[0m # HADOOP_OPTS and/or YARN_RESOURCEMANAGER_OPTS.
[36malgo-1    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX
[36malgo-1    |[0m #export YARN_RESOURCEMANAGER_HEAPSIZE=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the ResourceManager.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # Examples for a Sun/Oracle JDK:
[36malgo-1    |[0m # a) override the appsummary log file:
[36malgo-1    |[0m # export YARN_RESOURCEMANAGER_OPTS="-Dyarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log -Dyarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY"
[36malgo-1    |[0m #
[36malgo-1    |[0m # b) Set JMX options
[36malgo-1    |[0m # export YARN_RESOURCEMANAGER_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
[36malgo-1    |[0m #
[36malgo-1    |[0m # c) Set garbage collection logs from hadoop-env.sh
[36malgo-1    |[0m # export YARN_RESOURCE_MANAGER_OPTS="${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[36malgo-1    |[0m #
[36malgo-1    |[0m # d) ... or set them directly
[36malgo-1    |[0m # export YARN_RESOURCEMANAGER_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[36malgo-1    |[0m #
[36malgo-1    |[0m #
[36malgo-1    |[0m # export YARN_RESOURCEMANAGER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Node Manager specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the max heapsize for the NodeManager.  If no units are
[36malgo-1    |[0m # given, it will be assumed to be in MB.
[36malgo-1    |[0m # This value will be overridden by an Xmx setting specified in either
[36malgo-1    |[0m # HADOOP_OPTS and/or YARN_NODEMANAGER_OPTS.
[36malgo-1    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[36malgo-1    |[0m #export YARN_NODEMANAGER_HEAPSIZE=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the NodeManager.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_NODEMANAGER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # TimeLineServer specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the max heapsize for the timelineserver.  If no units are
[36malgo-1    |[0m # given, it will be assumed to be in MB.
[36malgo-1    |[0m # This value will be overridden by an Xmx setting specified in either
[36malgo-1    |[0m # HADOOP_OPTS and/or YARN_TIMELINESERVER_OPTS.
[36malgo-1    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[36malgo-1    |[0m #export YARN_TIMELINE_HEAPSIZE=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the TimeLineServer.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_TIMELINESERVER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # TimeLineReader specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the TimeLineReader.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_TIMELINEREADER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Web App Proxy Server specifc parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the max heapsize for the web app proxy server.  If no units are
[36malgo-1    |[0m # given, it will be assumed to be in MB.
[36malgo-1    |[0m # This value will be overridden by an Xmx setting specified in either
[36malgo-1    |[0m # HADOOP_OPTS and/or YARN_PROXYSERVER_OPTS.
[36malgo-1    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[36malgo-1    |[0m #export YARN_PROXYSERVER_HEAPSIZE=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the proxy server.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_PROXYSERVER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Shared Cache Manager specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the
[36malgo-1    |[0m # shared cache manager server.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_SHAREDCACHEMANAGER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Router specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the Router.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_ROUTER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Registry DNS specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # For privileged registry DNS, user to run as after dropping privileges
[36malgo-1    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[36malgo-1    |[0m # export YARN_REGISTRYDNS_SECURE_USER=yarn
[36malgo-1    |[0m 
[36malgo-1    |[0m # Supplemental options for privileged registry DNS
[36malgo-1    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[36malgo-1    |[0m # server jvm.
[36malgo-1    |[0m # export YARN_REGISTRYDNS_SECURE_EXTRA_OPTS="-jvm server"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # YARN Services parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Directory containing service examples
[36malgo-1    |[0m # export YARN_SERVICE_EXAMPLES_DIR = $HADOOP_YARN_HOME/share/hadoop/yarn/yarn-service-examples
[36malgo-1    |[0m # export YARN_CONTAINER_RUNTIME_DOCKER_RUN_OVERRIDE_DISABLE=true
[36malgo-1    |[0m export YARN_LOG_DIR=/var/log/yarn/export HADOOP_DATANODE_HEAPSIZE=2048
[36malgo-1    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[36malgo-1    |[0m 
[36malgo-1    |[0m WARNING: HADOOP_NAMENODE_OPTS has been replaced by HDFS_NAMENODE_OPTS. Using value of HADOOP_NAMENODE_OPTS.
[36malgo-1    |[0m WARNING: /usr/lib/hadoop/logs does not exist. Creating.
[33malgo-2    |[0m 2021-11-14 15:21:26,562 INFO nodemanager.NodeManager: STARTUP_MSG: 
[33malgo-2    |[0m /************************************************************
[33malgo-2    |[0m STARTUP_MSG: Starting NodeManager
[33malgo-2    |[0m STARTUP_MSG:   host = algo-2/192.168.240.2
[33malgo-2    |[0m STARTUP_MSG:   args = []
[33malgo-2    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[33malgo-2    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar
[33malgo-2    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[33malgo-2    |[0m STARTUP_MSG:   java = 1.8.0_302
[33malgo-2    |[0m ************************************************************/
[33malgo-2    |[0m 2021-11-14 15:21:26,573 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]
[33malgo-2    |[0m 2021-11-14 15:21:26,610 INFO datanode.DataNode: STARTUP_MSG: 
[33malgo-2    |[0m /************************************************************
[33malgo-2    |[0m STARTUP_MSG: Starting DataNode
[33malgo-2    |[0m STARTUP_MSG:   host = algo-2/192.168.240.2
[33malgo-2    |[0m STARTUP_MSG:   args = []
[33malgo-2    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[33malgo-2    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar
[33malgo-2    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[33malgo-2    |[0m STARTUP_MSG:   java = 1.8.0_302
[33malgo-2    |[0m ************************************************************/
[33malgo-2    |[0m 2021-11-14 15:21:26,621 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 15:21:26,834 INFO namenode.NameNode: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting NameNode
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/192.168.240.3
[36malgo-1    |[0m STARTUP_MSG:   args = [-format, -force]
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 15:21:26,845 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 15:21:26,912 INFO namenode.NameNode: createNameNode [-format, -force]
[33malgo-2    |[0m 2021-11-14 15:21:26,943 INFO resourceplugin.ResourcePluginManager: No Resource plugins found from configuration!
[33malgo-2    |[0m 2021-11-14 15:21:26,943 INFO resourceplugin.ResourcePluginManager: Found Resource plugins from configuration: null
[33malgo-2    |[0m 2021-11-14 15:21:26,967 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode
[33malgo-2    |[0m 2021-11-14 15:21:27,027 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.
[33malgo-2    |[0m 2021-11-14 15:21:27,062 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[33malgo-2    |[0m 2021-11-14 15:21:27,071 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher
[33malgo-2    |[0m 2021-11-14 15:21:27,072 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher
[33malgo-2    |[0m 2021-11-14 15:21:27,073 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper
[33malgo-2    |[0m 2021-11-14 15:21:27,074 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices
[33malgo-2    |[0m 2021-11-14 15:21:27,074 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl
[33malgo-2    |[0m 2021-11-14 15:21:27,075 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher
[33malgo-2    |[0m 2021-11-14 15:21:27,075 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler
[33malgo-2    |[0m 2021-11-14 15:21:27,077 INFO tracker.NMLogAggregationStatusTracker: the rolling interval seconds for the NodeManager Cached Log aggregation status is 600
[33malgo-2    |[0m 2021-11-14 15:21:27,095 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl
[33malgo-2    |[0m 2021-11-14 15:21:27,096 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager
[33malgo-2    |[0m 2021-11-14 15:21:27,138 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[33malgo-2    |[0m 2021-11-14 15:21:27,138 INFO impl.MetricsSystemImpl: DataNode metrics system started
[33malgo-2    |[0m 2021-11-14 15:21:27,161 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[33malgo-2    |[0m 2021-11-14 15:21:27,240 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[33malgo-2    |[0m 2021-11-14 15:21:27,240 INFO impl.MetricsSystemImpl: NodeManager metrics system started
[33malgo-2    |[0m 2021-11-14 15:21:27,260 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.
[33malgo-2    |[0m 2021-11-14 15:21:27,269 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.
[36malgo-1    |[0m Formatting using clusterid: CID-f7f17664-f4ce-4750-ac10-52bdfb51d834
[33malgo-2    |[0m 2021-11-14 15:21:27,301 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@4c60d6e9
[33malgo-2    |[0m 2021-11-14 15:21:27,302 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[33malgo-2    |[0m 2021-11-14 15:21:27,303 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler
[33malgo-2    |[0m 2021-11-14 15:21:27,304 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService
[33malgo-2    |[0m 2021-11-14 15:21:27,304 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled
[33malgo-2    |[0m 2021-11-14 15:21:27,304 INFO localizer.ResourceLocalizationService: per directory file limit = 8192
[33malgo-2    |[0m 2021-11-14 15:21:27,305 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
[33malgo-2    |[0m 2021-11-14 15:21:27,309 INFO datanode.DataNode: Configured hostname is algo-2
[33malgo-2    |[0m 2021-11-14 15:21:27,310 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[33malgo-2    |[0m 2021-11-14 15:21:27,313 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0
[33malgo-2    |[0m 2021-11-14 15:21:27,321 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker
[33malgo-2    |[0m 2021-11-14 15:21:27,327 INFO resources.ResourceHandlerModule: Using traffic control bandwidth handler
[33malgo-2    |[0m 2021-11-14 15:21:27,329 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@221a3fa4
[33malgo-2    |[0m 2021-11-14 15:21:27,330 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null
[33malgo-2    |[0m 2021-11-14 15:21:27,331 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true
[33malgo-2    |[0m 2021-11-14 15:21:27,331 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true
[33malgo-2    |[0m 2021-11-14 15:21:27,331 INFO monitor.ContainersMonitorImpl: Elastic memory control enabled: false
[33malgo-2    |[0m 2021-11-14 15:21:27,331 INFO monitor.ContainersMonitorImpl: Strict memory control enabled: true
[33malgo-2    |[0m 2021-11-14 15:21:27,331 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true
[33malgo-2    |[0m 2021-11-14 15:21:27,332 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866
[33malgo-2    |[0m 2021-11-14 15:21:27,333 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.
[33malgo-2    |[0m 2021-11-14 15:21:27,334 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s
[33malgo-2    |[0m 2021-11-14 15:21:27,334 INFO datanode.DataNode: Number threads for balancing is 50
[36malgo-1    |[0m 2021-11-14 15:21:27,334 INFO namenode.FSEditLog: Edit logging is async:true
[36malgo-1    |[0m 2021-11-14 15:21:27,349 INFO namenode.FSNamesystem: KeyProvider: null
[36malgo-1    |[0m 2021-11-14 15:21:27,351 INFO namenode.FSNamesystem: fsLock is fair: true
[36malgo-1    |[0m 2021-11-14 15:21:27,351 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
[36malgo-1    |[0m 2021-11-14 15:21:27,356 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 15:21:27,356 INFO namenode.FSNamesystem: supergroup          = supergroup
[36malgo-1    |[0m 2021-11-14 15:21:27,357 INFO namenode.FSNamesystem: isPermissionEnabled = true
[36malgo-1    |[0m 2021-11-14 15:21:27,357 INFO namenode.FSNamesystem: HA Enabled: false
[33malgo-2    |[0m 2021-11-14 15:21:27,361 INFO conf.Configuration: resource-types.xml not found
[33malgo-2    |[0m 2021-11-14 15:21:27,361 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
[33malgo-2    |[0m 2021-11-14 15:21:27,366 INFO conf.Configuration: node-resources.xml not found
[33malgo-2    |[0m 2021-11-14 15:21:27,366 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.
[33malgo-2    |[0m 2021-11-14 15:21:27,368 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:15892, vCores:4>
[33malgo-2    |[0m 2021-11-14 15:21:27,371 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=15892 virtual-memory=79460 virtual-cores=4
[33malgo-2    |[0m 2021-11-14 15:21:27,373 INFO util.log: Logging initialized @1254ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m 2021-11-14 15:21:27,403 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[33malgo-2    |[0m 2021-11-14 15:21:27,408 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 15:21:27,416 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
[36malgo-1    |[0m 2021-11-14 15:21:27,416 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
[36malgo-1    |[0m 2021-11-14 15:21:27,421 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
[36malgo-1    |[0m 2021-11-14 15:21:27,421 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Nov 14 15:21:27
[36malgo-1    |[0m 2021-11-14 15:21:27,423 INFO util.GSet: Computing capacity for map BlocksMap
[36malgo-1    |[0m 2021-11-14 15:21:27,423 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 15:21:27,424 INFO util.GSet: 2.0% max memory 6.5 GB = 133.6 MB
[36malgo-1    |[0m 2021-11-14 15:21:27,424 INFO util.GSet: capacity      = 2^24 = 16777216 entries
[33malgo-2    |[0m 2021-11-14 15:21:27,424 INFO ipc.Server: Starting Socket Reader #1 for port 0
[36malgo-1    |[0m 2021-11-14 15:21:27,466 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
[36malgo-1    |[0m 2021-11-14 15:21:27,466 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
[36malgo-1    |[0m 2021-11-14 15:21:27,474 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
[36malgo-1    |[0m 2021-11-14 15:21:27,474 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
[36malgo-1    |[0m 2021-11-14 15:21:27,474 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
[36malgo-1    |[0m 2021-11-14 15:21:27,474 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
[36malgo-1    |[0m 2021-11-14 15:21:27,475 INFO blockmanagement.BlockManager: defaultReplication         = 3
[36malgo-1    |[0m 2021-11-14 15:21:27,475 INFO blockmanagement.BlockManager: maxReplication             = 512
[36malgo-1    |[0m 2021-11-14 15:21:27,475 INFO blockmanagement.BlockManager: minReplication             = 1
[36malgo-1    |[0m 2021-11-14 15:21:27,475 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
[36malgo-1    |[0m 2021-11-14 15:21:27,476 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
[36malgo-1    |[0m 2021-11-14 15:21:27,476 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
[36malgo-1    |[0m 2021-11-14 15:21:27,476 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
[36malgo-1    |[0m 2021-11-14 15:21:27,501 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
[36malgo-1    |[0m 2021-11-14 15:21:27,501 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 15:21:27,501 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 15:21:27,501 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
[33malgo-2    |[0m 2021-11-14 15:21:27,507 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[33malgo-2    |[0m 2021-11-14 15:21:27,513 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined
[36malgo-1    |[0m 2021-11-14 15:21:27,514 INFO util.GSet: Computing capacity for map INodeMap
[36malgo-1    |[0m 2021-11-14 15:21:27,514 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 15:21:27,514 INFO util.GSet: 1.0% max memory 6.5 GB = 66.8 MB
[36malgo-1    |[0m 2021-11-14 15:21:27,514 INFO util.GSet: capacity      = 2^23 = 8388608 entries
[33malgo-2    |[0m 2021-11-14 15:21:27,520 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[33malgo-2    |[0m 2021-11-14 15:21:27,522 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
[33malgo-2    |[0m 2021-11-14 15:21:27,522 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[33malgo-2    |[0m 2021-11-14 15:21:27,522 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 2021-11-14 15:21:27,532 INFO namenode.FSDirectory: ACLs enabled? false
[36malgo-1    |[0m 2021-11-14 15:21:27,532 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
[36malgo-1    |[0m 2021-11-14 15:21:27,532 INFO namenode.FSDirectory: XAttrs enabled? true
[36malgo-1    |[0m 2021-11-14 15:21:27,532 INFO namenode.NameNode: Caching file names occurring more than 10 times
[36malgo-1    |[0m 2021-11-14 15:21:27,537 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
[36malgo-1    |[0m 2021-11-14 15:21:27,539 INFO snapshot.SnapshotManager: SkipList is disabled
[36malgo-1    |[0m 2021-11-14 15:21:27,543 INFO util.GSet: Computing capacity for map cachedBlocks
[36malgo-1    |[0m 2021-11-14 15:21:27,543 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 15:21:27,543 INFO util.GSet: 0.25% max memory 6.5 GB = 16.7 MB
[36malgo-1    |[0m 2021-11-14 15:21:27,543 INFO util.GSet: capacity      = 2^21 = 2097152 entries
[33malgo-2    |[0m 2021-11-14 15:21:27,547 INFO http.HttpServer2: Jetty bound to port 34895
[33malgo-2    |[0m 2021-11-14 15:21:27,548 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[36malgo-1    |[0m 2021-11-14 15:21:27,551 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
[36malgo-1    |[0m 2021-11-14 15:21:27,551 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
[36malgo-1    |[0m 2021-11-14 15:21:27,551 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
[36malgo-1    |[0m 2021-11-14 15:21:27,554 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
[36malgo-1    |[0m 2021-11-14 15:21:27,555 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
[36malgo-1    |[0m 2021-11-14 15:21:27,556 INFO util.GSet: Computing capacity for map NameNodeRetryCache
[36malgo-1    |[0m 2021-11-14 15:21:27,556 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 15:21:27,557 INFO util.GSet: 0.029999999329447746% max memory 6.5 GB = 2.0 MB
[36malgo-1    |[0m 2021-11-14 15:21:27,557 INFO util.GSet: capacity      = 2^18 = 262144 entries
[33malgo-2    |[0m 2021-11-14 15:21:27,572 INFO server.session: DefaultSessionIdManager workerName=node0
[33malgo-2    |[0m 2021-11-14 15:21:27,572 INFO server.session: No SessionScavenger set, using defaults
[33malgo-2    |[0m 2021-11-14 15:21:27,573 INFO server.session: node0 Scavenging every 660000ms
[33malgo-2    |[0m 2021-11-14 15:21:27,582 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a3793c7{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}
[33malgo-2    |[0m 2021-11-14 15:21:27,583 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@bb9e6dc{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 15:21:27,585 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1677061777-192.168.240.3-1636903287577
[36malgo-1    |[0m 2021-11-14 15:21:27,605 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.
[33malgo-2    |[0m 2021-11-14 15:21:27,609 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server
[33malgo-2    |[0m 2021-11-14 15:21:27,610 INFO ipc.Server: IPC Server Responder: starting
[33malgo-2    |[0m 2021-11-14 15:21:27,610 INFO ipc.Server: IPC Server listener on 0: starting
[33malgo-2    |[0m 2021-11-14 15:21:27,616 INFO security.NMContainerTokenSecretManager: Updating node address : algo-2:40975
[33malgo-2    |[0m 2021-11-14 15:21:27,621 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 500, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[33malgo-2    |[0m 2021-11-14 15:21:27,622 INFO ipc.Server: Starting Socket Reader #1 for port 8040
[33malgo-2    |[0m 2021-11-14 15:21:27,624 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server
[33malgo-2    |[0m 2021-11-14 15:21:27,625 INFO ipc.Server: IPC Server Responder: starting
[33malgo-2    |[0m 2021-11-14 15:21:27,625 INFO ipc.Server: IPC Server listener on 8040: starting
[33malgo-2    |[0m 2021-11-14 15:21:27,625 INFO localizer.ResourceLocalizationService: Localizer started on port 8040
[33malgo-2    |[0m 2021-11-14 15:21:27,628 INFO containermanager.ContainerManagerImpl: ContainerManager started at /192.168.240.2:40975
[33malgo-2    |[0m 2021-11-14 15:21:27,628 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-2/192.168.240.2:0
[33malgo-2    |[0m 2021-11-14 15:21:27,628 WARN tracker.NMLogAggregationStatusTracker: Log Aggregation is disabled.So is the LogAggregationStatusTracker.
[36malgo-1    |[0m 2021-11-14 15:21:27,631 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression
[33malgo-2    |[0m 2021-11-14 15:21:27,632 INFO webapp.WebServer: Instantiating NMWebApp at algo-2:8042
[33malgo-2    |[0m 2021-11-14 15:21:27,639 INFO util.TypeUtil: JVM Runtime does not support Modules
[33malgo-2    |[0m 2021-11-14 15:21:27,648 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@338c99c8{datanode,/,file:///usr/lib/hadoop-hdfs/webapps/datanode/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/datanode}
[33malgo-2    |[0m 2021-11-14 15:21:27,654 INFO util.log: Logging initialized @1535ms to org.eclipse.jetty.util.log.Slf4jLog
[33malgo-2    |[0m 2021-11-14 15:21:27,655 INFO server.AbstractConnector: Started ServerConnector@6c372fe6{HTTP/1.1,[http/1.1]}{localhost:34895}
[33malgo-2    |[0m 2021-11-14 15:21:27,655 INFO server.Server: Started @1536ms
[36malgo-1    |[0m 2021-11-14 15:21:27,719 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 396 bytes saved in 0 seconds .
[36malgo-1    |[0m 2021-11-14 15:21:27,729 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
[36malgo-1    |[0m 2021-11-14 15:21:27,735 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
[36malgo-1    |[0m 2021-11-14 15:21:27,735 INFO namenode.NameNode: SHUTDOWN_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m SHUTDOWN_MSG: Shutting down NameNode at algo-1/192.168.240.3
[36malgo-1    |[0m ************************************************************/
[33malgo-2    |[0m 2021-11-14 15:21:27,739 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[33malgo-2    |[0m 2021-11-14 15:21:27,742 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined
[33malgo-2    |[0m 2021-11-14 15:21:27,747 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[33malgo-2    |[0m 2021-11-14 15:21:27,748 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node
[33malgo-2    |[0m 2021-11-14 15:21:27,748 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[33malgo-2    |[0m 2021-11-14 15:21:27,748 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[33malgo-2    |[0m 2021-11-14 15:21:27,749 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context node
[33malgo-2    |[0m 2021-11-14 15:21:27,749 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
[33malgo-2    |[0m 2021-11-14 15:21:27,749 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
[33malgo-2    |[0m 2021-11-14 15:21:27,750 INFO http.HttpServer2: adding path spec: /node/*
[33malgo-2    |[0m 2021-11-14 15:21:27,751 INFO http.HttpServer2: adding path spec: /ws/*
[36malgo-1    |[0m 11-14 15:21 smspark-submit INFO     waiting for cluster to be up
[33malgo-2    |[0m 2021-11-14 15:21:27,778 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864
[33malgo-2    |[0m 2021-11-14 15:21:27,783 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[33malgo-2    |[0m 2021-11-14 15:21:27,784 INFO datanode.DataNode: dnUserName = root
[33malgo-2    |[0m 2021-11-14 15:21:27,784 INFO datanode.DataNode: supergroup = supergroup
[33malgo-2    |[0m 2021-11-14 15:21:27,817 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[33malgo-2    |[0m 2021-11-14 15:21:27,828 INFO ipc.Server: Starting Socket Reader #1 for port 9867
[36malgo-1    |[0m WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.
[36malgo-1    |[0m WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.
[36malgo-1    |[0m WARNING: HADOOP_NAMENODE_OPTS has been replaced by HDFS_NAMENODE_OPTS. Using value of HADOOP_NAMENODE_OPTS.
[36malgo-1    |[0m WARNING: /var/log/yarn/export does not exist. Creating.
[33malgo-2    |[0m 2021-11-14 15:21:28,031 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:9867
[33malgo-2    |[0m 2021-11-14 15:21:28,051 INFO datanode.DataNode: Refresh request received for nameservices: null
[33malgo-2    |[0m 2021-11-14 15:21:28,054 INFO webapp.WebApps: Registered webapp guice modules
[33malgo-2    |[0m 2021-11-14 15:21:28,057 INFO http.HttpServer2: Jetty bound to port 8042
[33malgo-2    |[0m 2021-11-14 15:21:28,059 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[33malgo-2    |[0m 2021-11-14 15:21:28,062 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>
[33malgo-2    |[0m 2021-11-14 15:21:28,072 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1.spark-network/192.168.240.3:8020 starting to offer service
[33malgo-2    |[0m 2021-11-14 15:21:28,078 INFO ipc.Server: IPC Server Responder: starting
[33malgo-2    |[0m 2021-11-14 15:21:28,078 INFO ipc.Server: IPC Server listener on 9867: starting
[33malgo-2    |[0m 2021-11-14 15:21:28,088 INFO server.session: DefaultSessionIdManager workerName=node0
[33malgo-2    |[0m 2021-11-14 15:21:28,088 INFO server.session: No SessionScavenger set, using defaults
[33malgo-2    |[0m 2021-11-14 15:21:28,089 INFO server.session: node0 Scavenging every 600000ms
[33malgo-2    |[0m 2021-11-14 15:21:28,100 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[33malgo-2    |[0m 2021-11-14 15:21:28,103 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4bff64c2{logs,/logs,file:///var/log/yarn/export/,AVAILABLE}
[33malgo-2    |[0m 2021-11-14 15:21:28,103 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3cc41abc{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/static,AVAILABLE}
[33malgo-2    |[0m 2021-11-14 15:21:28,113 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException
[33malgo-2    |[0m 2021-11-14 15:21:28,239 INFO util.TypeUtil: JVM Runtime does not support Modules
[33malgo-2    |[0m Nov 14, 2021 3:21:28 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[33malgo-2    |[0m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class
[33malgo-2    |[0m Nov 14, 2021 3:21:28 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[33malgo-2    |[0m INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
[33malgo-2    |[0m Nov 14, 2021 3:21:28 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[33malgo-2    |[0m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class
[33malgo-2    |[0m Nov 14, 2021 3:21:28 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
[33malgo-2    |[0m INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'
[36malgo-1    |[0m 2021-11-14 15:21:28,413 INFO nodemanager.NodeManager: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting NodeManager
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/192.168.240.3
[36malgo-1    |[0m STARTUP_MSG:   args = []
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 15:21:28,423 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]
[33malgo-2    |[0m Nov 14, 2021 3:21:28 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[33malgo-2    |[0m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 15:21:28,461 INFO resourcemanager.ResourceManager: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting ResourceManager
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/192.168.240.3
[36malgo-1    |[0m STARTUP_MSG:   args = []
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 15:21:28,466 INFO namenode.NameNode: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting NameNode
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/192.168.240.3
[36malgo-1    |[0m STARTUP_MSG:   args = []
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 15:21:28,476 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 15:21:28,481 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 15:21:28,521 INFO datanode.DataNode: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting DataNode
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/192.168.240.3
[36malgo-1    |[0m STARTUP_MSG:   args = []
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 15:21:28,532 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 15:21:28,574 INFO namenode.NameNode: createNameNode []
[33malgo-2    |[0m Nov 14, 2021 3:21:28 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[33malgo-2    |[0m INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 15:21:28,754 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m 2021-11-14 15:21:28,861 INFO conf.Configuration: found resource core-site.xml at file:/etc/hadoop/conf.empty/core-site.xml
[36malgo-1    |[0m 2021-11-14 15:21:28,917 INFO conf.Configuration: resource-types.xml not found
[36malgo-1    |[0m 2021-11-14 15:21:28,918 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
[36malgo-1    |[0m 2021-11-14 15:21:28,920 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m 2021-11-14 15:21:28,924 INFO impl.MetricsSystemImpl: NameNode metrics system started
[36malgo-1    |[0m 2021-11-14 15:21:28,953 INFO namenode.NameNodeUtils: fs.defaultFS is hdfs://192.168.240.3/
[36malgo-1    |[0m 2021-11-14 15:21:28,961 INFO resourceplugin.ResourcePluginManager: No Resource plugins found from configuration!
[36malgo-1    |[0m 2021-11-14 15:21:28,961 INFO resourceplugin.ResourcePluginManager: Found Resource plugins from configuration: null
[36malgo-1    |[0m 2021-11-14 15:21:28,979 INFO conf.Configuration: found resource yarn-site.xml at file:/etc/hadoop/conf.empty/yarn-site.xml
[36malgo-1    |[0m 2021-11-14 15:21:28,996 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher
[36malgo-1    |[0m 2021-11-14 15:21:29,048 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode
[36malgo-1    |[0m 2021-11-14 15:21:29,052 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms
[33malgo-2    |[0m Nov 14, 2021 3:21:29 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[33malgo-2    |[0m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 15:21:29,059 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.
[36malgo-1    |[0m 2021-11-14 15:21:29,080 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms
[33malgo-2    |[0m 2021-11-14 15:21:29,097 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5d5b5fa7{node,/,file:///tmp/jetty-algo-2-8042-_-any-3805016250366403724.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/node}
[36malgo-1    |[0m 2021-11-14 15:21:29,097 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms
[33malgo-2    |[0m 2021-11-14 15:21:29,108 INFO server.AbstractConnector: Started ServerConnector@3f390d63{HTTP/1.1,[http/1.1]}{algo-2:8042}
[33malgo-2    |[0m 2021-11-14 15:21:29,108 INFO server.Server: Started @2988ms
[33malgo-2    |[0m 2021-11-14 15:21:29,108 INFO webapp.WebApps: Web app node started at 8042
[33malgo-2    |[0m 2021-11-14 15:21:29,109 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-2:40975
[33malgo-2    |[0m 2021-11-14 15:21:29,110 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[33malgo-2    |[0m 2021-11-14 15:21:29,117 INFO client.RMProxy: Connecting to ResourceManager at /192.168.240.3:8031
[36malgo-1    |[0m 2021-11-14 15:21:29,125 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher
[36malgo-1    |[0m 2021-11-14 15:21:29,126 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher
[36malgo-1    |[0m 2021-11-14 15:21:29,128 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper
[36malgo-1    |[0m 2021-11-14 15:21:29,129 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices
[36malgo-1    |[0m 2021-11-14 15:21:29,129 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl
[36malgo-1    |[0m 2021-11-14 15:21:29,130 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher
[36malgo-1    |[0m 2021-11-14 15:21:29,131 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler
[36malgo-1    |[0m 2021-11-14 15:21:29,138 INFO tracker.NMLogAggregationStatusTracker: the rolling interval seconds for the NodeManager Cached Log aggregation status is 600
[33malgo-2    |[0m 2021-11-14 15:21:29,151 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/192.168.240.3:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 15:21:29,151 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 15:21:29,163 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl
[36malgo-1    |[0m 2021-11-14 15:21:29,164 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager
[36malgo-1    |[0m 2021-11-14 15:21:29,169 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler
[33malgo-2    |[0m 2021-11-14 15:21:29,170 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []
[36malgo-1    |[0m 2021-11-14 15:21:29,174 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager
[36malgo-1    |[0m 2021-11-14 15:21:29,174 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
[36malgo-1    |[0m 2021-11-14 15:21:29,175 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[33malgo-2    |[0m 2021-11-14 15:21:29,179 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]
[36malgo-1    |[0m 2021-11-14 15:21:29,191 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:9870
[36malgo-1    |[0m 2021-11-14 15:21:29,212 INFO util.log: Logging initialized @1339ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m 2021-11-14 15:21:29,225 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher
[36malgo-1    |[0m 2021-11-14 15:21:29,227 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher
[36malgo-1    |[0m 2021-11-14 15:21:29,228 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher
[36malgo-1    |[0m 2021-11-14 15:21:29,229 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher
[36malgo-1    |[0m 2021-11-14 15:21:29,237 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m 2021-11-14 15:21:29,278 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m 2021-11-14 15:21:29,278 INFO impl.MetricsSystemImpl: DataNode metrics system started
[36malgo-1    |[0m 2021-11-14 15:21:29,321 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m 2021-11-14 15:21:29,339 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m 2021-11-14 15:21:29,339 INFO impl.MetricsSystemImpl: NodeManager metrics system started
[36malgo-1    |[0m 2021-11-14 15:21:29,361 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.
[36malgo-1    |[0m 2021-11-14 15:21:29,361 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 15:21:29,372 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.
[36malgo-1    |[0m 2021-11-14 15:21:29,378 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined
[36malgo-1    |[0m 2021-11-14 15:21:29,388 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[36malgo-1    |[0m 2021-11-14 15:21:29,391 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
[36malgo-1    |[0m 2021-11-14 15:21:29,391 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[36malgo-1    |[0m 2021-11-14 15:21:29,391 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 2021-11-14 15:21:29,420 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@4c60d6e9
[36malgo-1    |[0m 2021-11-14 15:21:29,422 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler
[36malgo-1    |[0m 2021-11-14 15:21:29,424 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService
[36malgo-1    |[0m 2021-11-14 15:21:29,424 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled
[36malgo-1    |[0m 2021-11-14 15:21:29,424 INFO localizer.ResourceLocalizationService: per directory file limit = 8192
[36malgo-1    |[0m 2021-11-14 15:21:29,425 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m 2021-11-14 15:21:29,426 INFO impl.MetricsSystemImpl: ResourceManager metrics system started
[36malgo-1    |[0m 2021-11-14 15:21:29,426 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
[36malgo-1    |[0m 2021-11-14 15:21:29,426 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
[36malgo-1    |[0m 2021-11-14 15:21:29,437 INFO http.HttpServer2: Jetty bound to port 9870
[36malgo-1    |[0m 2021-11-14 15:21:29,439 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[33malgo-2    |[0m 11-14 15:21 smspark-submit INFO     cluster is up
[33malgo-2    |[0m 11-14 15:21 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING
[33malgo-2    |[0m 11-14 15:21 smspark-submit INFO     starting executor logs watcher
[33malgo-2    |[0m 11-14 15:21 smspark-submit INFO     waiting for the primary to come up
[33malgo-2    |[0m Starting executor logs watcher on log_dir: /var/log/yarn
[36malgo-1    |[0m 2021-11-14 15:21:29,448 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker
[33malgo-2    |[0m 11-14 15:21 smspark-submit INFO     waiting for the primary to go down
[36malgo-1    |[0m 2021-11-14 15:21:29,455 INFO resources.ResourceHandlerModule: Using traffic control bandwidth handler
[36malgo-1    |[0m 2021-11-14 15:21:29,458 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@221a3fa4
[36malgo-1    |[0m 2021-11-14 15:21:29,459 INFO security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.
[36malgo-1    |[0m 2021-11-14 15:21:29,459 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null
[36malgo-1    |[0m 2021-11-14 15:21:29,461 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true
[36malgo-1    |[0m 2021-11-14 15:21:29,461 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true
[36malgo-1    |[0m 2021-11-14 15:21:29,461 INFO monitor.ContainersMonitorImpl: Elastic memory control enabled: false
[36malgo-1    |[0m 2021-11-14 15:21:29,461 INFO monitor.ContainersMonitorImpl: Strict memory control enabled: true
[36malgo-1    |[0m 2021-11-14 15:21:29,461 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true
[36malgo-1    |[0m 2021-11-14 15:21:29,463 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.
[36malgo-1    |[0m 2021-11-14 15:21:29,463 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager
[36malgo-1    |[0m 2021-11-14 15:21:29,473 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher
[36malgo-1    |[0m 2021-11-14 15:21:29,475 INFO server.session: DefaultSessionIdManager workerName=node0
[36malgo-1    |[0m 2021-11-14 15:21:29,475 INFO server.session: No SessionScavenger set, using defaults
[36malgo-1    |[0m 2021-11-14 15:21:29,476 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean
[36malgo-1    |[0m 2021-11-14 15:21:29,476 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.
[36malgo-1    |[0m 2021-11-14 15:21:29,477 INFO server.session: node0 Scavenging every 660000ms
[36malgo-1    |[0m 2021-11-14 15:21:29,484 INFO placement.MultiNodeSortingManager: Initializing NodeSortingService=MultiNodeSortingManager
[36malgo-1    |[0m 2021-11-14 15:21:29,487 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list
[36malgo-1    |[0m 2021-11-14 15:21:29,492 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6d60fe40{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 15:21:29,493 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@73eb439a{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 15:21:29,495 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/etc/hadoop/conf.empty/capacity-scheduler.xml
[36malgo-1    |[0m 2021-11-14 15:21:29,510 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[36malgo-1    |[0m 2021-11-14 15:21:29,510 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1, vCores:1>
[36malgo-1    |[0m 2021-11-14 15:21:29,510 INFO conf.Configuration: resource-types.xml not found
[36malgo-1    |[0m 2021-11-14 15:21:29,511 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 15:21:29,511 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
[36malgo-1    |[0m 2021-11-14 15:21:29,513 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
[36malgo-1    |[0m 2021-11-14 15:21:29,517 INFO datanode.DataNode: Configured hostname is algo-1
[36malgo-1    |[0m 2021-11-14 15:21:29,518 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[36malgo-1    |[0m 2021-11-14 15:21:29,519 INFO conf.Configuration: node-resources.xml not found
[36malgo-1    |[0m 2021-11-14 15:21:29,519 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.
[36malgo-1    |[0m 2021-11-14 15:21:29,521 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 15:21:29,521 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0
[36malgo-1    |[0m 2021-11-14 15:21:29,527 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=15892 virtual-memory=79460 virtual-cores=4
[36malgo-1    |[0m 2021-11-14 15:21:29,543 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866
[36malgo-1    |[0m 2021-11-14 15:21:29,545 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s
[36malgo-1    |[0m 2021-11-14 15:21:29,545 INFO datanode.DataNode: Number threads for balancing is 50
[36malgo-1    |[0m 2021-11-14 15:21:29,557 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined
[36malgo-1    |[0m 2021-11-14 15:21:29,557 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined
[36malgo-1    |[0m 2021-11-14 15:21:29,563 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m 2021-11-14 15:21:29,567 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=ADMINISTER_QUEUE:*SUBMIT_APP:*, labels=*,
[36malgo-1    |[0m , reservationsContinueLooking=true, orderingPolicy=utilization, priority=0
[36malgo-1    |[0m 2021-11-14 15:21:29,567 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root
[36malgo-1    |[0m 2021-11-14 15:21:29,577 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4dd6fd0a{hdfs,/,file:///usr/lib/hadoop-hdfs/webapps/hdfs/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/hdfs}
[36malgo-1    |[0m 2021-11-14 15:21:29,580 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined
[36malgo-1    |[0m 2021-11-14 15:21:29,581 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined
[36malgo-1    |[0m 2021-11-14 15:21:29,581 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 15:21:29,584 INFO capacity.LeafQueue: Initializing default
[36malgo-1    |[0m capacity = 1.0 [= (float) configuredCapacity / 100 ]
[36malgo-1    |[0m absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]
[36malgo-1    |[0m maxCapacity = 1.0 [= configuredMaxCapacity ]
[36malgo-1    |[0m absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]
[36malgo-1    |[0m effectiveMinResource=<memory:0, vCores:0>
[36malgo-1    |[0m  , effectiveMaxResource=<memory:0, vCores:0>
[36malgo-1    |[0m userLimit = 100 [= configuredUserLimit ]
[36malgo-1    |[0m userLimitFactor = 1.0 [= configuredUserLimitFactor ]
[36malgo-1    |[0m maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]
[36malgo-1    |[0m maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]
[36malgo-1    |[0m usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]
[36malgo-1    |[0m absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]
[36malgo-1    |[0m maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]
[36malgo-1    |[0m minimumAllocationFactor = 0.99993706 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]
[36malgo-1    |[0m maximumAllocation = <memory:15892, vCores:4> [= configuredMaxAllocation ]
[36malgo-1    |[0m numContainers = 0 [= currentNumContainers ]
[36malgo-1    |[0m state = RUNNING [= configuredState ]
[36malgo-1    |[0m acls = ADMINISTER_QUEUE:*SUBMIT_APP:* [= configuredAcls ]
[36malgo-1    |[0m nodeLocalityDelay = 40
[36malgo-1    |[0m rackLocalityAdditionalDelay = -1
[36malgo-1    |[0m labels=*,
[36malgo-1    |[0m reservationsContinueLooking = true
[36malgo-1    |[0m preemptionDisabled = true
[36malgo-1    |[0m defaultAppPriorityPerQueue = 0
[36malgo-1    |[0m priority = 0
[36malgo-1    |[0m maxLifetime = -1 seconds
[36malgo-1    |[0m defaultLifetime = -1 seconds
[36malgo-1    |[0m 2021-11-14 15:21:29,584 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>
[36malgo-1    |[0m 2021-11-14 15:21:29,584 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
[36malgo-1    |[0m 2021-11-14 15:21:29,585 INFO util.log: Logging initialized @1719ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m 2021-11-14 15:21:29,586 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
[36malgo-1    |[0m 2021-11-14 15:21:29,587 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false
[36malgo-1    |[0m 2021-11-14 15:21:29,588 INFO placement.MultiNodeSortingManager: MultiNode scheduling is 'false', and configured policies are 
[36malgo-1    |[0m 2021-11-14 15:21:29,588 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1, vCores:1>>, maximumAllocation=<<memory:15892, vCores:4>>, asynchronousScheduling=false, asyncScheduleInterval=5ms,multiNodePlacementEnabled=false
[36malgo-1    |[0m 2021-11-14 15:21:29,591 INFO conf.Configuration: dynamic-resources.xml not found
[36malgo-1    |[0m 2021-11-14 15:21:29,593 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].
[36malgo-1    |[0m 2021-11-14 15:21:29,593 INFO resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.
[36malgo-1    |[0m 2021-11-14 15:21:29,594 INFO resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. 
[36malgo-1    |[0m 2021-11-14 15:21:29,599 INFO resourcemanager.ResourceManager: TimelineServicePublisher is not configured
[36malgo-1    |[0m 2021-11-14 15:21:29,603 INFO server.AbstractConnector: Started ServerConnector@4d14b6c2{HTTP/1.1,[http/1.1]}{0.0.0.0:9870}
[36malgo-1    |[0m 2021-11-14 15:21:29,603 INFO server.Server: Started @1731ms
[36malgo-1    |[0m 2021-11-14 15:21:29,605 INFO ipc.Server: Starting Socket Reader #1 for port 0
[36malgo-1    |[0m 2021-11-14 15:21:29,639 INFO util.log: Logging initialized @1765ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m 2021-11-14 15:21:29,741 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 15:21:29,745 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 15:21:29,747 INFO http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined
[36malgo-1    |[0m 2021-11-14 15:21:29,753 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[36malgo-1    |[0m 2021-11-14 15:21:29,754 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined
[36malgo-1    |[0m 2021-11-14 15:21:29,757 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster
[36malgo-1    |[0m 2021-11-14 15:21:29,757 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs
[36malgo-1    |[0m 2021-11-14 15:21:29,757 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static
[36malgo-1    |[0m 2021-11-14 15:21:29,757 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster
[36malgo-1    |[0m 2021-11-14 15:21:29,757 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[36malgo-1    |[0m 2021-11-14 15:21:29,757 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 2021-11-14 15:21:29,759 INFO http.HttpServer2: adding path spec: /cluster/*
[36malgo-1    |[0m 2021-11-14 15:21:29,759 INFO http.HttpServer2: adding path spec: /ws/*
[36malgo-1    |[0m 2021-11-14 15:21:29,759 INFO http.HttpServer2: adding path spec: /app/*
[36malgo-1    |[0m 2021-11-14 15:21:29,762 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[36malgo-1    |[0m 2021-11-14 15:21:29,764 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
[36malgo-1    |[0m 2021-11-14 15:21:29,764 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[36malgo-1    |[0m 2021-11-14 15:21:29,764 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 2021-11-14 15:21:29,795 INFO http.HttpServer2: Jetty bound to port 39647
[36malgo-1    |[0m 2021-11-14 15:21:29,797 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[36malgo-1    |[0m 2021-11-14 15:21:29,823 INFO server.session: DefaultSessionIdManager workerName=node0
[36malgo-1    |[0m 2021-11-14 15:21:29,823 INFO server.session: No SessionScavenger set, using defaults
[36malgo-1    |[0m 2021-11-14 15:21:29,825 INFO server.session: node0 Scavenging every 660000ms
[36malgo-1    |[0m 2021-11-14 15:21:29,835 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a3793c7{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 15:21:29,836 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@bb9e6dc{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 15:21:29,837 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
[36malgo-1    |[0m 2021-11-14 15:21:29,837 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
[36malgo-1    |[0m 2021-11-14 15:21:29,841 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 15:21:29,844 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 15:21:29,844 INFO ipc.Server: IPC Server listener on 0: starting
[36malgo-1    |[0m 2021-11-14 15:21:29,851 INFO security.NMContainerTokenSecretManager: Updating node address : algo-1:33137
[36malgo-1    |[0m 2021-11-14 15:21:29,862 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 500, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 15:21:29,864 INFO ipc.Server: Starting Socket Reader #1 for port 8040
[36malgo-1    |[0m 2021-11-14 15:21:29,868 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 15:21:29,868 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 15:21:29,868 INFO ipc.Server: IPC Server listener on 8040: starting
[36malgo-1    |[0m 2021-11-14 15:21:29,869 INFO localizer.ResourceLocalizationService: Localizer started on port 8040
[36malgo-1    |[0m 2021-11-14 15:21:29,872 INFO containermanager.ContainerManagerImpl: ContainerManager started at /192.168.240.3:33137
[36malgo-1    |[0m 2021-11-14 15:21:29,872 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-1/192.168.240.3:0
[36malgo-1    |[0m 2021-11-14 15:21:29,872 WARN tracker.NMLogAggregationStatusTracker: Log Aggregation is disabled.So is the LogAggregationStatusTracker.
[36malgo-1    |[0m 2021-11-14 15:21:29,876 INFO webapp.WebServer: Instantiating NMWebApp at algo-1:8042
[36malgo-1    |[0m 2021-11-14 15:21:29,883 INFO namenode.FSEditLog: Edit logging is async:true
[36malgo-1    |[0m 2021-11-14 15:21:29,895 INFO namenode.FSNamesystem: KeyProvider: null
[36malgo-1    |[0m 2021-11-14 15:21:29,895 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m 2021-11-14 15:21:29,896 INFO namenode.FSNamesystem: fsLock is fair: true
[36malgo-1    |[0m 2021-11-14 15:21:29,896 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
[36malgo-1    |[0m 2021-11-14 15:21:29,901 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 15:21:29,901 INFO namenode.FSNamesystem: supergroup          = supergroup
[36malgo-1    |[0m 2021-11-14 15:21:29,901 INFO namenode.FSNamesystem: isPermissionEnabled = true
[36malgo-1    |[0m 2021-11-14 15:21:29,901 INFO namenode.FSNamesystem: HA Enabled: false
[36malgo-1    |[0m 2021-11-14 15:21:29,902 INFO util.log: Logging initialized @2029ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m 2021-11-14 15:21:29,905 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@338c99c8{datanode,/,file:///usr/lib/hadoop-hdfs/webapps/datanode/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/datanode}
[36malgo-1    |[0m 2021-11-14 15:21:29,912 INFO server.AbstractConnector: Started ServerConnector@6c372fe6{HTTP/1.1,[http/1.1]}{localhost:39647}
[36malgo-1    |[0m 2021-11-14 15:21:29,912 INFO server.Server: Started @2046ms
[36malgo-1    |[0m 2021-11-14 15:21:29,945 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[36malgo-1    |[0m 2021-11-14 15:21:29,956 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
[36malgo-1    |[0m 2021-11-14 15:21:29,956 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
[36malgo-1    |[0m 2021-11-14 15:21:29,960 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
[36malgo-1    |[0m 2021-11-14 15:21:29,960 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Nov 14 15:21:29
[36malgo-1    |[0m 2021-11-14 15:21:29,962 INFO util.GSet: Computing capacity for map BlocksMap
[36malgo-1    |[0m 2021-11-14 15:21:29,962 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 15:21:29,963 INFO util.GSet: 2.0% max memory 6.5 GB = 133.6 MB
[36malgo-1    |[0m 2021-11-14 15:21:29,963 INFO util.GSet: capacity      = 2^24 = 16777216 entries
[36malgo-1    |[0m 2021-11-14 15:21:29,984 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
[36malgo-1    |[0m 2021-11-14 15:21:29,984 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
[36malgo-1    |[0m 2021-11-14 15:21:29,991 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
[36malgo-1    |[0m 2021-11-14 15:21:29,991 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
[36malgo-1    |[0m 2021-11-14 15:21:29,991 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
[36malgo-1    |[0m 2021-11-14 15:21:29,991 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
[36malgo-1    |[0m 2021-11-14 15:21:29,992 INFO blockmanagement.BlockManager: defaultReplication         = 3
[36malgo-1    |[0m 2021-11-14 15:21:29,992 INFO blockmanagement.BlockManager: maxReplication             = 512
[36malgo-1    |[0m 2021-11-14 15:21:29,992 INFO blockmanagement.BlockManager: minReplication             = 1
[36malgo-1    |[0m 2021-11-14 15:21:29,992 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
[36malgo-1    |[0m 2021-11-14 15:21:29,992 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
[36malgo-1    |[0m 2021-11-14 15:21:29,992 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
[36malgo-1    |[0m 2021-11-14 15:21:29,992 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
[36malgo-1    |[0m 2021-11-14 15:21:29,999 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 15:21:30,002 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined
[36malgo-1    |[0m 2021-11-14 15:21:30,008 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[36malgo-1    |[0m 2021-11-14 15:21:30,010 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node
[36malgo-1    |[0m 2021-11-14 15:21:30,010 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 2021-11-14 15:21:30,010 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[36malgo-1    |[0m 2021-11-14 15:21:30,010 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context node
[36malgo-1    |[0m 2021-11-14 15:21:30,010 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
[36malgo-1    |[0m 2021-11-14 15:21:30,010 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
[36malgo-1    |[0m 2021-11-14 15:21:30,012 INFO http.HttpServer2: adding path spec: /node/*
[36malgo-1    |[0m 2021-11-14 15:21:30,012 INFO http.HttpServer2: adding path spec: /ws/*
[36malgo-1    |[0m 2021-11-14 15:21:30,021 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
[36malgo-1    |[0m 2021-11-14 15:21:30,021 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 15:21:30,021 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 15:21:30,021 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 15:21:30,035 INFO util.GSet: Computing capacity for map INodeMap
[36malgo-1    |[0m 2021-11-14 15:21:30,035 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 15:21:30,035 INFO util.GSet: 1.0% max memory 6.5 GB = 66.8 MB
[36malgo-1    |[0m 2021-11-14 15:21:30,035 INFO util.GSet: capacity      = 2^23 = 8388608 entries
[36malgo-1    |[0m 2021-11-14 15:21:30,062 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864
[36malgo-1    |[0m 2021-11-14 15:21:30,072 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 15:21:30,073 INFO datanode.DataNode: dnUserName = root
[36malgo-1    |[0m 2021-11-14 15:21:30,073 INFO datanode.DataNode: supergroup = supergroup
[36malgo-1    |[0m 2021-11-14 15:21:30,119 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[33malgo-2    |[0m 2021-11-14 15:21:30,152 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/192.168.240.3:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 15:21:30,152 INFO ipc.Server: Starting Socket Reader #1 for port 9867
[36malgo-1    |[0m 2021-11-14 15:21:30,166 INFO namenode.FSDirectory: ACLs enabled? false
[36malgo-1    |[0m 2021-11-14 15:21:30,166 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
[36malgo-1    |[0m 2021-11-14 15:21:30,166 INFO namenode.FSDirectory: XAttrs enabled? true
[36malgo-1    |[0m 2021-11-14 15:21:30,167 INFO namenode.NameNode: Caching file names occurring more than 10 times
[36malgo-1    |[0m 2021-11-14 15:21:30,171 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
[36malgo-1    |[0m 2021-11-14 15:21:30,173 INFO snapshot.SnapshotManager: SkipList is disabled
[36malgo-1    |[0m 2021-11-14 15:21:30,177 INFO util.GSet: Computing capacity for map cachedBlocks
[36malgo-1    |[0m 2021-11-14 15:21:30,177 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 15:21:30,177 INFO util.GSet: 0.25% max memory 6.5 GB = 16.7 MB
[36malgo-1    |[0m 2021-11-14 15:21:30,177 INFO util.GSet: capacity      = 2^21 = 2097152 entries
[36malgo-1    |[0m 2021-11-14 15:21:30,177 INFO webapp.WebApps: Registered webapp guice modules
[36malgo-1    |[0m 2021-11-14 15:21:30,184 INFO http.HttpServer2: Jetty bound to port 8088
[36malgo-1    |[0m 2021-11-14 15:21:30,185 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
[36malgo-1    |[0m 2021-11-14 15:21:30,185 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
[36malgo-1    |[0m 2021-11-14 15:21:30,185 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
[36malgo-1    |[0m 2021-11-14 15:21:30,185 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[36malgo-1    |[0m 2021-11-14 15:21:30,188 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
[36malgo-1    |[0m 2021-11-14 15:21:30,188 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
[36malgo-1    |[0m 2021-11-14 15:21:30,190 INFO util.GSet: Computing capacity for map NameNodeRetryCache
[36malgo-1    |[0m 2021-11-14 15:21:30,190 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 15:21:30,190 INFO util.GSet: 0.029999999329447746% max memory 6.5 GB = 2.0 MB
[36malgo-1    |[0m 2021-11-14 15:21:30,190 INFO util.GSet: capacity      = 2^18 = 262144 entries
[36malgo-1    |[0m 2021-11-14 15:21:30,211 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/namenode/in_use.lock acquired by nodename 134@algo-1
[36malgo-1    |[0m 2021-11-14 15:21:30,211 INFO server.session: DefaultSessionIdManager workerName=node0
[36malgo-1    |[0m 2021-11-14 15:21:30,211 INFO server.session: No SessionScavenger set, using defaults
[36malgo-1    |[0m 2021-11-14 15:21:30,212 INFO server.session: node0 Scavenging every 660000ms
[36malgo-1    |[0m 2021-11-14 15:21:30,223 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 15:21:30,231 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
[36malgo-1    |[0m 2021-11-14 15:21:30,232 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
[36malgo-1    |[0m 2021-11-14 15:21:30,233 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
[36malgo-1    |[0m 2021-11-14 15:21:30,233 INFO namenode.FileJournalManager: Recovering unfinalized segments in /opt/amazon/hadoop/hdfs/namenode/current
[36malgo-1    |[0m 2021-11-14 15:21:30,233 INFO namenode.FSImage: No edit log streams selected.
[36malgo-1    |[0m 2021-11-14 15:21:30,233 INFO namenode.FSImage: Planning to load image: FSImageFile(file=/opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
[36malgo-1    |[0m 2021-11-14 15:21:30,254 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@21129f1f{logs,/logs,file:///var/log/yarn/export/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 15:21:30,254 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@48d61b48{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/static,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 15:21:30,266 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException
[33malgo-2    |[0m 2021-11-14 15:21:30,290 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/192.168.240.3:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 15:21:30,290 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.
[36malgo-1    |[0m 2021-11-14 15:21:30,316 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
[36malgo-1    |[0m 2021-11-14 15:21:30,317 INFO namenode.FSImage: Loaded image for txid 0 from /opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000
[36malgo-1    |[0m 2021-11-14 15:21:30,321 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
[36malgo-1    |[0m 2021-11-14 15:21:30,321 INFO namenode.FSEditLog: Starting log segment at 1
[36malgo-1    |[0m 2021-11-14 15:21:30,352 INFO webapp.WebApps: Registered webapp guice modules
[36malgo-1    |[0m 2021-11-14 15:21:30,355 INFO http.HttpServer2: Jetty bound to port 8042
[36malgo-1    |[0m 2021-11-14 15:21:30,357 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[36malgo-1    |[0m 2021-11-14 15:21:30,362 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:9867
[36malgo-1    |[0m 2021-11-14 15:21:30,369 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m 2021-11-14 15:21:30,379 INFO datanode.DataNode: Refresh request received for nameservices: null
[36malgo-1    |[0m 2021-11-14 15:21:30,386 INFO server.session: DefaultSessionIdManager workerName=node0
[36malgo-1    |[0m 2021-11-14 15:21:30,386 INFO server.session: No SessionScavenger set, using defaults
[36malgo-1    |[0m 2021-11-14 15:21:30,387 INFO server.session: node0 Scavenging every 600000ms
[36malgo-1    |[0m 2021-11-14 15:21:30,391 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>
[36malgo-1    |[0m 2021-11-14 15:21:30,396 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 15:21:30,398 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4bff64c2{logs,/logs,file:///var/log/yarn/export/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 15:21:30,399 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3cc41abc{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/static,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 15:21:30,403 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1/192.168.240.3:8020 starting to offer service
[36malgo-1    |[0m 2021-11-14 15:21:30,410 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 15:21:30,410 INFO ipc.Server: IPC Server listener on 9867: starting
[36malgo-1    |[0m 2021-11-14 15:21:30,411 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException
[36malgo-1    |[0m 2021-11-14 15:21:30,427 INFO namenode.NameCache: initialized with 0 entries 0 lookups
[36malgo-1    |[0m 2021-11-14 15:21:30,427 INFO namenode.FSNamesystem: Finished loading FSImage in 234 msecs
[36malgo-1    |[0m Nov 14, 2021 3:21:30 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class
[36malgo-1    |[0m Nov 14, 2021 3:21:30 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class
[36malgo-1    |[0m Nov 14, 2021 3:21:30 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
[36malgo-1    |[0m Nov 14, 2021 3:21:30 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
[36malgo-1    |[0m INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'
[36malgo-1    |[0m 2021-11-14 15:21:30,498 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m Nov 14, 2021 3:21:30 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m Nov 14, 2021 3:21:30 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class
[36malgo-1    |[0m Nov 14, 2021 3:21:30 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
[36malgo-1    |[0m Nov 14, 2021 3:21:30 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class
[36malgo-1    |[0m Nov 14, 2021 3:21:30 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
[36malgo-1    |[0m INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'
[36malgo-1    |[0m Nov 14, 2021 3:21:30 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 15:21:30,818 INFO namenode.NameNode: RPC server is binding to algo-1:8020
[36malgo-1    |[0m Nov 14, 2021 3:21:30 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 15:21:30,855 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 15:21:30,867 INFO ipc.Server: Starting Socket Reader #1 for port 8020
[36malgo-1    |[0m Nov 14, 2021 3:21:30 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 15:21:31,052 INFO namenode.NameNode: Clients are to use algo-1:8020 to access this namenode/service.
[36malgo-1    |[0m 2021-11-14 15:21:31,055 INFO namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
[36malgo-1    |[0m 2021-11-14 15:21:31,066 INFO namenode.LeaseManager: Number of blocks under construction: 0
[36malgo-1    |[0m 2021-11-14 15:21:31,078 INFO blockmanagement.BlockManager: initializing replication queues
[36malgo-1    |[0m 2021-11-14 15:21:31,100 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs
[36malgo-1    |[0m 2021-11-14 15:21:31,100 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
[36malgo-1    |[0m 2021-11-14 15:21:31,101 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
[36malgo-1    |[0m 2021-11-14 15:21:31,127 INFO blockmanagement.BlockManager: Total number of blocks            = 0
[36malgo-1    |[0m 2021-11-14 15:21:31,127 INFO blockmanagement.BlockManager: Number of invalid blocks          = 0
[36malgo-1    |[0m 2021-11-14 15:21:31,127 INFO blockmanagement.BlockManager: Number of under-replicated blocks = 0
[36malgo-1    |[0m 2021-11-14 15:21:31,127 INFO blockmanagement.BlockManager: Number of  over-replicated blocks = 0
[36malgo-1    |[0m 2021-11-14 15:21:31,127 INFO blockmanagement.BlockManager: Number of blocks being written    = 0
[36malgo-1    |[0m 2021-11-14 15:21:31,127 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 27 msec
[36malgo-1    |[0m 2021-11-14 15:21:31,139 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 15:21:31,152 INFO ipc.Server: IPC Server listener on 8020: starting
[33malgo-2    |[0m 2021-11-14 15:21:31,153 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/192.168.240.3:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 15:21:31,202 INFO namenode.NameNode: NameNode RPC up at: algo-1/192.168.240.3:8020
[36malgo-1    |[0m 2021-11-14 15:21:31,205 INFO namenode.FSNamesystem: Starting services required for active state
[36malgo-1    |[0m 2021-11-14 15:21:31,205 INFO namenode.FSDirectory: Initializing quota with 4 thread(s)
[36malgo-1    |[0m 2021-11-14 15:21:31,237 INFO namenode.FSDirectory: Quota initialization completed in 31 milliseconds
[36malgo-1    |[0m name space=1
[36malgo-1    |[0m storage space=0
[36malgo-1    |[0m storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
[36malgo-1    |[0m Nov 14, 2021 3:21:31 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope "Singleton"
[33malgo-2    |[0m 2021-11-14 15:21:31,291 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/192.168.240.3:8031. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 15:21:31,312 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
[36malgo-1    |[0m 2021-11-14 15:21:31,323 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5d5b5fa7{node,/,file:///tmp/jetty-algo-1-8042-_-any-4919619118336284921.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/node}
[36malgo-1    |[0m 2021-11-14 15:21:31,351 INFO server.AbstractConnector: Started ServerConnector@3f390d63{HTTP/1.1,[http/1.1]}{algo-1:8042}
[36malgo-1    |[0m 2021-11-14 15:21:31,352 INFO server.Server: Started @3479ms
[36malgo-1    |[0m 2021-11-14 15:21:31,352 INFO webapp.WebApps: Web app node started at 8042
[36malgo-1    |[0m 2021-11-14 15:21:31,375 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-1:33137
[36malgo-1    |[0m 2021-11-14 15:21:31,382 INFO client.RMProxy: Connecting to ResourceManager at /192.168.240.3:8031
[36malgo-1    |[0m 2021-11-14 15:21:31,387 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 15:21:31,436 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []
[36malgo-1    |[0m 2021-11-14 15:21:31,449 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]
[36malgo-1    |[0m 2021-11-14 15:21:31,470 INFO ipc.Client: Retrying connect to server: algo-1/192.168.240.3:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[33malgo-2    |[0m 2021-11-14 15:21:31,476 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1.spark-network/192.168.240.3:8020
[33malgo-2    |[0m 2021-11-14 15:21:31,478 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
[33malgo-2    |[0m 2021-11-14 15:21:31,488 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 18@algo-2
[36malgo-1    |[0m Nov 14, 2021 3:21:31 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope "Singleton"
[33malgo-2    |[0m 2021-11-14 15:21:31,490 INFO common.Storage: Storage directory with location [DISK]file:/opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 114424355. Formatting...
[33malgo-2    |[0m 2021-11-14 15:21:31,491 INFO common.Storage: Generated new storageID DS-82d29f09-aa8e-44f9-85fb-990c9df7f455 for directory /opt/amazon/hadoop/hdfs/datanode 
[33malgo-2    |[0m 2021-11-14 15:21:31,517 INFO common.Storage: Analyzing storage directories for bpid BP-1677061777-192.168.240.3-1636903287577
[33malgo-2    |[0m 2021-11-14 15:21:31,517 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-1677061777-192.168.240.3-1636903287577
[33malgo-2    |[0m 2021-11-14 15:21:31,518 INFO common.Storage: Block pool storage directory for location [DISK]file:/opt/amazon/hadoop/hdfs/datanode and block pool id BP-1677061777-192.168.240.3-1636903287577 is not formatted. Formatting ...
[33malgo-2    |[0m 2021-11-14 15:21:31,518 INFO common.Storage: Formatting block pool BP-1677061777-192.168.240.3-1636903287577 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-1677061777-192.168.240.3-1636903287577/current
[36malgo-1    |[0m 2021-11-14 15:21:31,524 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@25da615a{cluster,/,file:///tmp/jetty-192_168_240_3-8088-_-any-8046115131916512887.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/cluster}
[33malgo-2    |[0m 2021-11-14 15:21:31,528 INFO datanode.DataNode: Setting up storage: nsid=114424355;bpid=BP-1677061777-192.168.240.3-1636903287577;lv=-57;nsInfo=lv=-65;cid=CID-f7f17664-f4ce-4750-ac10-52bdfb51d834;nsid=114424355;c=1636903287577;bpid=BP-1677061777-192.168.240.3-1636903287577;dnuuid=null
[33malgo-2    |[0m 2021-11-14 15:21:31,531 INFO datanode.DataNode: Generated and persisted new Datanode UUID 3fcb146c-fa78-4453-a820-a655ecbeb032
[36malgo-1    |[0m 2021-11-14 15:21:31,537 INFO server.AbstractConnector: Started ServerConnector@71e9ebae{HTTP/1.1,[http/1.1]}{192.168.240.3:8088}
[36malgo-1    |[0m 2021-11-14 15:21:31,537 INFO server.Server: Started @3664ms
[36malgo-1    |[0m 2021-11-14 15:21:31,537 INFO webapp.WebApps: Web app cluster started at 8088
[36malgo-1    |[0m 2021-11-14 15:21:31,572 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1/192.168.240.3:8020
[36malgo-1    |[0m 2021-11-14 15:21:31,575 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
[36malgo-1    |[0m 2021-11-14 15:21:31,583 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 135@algo-1
[36malgo-1    |[0m 2021-11-14 15:21:31,585 INFO common.Storage: Storage directory with location [DISK]file:/opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 114424355. Formatting...
[36malgo-1    |[0m 2021-11-14 15:21:31,586 INFO common.Storage: Generated new storageID DS-5b50ee7f-d3bc-4683-9400-89d766bc6ebc for directory /opt/amazon/hadoop/hdfs/datanode 
[36malgo-1    |[0m 2021-11-14 15:21:31,608 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 15:21:31,616 INFO common.Storage: Analyzing storage directories for bpid BP-1677061777-192.168.240.3-1636903287577
[36malgo-1    |[0m 2021-11-14 15:21:31,616 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-1677061777-192.168.240.3-1636903287577
[36malgo-1    |[0m 2021-11-14 15:21:31,617 INFO common.Storage: Block pool storage directory for location [DISK]file:/opt/amazon/hadoop/hdfs/datanode and block pool id BP-1677061777-192.168.240.3-1636903287577 is not formatted. Formatting ...
[36malgo-1    |[0m 2021-11-14 15:21:31,617 INFO common.Storage: Formatting block pool BP-1677061777-192.168.240.3-1636903287577 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-1677061777-192.168.240.3-1636903287577/current
[36malgo-1    |[0m 2021-11-14 15:21:31,622 INFO ipc.Server: Starting Socket Reader #1 for port 8033
[33malgo-2    |[0m 2021-11-14 15:21:31,643 INFO impl.FsDatasetImpl: Added new volume: DS-82d29f09-aa8e-44f9-85fb-990c9df7f455
[33malgo-2    |[0m 2021-11-14 15:21:31,644 INFO impl.FsDatasetImpl: Added volume - [DISK]file:/opt/amazon/hadoop/hdfs/datanode, StorageType: DISK
[33malgo-2    |[0m 2021-11-14 15:21:31,648 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean
[36malgo-1    |[0m 2021-11-14 15:21:31,649 INFO datanode.DataNode: Setting up storage: nsid=114424355;bpid=BP-1677061777-192.168.240.3-1636903287577;lv=-57;nsInfo=lv=-65;cid=CID-f7f17664-f4ce-4750-ac10-52bdfb51d834;nsid=114424355;c=1636903287577;bpid=BP-1677061777-192.168.240.3-1636903287577;dnuuid=null
[36malgo-1    |[0m 2021-11-14 15:21:31,652 INFO datanode.DataNode: Generated and persisted new Datanode UUID cced7f96-009c-49df-9116-cc49c6ef35ea
[33malgo-2    |[0m 2021-11-14 15:21:31,656 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode
[33malgo-2    |[0m 2021-11-14 15:21:31,665 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode
[33malgo-2    |[0m 2021-11-14 15:21:31,666 INFO impl.FsDatasetImpl: Adding block pool BP-1677061777-192.168.240.3-1636903287577
[33malgo-2    |[0m 2021-11-14 15:21:31,667 INFO impl.FsDatasetImpl: Scanning block pool BP-1677061777-192.168.240.3-1636903287577 on volume /opt/amazon/hadoop/hdfs/datanode...
[33malgo-2    |[0m 2021-11-14 15:21:31,715 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-1677061777-192.168.240.3-1636903287577 on /opt/amazon/hadoop/hdfs/datanode: 47ms
[33malgo-2    |[0m 2021-11-14 15:21:31,716 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1677061777-192.168.240.3-1636903287577: 50ms
[33malgo-2    |[0m 2021-11-14 15:21:31,721 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-1677061777-192.168.240.3-1636903287577 on volume /opt/amazon/hadoop/hdfs/datanode...
[33malgo-2    |[0m 2021-11-14 15:21:31,721 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-1677061777-192.168.240.3-1636903287577/current/replicas doesn't exist 
[33malgo-2    |[0m 2021-11-14 15:21:31,725 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1677061777-192.168.240.3-1636903287577 on volume /opt/amazon/hadoop/hdfs/datanode: 4ms
[33malgo-2    |[0m 2021-11-14 15:21:31,725 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-1677061777-192.168.240.3-1636903287577: 7ms
[33malgo-2    |[0m 2021-11-14 15:21:31,734 INFO datanode.VolumeScanner: Now scanning bpid BP-1677061777-192.168.240.3-1636903287577 on volume /opt/amazon/hadoop/hdfs/datanode
[33malgo-2    |[0m 2021-11-14 15:21:31,736 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-82d29f09-aa8e-44f9-85fb-990c9df7f455): finished scanning block pool BP-1677061777-192.168.240.3-1636903287577
[33malgo-2    |[0m 2021-11-14 15:21:31,741 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 11/14/21 6:59 PM with interval of 21600000ms
[33malgo-2    |[0m 2021-11-14 15:21:31,748 INFO datanode.DataNode: Block pool BP-1677061777-192.168.240.3-1636903287577 (Datanode Uuid 3fcb146c-fa78-4453-a820-a655ecbeb032) service to algo-1.spark-network/192.168.240.3:8020 beginning handshake with NN
[33malgo-2    |[0m 2021-11-14 15:21:31,751 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-82d29f09-aa8e-44f9-85fb-990c9df7f455): no suitable block pools found to scan.  Waiting 1814399975 ms.
[36malgo-1    |[0m 2021-11-14 15:21:31,759 INFO impl.FsDatasetImpl: Added new volume: DS-5b50ee7f-d3bc-4683-9400-89d766bc6ebc
[36malgo-1    |[0m 2021-11-14 15:21:31,759 INFO impl.FsDatasetImpl: Added volume - [DISK]file:/opt/amazon/hadoop/hdfs/datanode, StorageType: DISK
[36malgo-1    |[0m 2021-11-14 15:21:31,762 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean
[36malgo-1    |[0m 2021-11-14 15:21:31,769 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode
[36malgo-1    |[0m 2021-11-14 15:21:31,777 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode
[36malgo-1    |[0m 2021-11-14 15:21:31,778 INFO impl.FsDatasetImpl: Adding block pool BP-1677061777-192.168.240.3-1636903287577
[36malgo-1    |[0m 2021-11-14 15:21:31,779 INFO impl.FsDatasetImpl: Scanning block pool BP-1677061777-192.168.240.3-1636903287577 on volume /opt/amazon/hadoop/hdfs/datanode...
[36malgo-1    |[0m 2021-11-14 15:21:31,792 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(192.168.240.2:9866, datanodeUuid=3fcb146c-fa78-4453-a820-a655ecbeb032, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-f7f17664-f4ce-4750-ac10-52bdfb51d834;nsid=114424355;c=1636903287577) storage 3fcb146c-fa78-4453-a820-a655ecbeb032
[36malgo-1    |[0m 2021-11-14 15:21:31,794 INFO net.NetworkTopology: Adding a new node: /default-rack/192.168.240.2:9866
[36malgo-1    |[0m 2021-11-14 15:21:31,794 INFO blockmanagement.BlockReportLeaseManager: Registered DN 3fcb146c-fa78-4453-a820-a655ecbeb032 (192.168.240.2:9866).
[36malgo-1    |[0m 2021-11-14 15:21:31,803 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 15:21:31,804 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 15:21:31,804 INFO ipc.Server: IPC Server listener on 8033: starting
[36malgo-1    |[0m 2021-11-14 15:21:31,805 INFO resourcemanager.ResourceManager: Transitioning to active state
[33malgo-2    |[0m 2021-11-14 15:21:31,808 INFO datanode.DataNode: Block pool Block pool BP-1677061777-192.168.240.3-1636903287577 (Datanode Uuid 3fcb146c-fa78-4453-a820-a655ecbeb032) service to algo-1.spark-network/192.168.240.3:8020 successfully registered with NN
[33malgo-2    |[0m 2021-11-14 15:21:31,808 INFO datanode.DataNode: For namenode algo-1.spark-network/192.168.240.3:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
[36malgo-1    |[0m 2021-11-14 15:21:31,818 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-1677061777-192.168.240.3-1636903287577 on /opt/amazon/hadoop/hdfs/datanode: 39ms
[36malgo-1    |[0m 2021-11-14 15:21:31,819 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1677061777-192.168.240.3-1636903287577: 40ms
[36malgo-1    |[0m 2021-11-14 15:21:31,820 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-1677061777-192.168.240.3-1636903287577 on volume /opt/amazon/hadoop/hdfs/datanode...
[36malgo-1    |[0m 2021-11-14 15:21:31,820 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-1677061777-192.168.240.3-1636903287577/current/replicas doesn't exist 
[36malgo-1    |[0m 2021-11-14 15:21:31,821 INFO recovery.RMStateStore: Updating AMRMToken
[36malgo-1    |[0m 2021-11-14 15:21:31,821 INFO security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
[36malgo-1    |[0m 2021-11-14 15:21:31,822 INFO security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
[36malgo-1    |[0m 2021-11-14 15:21:31,822 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
[36malgo-1    |[0m 2021-11-14 15:21:31,822 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 1
[36malgo-1    |[0m 2021-11-14 15:21:31,822 INFO recovery.RMStateStore: Storing RMDTMasterKey.
[36malgo-1    |[0m 2021-11-14 15:21:31,822 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1677061777-192.168.240.3-1636903287577 on volume /opt/amazon/hadoop/hdfs/datanode: 2ms
[36malgo-1    |[0m 2021-11-14 15:21:31,823 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-1677061777-192.168.240.3-1636903287577: 3ms
[36malgo-1    |[0m 2021-11-14 15:21:31,823 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
[36malgo-1    |[0m 2021-11-14 15:21:31,824 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
[36malgo-1    |[0m 2021-11-14 15:21:31,824 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 2
[36malgo-1    |[0m 2021-11-14 15:21:31,824 INFO recovery.RMStateStore: Storing RMDTMasterKey.
[36malgo-1    |[0m 2021-11-14 15:21:31,825 INFO datanode.VolumeScanner: Now scanning bpid BP-1677061777-192.168.240.3-1636903287577 on volume /opt/amazon/hadoop/hdfs/datanode
[36malgo-1    |[0m 2021-11-14 15:21:31,827 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-5b50ee7f-d3bc-4683-9400-89d766bc6ebc): finished scanning block pool BP-1677061777-192.168.240.3-1636903287577
[36malgo-1    |[0m 2021-11-14 15:21:31,838 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-5b50ee7f-d3bc-4683-9400-89d766bc6ebc): no suitable block pools found to scan.  Waiting 1814399987 ms.
[36malgo-1    |[0m 2021-11-14 15:21:31,839 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler
[36malgo-1    |[0m 2021-11-14 15:21:31,842 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 11/14/21 3:47 PM with interval of 21600000ms
[36malgo-1    |[0m 2021-11-14 15:21:31,849 INFO datanode.DataNode: Block pool BP-1677061777-192.168.240.3-1636903287577 (Datanode Uuid cced7f96-009c-49df-9116-cc49c6ef35ea) service to algo-1/192.168.240.3:8020 beginning handshake with NN
[36malgo-1    |[0m 11-14 15:21 smspark-submit INFO     cluster is up
[36malgo-1    |[0m 11-14 15:21 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING
[36malgo-1    |[0m 11-14 15:21 smspark-submit INFO     starting executor logs watcher
[36malgo-1    |[0m 11-14 15:21 smspark-submit INFO     start log event log publisher
[36malgo-1    |[0m Starting executor logs watcher on log_dir: /var/log/yarn
[36malgo-1    |[0m 11-14 15:21 sagemaker-spark-event-logs-publisher INFO     Spark event log not enabled.
[36malgo-1    |[0m 11-14 15:21 smspark-submit INFO     Waiting for hosts to bootstrap: ['algo-1', 'algo-2']
[36malgo-1    |[0m 11-14 15:21 smspark-submit INFO     Received host statuses: dict_items([('algo-1', StatusMessage(status='WAITING', timestamp='2021-11-14T15:21:31.856439')), ('algo-2', StatusMessage(status='WAITING', timestamp='2021-11-14T15:21:31.859848'))])
[36malgo-1    |[0m 2021-11-14 15:21:31,871 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(192.168.240.3:9866, datanodeUuid=cced7f96-009c-49df-9116-cc49c6ef35ea, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-f7f17664-f4ce-4750-ac10-52bdfb51d834;nsid=114424355;c=1636903287577) storage cced7f96-009c-49df-9116-cc49c6ef35ea
[36malgo-1    |[0m 2021-11-14 15:21:31,871 INFO net.NetworkTopology: Adding a new node: /default-rack/192.168.240.3:9866
[36malgo-1    |[0m 2021-11-14 15:21:31,871 INFO blockmanagement.BlockReportLeaseManager: Registered DN cced7f96-009c-49df-9116-cc49c6ef35ea (192.168.240.3:9866).
[36malgo-1    |[0m 2021-11-14 15:21:31,874 INFO datanode.DataNode: Block pool Block pool BP-1677061777-192.168.240.3-1636903287577 (Datanode Uuid cced7f96-009c-49df-9116-cc49c6ef35ea) service to algo-1/192.168.240.3:8020 successfully registered with NN
[36malgo-1    |[0m 2021-11-14 15:21:31,874 INFO datanode.DataNode: For namenode algo-1/192.168.240.3:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
[36malgo-1    |[0m 2021-11-14 15:21:31,877 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-82d29f09-aa8e-44f9-85fb-990c9df7f455 for DN 192.168.240.2:9866
[36malgo-1    |[0m 2021-11-14 15:21:31,915 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-5b50ee7f-d3bc-4683-9400-89d766bc6ebc for DN 192.168.240.3:9866
[36malgo-1    |[0m 2021-11-14 15:21:32,055 INFO BlockStateChange: BLOCK* processReport 0x923e8db76a920868: Processing first storage report for DS-5b50ee7f-d3bc-4683-9400-89d766bc6ebc from datanode cced7f96-009c-49df-9116-cc49c6ef35ea
[36malgo-1    |[0m 2021-11-14 15:21:32,057 INFO BlockStateChange: BLOCK* processReport 0x923e8db76a920868: from storage DS-5b50ee7f-d3bc-4683-9400-89d766bc6ebc node DatanodeRegistration(192.168.240.3:9866, datanodeUuid=cced7f96-009c-49df-9116-cc49c6ef35ea, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-f7f17664-f4ce-4750-ac10-52bdfb51d834;nsid=114424355;c=1636903287577), blocks: 0, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0
[36malgo-1    |[0m 2021-11-14 15:21:32,059 INFO BlockStateChange: BLOCK* processReport 0x57bd44d1f5666a4d: Processing first storage report for DS-82d29f09-aa8e-44f9-85fb-990c9df7f455 from datanode 3fcb146c-fa78-4453-a820-a655ecbeb032
[36malgo-1    |[0m 2021-11-14 15:21:32,059 INFO BlockStateChange: BLOCK* processReport 0x57bd44d1f5666a4d: from storage DS-82d29f09-aa8e-44f9-85fb-990c9df7f455 node DatanodeRegistration(192.168.240.2:9866, datanodeUuid=3fcb146c-fa78-4453-a820-a655ecbeb032, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-f7f17664-f4ce-4750-ac10-52bdfb51d834;nsid=114424355;c=1636903287577), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
[36malgo-1    |[0m 2021-11-14 15:21:32,073 INFO store.AbstractFSNodeStore: Created store directory :file:/tmp/hadoop-yarn-root/node-attribute
[36malgo-1    |[0m 2021-11-14 15:21:32,086 INFO store.AbstractFSNodeStore: Finished write mirror at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.mirror
[36malgo-1    |[0m 2021-11-14 15:21:32,086 INFO store.AbstractFSNodeStore: Finished create editlog file at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.editlog
[33malgo-2    |[0m 2021-11-14 15:21:32,088 INFO datanode.DataNode: Successfully sent block report 0x57bd44d1f5666a4d,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 181 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
[33malgo-2    |[0m 2021-11-14 15:21:32,088 INFO datanode.DataNode: Got finalize command for block pool BP-1677061777-192.168.240.3-1636903287577
[36malgo-1    |[0m 2021-11-14 15:21:32,089 INFO datanode.DataNode: Successfully sent block report 0x923e8db76a920868,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 158 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
[36malgo-1    |[0m 2021-11-14 15:21:32,089 INFO datanode.DataNode: Got finalize command for block pool BP-1677061777-192.168.240.3-1636903287577
[36malgo-1    |[0m 2021-11-14 15:21:32,097 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl$ForwardingEventHandler
[36malgo-1    |[0m 2021-11-14 15:21:32,098 INFO placement.MultiNodeSortingManager: Starting NodeSortingService=MultiNodeSortingManager
[36malgo-1    |[0m 2021-11-14 15:21:32,110 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 15:21:32,111 INFO ipc.Server: Starting Socket Reader #1 for port 8031
[36malgo-1    |[0m 2021-11-14 15:21:32,113 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server
[36malgo-1    |[0m 2021-11-14 15:21:32,113 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 15:21:32,113 INFO ipc.Server: IPC Server listener on 8031: starting
[36malgo-1    |[0m 2021-11-14 15:21:32,122 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 15:21:32,131 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 15:21:32,135 INFO ipc.Server: Starting Socket Reader #1 for port 8030
[36malgo-1    |[0m 2021-11-14 15:21:32,142 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 15:21:32,142 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 15:21:32,142 INFO ipc.Server: IPC Server listener on 8030: starting
[36malgo-1    |[0m 2021-11-14 15:21:32,206 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 15:21:32,206 INFO ipc.Server: Starting Socket Reader #1 for port 8032
[36malgo-1    |[0m 2021-11-14 15:21:32,209 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 15:21:32,209 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 15:21:32,209 INFO ipc.Server: IPC Server listener on 8032: starting
[36malgo-1    |[0m 2021-11-14 15:21:32,217 INFO resourcemanager.ResourceManager: Transitioned to active state
[33malgo-2    |[0m 2021-11-14 15:21:32,292 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/192.168.240.3:8031. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 15:21:32,478 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-2(cmPort: 40975 httpPort: 8042) registered with capability: <memory:15892, vCores:4>, assigned nodeId algo-2:40975
[36malgo-1    |[0m 2021-11-14 15:21:32,482 INFO rmnode.RMNodeImpl: algo-2:40975 Node Transitioned from NEW to RUNNING
[33malgo-2    |[0m 2021-11-14 15:21:32,497 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id -1857069384
[33malgo-2    |[0m 2021-11-14 15:21:32,498 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id 235095861
[33malgo-2    |[0m 2021-11-14 15:21:32,498 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-2:40975 with total resource of <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 15:21:32,504 INFO ipc.Client: Retrying connect to server: algo-1/192.168.240.3:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 15:21:32,504 INFO capacity.CapacityScheduler: Added node algo-2:40975 clusterResource: <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 15:21:32,516 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-1(cmPort: 33137 httpPort: 8042) registered with capability: <memory:15892, vCores:4>, assigned nodeId algo-1:33137
[36malgo-1    |[0m 2021-11-14 15:21:32,516 INFO rmnode.RMNodeImpl: algo-1:33137 Node Transitioned from NEW to RUNNING
[36malgo-1    |[0m 2021-11-14 15:21:32,519 INFO capacity.CapacityScheduler: Added node algo-1:33137 clusterResource: <memory:31784, vCores:8>
[36malgo-1    |[0m 2021-11-14 15:21:32,526 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id -1857069384
[36malgo-1    |[0m 2021-11-14 15:21:32,527 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id 235095861
[36malgo-1    |[0m 2021-11-14 15:21:32,528 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-1:33137 with total resource of <memory:15892, vCores:4>
[36malgo-1    |[0m Using properties file: /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m Adding default property: spark.driver.host=192.168.240.3
[36malgo-1    |[0m Adding default property: spark.executor.memoryOverhead=1239m
[36malgo-1    |[0m Adding default property: spark.driver.extraLibraryPath=/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m Adding default property: spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[36malgo-1    |[0m Adding default property: spark.rpc.askTimeout=300s
[36malgo-1    |[0m Adding default property: spark.driver.memory=2048m
[36malgo-1    |[0m Adding default property: spark.executor.instances=2
[36malgo-1    |[0m Adding default property: spark.driver.memoryOverhead=204m
[36malgo-1    |[0m Adding default property: key=value
[36malgo-1    |[0m Adding default property: spark.default.parallelism=16
[36malgo-1    |[0m Adding default property: spark.executor.defaultJavaOptions=-verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3
[36malgo-1    |[0m Adding default property: spark.driver.defaultJavaOptions=-XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[36malgo-1    |[0m Adding default property: spark.executor.extraLibraryPath=/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m Adding default property: spark.executor.memory=2g
[36malgo-1    |[0m Adding default property: spark.driver.extraClassPath=/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m Adding default property: spark.executor.extraClassPath=/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m Adding default property: spark.executor.cores=1
[36malgo-1    |[0m Warning: Ignoring non-Spark config property: key
[36malgo-1    |[0m Parsed arguments:
[36malgo-1    |[0m   master                  yarn
[36malgo-1    |[0m   deployMode              client
[36malgo-1    |[0m   executorMemory          2g
[36malgo-1    |[0m   executorCores           1
[36malgo-1    |[0m   totalExecutorCores      null
[36malgo-1    |[0m   propertiesFile          /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m   driverMemory            2048m
[36malgo-1    |[0m   driverCores             null
[36malgo-1    |[0m   driverExtraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m   driverExtraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m   driverExtraJavaOptions  null
[36malgo-1    |[0m   supervise               false
[36malgo-1    |[0m   queue                   null
[36malgo-1    |[0m   numExecutors            2
[36malgo-1    |[0m   files                   null
[36malgo-1    |[0m   pyFiles                 null
[36malgo-1    |[0m   archives                null
[36malgo-1    |[0m   mainClass               com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp
[36malgo-1    |[0m   primaryResource         file:/opt/ml/processing/input/code/scala/hello-scala-spark/target/scala-2.12/hello-scala-spark_2.12-1.0.jar
[36malgo-1    |[0m   name                    com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp
[36malgo-1    |[0m   childArgs               [--input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data]
[36malgo-1    |[0m   jars                    file:/opt/ml/processing/input/jars/json4s-native_2.12-3.6.9.jar
[36malgo-1    |[0m   packages                null
[36malgo-1    |[0m   packagesExclusions      null
[36malgo-1    |[0m   repositories            null
[36malgo-1    |[0m   verbose                 true
[36malgo-1    |[0m 
[36malgo-1    |[0m Spark properties used, including those specified through
[36malgo-1    |[0m  --conf and those from the properties file /usr/lib/spark/conf/spark-defaults.conf:
[36malgo-1    |[0m   (spark.executor.defaultJavaOptions,-verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3)
[36malgo-1    |[0m   (spark.default.parallelism,16)
[36malgo-1    |[0m   (spark.executor.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)
[36malgo-1    |[0m   (spark.executor.memory,2g)
[36malgo-1    |[0m   (spark.driver.memory,2048m)
[36malgo-1    |[0m   (spark.driver.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)
[36malgo-1    |[0m   (spark.executor.instances,2)
[36malgo-1    |[0m   (spark.executor.memoryOverhead,1239m)
[36malgo-1    |[0m   (spark.rpc.askTimeout,300s)
[36malgo-1    |[0m   (spark.driver.memoryOverhead,204m)
[36malgo-1    |[0m   (spark.driver.host,192.168.240.3)
[36malgo-1    |[0m   (spark.driver.defaultJavaOptions,-XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled)
[36malgo-1    |[0m   (spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version,2)
[36malgo-1    |[0m   (spark.executor.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)
[36malgo-1    |[0m   (spark.executor.cores,1)
[36malgo-1    |[0m   (spark.driver.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)
[36malgo-1    |[0m 
[36malgo-1    |[0m     
[36malgo-1    |[0m Main class:
[36malgo-1    |[0m com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp
[36malgo-1    |[0m Arguments:
[36malgo-1    |[0m --input
[36malgo-1    |[0m file:///opt/ml/processing/input/data/data.jsonl
[36malgo-1    |[0m --output
[36malgo-1    |[0m file:///opt/ml/processing/output/data
[36malgo-1    |[0m Spark config:
[36malgo-1    |[0m (spark.driver.host,192.168.240.3)
[36malgo-1    |[0m (spark.executor.memoryOverhead,1239m)
[36malgo-1    |[0m (spark.driver.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)
[36malgo-1    |[0m (spark.jars,file:/opt/ml/processing/input/code/scala/hello-scala-spark/target/scala-2.12/hello-scala-spark_2.12-1.0.jar)
[36malgo-1    |[0m (spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version,2)
[36malgo-1    |[0m (spark.app.name,com.amazonaws.sagemaker.spark.test.HelloScalaSparkApp)
[36malgo-1    |[0m (spark.rpc.askTimeout,300s)
[36malgo-1    |[0m (spark.driver.memory,2048m)
[36malgo-1    |[0m (spark.executor.instances,2)
[36malgo-1    |[0m (spark.submit.pyFiles,)
[36malgo-1    |[0m (spark.driver.memoryOverhead,204m)
[36malgo-1    |[0m (spark.default.parallelism,16)
[36malgo-1    |[0m (spark.executor.defaultJavaOptions,-verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3)
[36malgo-1    |[0m (spark.submit.deployMode,client)
[36malgo-1    |[0m (spark.master,yarn)
[36malgo-1    |[0m (spark.driver.defaultJavaOptions,-XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled)
[36malgo-1    |[0m (spark.executor.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)
[36malgo-1    |[0m (spark.executor.memory,2g)
[36malgo-1    |[0m (spark.driver.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)
[36malgo-1    |[0m (spark.executor.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)
[36malgo-1    |[0m (spark.executor.cores,1)
[36malgo-1    |[0m (spark.repl.local.jars,file:///opt/ml/processing/input/jars/json4s-native_2.12-3.6.9.jar)
[36malgo-1    |[0m (spark.yarn.dist.jars,file:///opt/ml/processing/input/jars/json4s-native_2.12-3.6.9.jar)
[36malgo-1    |[0m Classpath elements:
[36malgo-1    |[0m file:/opt/ml/processing/input/code/scala/hello-scala-spark/target/scala-2.12/hello-scala-spark_2.12-1.0.jar
[36malgo-1    |[0m file:///opt/ml/processing/input/jars/json4s-native_2.12-3.6.9.jar
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m Hello World, this is Scala-Spark!
[36malgo-1    |[0m Parsing command-line args: --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data
[36malgo-1    |[0m Parsed command-line args: {input -> file:///opt/ml/processing/input/data/data.jsonl, output -> file:///opt/ml/processing/output/data}
[36malgo-1    |[0m Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[36malgo-1    |[0m 21/11/14 15:21:34 INFO SparkContext: Running Spark version 3.0.0-amzn-0
[36malgo-1    |[0m 21/11/14 15:21:34 INFO ResourceUtils: ==============================================================
[36malgo-1    |[0m 21/11/14 15:21:34 INFO ResourceUtils: Resources for spark.driver:
[36malgo-1    |[0m 
[36malgo-1    |[0m 21/11/14 15:21:34 INFO ResourceUtils: ==============================================================
[36malgo-1    |[0m 21/11/14 15:21:34 INFO SparkContext: Submitted application: Hello Spark App
[36malgo-1    |[0m 21/11/14 15:21:34 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m 21/11/14 15:21:34 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m 21/11/14 15:21:34 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m 21/11/14 15:21:34 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m 21/11/14 15:21:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m 21/11/14 15:21:34 INFO Utils: Successfully started service 'sparkDriver' on port 41511.
[36malgo-1    |[0m 21/11/14 15:21:34 INFO SparkEnv: Registering MapOutputTracker
[36malgo-1    |[0m 21/11/14 15:21:34 INFO SparkEnv: Registering BlockManagerMaster
[36malgo-1    |[0m 21/11/14 15:21:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[36malgo-1    |[0m 21/11/14 15:21:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[36malgo-1    |[0m 21/11/14 15:21:34 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[36malgo-1    |[0m 21/11/14 15:21:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c32df4b2-cd00-4354-bda2-c6920d3f6719
[36malgo-1    |[0m 21/11/14 15:21:34 INFO MemoryStore: MemoryStore started with capacity 1007.8 MiB
[36malgo-1    |[0m 21/11/14 15:21:34 INFO SparkEnv: Registering OutputCommitCoordinator
[36malgo-1    |[0m 21/11/14 15:21:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[36malgo-1    |[0m 21/11/14 15:21:35 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.240.3:4040
[36malgo-1    |[0m 21/11/14 15:21:35 INFO SparkContext: Added JAR file:/opt/ml/processing/input/code/scala/hello-scala-spark/target/scala-2.12/hello-scala-spark_2.12-1.0.jar at spark://192.168.240.3:41511/jars/hello-scala-spark_2.12-1.0.jar with timestamp 1636903295031
[36malgo-1    |[0m 21/11/14 15:21:35 INFO RMProxy: Connecting to ResourceManager at /192.168.240.3:8032
[36malgo-1    |[0m 21/11/14 15:21:35 INFO Client: Requesting a new application from cluster with 2 NodeManagers
[36malgo-1    |[0m 2021-11-14 15:21:35,589 INFO resourcemanager.ClientRMService: Allocated new applicationId: 1
[36malgo-1    |[0m 21/11/14 15:21:36 INFO Configuration: resource-types.xml not found
[36malgo-1    |[0m 21/11/14 15:21:36 INFO ResourceUtils: Unable to find 'resource-types.xml'.
[36malgo-1    |[0m 21/11/14 15:21:36 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15892 MB per container)
[36malgo-1    |[0m 21/11/14 15:21:36 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
[36malgo-1    |[0m 21/11/14 15:21:36 INFO Client: Setting up container launch context for our AM
[36malgo-1    |[0m 21/11/14 15:21:36 INFO Client: Setting up the launch environment for our AM container
[36malgo-1    |[0m 21/11/14 15:21:36 INFO Client: Preparing resources for our AM container
[36malgo-1    |[0m 21/11/14 15:21:36 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
[36malgo-1    |[0m 21/11/14 15:21:41 INFO Client: Uploading resource file:/tmp/spark-2683eb3f-8962-49e0-b873-17f3ee141200/__spark_libs__4802459339841135843.zip -> hdfs://192.168.240.3/user/root/.sparkStaging/application_1636903291807_0001/__spark_libs__4802459339841135843.zip
[36malgo-1    |[0m 2021-11-14 15:21:42,112 INFO hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=192.168.240.3:9866, 192.168.240.2:9866 for /user/root/.sparkStaging/application_1636903291807_0001/__spark_libs__4802459339841135843.zip
[36malgo-1    |[0m 2021-11-14 15:21:42,241 INFO datanode.DataNode: Receiving BP-1677061777-192.168.240.3-1636903287577:blk_1073741825_1001 src: /192.168.240.3:39348 dest: /192.168.240.3:9866
[33malgo-2    |[0m 2021-11-14 15:21:42,303 INFO datanode.DataNode: Receiving BP-1677061777-192.168.240.3-1636903287577:blk_1073741825_1001 src: /192.168.240.3:59124 dest: /192.168.240.2:9866
[33malgo-2    |[0m 2021-11-14 15:21:42,717 INFO DataNode.clienttrace: src: /192.168.240.3:59124, dest: /192.168.240.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2105280782_1, offset: 0, srvID: 3fcb146c-fa78-4453-a820-a655ecbeb032, blockid: BP-1677061777-192.168.240.3-1636903287577:blk_1073741825_1001, duration(ns): 391365711
[33malgo-2    |[0m 2021-11-14 15:21:42,717 INFO datanode.DataNode: PacketResponder: BP-1677061777-192.168.240.3-1636903287577:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 15:21:42,721 INFO DataNode.clienttrace: src: /192.168.240.3:39348, dest: /192.168.240.3:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2105280782_1, offset: 0, srvID: cced7f96-009c-49df-9116-cc49c6ef35ea, blockid: BP-1677061777-192.168.240.3-1636903287577:blk_1073741825_1001, duration(ns): 390038490
[36malgo-1    |[0m 2021-11-14 15:21:42,721 INFO datanode.DataNode: PacketResponder: BP-1677061777-192.168.240.3-1636903287577:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.240.2:9866] terminating
[36malgo-1    |[0m 2021-11-14 15:21:42,728 INFO hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=192.168.240.3:9866, 192.168.240.2:9866 for /user/root/.sparkStaging/application_1636903291807_0001/__spark_libs__4802459339841135843.zip
[36malgo-1    |[0m 2021-11-14 15:21:42,731 INFO datanode.DataNode: Receiving BP-1677061777-192.168.240.3-1636903287577:blk_1073741826_1002 src: /192.168.240.3:39352 dest: /192.168.240.3:9866
[33malgo-2    |[0m 2021-11-14 15:21:42,733 INFO datanode.DataNode: Receiving BP-1677061777-192.168.240.3-1636903287577:blk_1073741826_1002 src: /192.168.240.3:59128 dest: /192.168.240.2:9866
[33malgo-2    |[0m 2021-11-14 15:21:43,052 INFO DataNode.clienttrace: src: /192.168.240.3:59128, dest: /192.168.240.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2105280782_1, offset: 0, srvID: 3fcb146c-fa78-4453-a820-a655ecbeb032, blockid: BP-1677061777-192.168.240.3-1636903287577:blk_1073741826_1002, duration(ns): 317177225
[33malgo-2    |[0m 2021-11-14 15:21:43,052 INFO datanode.DataNode: PacketResponder: BP-1677061777-192.168.240.3-1636903287577:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 15:21:43,053 INFO DataNode.clienttrace: src: /192.168.240.3:39352, dest: /192.168.240.3:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2105280782_1, offset: 0, srvID: cced7f96-009c-49df-9116-cc49c6ef35ea, blockid: BP-1677061777-192.168.240.3-1636903287577:blk_1073741826_1002, duration(ns): 318223285
[36malgo-1    |[0m 2021-11-14 15:21:43,053 INFO datanode.DataNode: PacketResponder: BP-1677061777-192.168.240.3-1636903287577:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.240.2:9866] terminating
[36malgo-1    |[0m 2021-11-14 15:21:43,055 INFO hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=192.168.240.3:9866, 192.168.240.2:9866 for /user/root/.sparkStaging/application_1636903291807_0001/__spark_libs__4802459339841135843.zip
[36malgo-1    |[0m 2021-11-14 15:21:43,058 INFO datanode.DataNode: Receiving BP-1677061777-192.168.240.3-1636903287577:blk_1073741827_1003 src: /192.168.240.3:39356 dest: /192.168.240.3:9866
[33malgo-2    |[0m 2021-11-14 15:21:43,060 INFO datanode.DataNode: Receiving BP-1677061777-192.168.240.3-1636903287577:blk_1073741827_1003 src: /192.168.240.3:59132 dest: /192.168.240.2:9866
[33malgo-2    |[0m 2021-11-14 15:21:43,367 INFO DataNode.clienttrace: src: /192.168.240.3:59132, dest: /192.168.240.2:9866, bytes: 128628638, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2105280782_1, offset: 0, srvID: 3fcb146c-fa78-4453-a820-a655ecbeb032, blockid: BP-1677061777-192.168.240.3-1636903287577:blk_1073741827_1003, duration(ns): 305826156
[33malgo-2    |[0m 2021-11-14 15:21:43,367 INFO datanode.DataNode: PacketResponder: BP-1677061777-192.168.240.3-1636903287577:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 15:21:43,369 INFO DataNode.clienttrace: src: /192.168.240.3:39356, dest: /192.168.240.3:9866, bytes: 128628638, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2105280782_1, offset: 0, srvID: cced7f96-009c-49df-9116-cc49c6ef35ea, blockid: BP-1677061777-192.168.240.3-1636903287577:blk_1073741827_1003, duration(ns): 306935270
[36malgo-1    |[0m 2021-11-14 15:21:43,369 INFO datanode.DataNode: PacketResponder: BP-1677061777-192.168.240.3-1636903287577:blk_1073741827_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.240.2:9866] terminating
[36malgo-1    |[0m 2021-11-14 15:21:43,374 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1636903291807_0001/__spark_libs__4802459339841135843.zip is closed by DFSClient_NONMAPREDUCE_2105280782_1
[36malgo-1    |[0m 21/11/14 15:21:43 INFO Client: Uploading resource file:/opt/ml/processing/input/jars/json4s-native_2.12-3.6.9.jar -> hdfs://192.168.240.3/user/root/.sparkStaging/application_1636903291807_0001/json4s-native_2.12-3.6.9.jar
[36malgo-1    |[0m 2021-11-14 15:21:43,470 INFO hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=192.168.240.3:9866, 192.168.240.2:9866 for /user/root/.sparkStaging/application_1636903291807_0001/json4s-native_2.12-3.6.9.jar
[36malgo-1    |[0m 2021-11-14 15:21:43,474 INFO datanode.DataNode: Receiving BP-1677061777-192.168.240.3-1636903287577:blk_1073741828_1004 src: /192.168.240.3:39360 dest: /192.168.240.3:9866
[33malgo-2    |[0m 2021-11-14 15:21:43,475 INFO datanode.DataNode: Receiving BP-1677061777-192.168.240.3-1636903287577:blk_1073741828_1004 src: /192.168.240.3:59136 dest: /192.168.240.2:9866
[33malgo-2    |[0m 2021-11-14 15:21:43,479 INFO DataNode.clienttrace: src: /192.168.240.3:59136, dest: /192.168.240.2:9866, bytes: 93283, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2105280782_1, offset: 0, srvID: 3fcb146c-fa78-4453-a820-a655ecbeb032, blockid: BP-1677061777-192.168.240.3-1636903287577:blk_1073741828_1004, duration(ns): 2182983
[33malgo-2    |[0m 2021-11-14 15:21:43,479 INFO datanode.DataNode: PacketResponder: BP-1677061777-192.168.240.3-1636903287577:blk_1073741828_1004, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 15:21:43,480 INFO DataNode.clienttrace: src: /192.168.240.3:39360, dest: /192.168.240.3:9866, bytes: 93283, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2105280782_1, offset: 0, srvID: cced7f96-009c-49df-9116-cc49c6ef35ea, blockid: BP-1677061777-192.168.240.3-1636903287577:blk_1073741828_1004, duration(ns): 2993213
[36malgo-1    |[0m 2021-11-14 15:21:43,480 INFO datanode.DataNode: PacketResponder: BP-1677061777-192.168.240.3-1636903287577:blk_1073741828_1004, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.240.2:9866] terminating
[36malgo-1    |[0m 2021-11-14 15:21:43,481 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1636903291807_0001/json4s-native_2.12-3.6.9.jar is closed by DFSClient_NONMAPREDUCE_2105280782_1
[36malgo-1    |[0m 21/11/14 15:21:43 INFO Client: Uploading resource file:/tmp/spark-2683eb3f-8962-49e0-b873-17f3ee141200/__spark_conf__6963778243465608092.zip -> hdfs://192.168.240.3/user/root/.sparkStaging/application_1636903291807_0001/__spark_conf__.zip
[36malgo-1    |[0m 2021-11-14 15:21:43,599 INFO hdfs.StateChange: BLOCK* allocate blk_1073741829_1005, replicas=192.168.240.3:9866, 192.168.240.2:9866 for /user/root/.sparkStaging/application_1636903291807_0001/__spark_conf__.zip
[36malgo-1    |[0m 2021-11-14 15:21:43,602 INFO datanode.DataNode: Receiving BP-1677061777-192.168.240.3-1636903287577:blk_1073741829_1005 src: /192.168.240.3:39364 dest: /192.168.240.3:9866
[33malgo-2    |[0m 2021-11-14 15:21:43,604 INFO datanode.DataNode: Receiving BP-1677061777-192.168.240.3-1636903287577:blk_1073741829_1005 src: /192.168.240.3:59140 dest: /192.168.240.2:9866
[33malgo-2    |[0m 2021-11-14 15:21:43,607 INFO DataNode.clienttrace: src: /192.168.240.3:59140, dest: /192.168.240.2:9866, bytes: 252294, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2105280782_1, offset: 0, srvID: 3fcb146c-fa78-4453-a820-a655ecbeb032, blockid: BP-1677061777-192.168.240.3-1636903287577:blk_1073741829_1005, duration(ns): 2324869
[33malgo-2    |[0m 2021-11-14 15:21:43,607 INFO datanode.DataNode: PacketResponder: BP-1677061777-192.168.240.3-1636903287577:blk_1073741829_1005, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 15:21:43,609 INFO DataNode.clienttrace: src: /192.168.240.3:39364, dest: /192.168.240.3:9866, bytes: 252294, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2105280782_1, offset: 0, srvID: cced7f96-009c-49df-9116-cc49c6ef35ea, blockid: BP-1677061777-192.168.240.3-1636903287577:blk_1073741829_1005, duration(ns): 3040913
[36malgo-1    |[0m 2021-11-14 15:21:43,609 INFO datanode.DataNode: PacketResponder: BP-1677061777-192.168.240.3-1636903287577:blk_1073741829_1005, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.240.2:9866] terminating
[36malgo-1    |[0m 2021-11-14 15:21:43,610 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1636903291807_0001/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_2105280782_1
[36malgo-1    |[0m 21/11/14 15:21:43 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m 21/11/14 15:21:43 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m 21/11/14 15:21:43 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m 21/11/14 15:21:43 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m 21/11/14 15:21:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m 21/11/14 15:21:43 INFO Client: Submitting application application_1636903291807_0001 to ResourceManager
[36malgo-1    |[0m 2021-11-14 15:21:43,746 INFO capacity.CapacityScheduler: Application 'application_1636903291807_0001' is submitted without priority hence considering default queue/cluster priority: 0
[36malgo-1    |[0m 2021-11-14 15:21:43,746 INFO capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1636903291807_0001
[36malgo-1    |[0m 2021-11-14 15:21:43,760 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 1]. Use the global max attempts instead.
[36malgo-1    |[0m 2021-11-14 15:21:43,762 INFO resourcemanager.ClientRMService: Application with id 1 submitted by user root
[36malgo-1    |[0m 2021-11-14 15:21:43,762 INFO rmapp.RMAppImpl: Storing application with id application_1636903291807_0001
[36malgo-1    |[0m 2021-11-14 15:21:43,764 INFO resourcemanager.RMAuditLogger: USER=root	IP=192.168.240.3	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1636903291807_0001	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 15:21:43,769 INFO recovery.RMStateStore: Storing info for app: application_1636903291807_0001
[36malgo-1    |[0m 2021-11-14 15:21:43,769 INFO rmapp.RMAppImpl: application_1636903291807_0001 State change from NEW to NEW_SAVING on event = START
[36malgo-1    |[0m 2021-11-14 15:21:43,770 INFO rmapp.RMAppImpl: application_1636903291807_0001 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
[36malgo-1    |[0m 2021-11-14 15:21:43,771 INFO capacity.ParentQueue: Application added - appId: application_1636903291807_0001 user: root leaf-queue of parent: root #applications: 1
[36malgo-1    |[0m 2021-11-14 15:21:43,771 INFO capacity.CapacityScheduler: Accepted application application_1636903291807_0001 from user: root, in queue: default
[36malgo-1    |[0m 2021-11-14 15:21:43,781 INFO rmapp.RMAppImpl: application_1636903291807_0001 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
[36malgo-1    |[0m 2021-11-14 15:21:43,804 INFO resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1636903291807_0001_000001
[36malgo-1    |[0m 2021-11-14 15:21:43,805 INFO attempt.RMAppAttemptImpl: appattempt_1636903291807_0001_000001 State change from NEW to SUBMITTED on event = START
[36malgo-1    |[0m 21/11/14 15:21:43 INFO YarnClientImpl: Submitted application application_1636903291807_0001
[36malgo-1    |[0m 2021-11-14 15:21:43,854 INFO capacity.LeafQueue: Application application_1636903291807_0001 from user: root activated in queue: default
[36malgo-1    |[0m 2021-11-14 15:21:43,854 INFO capacity.LeafQueue: Application added - appId: application_1636903291807_0001 user: root, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
[36malgo-1    |[0m 2021-11-14 15:21:43,854 INFO capacity.CapacityScheduler: Added Application Attempt appattempt_1636903291807_0001_000001 to scheduler from user root in queue default
[36malgo-1    |[0m 2021-11-14 15:21:43,860 INFO attempt.RMAppAttemptImpl: appattempt_1636903291807_0001_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
[36malgo-1    |[0m 2021-11-14 15:21:44,615 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1636903291807_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=
[36malgo-1    |[0m 2021-11-14 15:21:44,619 INFO rmcontainer.RMContainerImpl: container_1636903291807_0001_01_000001 Container Transitioned from NEW to ALLOCATED
[36malgo-1    |[0m 2021-11-14 15:21:44,620 INFO fica.FiCaSchedulerNode: Assigned container container_1636903291807_0001_01_000001 of capacity <memory:896, vCores:1> on host algo-2:40975, which has 1 containers, <memory:896, vCores:1> used and <memory:14996, vCores:3> available after allocation
[36malgo-1    |[0m 2021-11-14 15:21:44,620 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636903291807_0001	CONTAINERID=container_1636903291807_0001_01_000001	RESOURCE=<memory:896, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 15:21:44,639 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-2:40975 for container : container_1636903291807_0001_01_000001
[36malgo-1    |[0m 2021-11-14 15:21:44,647 INFO rmcontainer.RMContainerImpl: container_1636903291807_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
[36malgo-1    |[0m 2021-11-14 15:21:44,647 INFO security.NMTokenSecretManagerInRM: Clear node set for appattempt_1636903291807_0001_000001
[36malgo-1    |[0m 2021-11-14 15:21:44,647 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.028190285 absoluteUsedCapacity=0.028190285 used=<memory:896, vCores:1> cluster=<memory:31784, vCores:8>
[36malgo-1    |[0m 2021-11-14 15:21:44,648 INFO capacity.CapacityScheduler: Allocation proposal accepted
[36malgo-1    |[0m 2021-11-14 15:21:44,648 INFO attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1636903291807_0001 AttemptId: appattempt_1636903291807_0001_000001 MasterContainer: Container: [ContainerId: container_1636903291807_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-2:40975, NodeHttpAddress: algo-2:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.240.2:40975 }, ExecutionType: GUARANTEED, ]
[36malgo-1    |[0m 2021-11-14 15:21:44,659 INFO attempt.RMAppAttemptImpl: appattempt_1636903291807_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
[36malgo-1    |[0m 2021-11-14 15:21:44,662 INFO attempt.RMAppAttemptImpl: appattempt_1636903291807_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
[36malgo-1    |[0m 2021-11-14 15:21:44,673 INFO amlauncher.AMLauncher: Launching masterappattempt_1636903291807_0001_000001
[36malgo-1    |[0m 2021-11-14 15:21:44,725 INFO amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1636903291807_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-2:40975, NodeHttpAddress: algo-2:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.240.2:40975 }, ExecutionType: GUARANTEED, ] for AM appattempt_1636903291807_0001_000001
[36malgo-1    |[0m 2021-11-14 15:21:44,726 INFO security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1636903291807_0001_000001
[36malgo-1    |[0m 2021-11-14 15:21:44,729 INFO security.AMRMTokenSecretManager: Creating password for appattempt_1636903291807_0001_000001
[33malgo-2    |[0m 2021-11-14 15:21:44,845 INFO ipc.Server: Auth successful for appattempt_1636903291807_0001_000001 (auth:SIMPLE)
[36malgo-1    |[0m 21/11/14 15:21:44 INFO Client: Application report for application_1636903291807_0001 (state: ACCEPTED)
[36malgo-1    |[0m 21/11/14 15:21:44 INFO Client: 
[36malgo-1    |[0m 	 client token: N/A
[36malgo-1    |[0m 	 diagnostics: [Sun Nov 14 15:21:44 +0000 2021] Scheduler has assigned a container for AM, waiting for AM container to be launched
[36malgo-1    |[0m 	 ApplicationMaster host: N/A
[36malgo-1    |[0m 	 ApplicationMaster RPC port: -1
[36malgo-1    |[0m 	 queue: default
[36malgo-1    |[0m 	 start time: 1636903303760
[36malgo-1    |[0m 	 final status: UNDEFINED
[36malgo-1    |[0m 	 tracking URL: http://algo-1:8088/proxy/application_1636903291807_0001/
[36malgo-1    |[0m 	 user: root
[33malgo-2    |[0m 2021-11-14 15:21:44,927 INFO containermanager.ContainerManagerImpl: Start request for container_1636903291807_0001_01_000001 by user root
[33malgo-2    |[0m 2021-11-14 15:21:44,973 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1636903291807_0001
[33malgo-2    |[0m 2021-11-14 15:21:44,980 INFO application.ApplicationImpl: Application application_1636903291807_0001 transitioned from NEW to INITING
[33malgo-2    |[0m 2021-11-14 15:21:44,980 INFO nodemanager.NMAuditLogger: USER=root	IP=192.168.240.3	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1636903291807_0001	CONTAINERID=container_1636903291807_0001_01_000001
[33malgo-2    |[0m 2021-11-14 15:21:44,980 INFO application.ApplicationImpl: Adding container_1636903291807_0001_01_000001 to application application_1636903291807_0001
[33malgo-2    |[0m 2021-11-14 15:21:44,984 INFO application.ApplicationImpl: Application application_1636903291807_0001 transitioned from INITING to RUNNING
[33malgo-2    |[0m 2021-11-14 15:21:44,986 INFO container.ContainerImpl: Container container_1636903291807_0001_01_000001 transitioned from NEW to LOCALIZING
[33malgo-2    |[0m 2021-11-14 15:21:44,987 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1636903291807_0001
[36malgo-1    |[0m 2021-11-14 15:21:44,992 INFO amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1636903291807_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-2:40975, NodeHttpAddress: algo-2:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.240.2:40975 }, ExecutionType: GUARANTEED, ] for AM appattempt_1636903291807_0001_000001
[36malgo-1    |[0m 2021-11-14 15:21:44,993 INFO attempt.RMAppAttemptImpl: appattempt_1636903291807_0001_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
[36malgo-1    |[0m 2021-11-14 15:21:44,993 INFO rmapp.RMAppImpl: update the launch time for applicationId: application_1636903291807_0001, attemptId: appattempt_1636903291807_0001_000001launchTime: 1636903304992
[36malgo-1    |[0m 2021-11-14 15:21:44,993 INFO recovery.RMStateStore: Updating info for app: application_1636903291807_0001
[33malgo-2    |[0m 2021-11-14 15:21:44,995 INFO localizer.ResourceLocalizationService: Created localizer for container_1636903291807_0001_01_000001
[33malgo-2    |[0m 2021-11-14 15:21:45,060 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1636903291807_0001_01_000001.tokens
[33malgo-2    |[0m 2021-11-14 15:21:45,074 INFO nodemanager.DefaultContainerExecutor: Initializing user root
[33malgo-2    |[0m 2021-11-14 15:21:45,079 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1636903291807_0001_01_000001.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000001.tokens
[33malgo-2    |[0m 2021-11-14 15:21:45,079 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001
[36malgo-1    |[0m 2021-11-14 15:21:45,605 INFO rmcontainer.RMContainerImpl: container_1636903291807_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING
[36malgo-1    |[0m 21/11/14 15:21:45 INFO Client: Application report for application_1636903291807_0001 (state: ACCEPTED)
[36malgo-1    |[0m 21/11/14 15:21:46 INFO Client: Application report for application_1636903291807_0001 (state: ACCEPTED)
[36malgo-1    |[0m 21/11/14 15:21:47 INFO Client: Application report for application_1636903291807_0001 (state: ACCEPTED)
[33malgo-2    |[0m 2021-11-14 15:21:48,332 INFO container.ContainerImpl: Container container_1636903291807_0001_01_000001 transitioned from LOCALIZING to SCHEDULED
[33malgo-2    |[0m 2021-11-14 15:21:48,333 INFO scheduler.ContainerScheduler: Starting container [container_1636903291807_0001_01_000001]
[33malgo-2    |[0m 2021-11-14 15:21:48,356 INFO container.ContainerImpl: Container container_1636903291807_0001_01_000001 transitioned from SCHEDULED to RUNNING
[33malgo-2    |[0m 2021-11-14 15:21:48,357 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1636903291807_0001_01_000001
[33malgo-2    |[0m 2021-11-14 15:21:48,360 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000001/default_container_executor.sh]
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/prelaunch.out
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/prelaunch.err
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/launch_container.sh
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/directory.info
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/directory.info] 1553046    4 -r-x------   1 root     root         1162 Nov 14 15:21 ./__spark_conf__/__hadoop_conf__/core-site.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/directory.info] 1553055    4 -r-x------   1 root     root         2316 Nov 14 15:21 ./__spark_conf__/__hadoop_conf__/ssl-client.xml.example
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/directory.info] 1553048    4 -r-x------   1 root     root         3593 Nov 14 15:21 ./__spark_conf__/__hadoop_conf__/container-log4j.properties.default
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/directory.info] 1553061    8 -r-x------   1 root     root         4113 Nov 14 15:21 ./__spark_conf__/__hadoop_conf__/mapred-queues.xml.template
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/directory.info] 1553054    4 -r-x------   1 root     root          188 Nov 14 15:21 ./__spark_conf__/__hadoop_conf__/hive-site.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/directory.info] 1553043    4 -r-x------   1 root     root         1976 Nov 14 15:21 ./__spark_conf__/__hadoop_conf__/yarn-site.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/directory.info] 1553042    4 -r-x------   1 root     root         3321 Nov 14 15:21 ./__spark_conf__/__hadoop_conf__/hadoop-metrics2.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/directory.info] 1553051    8 -r-x------   1 root     root         6174 Nov 14 15:21 ./__spark_conf__/__hadoop_conf__/yarn-env.sh
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/directory.info] 1553059    4 -r-x------   1 root     root         2697 Nov 14 15:21 ./__spark_conf__/__hadoop_conf__/ssl-server.xml.example
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/directory.info] 1553056   16 -r-x------   1 root     root        14890 Nov 14 15:21 ./__spark_conf__/__hadoop_conf__/log4j.properties.default
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/directory.info] 1553044   12 -r-x------   1 root     root         8260 Nov 14 15:21 ./__spark_conf__/__hadoop_conf__/capacity-scheduler.xml.default
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/directory.info] 1553041   20 -r-x------   1 root     root        16406 Nov 14 15:21 ./__spark_conf__/__hadoop_conf__/hadoop-env.sh
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/directory.info] 1553053    4 -r-x------   1 root     root         1764 Nov 14 15:21 ./__spark_conf__/__hadoop_conf__/mapred-env.sh
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/directory.info] 1553040    4 -r-x------   1 root     root          758 Nov 14 15:21 ./__spark_conf__/__hadoop_conf__/mapred-site.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/directory.info] 1553039   16 -r-x------   1 root     root        14900 Nov 14 15:21 ./__spark_conf__/__hadoop_conf__/log4j.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/directory.info] 1553047   12 -r-x------   1 root     root         8260 Nov 14 15:21 ./__spark_conf__/__hadoop_conf__/capacity-scheduler.xml
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/directory.info] 1553052    4 -r-x------   1 root     root         1940 Nov 14 15:21 ./__spark_conf__/__hadoop_conf__/container-executor.cfg
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/applicaHandling create event for file: /var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stdout
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr
[33malgo-2    |[0m 2021-11-14 15:21:48,634 INFO monitor.ContainersMonitorImpl: container_1636903291807_0001_01_000001's ip = 192.168.240.2, and hostname = algo-2
[33malgo-2    |[0m 2021-11-14 15:21:48,639 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1636903291807_0001_01_000001 since CPU usage is not yet available.
[36malgo-1    |[0m 21/11/14 15:21:48 INFO Client: Application report for application_1636903291807_0001 (state: ACCEPTED)
[36malgo-1    |[0m 21/11/14 15:21:49 INFO Client: Application report for application_1636903291807_0001 (state: ACCEPTED)
[36malgo-1    |[0m 2021-11-14 15:21:50,601 INFO ipc.Server: Auth successful for appattempt_1636903291807_0001_000001 (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 15:21:50,627 INFO resourcemanager.DefaultAMSProcessor: AM registration appattempt_1636903291807_0001_000001
[36malgo-1    |[0m 2021-11-14 15:21:50,628 INFO resourcemanager.RMAuditLogger: USER=root	IP=192.168.240.2	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1636903291807_0001	APPATTEMPTID=appattempt_1636903291807_0001_000001
[36malgo-1    |[0m 2021-11-14 15:21:50,628 INFO attempt.RMAppAttemptImpl: appattempt_1636903291807_0001_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
[36malgo-1    |[0m 2021-11-14 15:21:50,629 INFO rmapp.RMAppImpl: application_1636903291807_0001 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
[36malgo-1    |[0m 21/11/14 15:21:50 INFO Client: Application report for application_1636903291807_0001 (state: RUNNING)
[36malgo-1    |[0m 21/11/14 15:21:50 INFO Client: 
[36malgo-1    |[0m 	 client token: N/A
[36malgo-1    |[0m 	 diagnostics: N/A
[36malgo-1    |[0m 	 ApplicationMaster host: 192.168.240.2
[36malgo-1    |[0m 	 ApplicationMaster RPC port: -1
[36malgo-1    |[0m 	 queue: default
[36malgo-1    |[0m 	 start time: 1636903303760
[36malgo-1    |[0m 	 final status: UNDEFINED
[36malgo-1    |[0m 	 tracking URL: http://algo-1:8088/proxy/application_1636903291807_0001/
[36malgo-1    |[0m 	 user: root
[36malgo-1    |[0m 21/11/14 15:21:50 INFO YarnClientSchedulerBackend: Application application_1636903291807_0001 has started running.
[36malgo-1    |[0m 21/11/14 15:21:50 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1.spark-network, PROXY_URI_BASES -> http://algo-1.spark-network:8088/proxy/application_1636903291807_0001), /proxy/application_1636903291807_0001
[36malgo-1    |[0m 21/11/14 15:21:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40105.
[36malgo-1    |[0m 21/11/14 15:21:50 INFO NettyBlockTransferService: Server created on 192.168.240.3:40105
[36malgo-1    |[0m 21/11/14 15:21:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[36malgo-1    |[0m 21/11/14 15:21:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.240.3, 40105, None)
[36malgo-1    |[0m 21/11/14 15:21:50 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.240.3:40105 with 1007.8 MiB RAM, BlockManagerId(driver, 192.168.240.3, 40105, None)
[36malgo-1    |[0m 21/11/14 15:21:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.240.3, 40105, None)
[36malgo-1    |[0m 21/11/14 15:21:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.240.3, 40105, None)
[36malgo-1    |[0m 21/11/14 15:21:51 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 15:21:51 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[36malgo-1    |[0m Got a Spark session with version: 3.0.0-amzn-0
[36malgo-1    |[0m 21/11/14 15:21:51 INFO SparkContext: Starting job: foreach at HelloScalaSparkApp.scala:39
[36malgo-1    |[0m 21/11/14 15:21:51 INFO DAGScheduler: Got job 0 (foreach at HelloScalaSparkApp.scala:39) with 16 output partitions
[36malgo-1    |[0m 21/11/14 15:21:51 INFO DAGScheduler: Final stage: ResultStage 0 (foreach at HelloScalaSparkApp.scala:39)
[36malgo-1    |[0m 21/11/14 15:21:51 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:21:51 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:21:51 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at HelloScalaSparkApp.scala:39), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:21:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.3 KiB, free 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:21:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1410.0 B, free 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:21:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.240.3:40105 (size: 1410.0 B, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:21:51 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:21:51 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
[36malgo-1    |[0m 21/11/14 15:21:51 INFO DAGScheduler: Submitting 16 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at HelloScalaSparkApp.scala:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[36malgo-1    |[0m 21/11/14 15:21:51 INFO YarnScheduler: Adding task set 0.0 with 16 tasks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr] Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr] 21/11/14 15:21:49 INFO SignalUtils: Registered signal handler for TERM
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr] 21/11/14 15:21:49 INFO SignalUtils: Registered signal handler for HUP
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr] 21/11/14 15:21:49 INFO SignalUtils: Registered signal handler for INT
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr] 21/11/14 15:21:49 INFO SecurityManager: Changing view acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr] 21/11/14 15:21:49 INFO SecurityManager: Changing modify acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr] 21/11/14 15:21:49 INFO SecurityManager: Changing view acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr] 21/11/14 15:21:49 INFO SecurityManager: Changing modify acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr] 21/11/14 15:21:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr] 21/11/14 15:21:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr] 21/11/14 15:21:50 INFO ApplicationMaster: ApplicationAttemptId: appattempt_1636903291807_0001_000001
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr] 21/11/14 15:21:50 INFO RMProxy: Connecting to ResourceManager at /192.168.240.3:8030
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr] 21/11/14 15:21:50 INFO YarnRMClient: Registering the ApplicationMaster
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr] 21/11/14 15:21:50 INFO TransportClientFactory: Successfully created connection to /192.168.240.3:41511 after 81 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr] 21/11/14 15:21:50 INFO ApplicationMaster: Preparing Local resources
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr] 21/11/14 15:21:51 INFO ApplicationMaster: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr] ===============================================================================
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr] Default YARN executor launch context:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]   env:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]     CLASSPATH -> /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]     SPARK_YARN_STAGING_DIR -> hdfs://192.168.240.3/user/root/.sparkStaging/application_1636903291807_0001
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]     SPARK_USER -> root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]   command:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]     LD_LIBRARY_PATH=\"/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:$LD_LIBRARY_PATH\" \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       {{JAVA_HOME}}/bin/java \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       -server \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       -Xmx2048m \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       '-verbose:gc' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       '-XX:OnOutOfMemoryError=kill -9 %p' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       '-XX:+PrintGCDetails' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       '-XX:+PrintGCDateStamps' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       '-XX:+UseParallelGC' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       '-XX:InitiatingHeapOccupancyPercent=70' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       '-XX:ConcGCThreads=1' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       '-XX:ParallelGCThreads=3' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       -Djava.io.tmpdir={{PWD}}/tmp \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       '-Dspark.driver.port=41511' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       '-Dspark.rpc.askTimeout=300s' \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       -Dspark.yarn.app.container.log.dir=<LOG_DIR> \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       org.apache.spark.executor.YarnCoarseGrainedExecutorBackend \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       --driver-url \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       spark://CoarseGrainedScheduler@192.168.240.3:41511 \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       --executor-id \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       <executorId> \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       --hostname \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       <hostname> \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       --cores \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       1 \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       --app-id \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       application_1636903291807_0001 \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       --resourceProfileId \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       0 \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       --user-class-path \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       file:$PWD/__app__.jar \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       --user-class-path \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       file:$PWD/json4s-native_2.12-3.6.9.jar \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       1><LOG_DIR>/stdout \ 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]       2><LOG_DIR>/stderr
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]   resources:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]     __spark_libs__ -> resource { scheme: "hdfs" host: "192.168.240.3" port: -1 file: "/user/root/.sparkStaging/application_1636903291807_0001/__spark_libs__4802459339841135843.zip" } size: 397064094 timestamp: 1636903303374 type: ARCHIVE visibility: PRIVATE
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]     __spark_conf__ -> resource { scheme: "hdfs" host: "192.168.240.3" port: -1 file: "/user/root/.sparkStaging/application_1636903291807_0001/__spark_conf__.zip" } size: 252294 timestamp: 1636903303610 type: ARCHIVE visibility: PRIVATE
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr]     json4s-native_2.12-3.6.9.jar -> resource { scheme: "hdfs" host: "192.168.240.3" port: -1 file: "/user/root/.sparkStaging/application_1636903291807_0001/json4s-native_2.12-3.6.9.jar" } size: 93283 timestamp: 1636903303481 type: FILE visibility: PRIVATE
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr] ===============================================================================
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000001/stderr] 21/11/14 15:21:51 INFO Configuration: resource-types.xml not found
[36malgo-1    |[0m 2021-11-14 15:21:52,609 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1636903291807_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=
[36malgo-1    |[0m 2021-11-14 15:21:52,609 INFO rmcontainer.RMContainerImpl: container_1636903291807_0001_01_000002 Container Transitioned from NEW to ALLOCATED
[36malgo-1    |[0m 2021-11-14 15:21:52,610 INFO fica.FiCaSchedulerNode: Assigned container container_1636903291807_0001_01_000002 of capacity <memory:3287, vCores:1> on host algo-1:33137, which has 1 containers, <memory:3287, vCores:1> used and <memory:12605, vCores:3> available after allocation
[36malgo-1    |[0m 2021-11-14 15:21:52,610 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636903291807_0001	CONTAINERID=container_1636903291807_0001_01_000002	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 15:21:52,610 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.1316071 absoluteUsedCapacity=0.1316071 used=<memory:4183, vCores:2> cluster=<memory:31784, vCores:8>
[36malgo-1    |[0m 2021-11-14 15:21:52,610 INFO capacity.CapacityScheduler: Allocation proposal accepted
[36malgo-1    |[0m 2021-11-14 15:21:52,624 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1636903291807_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=
[36malgo-1    |[0m 2021-11-14 15:21:52,624 INFO rmcontainer.RMContainerImpl: container_1636903291807_0001_01_000003 Container Transitioned from NEW to ALLOCATED
[36malgo-1    |[0m 2021-11-14 15:21:52,624 INFO fica.FiCaSchedulerNode: Assigned container container_1636903291807_0001_01_000003 of capacity <memory:3287, vCores:1> on host algo-2:40975, which has 2 containers, <memory:4183, vCores:2> used and <memory:11709, vCores:2> available after allocation
[36malgo-1    |[0m 2021-11-14 15:21:52,624 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636903291807_0001	CONTAINERID=container_1636903291807_0001_01_000003	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 15:21:52,625 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.23502392 absoluteUsedCapacity=0.23502392 used=<memory:7470, vCores:3> cluster=<memory:31784, vCores:8>
[36malgo-1    |[0m 2021-11-14 15:21:52,625 INFO capacity.CapacityScheduler: Allocation proposal accepted
[36malgo-1    |[0m 2021-11-14 15:21:52,671 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:33137 for container : container_1636903291807_0001_01_000002
[36malgo-1    |[0m 2021-11-14 15:21:52,672 INFO rmcontainer.RMContainerImpl: container_1636903291807_0001_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
[36malgo-1    |[0m 2021-11-14 15:21:52,673 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-2:40975 for container : container_1636903291807_0001_01_000003
[36malgo-1    |[0m 2021-11-14 15:21:52,674 INFO rmcontainer.RMContainerImpl: container_1636903291807_0001_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_2021-11-14 15:21:52,793 INFO ipc.Server: Auth successful for appattempt_1636903291807_0001_000001 (auth:SIMPLE)
[33malgo-2    |[0m 2021-11-14 15:21:52,797 INFO containermanager.ContainerManagerImpl: Start request for container_1636903291807_0001_01_000003 by user root
[33malgo-2    |[0m 2021-11-14 15:21:52,799 INFO nodemanager.NMAuditLogger: USER=root	IP=192.168.240.2	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1636903291807_0001	CONTAINERID=container_1636903291807_0001_01_000003
[33malgo-2    |[0m 2021-11-14 15:21:52,799 INFO application.ApplicationImpl: Adding container_1636903291807_0001_01_000003 to application application_1636903291807_0001
[33malgo-2    |[0m 2021-11-14 15:21:52,800 INFO container.ContainerImpl: Container container_1636903291807_0001_01_000003 transitioned from NEW to LOCALIZING
[33malgo-2    |[0m 2021-11-14 15:21:52,800 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1636903291807_0001
[33malgo-2    |[0m 2021-11-14 15:21:52,800 INFO container.ContainerImpl: Container container_1636903291807_0001_01_000003 transitioned from LOCALIZING to SCHEDULED
[33malgo-2    |[0m 2021-11-14 15:21:52,800 INFO scheduler.ContainerScheduler: Starting container [container_1636903291807_0001_01_000003]
[33malgo-2    |[0m 2021-11-14 15:21:52,814 INFO container.ContainerImpl: Container container_1636903291807_0001_01_000003 transitioned from SCHEDULED to RUNNING
[33malgo-2    |[0m 2021-11-14 15:21:52,814 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1636903291807_0001_01_000003
[33malgo-2    |[0m 2021-11-14 15:21:52,817 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000003/default_container_executor.sh]
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/prelaunch.out
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/prelaunch.err
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/launch_container.sh
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/directory.info
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stdout
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr
[36malgo-1    |[0m 2021-11-14 15:21:52,866 INFO ipc.Server: Auth successful for appattempt_1636903291807_0001_000001 (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 15:21:52,946 INFO containermanager.ContainerManagerImpl: Start request for container_1636903291807_0001_01_000002 by user root
[36malgo-1    |[0m 2021-11-14 15:21:52,992 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1636903291807_0001
[36malgo-1    |[0m 2021-11-14 15:21:53,000 INFO application.ApplicationImpl: Application application_1636903291807_0001 transitioned from NEW to INITING
[36malgo-1    |[0m 2021-11-14 15:21:53,000 INFO application.ApplicationImpl: Adding container_1636903291807_0001_01_000002 to application application_1636903291807_0001
[36malgo-1    |[0m 2021-11-14 15:21:53,001 INFO nodemanager.NMAuditLogger: USER=root	IP=192.168.240.2	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1636903291807_0001	CONTAINERID=container_1636903291807_0001_01_000002
[36malgo-1    |[0m 2021-11-14 15:21:53,005 INFO application.ApplicationImpl: Application application_1636903291807_0001 transitioned from INITING to RUNNING
[36malgo-1    |[0m 2021-11-14 15:21:53,009 INFO container.ContainerImpl: Container container_1636903291807_0001_01_000002 transitioned from NEW to LOCALIZING
[36malgo-1    |[0m 2021-11-14 15:21:53,009 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1636903291807_0001
[36malgo-1    |[0m 2021-11-14 15:21:53,019 INFO localizer.ResourceLocalizationService: Created localizer for container_1636903291807_0001_01_000002
[36malgo-1    |[0m 2021-11-14 15:21:53,081 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1636903291807_0001_01_000002.tokens
[36malgo-1    |[0m 2021-11-14 15:21:53,093 INFO nodemanager.DefaultContainerExecutor: Initializing user root
[36malgo-1    |[0m 2021-11-14 15:21:53,099 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1636903291807_0001_01_000002.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000002.tokens
[36malgo-1    |[0m 2021-11-14 15:21:53,099 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001
[36malgo-1    |[0m 2021-11-14 15:21:53,623 INFO rmcontainer.RMContainerImpl: container_1636903291807_0001_01_000002 Container Transitioned from ACQUIRED to RUNNING
[36malgo-1    |[0m 2021-11-14 15:21:53,625 INFO rmcontainer.RMContainerImpl: container_1636903291807_0001_01_000003 Container Transitioned from ACQUIRED to RUNNING
[33malgo-2    |[0m 2021-11-14 15:21:54,647 INFO monitor.ContainersMonitorImpl: container_1636903291807_0001_01_000003's ip = 192.168.240.2, and hostname = algo-2
[33malgo-2    |[0m 2021-11-14 15:21:54,650 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1636903291807_0001_01_000003 since CPU usage is not yet available.
[36malgo-1    |[0m 21/11/14 15:21:55 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 1239, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[36malgo-1    |[0m 21/11/14 15:21:55 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.240.2:58634) with ID 2
[36malgo-1    |[0m 2021-11-14 15:21:55,700 INFO scheduler.AppSchedulingInfo: checking for deactivate of application :application_1636903291807_0001
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:53 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 508@algo-2
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:53 INFO SignalUtils: Registered signal handler for TERM
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:53 INFO SignalUtils: Registered signal handler for HUP
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:53 INFO SignalUtils: Registered signal handler for INT
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:54 INFO SecurityManager: Changing view acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:54 INFO SecurityManager: Changing modify acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:54 INFO SecurityManager: Changing view acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:54 INFO SecurityManager: Changing modify acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:54 INFO TransportClientFactory: Successfully created connection to /192.168.240.3:41511 after 89 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO SecurityManager: Changing view acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO SecurityManager: Changing modify acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO SecurityManager: Changing view acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO SecurityManager: Changing modify acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO TransportClientFactory: Successfully created connection to /192.168.240.3:41511 after 1 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/blockmgr-0d453717-0249-4e5a-9140-939a0ae4764f
[36malgo-1    |[0m 21/11/14 15:21:55 INFO BlockManagerMasterEndpoint: Registering block manager algo-2:38277 with 912.3 MiB RAM, BlockManagerId(2, algo-2, 38277, None)
[36malgo-1    |[0m 21/11/14 15:21:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, algo-2, executor 2, partition 0, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-2:38277 (size: 1410.0 B, free: 912.3 MiB)
[36malgo-1    |[0m 2021-11-14 15:21:56,535 INFO container.ContainerImpl: Container container_1636903291807_0001_01_000002 transitioned from LOCALIZING to SCHEDULED
[36malgo-1    |[0m 2021-11-14 15:21:56,535 INFO scheduler.ContainerScheduler: Starting container [container_1636903291807_0001_01_000002]
[36malgo-1    |[0m 2021-11-14 15:21:56,566 INFO container.ContainerImpl: Container container_1636903291807_0001_01_000002 transitioned from SCHEDULED to RUNNING
[36malgo-1    |[0m 2021-11-14 15:21:56,566 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1636903291807_0001_01_000002
[36malgo-1    |[0m 2021-11-14 15:21:56,570 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000002/default_container_executor.sh]
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/prelaunch.out
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/prelaunch.err
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/launch_container.sh
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/directory.info
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stdout
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, algo-2, executor 2, partition 1, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 837 ms on algo-2 (executor 2) (1/16)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, algo-2, executor 2, partition 2, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 13 ms on algo-2 (executor 2) (2/16)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, algo-2, executor 2, partition 3, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 13 ms on algo-2 (executor 2) (3/16)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, algo-2, executor 2, partition 4, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 11 ms on algo-2 (executor 2) (4/16)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, algo-2, executor 2, partition 5, PROCESS_LOCAL, 7368 bytes)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 11 ms on algo-2 (executor 2) (5/16)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, algo-2, executor 2, partition 6, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 10 ms on algo-2 (executor 2) (6/16)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, algo-2, executor 2, partition 7, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 9 ms on algo-2 (executor 2) (7/16)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, algo-2, executor 2, partition 8, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 10 ms on algo-2 (executor 2) (8/16)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, algo-2, executor 2, partition 9, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 7 ms on algo-2 (executor 2) (9/16)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10, algo-2, executor 2, partition 10, PROCESS_LOCAL, 7367 bytes)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 8 ms on algo-2 (executor 2) (10/16)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11, algo-2, executor 2, partition 11, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 8 ms on algo-2 (executor 2) (11/16)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12, algo-2, executor 2, partition 12, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 7 ms on algo-2 (executor 2) (12/16)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13, algo-2, executor 2, partition 13, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 9 ms on algo-2 (executor 2) (13/16)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@192.168.240.3:41511
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO ResourceUtils: ==============================================================
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO ResourceUtils: Resources for spark.executor:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO ResourceUtils: ==============================================================
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO YarnCoarseGrainedExecutorBackend: Successfully registered with driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO Executor: Starting executor ID 2 on host algo-2
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38277.
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO NettyBlockTransferService: Server created on algo-2:38277
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(2, algo-2, 38277, None)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(2, algo-2, 38277, None)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(2, algo-2, 38277, None)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO MetricsConfig: Loaded properties from hadoop-metrics2.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 0
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO MetricsSystemImpl: s3a-file-system metrics system started
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO Executor: Fetching spark://192.168.240.3:41511/jars/hello-scala-spark_2.12-1.0.jar with timestamp 1636903295031
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO TransportClientFactory: Successfully created connection to /192.168.240.3:41511 after 1 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:55 INFO Utils: Fetching spark://192.168.240.3:41511/jars/hello-scala-spark_2.12-1.0.jar to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/spark-61c791bf-8d8b-4afe-8cbc-360979266916/fetchFileTemp8194120534491294014.tmp
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Utils: Copying /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/spark-61c791bf-8d8b-4afe-8cbc-360979266916/-4556996951636903295031_cache to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000003/./hello-scala-spark_2.12-1.0.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Adding file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000003/./hello-scala-spark_2.12-1.0.jar to class loader
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO TorrentBroadcast: Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO TransportClientFactory: Successfully created connection to /192.168.240.3:40105 after 1 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1410.0 B, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO TorrentBroadcast: Reading broadcast variable 0 took 122 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.3 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 880 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 1
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 837 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 2
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 837 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 3
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 837 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 4
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 837 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 5
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] I'm an executor, Hello!
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 837 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 6
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 837 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 7
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 837 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 8
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 794 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 9
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 794 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 10
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14, algo-2, executor 2, partition 14, PROCESS_LOCAL, 7360 bytes)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 7 ms on algo-2 (executor 2) (14/16)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15, algo-2, executor 2, partition 15, PROCESS_LOCAL, 7370 bytes)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 8 ms on algo-2 (executor 2) (15/16)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 12 ms on algo-2 (executor 2) (16/16)
[36malgo-1    |[0m 21/11/14 15:21:56 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:21:56 INFO DAGScheduler: ResultStage 0 (foreach at HelloScalaSparkApp.scala:39) finished in 5.188 s
[36malgo-1    |[0m 21/11/14 15:21:56 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:21:56 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished
[36malgo-1    |[0m 21/11/14 15:21:56 INFO DAGScheduler: Job 0 finished: foreach at HelloScalaSparkApp.scala:39, took 5.253629 s
[36malgo-1    |[0m Reading input from: file:///opt/ml/processing/input/data/data.jsonl
[36malgo-1    |[0m 2021-11-14 15:21:56,877 INFO monitor.ContainersMonitorImpl: container_1636903291807_0001_01_000002's ip = 192.168.240.3, and hostname = algo-1
[36malgo-1    |[0m 2021-11-14 15:21:56,883 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1636903291807_0001_01_000002 since CPU usage is not yet available.
[36malgo-1    |[0m 21/11/14 15:21:56 INFO SharedState: loading hive config file: file:/etc/spark/conf.dist/hive-site.xml
[36malgo-1    |[0m 21/11/14 15:21:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/lib/spark/spark-warehouse').
[36malgo-1    |[0m 21/11/14 15:21:56 INFO SharedState: Warehouse path is 'file:/usr/lib/spark/spark-warehouse'.
[36malgo-1    |[0m 21/11/14 15:21:56 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 15:21:56 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 15:21:56 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 15:21:56 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 15:21:56 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 15:21:57 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.240.3:40105 in memory (size: 1410.0 B, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:21:57 INFO BlockManagerInfo: Removed broadcast_0_piece0 on algo-2:38277 in memory (size: 1410.0 B, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:21:57 INFO InMemoryFileIndex: It took 13 ms to list leaf files for 1 paths.
[36malgo-1    |[0m 21/11/14 15:21:57 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[36malgo-1    |[0m 21/11/14 15:21:59 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.240.3:38674) with ID 1
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:57 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 1210@algo-1
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:57 INFO SignalUtils: Registered signal handler for TERM
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:57 INFO SignalUtils: Registered signal handler for HUP
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:57 INFO SignalUtils: Registered signal handler for INT
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:58 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:58 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:58 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:58 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:58 INFO TransportClientFactory: Successfully created connection to /192.168.240.3:41511 after 90 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:58 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:58 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:58 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:58 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:58 INFO TransportClientFactory: Successfully created connection to /192.168.240.3:41511 after 1 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:59 INFO DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/blockmgr-3d06b36a-b877-4c5a-8134-c7a22f65bb62
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:59 INFO MemoryStore: MemoryStore started with capacity 921/11/14 15:21:59 INFO BlockManagerMasterEndpoint: Registering block manager algo-1:41021 with 912.3 MiB RAM, BlockManagerId(1, algo-1, 41021, None)
[36malgo-1    |[0m 21/11/14 15:21:59 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 15:21:59 INFO FileSourceStrategy: Pushed Filters: 
[36malgo-1    |[0m 21/11/14 15:21:59 INFO FileSourceStrategy: Post-Scan Filters: 
[36malgo-1    |[0m 21/11/14 15:21:59 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[36malgo-1    |[0m 21/11/14 15:22:00 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 317.8 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:22:00 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:22:00 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.240.3:40105 (size: 30.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:00 INFO SparkContext: Created broadcast 1 from json at HelloScalaSparkApp.scala:45
[36malgo-1    |[0m 21/11/14 15:22:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 15:22:00 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 15:22:00 INFO SparkContext: Starting job: json at HelloScalaSparkApp.scala:45
[36malgo-1    |[0m 21/11/14 15:22:00 INFO DAGScheduler: Got job 1 (json at HelloScalaSparkApp.scala:45) with 1 output partitions
[36malgo-1    |[0m 21/11/14 15:22:00 INFO DAGScheduler: Final stage: ResultStage 1 (json at HelloScalaSparkApp.scala:45)
[36malgo-1    |[0m 21/11/14 15:22:00 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:22:00 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:22:00 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at json at HelloScalaSparkApp.scala:45), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:22:00 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 14.6 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:22:00 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:22:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.240.3:40105 (size: 7.3 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:00 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:22:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at json at HelloScalaSparkApp.scala:45) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 15:22:00 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 15:22:00 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 16, algo-2, executor 2, partition 0, PROCESS_LOCAL, 7755 bytes)
[36malgo-1    |[0m 21/11/14 15:22:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-2:38277 (size: 7.3 KiB, free: 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Running task 10.0 in stage 0.0 (TID 10)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] I'm an executor, Hola!
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Finished task 10.0 in stage 0.0 (TID 10). 837 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 11
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 794 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 12
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 794 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 13
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Running task 13.0 in stage 0.0 (TID 13)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Finished task 13.0 in stage 0.0 (TID 13). 794 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 14
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Running task 14.0 in stage 0.0 (TID 14)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Finished task 14.0 in stage 0.0 (TID 14). 794 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 15
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Running task 15.0 in stage 0.0 (TID 15)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] I'm an executor, Bonjour!
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:21:56 INFO Executor: Finished task 15.0 in stage 0.0 (TID 15). 794 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:00 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 16
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:00 INFO Executor: Running task 0.0 in stage 1.0 (TID 16)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:00 INFO TorrentBroadcast: Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m 21/11/14 15:22:01 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-2:38277 (size: 30.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:22:01 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 16) in 971 ms on algo-2 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 15:22:01 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:22:01 INFO DAGScheduler: ResultStage 1 (json at HelloScalaSparkApp.scala:45) finished in 1.022 s
[36malgo-1    |[0m 21/11/14 15:22:01 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:22:01 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
[36malgo-1    |[0m 21/11/14 15:22:01 INFO DAGScheduler: Job 1 finished: json at HelloScalaSparkApp.scala:45, took 1.030236 s
[36malgo-1    |[0m root
[36malgo-1    |[0m  |-- date: string (nullable = true)
[36malgo-1    |[0m  |-- sale: long (nullable = true)
[36malgo-1    |[0m 
[36malgo-1    |[0m 21/11/14 15:22:01 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.240.3:40105 in memory (size: 7.3 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:01 INFO BlockManagerInfo: Removed broadcast_2_piece0 on algo-2:38277 in memory (size: 7.3 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:22:01 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.240.3:40105 in memory (size: 30.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:01 INFO BlockManagerInfo: Removed broadcast_1_piece0 on algo-2:38277 in memory (size: 30.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:22:01 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 15:22:01 INFO FileSourceStrategy: Pushed Filters: 
[36malgo-1    |[0m 21/11/14 15:22:01 INFO FileSourceStrategy: Post-Scan Filters: 
[36malgo-1    |[0m 21/11/14 15:22:01 INFO FileSourceStrategy: Output Data Schema: struct<date: string, sale: bigint>
[36malgo-1    |[0m 21/11/14 15:22:01 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 317.8 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:22:01 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:22:01 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.240.3:40105 (size: 30.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:01 INFO SparkContext: Created broadcast 3 from rdd at HelloScalaSparkApp.scala:52
[36malgo-1    |[0m 21/11/14 15:22:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 15:22:01 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 15:22:01 INFO SparkContext: Starting job: first at HelloScalaSparkApp.scala:52
[36malgo-1    |[0m 21/11/14 15:22:01 INFO DAGScheduler: Got job 2 (first at HelloScalaSparkApp.scala:52) with 1 output partitions
[36malgo-1    |[0m 21/11/14 15:22:01 INFO DAGScheduler: Final stage: ResultStage 2 (first at HelloScalaSparkApp.scala:52)
[36malgo-1    |[0m 21/11/14 15:22:01 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:22:01 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:22:01 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at map at HelloScalaSparkApp.scala:52), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:22:01 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 21.5 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:22:01 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.1 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:22:01 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.240.3:40105 (size: 10.1 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:01 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:22:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at map at HelloScalaSparkApp.scala:52) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 15:22:01 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 15:22:01 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 17, algo-2, executor 2, partition 0, PROCESS_LOCAL, 7755 bytes)
[36malgo-1    |[0m 21/11/14 15:22:01 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-2:38277 (size: 10.1 KiB, free: 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:00 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:00 INFO TorrentBroadcast: Reading broadcast variable 2 took 9 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:00 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 14.6 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:00 INFO FileScanRDD: TID: 16 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:01 INFO CodeGenerator: Code generated in 332.239572 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:01 INFO TorrentBroadcast: Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:01 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:01 INFO TorrentBroadcast: Reading broadcast variable 1 took 8 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:01 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 445.1 KiB, free 911.8 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:01 INFO Executor: Finished task 0.0 in stage 1.0 (TID 16). 2013 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:01 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 17
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:01 INFO Executor: Running task 0.0 in stage 2.0 (TID 17)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:01 INFO TorrentBroadcast: Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:01 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.1 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:01 INFO TorrentBroadcast: Reading broadcast variable 4 took 6 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:01 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 21.5 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:02 WARN YarnCoarseGrainedExecutorBackend: eagerFSInit: Unable to eagerly init filesystem s3://does/not/exist
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] java.nio.file.AccessDeniedException: does: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@6729eb: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@11f00548: Failed to connect to service endpoint: ]
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:187)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:111)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:380)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:314)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3358)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:123)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3407)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3375)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:486)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at org.apache.spark.executor.CoarseGrainedExecutorBackend.$anonfun$eagerFSInit$1(CoarseGrainedExecutorBackend.scala:274)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at scala.collection.parallel.mutable.ParArray$ParArrayIterator.flatmap2combiner(ParArray.scala:419)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at scala.collection.parallel.ParIterableLike$FlatMap.leaf(ParIterableLike.scala:1074)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at scala.collection.parallel.ParIterableLike$FlatMap.tryLeaf(ParIterableLike.scala:1070)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at scala.collection.parallel.FutureTasks.$anonfun$exec$5(Tasks.scala:499)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at scala.util.Success.$anonfun$map$1(Try.scala:255)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at scala.util.Success.map(Try.scala:213)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at java.lang.Thread.run(Thread.java:748)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] Caused by: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@6729eb: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@11f00548: Failed to connect to service endpoint: ]
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:159)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1257)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:833)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:783)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5062)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:5850)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:5823)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5046)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5008)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1416)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1352)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$verifyBucketExists$1(S3AFileSystem.java:381)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	... 32 more
[36malgo-1    |[0m 21/11/14 15:22:02 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-2:38277 (size: 30.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:22:03 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 17) in 1685 ms on algo-2 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 15:22:03 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:22:03 INFO DAGScheduler: ResultStage 2 (first at HelloScalaSparkApp.scala:52) finished in 1.690 s
[36malgo-1    |[0m 21/11/14 15:22:03 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:22:03 INFO YarnScheduler: Killing all running tasks in stage 2: Stage finished
[36malgo-1    |[0m 21/11/14 15:22:03 INFO DAGScheduler: Job 2 finished: first at HelloScalaSparkApp.scala:52, took 1.694123 s
[36malgo-1    |[0m Parsing first line with json4s: JObject(List((date,JString(2020-01-04)), (sale,JInt(283))))
[36malgo-1    |[0m 21/11/14 15:22:03 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 15:22:03 INFO FileSourceStrategy: Pushed Filters: IsNotNull(sale),GreaterThan(sale,750)
[36malgo-1    |[0m 21/11/14 15:22:03 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(sale#8L),(sale#8L > 750)
[36malgo-1    |[0m 21/11/14 15:22:03 INFO FileSourceStrategy: Output Data Schema: struct<date: string, sale: bigint>
[36malgo-1    |[0m 21/11/14 15:22:03 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.240.3:40105 in memory (size: 30.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:03 INFO BlockManagerInfo: Removed broadcast_3_piece0 on algo-2:38277 in memory (size: 30.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:22:03 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.240.3:40105 in memory (size: 10.1 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:03 INFO BlockManagerInfo: Removed broadcast_4_piece0 on algo-2:38277 in memory (size: 10.1 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:22:03 INFO CodeGenerator: Code generated in 259.578477 ms
[36malgo-1    |[0m 21/11/14 15:22:03 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 317.8 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:22:03 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:22:03 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.240.3:40105 (size: 30.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:03 INFO SparkContext: Created broadcast 5 from show at HelloScalaSparkApp.scala:57
[36malgo-1    |[0m 21/11/14 15:22:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 15:22:03 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 15:22:03 INFO SparkContext: Starting job: show at HelloScalaSparkApp.scala:57
[36malgo-1    |[0m 21/11/14 15:22:03 INFO DAGScheduler: Got job 3 (show at HelloScalaSparkApp.scala:57) with 1 output partitions
[36malgo-1    |[0m 21/11/14 15:22:03 INFO DAGScheduler: Final stage: ResultStage 3 (show at HelloScalaSparkApp.scala:57)
[36malgo-1    |[0m 21/11/14 15:22:03 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:22:03 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:22:03 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[15] at show at HelloScalaSparkApp.scala:57), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:22:04 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 18.4 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:22:04 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 9.1 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:22:04 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.240.3:40105 (size: 9.1 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:04 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:22:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at show at HelloScalaSparkApp.scala:57) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 15:22:04 INFO YarnScheduler: Adding task set 3.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 15:22:04 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 18, algo-2, executor 2, partition 0, PROCESS_LOCAL, 7755 bytes)
[36malgo-1    |[0m 21/11/14 15:22:04 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-2:38277 (size: 9.1 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:22:04 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-2:38277 (size: 30.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:22:04 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 18) in 227 ms on algo-2 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 15:22:04 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:22:04 INFO DAGScheduler: ResultStage 3 (show at HelloScalaSparkApp.scala:57) finished in 0.234 s
[36malgo-1    |[0m 21/11/14 15:22:04 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:22:04 INFO YarnScheduler: Killing all running tasks in stage 3: Stage finished
[36malgo-1    |[0m 21/11/14 15:22:04 INFO DAGScheduler: Job 3 finished: show at HelloScalaSparkApp.scala:57, took 0.237637 s
[36malgo-1    |[0m 21/11/14 15:22:04 INFO CodeGenerator: Code generated in 13.95742 ms
[36malgo-1    |[0m +----------+----+
[36malgo-1    |[0m |      date|sale|
[36malgo-1    |[0m +----------+----+
[36malgo-1    |[0m |2020-01-03| 999|
[36malgo-1    |[0m |2020-01-02| 999|
[36malgo-1    |[0m |2020-01-06| 998|
[36malgo-1    |[0m |2020-01-05| 998|
[36malgo-1    |[0m |2020-01-07| 996|
[36malgo-1    |[0m |2020-01-07| 994|
[36malgo-1    |[0m |2020-01-07| 993|
[36malgo-1    |[0m |2020-01-01| 993|
[36malgo-1    |[0m |2020-01-02| 991|
[36malgo-1    |[0m |2020-01-05| 990|
[36malgo-1    |[0m |2020-01-07| 990|
[36malgo-1    |[0m |2020-01-03| 989|
[36malgo-1    |[0m |2020-01-04| 988|
[36malgo-1    |[0m |2020-01-05| 988|
[36malgo-1    |[0m |2020-01-07| 985|
[36malgo-1    |[0m |2020-01-03| 985|
[36malgo-1    |[0m |2020-01-04| 982|
[36malgo-1    |[0m |2020-01-02| 981|
[36malgo-1    |[0m |2020-01-03| 980|
[36malgo-1    |[0m |2020-01-05| 979|
[36malgo-1    |[0m +----------+----+
[36malgo-1    |[0m only showing top 20 rows
[36malgo-1    |[0m 
[36malgo-1    |[0m 21/11/14 15:22:04 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 15:22:04 INFO FileSourceStrategy: Pushed Filters: 
[36malgo-1    |[0m 21/11/14 15:22:04 INFO FileSourceStrategy: Post-Scan Filters: 
[36malgo-1    |[0m 21/11/14 15:22:04 INFO FileSourceStrategy: Output Data Schema: struct<date: string, sale: bigint>
[36malgo-1    |[0m 21/11/14 15:22:04 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.240.3:40105 in memory (size: 9.1 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:04 INFO BlockManagerInfo: Removed broadcast_6_piece0 on algo-2:38277 in memory (size: 9.1 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:22:04 INFO CodeGenerator: Code generated in 55.507398 ms
[36malgo-1    |[0m 21/11/14 15:22:04 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 317.8 KiB, free 1007.2 MiB)
[36malgo-1    |[0m 21/11/14 15:22:04 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 1007.2 MiB)
[36malgo-1    |[0m 21/11/14 15:22:04 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.240.3:40105 (size: 30.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:04 INFO SparkContext: Created broadcast 7 from collect at HelloScalaSparkApp.scala:61
[36malgo-1    |[0m 21/11/14 15:22:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 15:22:04 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 15:22:04 INFO DAGScheduler: Registering RDD 19 (collect at HelloScalaSparkApp.scala:61) as input to shuffle 0
[36malgo-1    |[0m 21/11/14 15:22:04 INFO DAGScheduler: Got map stage job 4 (collect at HelloScalaSparkApp.scala:61) with 1 output partitions
[36malgo-1    |[0m 21/11/14 15:22:04 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (collect at HelloScalaSparkApp.scala:61)
[36malgo-1    |[0m 21/11/14 15:22:04 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:22:04 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:22:04 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[19] at collect at HelloScalaSparkApp.scala:61), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:22:04 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 29.7 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 15:22:04 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 13.9 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 15:22:04 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.240.3:40105 (size: 13.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:04 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:22:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[19] at collect at HelloScalaSparkApp.scala:61) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 15:22:04 INFO YarnScheduler: Adding task set 4.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 15:22:04 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 19, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7744 bytes)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] Caused by: com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@6729eb: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@11f00548: Failed to connect to service endpoint: ]
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:136)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:137)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 	... 50 more
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:02 INFO CodeGenerator: Code generated in 17.191748 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:02 INFO FileScanRDD: TID: 17 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:02 INFO CodeGenerator: Code generated in 9.727355 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:02 INFO TorrentBroadcast: Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:02 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:02 INFO TorrentBroadcast: Reading broadcast variable 3 took 7 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:02 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 445.1 KiB, free 911.8 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:03 INFO CodeGenerator: Code generated in 28.142069 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:03 INFO Executor: Finished task 0.0 in stage 2.0 (TID 17). 2423 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:04 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 18
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:04 INFO Executor: Running task 0.0 in stage 3.0 (TID 18)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:04 INFO TorrentBroadcast: Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:04 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 9.1 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:04 INFO TorrentBroadcast: Reading broadcast variable 6 took 9 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:04 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 18.4 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:04 INFO CodeGenerator: Code generated in 30.299853 ms
[36malgo-1    |[0m 21/11/14 15:22:04 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:41021 (size: 13.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 12.3 MiB
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:59 INFO YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@192.168.240.3:41511
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:59 INFO ResourceUtils: ==============================================================
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:59 INFO ResourceUtils: Resources for spark.executor:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:59 INFO ResourceUtils: ==============================================================
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:59 INFO YarnCoarseGrainedExecutorBackend: Successfully registered with driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:59 INFO Executor: Starting executor ID 1 on host algo-1
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41021.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:59 INFO NettyBlockTransferService: Server created on algo-1:41021
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(1, algo-1, 41021, None)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(1, algo-1, 41021, None)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(1, algo-1, 41021, None)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:59 INFO MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:59 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:21:59 INFO MetricsSystemImpl: s3a-file-system metrics system started
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:04 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 19
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:04 INFO Executor: Running task 0.0 in stage 4.0 (TID 19)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:04 INFO Executor: Fetching spark://192.168.240.3:41511/jars/hello-scala-spark_2.12-1.0.jar with timestamp 1636903295031
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:04 INFO TransportClientFactory: Successfully created connection to /192.168.240.3:41511 after 2 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m 21/11/14 15:22:06 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:41021 (size: 30.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:22:06 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 19) in 1840 ms on algo-1 (executor 1) (1/1)
[36malgo-1    |[0m 21/11/14 15:22:06 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:22:06 INFO DAGScheduler: ShuffleMapStage 4 (collect at HelloScalaSparkApp.scala:61) finished in 1.860 s
[36malgo-1    |[0m 21/11/14 15:22:06 INFO DAGScheduler: looking for newly runnable stages
[36malgo-1    |[0m 21/11/14 15:22:06 INFO DAGScheduler: running: Set()
[36malgo-1    |[0m 21/11/14 15:22:06 INFO DAGScheduler: waiting: Set()
[36malgo-1    |[0m 21/11/14 15:22:06 INFO DAGScheduler: failed: Set()
[36malgo-1    |[0m 21/11/14 15:22:06 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 16.
[36malgo-1    |[0m 21/11/14 15:22:06 INFO CodeGenerator: Code generated in 18.430025 ms
[36malgo-1    |[0m 21/11/14 15:22:06 INFO CodeGenerator: Code generated in 11.717167 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:04 INFO Utils: Fetching spark://192.168.240.3:41511/jars/hello-scala-spark_2.12-1.0.jar to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/spark-c64f6490-87d9-4e1d-a689-77c50384f1c3/fetchFileTemp5753242536007663714.tmp
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:04 INFO Utils: Copying /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/spark-c64f6490-87d9-4e1d-a689-77c50384f1c3/-4556996951636903295031_cache to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000002/./hello-scala-spark_2.12-1.0.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:04 INFO Executor: Adding file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000002/./hello-scala-spark_2.12-1.0.jar to class loader
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:04 INFO TorrentBroadcast: Started reading broadcast variable 8 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:04 INFO TransportClientFactory: Successfully created connection to /192.168.240.3:40105 after 1 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:04 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 13.9 KiB, free 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:04 INFO TorrentBroadcast: Reading broadcast variable 8 took 91 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:05 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 29.7 KiB, free 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:05 INFO CodeGenerator: Code generated in 313.333264 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO CodeGenerator: Code generated in 15.812221 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO CodeGenerator: Code generated in 6.85822 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO CodeGenerator: Code generated in 9.02037 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO CodeGenerator: Code generated in 7.282407 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO FileScanRDD: TID: 19 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO CodeGenerator: Code generated in 7.859778 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO TorrentBroadcast: Started reading broadcast variable 7 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 879.9 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO TorrentBroadcast: Reading broadcast variable 7 took 7 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 445.1 KiB, free 879.5 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 WARN YarnCoarseGrainedExecutorBackend: eagerFSInit: Unable to eagerly init filesystem s3://does/not/exist
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] java.nio.file.AccessDeniedException: does: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@25225e7: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@4e08b1d4: Failed to connect to service endpoint: ]
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:187)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:111)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:380)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:314)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3358)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:123)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3407)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3375)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:486)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at org.apache.spark.executor.CoarseGrainedExecutorBackend.$anonfun$eagerFSInit$1(CoarseGrainedExecutorBackend.scala:274)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at scala.collection.parallel.mutable.ParArray$ParArrayIterator.flatmap2combiner(ParArray.scala:419)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at scala.collection.parallel.ParIterableLike$FlatMap.leaf(ParIterableLike.scala:1074)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at scala.collection.parallel.ParIterableLike$FlatMap.tryLeaf(ParIterableLike.scala:1070)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at scala.collection.parallel.FutureTasks.$anonfun$exec$5(Tasks.scala:499)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at scala.util.Success.$anonfun$map$1(Try.scala:255)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at scala.util.Success.map(Try.scala:213)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at java.lang.Thread.run(Thread.java:748)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] Caused by: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@25225e7: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@4e08b1d4: Failed to connect to service endpoint: ]
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:159)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1257)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:833)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:783)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5062)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:5850)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:5823)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5046)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5008)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1416)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1352)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at org.apac21/11/14 15:22:06 INFO SparkContext: Starting job: collect at HelloScalaSparkApp.scala:61
[36malgo-1    |[0m 21/11/14 15:22:06 INFO DAGScheduler: Got job 5 (collect at HelloScalaSparkApp.scala:61) with 15 output partitions
[36malgo-1    |[0m 21/11/14 15:22:06 INFO DAGScheduler: Final stage: ResultStage 6 (collect at HelloScalaSparkApp.scala:61)
[36malgo-1    |[0m 21/11/14 15:22:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
[36malgo-1    |[0m 21/11/14 15:22:06 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:22:06 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[24] at collect at HelloScalaSparkApp.scala:61), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:22:06 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 31.1 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 15:22:06 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.8 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 15:22:06 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.240.3:40105 (size: 14.8 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:06 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:22:06 INFO DAGScheduler: Submitting 15 missing tasks from ResultStage 6 (MapPartitionsRDD[24] at collect at HelloScalaSparkApp.scala:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[36malgo-1    |[0m 21/11/14 15:22:06 INFO YarnScheduler: Adding task set 6.0 with 15 tasks
[36malgo-1    |[0m 21/11/14 15:22:06 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 20, algo-2, executor 2, partition 0, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:22:06 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 21, algo-1, executor 1, partition 1, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:22:06 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-2:38277 (size: 14.8 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:22:06 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:41021 (size: 14.8 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:22:06 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.240.2:58634
[36malgo-1    |[0m 21/11/14 15:22:06 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.240.3:38674
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:04 INFO CodeGenerator: Code generated in 9.615521 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:04 INFO CodeGenerator: Code generated in 7.109335 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:04 INFO FileScanRDD: TID: 18 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:04 INFO TorrentBroadcast: Started reading broadcast variable 5 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:04 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:04 INFO TorrentBroadcast: Reading broadcast variable 5 took 7 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:04 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 445.1 KiB, free 911.7 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:04 INFO Executor: Finished task 0.0 in stage 3.0 (TID 18). 2327 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 20
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO Executor: Running task 0.0 in stage 6.0 (TID 20)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO TorrentBroadcast: Started reading broadcast variable 9 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.8 KiB, free 911.8 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO TorrentBroadcast: Reading broadcast variable 9 took 9 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 31.1 KiB, free 911.8 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@192.168.240.3:41511)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO MapOutputTrackerWorker: Got the output locations
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m 21/11/14 15:22:06 INFO TaskSetManager: Starting task 2.0 in stage 6.0 (TID 22, algo-2, executor 2, partition 2, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:22:06 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 20) in 274 ms on algo-2 (executor 2) (1/15)
[36malgo-1    |[0m 21/11/14 15:22:06 INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 23, algo-2, executor 2, partition 3, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:22:06 INFO TaskSetManager: Finished task 2.0 in stage 6.0 (TID 22) in 22 ms on algo-2 (executor 2) (2/15)
[36malgo-1    |[0m 21/11/14 15:22:06 INFO TaskSetManager: Starting task 4.0 in stage 6.0 (TID 24, algo-1, executor 1, partition 4, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:22:06 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 21) in 303 ms on algo-1 (executor 1) (3/15)
[36malgo-1    |[0m 21/11/14 15:22:06 INFO TaskSetManager: Starting task 5.0 in stage 6.0 (TID 25, algo-1, executor 1, partition 5, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:22:06 INFO TaskSetManager: Finished task 4.0 in stage 6.0 (TID 24) in 26 ms on algo-1 (executor 1) (4/15)
[36malgo-1    |[0m 21/11/14 15:22:06 INFO TaskSetManager: Starting task 6.0 in stage 6.0 (TID 26, algo-1, executor 1, partition 6, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:22:06 INFO TaskSetManager: Finished task 5.0 in stage 6.0 (TID 25) in 25 ms on algo-1 (executor 1) (5/15)
[36malgo-1    |[0m 21/11/14 15:22:06 INFO TaskSetManager: Starting task 7.0 in stage 6.0 (TID 27, algo-2, executor 2, partition 7, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:22:06 INFO TaskSetManager: Finished task 3.0 in stage 6.0 (TID 23) in 61 ms on algo-2 (executor 2) (6/15)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 8.0 in stage 6.0 (TID 28, algo-2, executor 2, partition 8, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 7.0 in stage 6.0 (TID 27) in 21 ms on algo-2 (executor 2) (7/15)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 9.0 in stage 6.0 (TID 29, algo-1, executor 1, partition 9, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 6.0 in stage 6.0 (TID 26) in 26 ms on algo-1 (executor 1) (8/15)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 10.0 in stage 6.0 (TID 30, algo-2, executor 2, partition 10, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 8.0 in stage 6.0 (TID 28) in 19 ms on algo-2 (executor 2) (9/15)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 11.0 in stage 6.0 (TID 31, algo-1, executor 1, partition 11, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 9.0 in stage 6.0 (TID 29) in 23 ms on algo-1 (executor 1) (10/15)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 12.0 in stage 6.0 (TID 32, algo-2, executor 2, partition 12, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 10.0 in stage 6.0 (TID 30) in 18 ms on algo-2 (executor 2) (11/15)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 13.0 in stage 6.0 (TID 33, algo-1, executor 1, partition 13, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 11.0 in stage 6.0 (TID 31) in 28 ms on algo-1 (executor 1) (12/15)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 14.0 in stage 6.0 (TID 34, algo-2, executor 2, partition 14, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 12.0 in stage 6.0 (TID 32) in 19 ms on algo-2 (executor 2) (13/15)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 13.0 in stage 6.0 (TID 33) in 25 ms on algo-1 (executor 1) (14/15)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 14.0 in stage 6.0 (TID 34) in 20 ms on algo-2 (executor 2) (15/15)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: ResultStage 6 (collect at HelloScalaSparkApp.scala:61) finished in 0.456 s
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:22:07 INFO YarnScheduler: Killing all running tasks in stage 6: Stage finished
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: Job 5 finished: collect at HelloScalaSparkApp.scala:61, took 0.463334 s
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: Registering RDD 25 (collect at HelloScalaSparkApp.scala:61) as input to shuffle 1
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: Got map stage job 6 (collect at HelloScalaSparkApp.scala:61) with 15 output partitions
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (collect at HelloScalaSparkApp.scala:61)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[25] at collect at HelloScalaSparkApp.scala:61), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:22:07 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 31.9 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 15.3 KiB, free 1007.0 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.240.3:40105 (size: 15.3 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: Submitting 15 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[25] at collect at HelloScalaSparkApp.scala:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[36malgo-1    |[0m 21/11/14 15:22:07 INFO YarnScheduler: Adding task set 8.0 with 15 tasks
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 35, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 36, algo-2, executor 2, partition 1, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:41021 (size: 15.3 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-2:38277 (size: 15.3 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 37, algo-1, executor 1, partition 2, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 35) in 90 ms on algo-1 (executor 1) (1/15)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 3.0 in stage 8.0 (TID 38, algo-2, executor 2, partition 3, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 36) in 105 ms on algo-2 (executor 2) (2/15)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 4.0 in stage 8.0 (TID 39, algo-1, executor 1, partition 4, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 37) in 29 ms on algo-1 (executor 1) (3/15)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 5.0 in stage 8.0 (TID 40, algo-2, executor 2, partition 5, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 3.0 in stage 8.0 (TID 38) in 30 ms on algo-2 (executor 2) (4/15)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 6.0 in stage 8.0 (TID 41, algo-1, executor 1, partition 6, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 4.0 in stage 8.0 (TID 39) in 25 ms on algo-1 (executor 1) (5/15)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 7.0 in stage 8.0 (TID 42, algo-1, executor 1, partition 7, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 8.0 in stage 8.0 (TID 43, algo-2, executor 2, partition 8, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 6.0 in stage 8.0 (TID 41) in 28 ms on algo-1 (executor 1) (6/15)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 5.0 in stage 8.0 (TID 40) in 36 ms on algo-2 (executor 2) (7/15)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 9.0 in stage 8.0 (TID 44, algo-2, executor 2, partition 9, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 8.0 in stage 8.0 (TID 43) in 27 ms on algo-2 (executor 2) (8/15)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 10.0 in stage 8.0 (TID 45, algo-1, executor 1, partition 10, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 7.0 in stage 8.0 (TID 42) in 46 ms on algo-1 (executor 1) (9/15)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 11.0 in stage 8.0 (TID 46, algo-2, executor 2, partition 11, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 9.0 in stage 8.0 (TID 44) in 25 ms on algo-2 (executor 2) (10/15)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 12.0 in stage 8.0 (TID 47, algo-1, executor 1, partition 12, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 10.0 in stage 8.0 (TID 45) in 20 ms on algo-1 (executor 1) (11/15)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 13.0 in stage 8.0 (TID 48, algo-2, executor 2, partition 13, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 11.0 in stage 8.0 (TID 46) in 28 ms on algo-2 (executor 2) (12/15)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 14.0 in stage 8.0 (TID 49, algo-1, executor 1, partition 14, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 12.0 in stage 8.0 (TID 47) in 23 ms on algo-1 (executor 1) (13/15)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 13.0 in stage 8.0 (TID 48) in 35 ms on algo-2 (executor 2) (14/15)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 14.0 in stage 8.0 (TID 49) in 35 ms on algo-1 (executor 1) (15/15)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: ShuffleMapStage 8 (collect at HelloScalaSparkApp.scala:61) finished in 0.301 s
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: looking for newly runnable stages
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: running: Set()
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: waiting: Set()
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: failed: Set()
[36malgo-1    |[0m 21/11/14 15:22:07 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 16.
[36malgo-1    |[0m 21/11/14 15:22:07 INFO CodeGenerator: Code generated in 19.938892 ms
[36malgo-1    |[0m 21/11/14 15:22:07 INFO SparkContext: Starting job: collect at HelloScalaSparkApp.scala:61
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: Got job 7 (collect at HelloScalaSparkApp.scala:61) with 7 output partitions
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: Final stage: ResultStage 11 (collect at HelloScalaSparkApp.scala:61)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[28] at collect at HelloScalaSparkApp.scala:61), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:22:07 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 26.7 KiB, free 1007.0 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 1007.0 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.240.3:40105 (size: 13.4 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: Submitting 7 missing tasks from ResultStage 11 (MapPartitionsRDD[28] at collect at HelloScalaSparkApp.scala:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))
[36malgo-1    |[0m 21/11/14 15:22:07 INFO YarnScheduler: Adding task set 11.0 with 7 tasks
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 50, algo-2, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 5.0 in stage 11.0 (TID 51, algo-1, executor 1, partition 5, NODE_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-2:38277 (size: 13.4 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:41021 (size: 13.4 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.240.2:58634
[36malgo-1    |[0m 21/11/14 15:22:07 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.240.3:38674
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 52, algo-2, executor 2, partition 1, NODE_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 50) in 76 ms on algo-2 (executor 2) (1/7)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 2.0 in stage 11.0 (TID 53, algo-2, executor 2, partition 2, NODE_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 52) in 20 ms on algo-2 (executor 2) (2/7)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 3.0 in stage 11.0 (TID 54, algo-2, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 2.0 in stage 11.0 (TID 53) in 19 ms on algo-2 (executor 2) (3/7)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 4.0 in stage 11.0 (TID 55, algo-1, executor 1, partition 4, RACK_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 5.0 in stage 11.0 (TID 51) in 121 ms on algo-1 (executor 1) (4/7)
[36malgo-1    |[0m he.hadoop.fs.s3a.S3AFileSystem.lambda$verifyBucketExists$1(S3AFileSystem.java:381)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	... 32 more
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] Caused by: com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@25225e7: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@4e08b1d4: Failed to connect to service endpoint: ]
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:136)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:137)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 	... 50 more
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO Executor: Finished task 0.0 in stage 4.0 (TID 19). 4574 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 21
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO Executor: Running task 1.0 in stage 6.0 (TID 21)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO TorrentBroadcast: Started reading broadcast variable 9 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.8 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO TorrentBroadcast: Reading broadcast variable 9 took 7 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 31.1 KiB, free 911.7 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@192.168.240.3:41511)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO MapOutputTrackerWorker: Got the output locations
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO CodeGenerator: Code generated in 24.070116 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO Executor: Finished task 1.0 in stage 6.0 (TID 21). 3890 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 24
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO Executor: Running task 4.0 in stage 6.0 (TID 24)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO Executor: Finished task 4.0 in stage 6.0 (TID 24). 3723 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 25
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO Executor: Running task 5.0 in stage 6.0 (TID 25)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO Executor: Finished task 5.0 in stage 6.0 (TID 25). 3890 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 26
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO Executor: Running task 6.0 in stage 6.0 (TID 26)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 6.0 in stage 6.0 (TID 26). 3723 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 29
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Running task 9.0 in stage 6.0 (TID 29)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 9.0 in stage 6.0 (TID 29). 3890 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 31
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Running task 11.0 in stage 6.0 (TID 31)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 11.0 in stage 6.0 (TID 31). 3890 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 33
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Running task 13.0 in stage 6.0 (TID 33)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 13.0 in stage 6.0 (TID 33). 3890 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 35
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Running task 0.0 in stage 8.0 (TID 35)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO TorrentBroadcast: Started reading broadcast variable 10 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 15.3 KiB, free 911.7 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO TorrentBroadcast: Reading broadcast variable 10 took 8 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 31.9 KiB, free 911.7 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO CodeGenerator: Code generated in 16.009127 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 0.0 in stage 8.0 (TID 35). 3766 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 37
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Running task 2.0 in stage 8.0 (TID 37)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 2.0 in stage 8.0 (TID 37). 3766 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 39
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Running task 4.0 in stage 8.0 (TID 39)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 4.0 in stage 8.0 (TID 39). 3766 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 41
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Running task 6.0 in stage 8.0 (TID 41)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 6.0 in stage 8.0 (TID 41). 3766 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 42
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Running task 7.0 in stage 8.0 (TID 42)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 7.0 in stage 8.0 (TID 42). 3902 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 45
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Running task 10.0 in stage 8.0 (TID 45)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 10.0 in stage 8.0 (TID 45). 3766 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 47
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Running task 12.0 in stage 8.0 (TID 47)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 12.0 in stage 8.0 (TID 47). 3766 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 49
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Running task 14.0 in stage 8.0 (TID 49)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/1421/11/14 15:22:07 INFO TaskSetManager: Starting task 6.0 in stage 11.0 (TID 56, algo-2, executor 2, partition 6, NODE_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 3.0 in stage 11.0 (TID 54) in 19 ms on algo-2 (executor 2) (5/7)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 192.168.240.3:40105 in memory (size: 14.8 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO BlockManagerInfo: Removed broadcast_9_piece0 on algo-2:38277 in memory (size: 14.8 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 6.0 in stage 11.0 (TID 56) in 40 ms on algo-2 (executor 2) (6/7)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:41021 in memory (size: 14.8 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Finished task 4.0 in stage 11.0 (TID 55) in 52 ms on algo-1 (executor 1) (7/7)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: ResultStage 11 (collect at HelloScalaSparkApp.scala:61) finished in 0.182 s
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:22:07 INFO YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:22:07 INFO YarnScheduler: Killing all running tasks in stage 11: Stage finished
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: Job 7 finished: collect at HelloScalaSparkApp.scala:61, took 0.193003 s
[36malgo-1    |[0m 21/11/14 15:22:07 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 192.168.240.3:40105 in memory (size: 13.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:41021 in memory (size: 13.9 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO CodeGenerator: Code generated in 10.99117 ms
[36malgo-1    |[0m 21/11/14 15:22:07 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 192.168.240.3:40105 in memory (size: 15.3 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:41021 in memory (size: 15.3 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO BlockManagerInfo: Removed broadcast_10_piece0 on algo-2:38277 in memory (size: 15.3 KiB, free: 912.3 MiB)
[36malgo-1    |[0m [Lorg.apache.spark.sql.Row;@24bc06dd
[36malgo-1    |[0m 21/11/14 15:22:07 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.240.3:40105 in memory (size: 30.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO BlockManagerInfo: Removed broadcast_5_piece0 on algo-2:38277 in memory (size: 30.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 15:22:07 INFO FileSourceStrategy: Pushed Filters: 
[36malgo-1    |[0m 21/11/14 15:22:07 INFO FileSourceStrategy: Post-Scan Filters: 
[36malgo-1    |[0m 21/11/14 15:22:07 INFO FileSourceStrategy: Output Data Schema: struct<date: string, sale: bigint>
[36malgo-1    |[0m 21/11/14 15:22:07 INFO CodeGenerator: Code generated in 9.434144 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO CodeGenerator: Code generated in 28.926027 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO CodeGenerator: Code generated in 6.877348 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO CodeGenerator: Code generated in 9.952986 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO Executor: Finished task 0.0 in stage 6.0 (TID 20). 3723 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 22
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO Executor: Running task 2.0 in stage 6.0 (TID 22)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO Executor: Finished task 2.0 in stage 6.0 (TID 22). 3723 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 23
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO Executor: Running task 3.0 in stage 6.0 (TID 23)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO TransportClientFactory: Successfully created connection to algo-1/192.168.240.3:41021 after 1 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 15 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO Executor: Finished task 3.0 in stage 6.0 (TID 23). 3890 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 27
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO Executor: Running task 7.0 in stage 6.0 (TID 27)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:06 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 7.0 in stage 6.0 (TID 27). 3890 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 28
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Running task 8.0 in stage 6.0 (TID 28)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 8.0 in stage 6.0 (TID 28). 3723 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 30
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Running task 10.0 in stage 6.0 (TID 30)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 10.0 in stage 6.0 (TID 30). 3723 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 32
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Running task 12.0 in stage 6.0 (TID 32)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 12.0 in stage 6.0 (TID 32). 3723 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 34
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Running task 14.0 in stage 6.0 (TID 34)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 14.0 in stage 6.0 (TID 34). 3723 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 36
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Running task 1.0 in stage 8.0 (TID 36)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO TorrentBroadcast: Started reading broadcast variable 10 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 15.3 KiB, free 911.8 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO TorrentBroadcast: Reading broadcast variable 10 took 9 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 31.9 KiB, free 911.7 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO CodeGenerator: Code generated in 10.765568 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 1.0 in stage 8.0 (TID 36). 3902 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 38
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Running task 3.0 in stage 8.0 (TID 38)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 3.0 in stage 8.0 (TID 38). 3902 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 40
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Running task 5.0 in stage 8.0 (TID 40)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 5.0 in stage 8.0 (TID 40). 3902 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 43
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Running task 8.0 in stage 8.0 (TID 43)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 8.0 in stage 8.0 (TID 43). 3766 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 44
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Running task 9.0 in stage 8.0 (TID 44)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 9.0 in stage 8.0 (TID 44). 3902 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 46
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Running task 11.0 in stage 8.0 (TID 46)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 11.0 in stage 8.0 (TID 46). 3902 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 48
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Running task 13.0 in stage 8.0 (TID 48)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 13.0 in stage 8.0 (TID 48). 3902 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 50
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Running task 0.0 in stage 11.0 (TID 50)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO MapOutputTrackerWorker: Updating epoch to 2 and clearing cache
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO TorrentBroadcast: Started reading broadcast variable 11 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 911.7 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO TorrentBroadcast: Reading broadcast variable 11 took 8 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 26.7 KiB, free 911.7 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@192.168.240.3:41511)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO MapOutputTrackerWorker: Got the output locations
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO CodeGenerator: Code generated in 18.32207 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO CodeGenerator: Code generated in 13.984792 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 0.0 in stage 11.0 (TID 50). 4645 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 52
[36malgo-1    |[0m 21/11/14 15:22:07 INFO CodeGenerator: Code generated in 17.583211 ms
[36malgo-1    |[0m 21/11/14 15:22:07 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 317.8 KiB, free 1007.2 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.240.3:40105 (size: 30.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO SparkContext: Created broadcast 12 from show at HelloScalaSparkApp.scala:68
[36malgo-1    |[0m 21/11/14 15:22:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 15:22:07 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 15:22:07 INFO SparkContext: Starting job: show at HelloScalaSparkApp.scala:68
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: Got job 8 (show at HelloScalaSparkApp.scala:68) with 1 output partitions
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: Final stage: ResultStage 12 (show at HelloScalaSparkApp.scala:68)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[33] at show at HelloScalaSparkApp.scala:68), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:22:07 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 15.9 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 192.168.240.3:40105 (size: 7.7 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:22:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[33] at show at HelloScalaSparkApp.scala:68) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 15:22:07 INFO YarnScheduler: Adding task set 12.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 15:22:07 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 57, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7755 bytes)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:41021 (size: 7.7 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:22:07 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:41021 (size: 30.2 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 57) in 154 ms on algo-1 (executor 1) (1/1)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO YarnScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: ResultStage 12 (show at HelloScalaSparkApp.scala:68) finished in 0.164 s
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:22:08 INFO YarnScheduler: Killing all running tasks in stage 12: Stage finished
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: Job 8 finished: show at HelloScalaSparkApp.scala:68, took 0.166670 s
[36malgo-1    |[0m 21/11/14 15:22:08 INFO CodeGenerator: Code generated in 9.992466 ms
[36malgo-1    |[0m 21/11/14 15:22:08 INFO CodeGenerator: Code generated in 8.014571 ms
[36malgo-1    |[0m +----------+----+-----------+
[36malgo-1    |[0m |      date|sale|sale_double|
[36malgo-1    |[0m +----------+----+-----------+
[36malgo-1    |[0m |2020-01-01|   5|         10|
[36malgo-1    |[0m |2020-01-01|   7|         14|
[36malgo-1    |[0m |2020-01-01|  15|         30|
[36malgo-1    |[0m |2020-01-01|  15|         30|
[36malgo-1    |[0m |2020-01-01|  23|         46|
[36malgo-1    |[0m |2020-01-01|  33|         66|
[36malgo-1    |[0m |2020-01-01|  34|         68|
[36malgo-1    |[0m |2020-01-01|  39|         78|
[36malgo-1    |[0m |2020-01-01|  40|         80|
[36malgo-1    |[0m |2020-01-01|  43|         86|
[36malgo-1    |[0m |2020-01-01|  48|         96|
[36malgo-1    |[0m |2020-01-01|  59|        118|
[36malgo-1    |[0m |2020-01-01|  64|        128|
[36malgo-1    |[0m |2020-01-01|  79|        158|
[36malgo-1    |[0m |2020-01-01|  82|        164|
[36malgo-1    |[0m |2020-01-01|  85|        170|
[36malgo-1    |[0m |2020-01-01| 103|        206|
[36malgo-1    |[0m |2020-01-01| 105|        210|
[36malgo-1    |[0m |2020-01-01| 105|        210|
[36malgo-1    |[0m |2020-01-01| 122|        244|
[36malgo-1    |[0m +----------+----+-----------+
[36malgo-1    |[0m only showing top 20 rows
[36malgo-1    |[0m 
[36malgo-1    |[0m Writing output to: file:///opt/ml/processing/output/data
[36malgo-1    |[0m 21/11/14 15:22:08 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 15:22:08 INFO FileSourceStrategy: Pushed Filters: 
[36malgo-1    |[0m 21/11/14 15:22:08 INFO FileSourceStrategy: Post-Scan Filters: 
[36malgo-1    |[0m 21/11/14 15:22:08 INFO FileSourceStrategy: Output Data Schema: struct<date: string, sale: bigint>
[36malgo-1    |[0m 21/11/14 15:22:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
[36malgo-1    |[0m 21/11/14 15:22:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DirectFileOutputCommitter: Direct Write: DISABLED
[36malgo-1    |[0m 21/11/14 15:22:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter
[36malgo-1    |[0m 21/11/14 15:22:08 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 317.8 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 192.168.240.3:40105 (size: 30.2 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO SparkContext: Created broadcast 14 from json at HelloScalaSparkApp.scala:72
[36malgo-1    |[0m 21/11/14 15:22:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 15:22:08 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 15:22:08 INFO SparkContext: Starting job: json at HelloScalaSparkApp.scala:72
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: Got job 9 (json at HelloScalaSparkApp.scala:72) with 1 output partitions
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: Final stage: ResultStage 13 (json at HelloScalaSparkApp.scala:72)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[39] at json at HelloScalaSparkApp.scala:72), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:22:08 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 15.4 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 1006.7 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 192.168.240.3:40105 (size: 7.6 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[39] at json at HelloScalaSparkApp.scala:72) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 15:22:08 INFO YarnScheduler: Adding task set 13.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 15:22:08 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 58, algo-2, executor 2, partition 0, PROCESS_LOCAL, 7755 bytes)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-2:38277 (size: 7.6 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-2:38277 (size: 30.2 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 58) in 72 ms on algo-2 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: ResultStage 13 (json at HelloScalaSparkApp.scala:72) finished in 0.078 s
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:22:08 INFO YarnScheduler: Killing all running tasks in stage 13: Stage finished
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: Job 9 finished: json at HelloScalaSparkApp.scala:72, took 0.080396 s
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: Registering RDD 40 (json at HelloScalaSparkApp.scala:72) as input to shuffle 2
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: Got map stage job 10 (json at HelloScalaSparkApp.scala:72) with 1 output partitions
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: Final stage: ShuffleMapStage 14 (json at HelloScalaSparkApp.scala:72)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[40] at json at HelloScalaSparkApp.scala:72), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:22:08 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 70.6 KiB, free 1006.7 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 15.2 KiB, free 1006.7 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 192.168.240.3:40105 (size: 15.2 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[40] at json at HelloScalaSparkApp.scala:72) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 15:22:08 INFO YarnScheduler: Adding task set 14.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 15:22:08 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 59, algo-2, executor 2, partition 0, PROCESS_LOCAL, 7744 bytes)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-2:38277 (size: 15.2 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 59) in 174 ms on algo-2 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: ShuffleMapStage 14 (json at HelloScalaSparkApp.scala:72) finished in 0.182 s
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: looking for newly runnable stages
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: running: Set()
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: waiting: Set()
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: failed: Set()
[36malgo-1    |[0m 21/11/14 15:22:08 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 2155.
[36malgo-1    |[0m 21/11/14 15:22:08 INFO CodeGenerator: Code generated in 11.405935 ms
[36malgo-1    |[0m  15:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 14.0 in stage 8.0 (TID 49). 3766 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 51
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Running task 5.0 in stage 11.0 (TID 51)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO MapOutputTrackerWorker: Updating epoch to 2 and clearing cache
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO TorrentBroadcast: Started reading broadcast variable 11 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 911.7 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO TorrentBroadcast: Reading broadcast variable 11 took 9 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 26.7 KiB, free 911.7 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@192.168.240.3:41511)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO MapOutputTrackerWorker: Got the output locations
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO CodeGenerator: Code generated in 17.4498 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO CodeGenerator: Code generated in 15.925354 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 5.0 in stage 11.0 (TID 51). 4646 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 55
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO Executor: Running task 4.0 in stage 11.0 (TID 55)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000002/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[36malgo-1    |[0m [/var/lo21/11/14 15:22:08 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 192.168.240.3:40105 in memory (size: 7.6 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO SparkContext: Starting job: json at HelloScalaSparkApp.scala:72
[36malgo-1    |[0m 21/11/14 15:22:08 INFO BlockManagerInfo: Removed broadcast_15_piece0 on algo-2:38277 in memory (size: 7.6 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: Got job 11 (json at HelloScalaSparkApp.scala:72) with 1 output partitions
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: Final stage: ResultStage 16 (json at HelloScalaSparkApp.scala:72)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: Submitting ResultStage 16 (CoalescedRDD[44] at json at HelloScalaSparkApp.scala:72), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:22:08 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 192.168.240.3:40105 in memory (size: 13.4 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO BlockManagerInfo: Removed broadcast_11_piece0 on algo-2:38277 in memory (size: 13.4 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:41021 in memory (size: 13.4 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO BlockManagerInfo: Removed broadcast_16_piece0 on algo-2:38277 in memory (size: 15.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 192.168.240.3:40105 in memory (size: 15.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 192.168.240.3:40105 in memory (size: 30.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:41021 in memory (size: 30.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 196.2 KiB, free 1007.0 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:41021 in memory (size: 7.7 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 73.2 KiB, free 1006.9 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 192.168.240.3:40105 (size: 73.2 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 192.168.240.3:40105 in memory (size: 7.7 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (CoalescedRDD[44] at json at HelloScalaSparkApp.scala:72) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 15:22:08 INFO YarnScheduler: Adding task set 16.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 15:22:08 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 60, algo-2, executor 2, partition 0, NODE_LOCAL, 8312 bytes)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-2:38277 (size: 73.2 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 192.168.240.2:58634
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Running task 1.0 in stage 11.0 (TID 52)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 1.0 in stage 11.0 (TID 52). 4642 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 53
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Running task 2.0 in stage 11.0 (TID 53)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 2.0 in stage 11.0 (TID 53). 4645 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 54
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Running task 3.0 in stage 11.0 (TID 54)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 3.0 in stage 11.0 (TID 54). 4646 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 56
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Running task 6.0 in stage 11.0 (TID 56)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:07 INFO Executor: Finished task 6.0 in stage 11.0 (TID 56). 4646 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 58
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO Executor: Running task 0.0 in stage 13.0 (TID 58)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO TorrentBroadcast: Started reading broadcast variable 15 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO TorrentBroadcast: Reading broadcast variable 15 took 5 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 15.4 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO CodeGenerator: Code generated in 11.620445 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO FileScanRDD: TID: 58 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO TorrentBroadcast: Started reading broadcast variable 14 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO TorrentBroadcast: Reading broadcast variable 14 took 6 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 445.1 KiB, free 911.8 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO Executor: Finished task 0.0 in stage 13.0 (TID 58). 58923 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 59
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO Executor: Running task 0.0 in stage 14.0 (TID 59)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO TorrentBroadcast: Started reading broadcast variable 16 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 15.2 KiB, free 911.8 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO TorrentBroadcast: Reading broadcast variable 16 took 4 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 70.6 KiB, free 911.7 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO CodeGenerator: Code generated in 7.222373 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO FileScanRDD: TID: 59 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO Executor: Finished task 0.0 in stage 14.0 (TID 59). 3742 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 60
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO Executor: Running task 0.0 in stage 16.0 (TID 60)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO MapOutputTrackerWorker: Updating epoch to 3 and clearing cache
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO TorrentBroadcast: Started reading broadcast variable 17 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 73.2 KiB, free 911.8 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO TorrentBroadcast: Reading broadcast variable 17 took 5 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 196.2 KiB, free 911.6 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO DirectFileOutputCommitter: Direct Write: DISABLED
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@192.168.240.3:41511)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO MapOutputTrackerWorker: Got the output locations
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO CodeGenerator: Code generated in 11.132921 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (3.1 KiB) non-empty blocks including 1 (3.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (3.1 KiB) non-empty blocks including 1 (3.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (3.0 KiB) non-empty blocks including 1 (3.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m 21/11/14 15:22:08 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 60) in 262 ms on algo-2 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 15:22:08 INFO YarnScheduler: Removed TaskSet 16.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: ResultStage 16 (json at HelloScalaSparkApp.scala:72) finished in 0.289 s
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:22:08 INFO YarnScheduler: Killing all running tasks in stage 16: Stage finished
[36malgo-1    |[0m 21/11/14 15:22:08 INFO DAGScheduler: Job 11 finished: json at HelloScalaSparkApp.scala:72, took 0.294536 s
[36malgo-1    |[0m 21/11/14 15:22:08 INFO FileFormatWriter: Write Job e83c4f68-d19b-4b7f-83ca-87bf54aefe86 committed.
[36malgo-1    |[0m 21/11/14 15:22:08 INFO FileFormatWriter: Finished processing stats for write job e83c4f68-d19b-4b7f-83ca-87bf54aefe86.
[36malgo-1    |[0m 21/11/14 15:22:08 INFO SparkUI: Stopped Spark web UI at http://192.168.240.3:4040
[36malgo-1    |[0m 21/11/14 15:22:08 INFO YarnClientSchedulerBackend: Interrupting monitor thread
[36malgo-1    |[0m 21/11/14 15:22:08 INFO YarnClientSchedulerBackend: Shutting down all executors
[36malgo-1    |[0m 21/11/14 15:22:08 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
[36malgo-1    |[0m 21/11/14 15:22:08 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
[36malgo-1    |[0m 21/11/14 15:22:08 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[36malgo-1    |[0m 21/11/14 15:22:08 INFO MemoryStore: MemoryStore cleared
[36malgo-1    |[0m 21/11/14 15:22:08 INFO BlockManager: BlockManager stopped
[36malgo-1    |[0m 21/11/14 15:22:08 INFO BlockManagerMaster: BlockManagerMaster stopped
[36malgo-1    |[0m 21/11/14 15:22:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[36malgo-1    |[0m 21/11/14 15:22:09 INFO SparkContext: Successfully stopped SparkContext
[36malgo-1    |[0m 21/11/14 15:22:09 INFO ShutdownHookManager: Shutdown hook called
[36malgo-1    |[0m 21/11/14 15:22:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-2683eb3f-8962-49e0-b873-17f3ee141200
[36malgo-1    |[0m 21/11/14 15:22:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-de9aae39-b142-49d7-9d70-bbc401aba056
[36malgo-1    |[0m 2021-11-14 15:22:09,029 INFO attempt.RMAppAttemptImpl: Updating application attempt appattempt_1636903291807_0001_000001 with final state: FINISHING, and exit status: -1000
[36malgo-1    |[0m 2021-11-14 15:22:09,030 INFO attempt.RMAppAttemptImpl: appattempt_1636903291807_0001_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
[36malgo-1    |[0m 2021-11-14 15:22:09,030 INFO rmapp.RMAppImpl: Updating application application_1636903291807_0001 with final state: FINISHING
[36malgo-1    |[0m 2021-11-14 15:22:09,030 INFO rmapp.RMAppImpl: application_1636903291807_0001 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
[36malgo-1    |[0m 2021-11-14 15:22:09,030 INFO recovery.RMStateStore: Updating info for app: application_1636903291807_0001
[36malgo-1    |[0m 2021-11-14 15:22:09,030 INFO attempt.RMAppAttemptImpl: appattempt_1636903291807_0001_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
[36malgo-1    |[0m 2021-11-14 15:22:09,030 INFO rmapp.RMAppImpl: application_1636903291807_0001 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 2021-11-14 15:22:09,047 INFO launcher.ContainerLaunch: Container container_1636903291807_0001_01_000003 succeeded 
[33malgo-2    |[0m 2021-11-14 15:22:09,054 INFO container.ContainerImpl: Container container_1636903291807_0001_01_000003 transitioned from RUNNING to EXITED_WITH_SUCCESS
[36malgo-1    |[0m 2021-11-14 15:22:09,055 INFO launcher.ContainerLaunch: Container container_1636903291807_0001_01_000002 succeeded 
[33malgo-2    |[0m 2021-11-14 15:22:09,055 INFO launcher.ContainerCleanup: Cleaning up container container_1636903291807_0001_01_000003
[33malgo-2    |[0m 2021-11-14 15:22:09,057 INFO nodemanager.NMAuditLogger: USER=root	OPERATION=Container Finished - Succeeded	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1636903291807_0001	CONTAINERID=container_1636903291807_0001_01_000003
[33malgo-2    |[0m 2021-11-14 15:22:09,057 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000003
[33malgo-2    |[0m 2021-11-14 15:22:09,059 INFO container.ContainerImpl: Container container_1636903291807_0001_01_000003 transitioned from EXITED_WITH_SUCCESS to DONE
[33malgo-2    |[0m 2021-11-14 15:22:09,059 INFO application.ApplicationImpl: Removing container_1636903291807_0001_01_000003 from application application_1636903291807_0001
[33malgo-2    |[0m 2021-11-14 15:22:09,060 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1636903291807_0001_01_000003
[33malgo-2    |[0m 2021-11-14 15:22:09,060 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1636903291807_0001
[36malgo-1    |[0m 2021-11-14 15:22:09,062 INFO rmcontainer.RMContainerImpl: container_1636903291807_0001_01_000003 Container Transitioned from RUNNING to COMPLETED
[36malgo-1    |[0m 2021-11-14 15:22:09,062 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636903291807_0001	CONTAINERID=container_1636903291807_0001_01_000003	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 15:22:09,063 INFO container.ContainerImpl: Container container_1636903291807_0001_01_000002 transitioned from RUNNING to EXITED_WITH_SUCCESS
[36malgo-1    |[0m 2021-11-14 15:22:09,065 INFO launcher.ContainerCleanup: Cleaning up container container_1636903291807_0001_01_000002
[36malgo-1    |[0m 2021-11-14 15:22:09,066 INFO nodemanager.NMAuditLogger: USER=root	OPERATION=Container Finished - Succeeded	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1636903291807_0001	CONTAINERID=container_1636903291807_0001_01_000002
[36malgo-1    |[0m 2021-11-14 15:22:09,067 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000002
[36malgo-1    |[0m 2021-11-14 15:22:09,068 INFO container.ContainerImpl: Container container_1636903291807_0001_01_000002 transitioned from EXITED_WITH_SUCCESS to DONE
[36malgo-1    |[0m 2021-11-14 15:22:09,068 INFO application.ApplicationImpl: Removing container_1636903291807_0001_01_000002 from application application_1636903291807_0001
[36malgo-1    |[0m 2021-11-14 15:22:09,069 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1636903291807_0001_01_000002
[36malgo-1    |[0m 2021-11-14 15:22:09,069 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1636903291807_0001
[36malgo-1    |[0m 2021-11-14 15:22:09,070 INFO rmcontainer.RMContainerImpl: container_1636903291807_0001_01_000002 Container Transitioned from RUNNING to COMPLETED
[36malgo-1    |[0m 2021-11-14 15:22:09,070 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636903291807_0001	CONTAINERID=container_1636903291807_0001_01_000002	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[33malgo-2    |[0m 2021-11-14 15:22:09,075 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000003/launch_container.sh
[33malgo-2    |[0m 2021-11-14 15:22:09,075 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000003/launch_container.sh]
[33malgo-2    |[0m 2021-11-14 15:22:09,075 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000003/container_tokens
[33malgo-2    |[0m 2021-11-14 15:22:09,075 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000003/container_tokens]
[33malgo-2    |[0m 2021-11-14 15:22:09,075 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000003/sysfs
[33malgo-2    |[0m 2021-11-14 15:22:09,075 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000003/sysfs]
[36malgo-1    |[0m 2021-11-14 15:22:09,082 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000002/launch_container.sh
[36malgo-1    |[0m 2021-11-14 15:22:09,082 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000002/launch_container.sh]
[36malgo-1    |[0m 2021-11-14 15:22:09,082 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000002/container_tokens
[36malgo-1    |[0m 2021-11-14 15:22:09,083 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000002/container_tokens]
[36malgo-1    |[0m 2021-11-14 15:22:09,083 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000002/sysfs
[36malgo-1    |[0m 2021-11-14 15:22:09,083 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000002/sysfs]
[36malgo-1    |[0m 2021-11-14 15:22:09,131 INFO resourcemanager.ApplicationMasterService: application_1636903291807_0001 unregistered successfully. 
[36malgo-1    |[0m 11-14 15:22 smspark-submit INFO     spark submit was successful. primary node exiting.
[33malgo-2    |[0m 2021-11-14 15:22:09,491 INFO launcher.ContainerLaunch: Container container_1636903291807_0001_01_000001 succeeded 
[33malgo-2    |[0m 2021-11-14 15:22:09,491 INFO container.ContainerImpl: Container container_1636903291807_0001_01_000001 transitioned from RUNNING to EXITED_WITH_SUCCESS
[33malgo-2    |[0m 2021-11-14 15:22:09,491 INFO launcher.ContainerCleanup: Cleaning up container container_1636903291807_0001_01_000001
[33malgo-2    |[0m 2021-11-14 15:22:09,492 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000001
[33malgo-2    |[0m 2021-11-14 15:22:09,492 INFO nodemanager.NMAuditLogger: USER=root	OPERATION=Container Finished - Succeeded	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1636903291807_0001	CONTAINERID=container_1636903291807_0001_01_000001
[33malgo-2    |[0m 2021-11-14 15:22:09,493 INFO container.ContainerImpl: Container container_1636903291807_0001_01_000001 transitioned from EXITED_WITH_SUCCESS to DONE
[33malgo-2    |[0m 2021-11-14 15:22:09,493 INFO application.ApplicationImpl: Removing container_1636903291807_0001_01_000001 from application application_1636903291807_0001
[33malgo-2    |[0m 2021-11-14 15:22:09,493 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1636903291807_0001_01_000001
[33malgo-2    |[0m 2021-11-14 15:22:09,493 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1636903291807_0001
[33malgo-2    |[0m 2021-11-14 15:22:09,497 INFO retry.RetryInvocationHandler: java.io.EOFException: End of File Exception between local host is: "algo-2/192.168.240.2"; destination host is: "algo-1.spark-network":8031; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException, while invoking ResourceTrackerPBClientImpl.nodeHeartbeat over null. Retrying after sleeping for 30000ms.
[33malgo-2    |[0m 2021-11-14 15:22:09,504 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000001/launch_container.sh
[33malgo-2    |[0m 2021-11-14 15:22:09,504 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000001/launch_container.sh]
[33malgo-2    |[0m 2021-11-14 15:22:09,504 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000001/container_tokens
[33malgo-2    |[0m 2021-11-14 15:22:09,504 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000001/container_tokens]
[33malgo-2    |[0m 2021-11-14 15:22:09,504 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000001/sysfs
[33malgo-2    |[0m 2021-11-14 15:22:09,504 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903291807_0001/container_1636903291807_0001_01_000001/sysfs]
[33malgo-2    |[0m 11-14 15:22 urllib3.connectionpool WARNING  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcfd5871790>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stdout] 2021-11-14T15:21:54.016+0000: [GC (Allocation Failure) [PSYoungGen: 120320K->8981K(140288K)] 120320K->8989K(461312K), 0.0075271 secs] [Times: user=0.01 sys=0.01, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stdout] 2021-11-14T15:21:54.454+0000: [GC (Allocation Failure) [PSYoungGen: 129301K->7746K(140288K)] 129309K->7762K(461312K), 0.0059463 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stdout] 2021-11-14T15:21:54.768+0000: [GC (Metadata GC Threshold) [PSYoungGen: 109370K->8658K(140288K)] 109386K->8682K(461312K), 0.0045980 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stdout] 2021-11-14T15:21:54.773+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 8658K->0K(140288K)] [ParOldGen: 24K->8404K(188928K)] 8682K->8404K(329216K), [Metaspace: 20376K->20376K(1067008K)], 0.0214246 secs] [Times: user=0.04 sys=0.01, real=0.02 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stdout] 2021-11-14T15:21:55.270+0000: [GC (Allocation Failure) [PSYoungGen: 120320K->4628K(184832K)] 128724K->13040K(373760K), 0.0044851 secs] [Times: user=0.00 sys=0.01, real=0.00 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stdout] 2021-11-14T15:21:55.695+0000: [GC (Allocation Failure) [PSYoungGen: 184340K->7675K(245760K)] 192752K->16087K(434688K), 0.0066344 secs] [Times: user=0.00 sys=0.01, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stdout] 2021-11-14T15:21:55.787+0000: [GC (Metadata GC Threshold) [PSYoungGen: 43850K->3889K(282624K)] 52262K->12309K(471552K), 0.0041598 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stdout] 2021-11-14T15:21:55.791+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 3889K->0K(282624K)] [ParOldGen: 8420K->9988K(274944K)] 12309K->9988K(557568K), [Metaspace: 33949K->33946K(1079296K)], 0.0248789 secs] [Times: user=0.06 sys=0.00, real=0.02 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stdout] 2021-11-14T15:21:56.533+0000: [GC (Allocation Failure) [PSYoungGen: 272384K->10734K(283136K)] 282372K->22536K(558080K), 0.0129040 secs] [Times: user=0.06 sys=0.01, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stdout] 2021-11-14T15:22:00.825+0000: [GC (Metadata GC Threshold) [PSYoungGen: 256302K->12769K(411136K)] 268104K->26407K(686080K), 0.0116249 secs] [Times: user=0.02 sys=0.01, real=0.02 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stdout] 2021-11-14T15:22:00.836+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 12769K->0K(411136K)] [ParOldGen: 13638K->23007K(373760K)] 26407K->23007K(784896K), [Metaspace: 54009K->54009K(1099776K)], 0.0803380 secs] [Times: user=0.18 sys=0.00, real=0.08 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stdout] 2021-11-14T15:22:02.549+0000: [GC (Allocation Failure) [PSYoungGen: 398336K->13417K(424960K)] 421343K->36433K(798720K), 0.0116734 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stdout] 2021-11-14T15:22:08.458+0000: [GC (Allocation Failure) [PSYoungGen: 423017K->14930K(517120K)] 446033K->103489K(890880K), 0.0327907 secs] [Times: user=0.06 sys=0.03, real=0.03 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/std1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (3.1 KiB) non-empty blocks including 1 (3.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (3.0 KiB) non-empty blocks including 1 (3.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (3.0 KiB) non-empty blocks including 1 (3.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (3.0 KiB) non-empty blocks including 1 (3.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Getting 1 (1408.0 B) non-empty blocks including 1 (1408.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/stderr] 21/11/14 15:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_20211114152208_0016_m_000000_60' to file:/opt/ml/processing/output/data
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903291807_0001/container_1636903291807_0001_01_000003/s11-14 15:22 urllib3.connectionpool WARNING  Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcfd5871890>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m 2021-11-14 15:22:10,812 WARN datanode.DataNode: IOException in offerService
[33malgo-2    |[0m java.io.EOFException: End of File Exception between local host is: "algo-2/192.168.240.2"; destination host is: "algo-1.spark-network":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
[33malgo-2    |[0m 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[33malgo-2    |[0m 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[33malgo-2    |[0m 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[33malgo-2    |[0m 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
[33malgo-2    |[0m 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
[33malgo-2    |[0m 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:791)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
[33malgo-2    |[0m 	at com.sun.proxy.$Proxy16.sendHeartbeat(Unknown Source)
[33malgo-2    |[0m 	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:168)
[33malgo-2    |[0m 	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:517)
[33malgo-2    |[0m 	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
[33malgo-2    |[0m 	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
[33malgo-2    |[0m 	at java.lang.Thread.run(Thread.java:748)
[33malgo-2    |[0m Caused by: java.io.EOFException
[33malgo-2    |[0m 	at java.io.DataInputStream.readInt(DataInputStream.java:392)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)
[33malgo-2    |[0m 11-14 15:22 urllib3.connectionpool WARNING  Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcfd580b210>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m 11-14 15:22 urllib3.connectionpool WARNING  Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcfd580b610>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[36malgo-1 exited with code 0
[0m[33malgo-2    |[0m 11-14 15:22 urllib3.connectionpool WARNING  Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcfd580ba10>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m 11-14 15:22 smspark-submit INFO     primary is down, worker now exiting
[33malgo-2 exited with code 0
[0mThe CMD variable is not set. Defaulting to a blank string.
Removing algo-1 ... 
Removing algo-2 ... 
[2A[2KRemoving algo-1 ... [32mdone[0m[2B[1A[2KRemoving algo-2 ... [32mdone[0m[1BRemoving network spark-network


Running docker-compose down ...
PASSED
test/integration/local/test_multinode_container.py::test_java_spark_multinode CMD='--class com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp --verbose /opt/ml/processing/input/code/java/hello-java-spark/target/hello-java-spark-1.0-SNAPSHOT.jar --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data' docker-compose up --force-recreate
Creating network "spark-network" with the default driver
Creating algo-2 ... 
Creating algo-1 ... 
[1A[2K
Creating algo-1 ... [32mdone[0m
[1B[2A[2K
Creating algo-2 ... [32mdone[0m
[2BAttaching to algo-1, algo-2
[36malgo-1    |[0m 11-14 15:22 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--class', 'com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp', '--verbose', '/opt/ml/processing/input/code/java/hello-java-spark/target/hello-java-spark-1.0-SNAPSHOT.jar', '--input', 'file:///opt/ml/processing/input/data/data.jsonl', '--output', 'file:///opt/ml/processing/output/data']
[36malgo-1    |[0m 11-14 15:22 smspark.cli  INFO     Raw spark options before processing: {'class_': 'com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp', 'verbose': True, 'jars': None, 'py_files': None, 'files': None}
[36malgo-1    |[0m 11-14 15:22 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/java/hello-java-spark/target/hello-java-spark-1.0-SNAPSHOT.jar', '--input', 'file:///opt/ml/processing/input/data/data.jsonl', '--output', 'file:///opt/ml/processing/output/data']
[36malgo-1    |[0m 11-14 15:22 smspark.cli  INFO     Rendered spark options: {'class_': 'com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp', 'verbose': True, 'jars': None, 'py_files': None, 'files': None}
[36malgo-1    |[0m 11-14 15:22 smspark.cli  INFO     Initializing processing job.
[36malgo-1    |[0m 11-14 15:22 smspark-submit INFO     {'current_host': 'algo-1', 'hosts': ['algo-1', 'algo-2']}
[36malgo-1    |[0m 11-14 15:22 smspark-submit INFO     {'ProcessingJobArn': 'processing_job_arn', 'ProcessingJobName': 'processing_job_name', 'AppSpecification': {'ImageUri': 'image_uri'}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30}}, 'RoleArn': 'iam_role'}
[36malgo-1    |[0m 11-14 15:22 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client --class com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp --verbose /opt/ml/processing/input/code/java/hello-java-spark/target/hello-java-spark-1.0-SNAPSHOT.jar --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data
[36malgo-1    |[0m 11-14 15:22 smspark-submit INFO     waiting for hosts
[36malgo-1    |[0m 11-14 15:22 smspark-submit INFO     starting status server
[36malgo-1    |[0m 11-14 15:22 smspark-submit INFO     Status server listening on algo-1:5555
[36malgo-1    |[0m 11-14 15:22 smspark-submit INFO     bootstrapping cluster
[36malgo-1    |[0m 11-14 15:22 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING
[36malgo-1    |[0m 11-14 15:22 smspark-submit INFO     copying aws jars
[36malgo-1    |[0m Serving on http://algo-1:5555
[33malgo-2    |[0m 11-14 15:22 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--class', 'com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp', '--verbose', '/opt/ml/processing/input/code/java/hello-java-spark/target/hello-java-spark-1.0-SNAPSHOT.jar', '--input', 'file:///opt/ml/processing/input/data/data.jsonl', '--output', 'file:///opt/ml/processing/output/data']
[33malgo-2    |[0m 11-14 15:22 smspark.cli  INFO     Raw spark options before processing: {'class_': 'com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp', 'verbose': True, 'jars': None, 'py_files': None, 'files': None}
[33malgo-2    |[0m 11-14 15:22 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/java/hello-java-spark/target/hello-java-spark-1.0-SNAPSHOT.jar', '--input', 'file:///opt/ml/processing/input/data/data.jsonl', '--output', 'file:///opt/ml/processing/output/data']
[33malgo-2    |[0m 11-14 15:22 smspark.cli  INFO     Rendered spark options: {'class_': 'com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp', 'verbose': True, 'jars': None, 'py_files': None, 'files': None}
[33malgo-2    |[0m 11-14 15:22 smspark.cli  INFO     Initializing processing job.
[33malgo-2    |[0m 11-14 15:22 smspark-submit INFO     {'current_host': 'algo-2', 'hosts': ['algo-1', 'algo-2']}
[33malgo-2    |[0m 11-14 15:22 smspark-submit INFO     {'ProcessingJobArn': 'processing_job_arn', 'ProcessingJobName': 'processing_job_name', 'AppSpecification': {'ImageUri': 'image_uri'}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30}}, 'RoleArn': 'iam_role'}
[33malgo-2    |[0m 11-14 15:22 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client --class com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp --verbose /opt/ml/processing/input/code/java/hello-java-spark/target/hello-java-spark-1.0-SNAPSHOT.jar --input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data
[33malgo-2    |[0m 11-14 15:22 smspark-submit INFO     waiting for hosts
[33malgo-2    |[0m 11-14 15:22 smspark-submit INFO     starting status server
[33malgo-2    |[0m 11-14 15:22 smspark-submit INFO     Status server listening on algo-2:5555
[33malgo-2    |[0m 11-14 15:22 smspark-submit INFO     bootstrapping cluster
[33malgo-2    |[0m 11-14 15:22 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING
[33malgo-2    |[0m 11-14 15:22 smspark-submit INFO     copying aws jars
[33malgo-2    |[0m Serving on http://algo-2:5555
[36malgo-1    |[0m 11-14 15:22 smspark-submit INFO     Found hadoop jar hadoop-aws.jar
[36malgo-1    |[0m 11-14 15:22 smspark-submit INFO     Optional jar jets3t-0.9.0.jar in /usr/lib/hadoop/lib does not exist
[36malgo-1    |[0m 11-14 15:22 smspark-submit INFO     copying cluster config
[36malgo-1    |[0m 11-14 15:22 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml
[36malgo-1    |[0m 11-14 15:22 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml
[36malgo-1    |[0m 11-14 15:22 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[36malgo-1    |[0m 11-14 15:22 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m 11-14 15:22 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh
[36malgo-1    |[0m 11-14 15:22 root         INFO     Detected instance type: m5.xlarge with total memory: 16384M and total cores: 4
[36malgo-1    |[0m 11-14 15:22 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[36malgo-1    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0"?>
[36malgo-1    |[0m <!-- Site specific YARN configuration properties -->
[36malgo-1    |[0m  <configuration>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.resourcemanager.hostname</name>
[36malgo-1    |[0m          <value>172.18.0.3</value>
[36malgo-1    |[0m          <description>The hostname of the RM.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.hostname</name>
[36malgo-1    |[0m          <value>algo-1</value>
[36malgo-1    |[0m          <description>The hostname of the NM.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.webapp.address</name>
[36malgo-1    |[0m          <value>algo-1:8042</value>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.vmem-pmem-ratio</name>
[36malgo-1    |[0m          <value>5</value>
[36malgo-1    |[0m          <description>Ratio between virtual memory to physical memory.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.resourcemanager.am.max-attempts</name>
[36malgo-1    |[0m          <value>1</value>
[36malgo-1    |[0m          <description>The maximum number of application attempts.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.env-whitelist</name>
[36malgo-1    |[0m          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>
[36malgo-1    |[0m          <description>Environment variable whitelist</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m  
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.minimum-allocation-mb</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.maximum-allocation-mb</name>
[36malgo-1    |[0m     <value>15892</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.minimum-allocation-vcores</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.maximum-allocation-vcores</name>
[36malgo-1    |[0m     <value>4</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.nodemanager.resource.memory-mb</name>
[36malgo-1    |[0m     <value>15892</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.nodemanager.resource.cpu-vcores</name>
[36malgo-1    |[0m     <value>4</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:22 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[36malgo-1    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.driver.host=172.18.0.3
[36malgo-1    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[36malgo-1    |[0m 
[36malgo-1    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[36malgo-1    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[36malgo-1    |[0m spark.rpc.askTimeout=300s
[36malgo-1    |[0m spark.driver.memory 2048m
[36malgo-1    |[0m spark.driver.memoryOverhead 204m
[36malgo-1    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[36malgo-1    |[0m spark.executor.memory 12399m
[36malgo-1    |[0m spark.executor.memoryOverhead 1239m
[36malgo-1    |[0m spark.executor.cores 4
[36malgo-1    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[36malgo-1    |[0m spark.executor.instances 2
[36malgo-1    |[0m spark.default.parallelism 16
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:22 root         INFO     Finished Yarn configuration files setup.
[36malgo-1    |[0m 11-14 15:22 root         INFO     reading user configuration from /opt/ml/processing/input/conf/configuration.json
[36malgo-1    |[0m 11-14 15:22 root         INFO     User configuration list or dict: [{'Classification': 'spark-defaults', 'Properties': {'spark.executor.memory': '2g', 'spark.executor.cores': '1'}}, {'Classification': 'hadoop-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'core-site', 'Properties': {'spark.executor.memory': '2g', 'spark.executor.cores': '1'}}, {'Classification': 'hadoop-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'hive-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-exec-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-site', 'Properties': {'key': 'value'}}, {'Classification': 'spark-defaults', 'Properties': {'key': 'value'}}, {'Classification': 'spark-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'spark-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'spark-hive-site', 'Properties': {'key': 'value'}}, {'Classification': 'spark-metrics', 'Properties': {'key': 'value'}}, {'Classification': 'yarn-site', 'Properties': {'key': 'value'}}, {'Classification': 'yarn-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}] , type <class 'list'>
[36malgo-1    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[36malgo-1    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.driver.host=172.18.0.3
[36malgo-1    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[36malgo-1    |[0m 
[36malgo-1    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[36malgo-1    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[36malgo-1    |[0m spark.rpc.askTimeout=300s
[36malgo-1    |[0m spark.driver.memory 2048m
[36malgo-1    |[0m spark.driver.memoryOverhead 204m
[36malgo-1    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[36malgo-1    |[0m spark.executor.memory 12399m
[36malgo-1    |[0m spark.executor.memoryOverhead 1239m
[36malgo-1    |[0m spark.executor.cores 4
[36malgo-1    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[36malgo-1    |[0m spark.executor.instances 2
[36malgo-1    |[0m spark.default.parallelism 16
[36malgo-1    |[0m spark.executor.memory 2g
[36malgo-1    |[0m spark.executor.cores 1
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/hadoop-env.sh
[36malgo-1    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/hadoop-env.sh is: 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[36malgo-1    |[0m # or more contributor license agreements.  See the NOTICE file
[36malgo-1    |[0m # distributed with this work for additional information
[36malgo-1    |[0m # regarding copyright ownership.  The ASF licenses this file
[36malgo-1    |[0m # to you under the Apache License, Version 2.0 (the
[36malgo-1    |[0m # "License"); you may not use this file except in compliance
[36malgo-1    |[0m # with the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m # Set Hadoop-specific environment variables here.
[36malgo-1    |[0m 
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## THIS FILE ACTS AS THE MASTER FILE FOR ALL HADOOP PROJECTS.
[36malgo-1    |[0m ## SETTINGS HERE WILL BE READ BY ALL HADOOP COMMANDS.  THEREFORE,
[36malgo-1    |[0m ## ONE CAN USE THIS FILE TO SET YARN, HDFS, AND MAPREDUCE
[36malgo-1    |[0m ## CONFIGURATION OPTIONS INSTEAD OF xxx-env.sh.
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## Precedence rules:
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## {yarn-env.sh|hdfs-env.sh} > hadoop-env.sh > hard-coded defaults
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## {YARN_xyz|HDFS_xyz} > HADOOP_xyz > hard-coded defaults
[36malgo-1    |[0m ##
[36malgo-1    |[0m 
[36malgo-1    |[0m # Many of the options here are built from the perspective that users
[36malgo-1    |[0m # may want to provide OVERWRITING values on the command line.
[36malgo-1    |[0m # For example:
[36malgo-1    |[0m #
[36malgo-1    |[0m #  JAVA_HOME=/usr/java/testing hdfs dfs -ls
[36malgo-1    |[0m #
[36malgo-1    |[0m # Therefore, the vast majority (BUT NOT ALL!) of these defaults
[36malgo-1    |[0m # are configured for substitution and not append.  If append
[36malgo-1    |[0m # is preferable, modify this file accordingly.
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Generic settings for HADOOP
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Technically, the only required environment variable is JAVA_HOME.
[36malgo-1    |[0m # All others are optional.  However, the defaults are probably not
[36malgo-1    |[0m # preferred.  Many sites configure these options outside of Hadoop,
[36malgo-1    |[0m # such as in /etc/profile.d
[36malgo-1    |[0m 
[36malgo-1    |[0m # The java implementation to use. By default, this environment
[36malgo-1    |[0m # variable is REQUIRED on ALL platforms except OS X!
[36malgo-1    |[0m # export JAVA_HOME=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Location of Hadoop.  By default, Hadoop will attempt to determine
[36malgo-1    |[0m # this location based upon its execution path.
[36malgo-1    |[0m # export HADOOP_HOME=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Location of Hadoop's configuration information.  i.e., where this
[36malgo-1    |[0m # file is living. If this is not defined, Hadoop will attempt to
[36malgo-1    |[0m # locate it based upon its execution path.
[36malgo-1    |[0m #
[36malgo-1    |[0m # NOTE: It is recommend that this variable not be set here but in
[36malgo-1    |[0m # /etc/profile.d or equivalent.  Some options (such as
[36malgo-1    |[0m # --config) may react strangely otherwise.
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
[36malgo-1    |[0m 
[36malgo-1    |[0m # The maximum amount of heap to use (Java -Xmx).  If no unit
[36malgo-1    |[0m # is provided, it will be converted to MB.  Daemons will
[36malgo-1    |[0m # prefer any Xmx setting in their respective _OPT variable.
[36malgo-1    |[0m # There is no default; the JVM will autoscale based upon machine
[36malgo-1    |[0m # memory size.
[36malgo-1    |[0m # export HADOOP_HEAPSIZE_MAX=
[36malgo-1    |[0m 
[36malgo-1    |[0m # The minimum amount of heap to use (Java -Xms).  If no unit
[36malgo-1    |[0m # is provided, it will be converted to MB.  Daemons will
[36malgo-1    |[0m # prefer any Xms setting in their respective _OPT variable.
[36malgo-1    |[0m # There is no default; the JVM will autoscale based upon machine
[36malgo-1    |[0m # memory size.
[36malgo-1    |[0m # export HADOOP_HEAPSIZE_MIN=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Enable extra debugging of Hadoop's JAAS binding, used to set up
[36malgo-1    |[0m # Kerberos security.
[36malgo-1    |[0m # export HADOOP_JAAS_DEBUG=true
[36malgo-1    |[0m 
[36malgo-1    |[0m # Extra Java runtime options for all Hadoop commands. We don't support
[36malgo-1    |[0m # IPv6 yet/still, so by default the preference is set to IPv4.
[36malgo-1    |[0m # export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true"
[36malgo-1    |[0m # For Kerberos debugging, an extended option set logs more information
[36malgo-1    |[0m # export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug"
[36malgo-1    |[0m 
[36malgo-1    |[0m # Some parts of the shell code may do special things dependent upon
[36malgo-1    |[0m # the operating system.  We have to set this here. See the next
[36malgo-1    |[0m # section as to why....
[36malgo-1    |[0m #export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}
[36malgo-1    |[0m 
[36malgo-1    |[0m # Extra Java runtime options for some Hadoop commands
[36malgo-1    |[0m # and clients (i.e., hdfs dfs -blah).  These get appended to HADOOP_OPTS for
[36malgo-1    |[0m # such commands.  In most cases, # this should be left empty and
[36malgo-1    |[0m # let users supply it on the command line.
[36malgo-1    |[0m # export HADOOP_CLIENT_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # A note about classpaths.
[36malgo-1    |[0m #
[36malgo-1    |[0m # By default, Apache Hadoop overrides Java's CLASSPATH
[36malgo-1    |[0m # environment variable.  It is configured such
[36malgo-1    |[0m # that it starts out blank with new entries added after passing
[36malgo-1    |[0m # a series of checks (file/dir exists, not already listed aka
[36malgo-1    |[0m # de-deduplication).  During de-deduplication, wildcards and/or
[36malgo-1    |[0m # directories are *NOT* expanded to keep it simple. Therefore,
[36malgo-1    |[0m # if the computed classpath has two specific mentions of
[36malgo-1    |[0m # awesome-methods-1.0.jar, only the first one added will be seen.
[36malgo-1    |[0m # If two directories are in the classpath that both contain
[36malgo-1    |[0m # awesome-methods-1.0.jar, then Java will pick up both versions.
[36malgo-1    |[0m 
[36malgo-1    |[0m # An additional, custom CLASSPATH. Site-wide configs should be
[36malgo-1    |[0m # handled via the shellprofile functionality, utilizing the
[36malgo-1    |[0m # hadoop_add_classpath function for greater control and much
[36malgo-1    |[0m # harder for apps/end-users to accidentally override.
[36malgo-1    |[0m # Similarly, end users should utilize ${HOME}/.hadooprc .
[36malgo-1    |[0m # This variable should ideally only be used as a short-cut,
[36malgo-1    |[0m # interactive way for temporary additions on the command line.
[36malgo-1    |[0m # export HADOOP_CLASSPATH="/some/cool/path/on/your/machine"
[36malgo-1    |[0m 
[36malgo-1    |[0m # Should HADOOP_CLASSPATH be first in the official CLASSPATH?
[36malgo-1    |[0m # export HADOOP_USER_CLASSPATH_FIRST="yes"
[36malgo-1    |[0m 
[36malgo-1    |[0m # If HADOOP_USE_CLIENT_CLASSLOADER is set, the classpath along
[36malgo-1    |[0m # with the main jar are handled by a separate isolated
[36malgo-1    |[0m # client classloader when 'hadoop jar', 'yarn jar', or 'mapred job'
[36malgo-1    |[0m # is utilized. If it is set, HADOOP_CLASSPATH and
[36malgo-1    |[0m # HADOOP_USER_CLASSPATH_FIRST are ignored.
[36malgo-1    |[0m # export HADOOP_USE_CLIENT_CLASSLOADER=true
[36malgo-1    |[0m 
[36malgo-1    |[0m # HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES overrides the default definition of
[36malgo-1    |[0m # system classes for the client classloader when HADOOP_USE_CLIENT_CLASSLOADER
[36malgo-1    |[0m # is enabled. Names ending in '.' (period) are treated as package names, and
[36malgo-1    |[0m # names starting with a '-' are treated as negative matches. For example,
[36malgo-1    |[0m # export HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES="-org.apache.hadoop.UserClass,java.,javax.,org.apache.hadoop."
[36malgo-1    |[0m 
[36malgo-1    |[0m # Enable optional, bundled Hadoop features
[36malgo-1    |[0m # This is a comma delimited list.  It may NOT be overridden via .hadooprc
[36malgo-1    |[0m # Entries may be added/removed as needed.
[36malgo-1    |[0m # export HADOOP_OPTIONAL_TOOLS="hadoop-azure,hadoop-azure-datalake,hadoop-aws,hadoop-openstack,hadoop-aliyun,hadoop-kafka"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Options for remote shell connectivity
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # There are some optional components of hadoop that allow for
[36malgo-1    |[0m # command and control of remote hosts.  For example,
[36malgo-1    |[0m # start-dfs.sh will attempt to bring up all NNs, DNS, etc.
[36malgo-1    |[0m 
[36malgo-1    |[0m # Options to pass to SSH when one of the "log into a host and
[36malgo-1    |[0m # start/stop daemons" scripts is executed
[36malgo-1    |[0m # export HADOOP_SSH_OPTS="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10s"
[36malgo-1    |[0m 
[36malgo-1    |[0m # The built-in ssh handler will limit itself to 10 simultaneous connections.
[36malgo-1    |[0m # For pdsh users, this sets the fanout size ( -f )
[36malgo-1    |[0m # Change this to increase/decrease as necessary.
[36malgo-1    |[0m # export HADOOP_SSH_PARALLEL=10
[36malgo-1    |[0m 
[36malgo-1    |[0m # Filename which contains all of the hosts for any remote execution
[36malgo-1    |[0m # helper scripts # such as workers.sh, start-dfs.sh, etc.
[36malgo-1    |[0m # export HADOOP_WORKERS="${HADOOP_CONF_DIR}/workers"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Options for all daemons
[36malgo-1    |[0m ###
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Many options may also be specified as Java properties.  It is
[36malgo-1    |[0m # very common, and in many cases, desirable, to hard-set these
[36malgo-1    |[0m # in daemon _OPTS variables.  Where applicable, the appropriate
[36malgo-1    |[0m # Java property is also identified.  Note that many are re-used
[36malgo-1    |[0m # or set differently in certain contexts (e.g., secure vs
[36malgo-1    |[0m # non-secure)
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m # Where (primarily) daemon log files are stored.
[36malgo-1    |[0m # ${HADOOP_HOME}/logs by default.
[36malgo-1    |[0m # Java property: hadoop.log.dir
[36malgo-1    |[0m # export HADOOP_LOG_DIR=${HADOOP_HOME}/logs
[36malgo-1    |[0m 
[36malgo-1    |[0m # A string representing this instance of hadoop. $USER by default.
[36malgo-1    |[0m # This is used in writing log and pid files, so keep that in mind!
[36malgo-1    |[0m # Java property: hadoop.id.str
[36malgo-1    |[0m # export HADOOP_IDENT_STRING=$USER
[36malgo-1    |[0m 
[36malgo-1    |[0m # How many seconds to pause after stopping a daemon
[36malgo-1    |[0m # export HADOOP_STOP_TIMEOUT=5
[36malgo-1    |[0m 
[36malgo-1    |[0m # Where pid files are stored.  /tmp by default.
[36malgo-1    |[0m # export HADOOP_PID_DIR=/tmp
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default log4j setting for interactive commands
[36malgo-1    |[0m # Java property: hadoop.root.logger
[36malgo-1    |[0m # export HADOOP_ROOT_LOGGER=INFO,console
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default log4j setting for daemons spawned explicitly by
[36malgo-1    |[0m # --daemon option of hadoop, hdfs, mapred and yarn command.
[36malgo-1    |[0m # Java property: hadoop.root.logger
[36malgo-1    |[0m # export HADOOP_DAEMON_ROOT_LOGGER=INFO,RFA
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default log level and output location for security-related messages.
[36malgo-1    |[0m # You will almost certainly want to change this on a per-daemon basis via
[36malgo-1    |[0m # the Java property (i.e., -Dhadoop.security.logger=foo). (Note that the
[36malgo-1    |[0m # defaults for the NN and 2NN override this by default.)
[36malgo-1    |[0m # Java property: hadoop.security.logger
[36malgo-1    |[0m # export HADOOP_SECURITY_LOGGER=INFO,NullAppender
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default process priority level
[36malgo-1    |[0m # Note that sub-processes will also run at this level!
[36malgo-1    |[0m # export HADOOP_NICENESS=0
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default name for the service level authorization file
[36malgo-1    |[0m # Java property: hadoop.policy.file
[36malgo-1    |[0m # export HADOOP_POLICYFILE="hadoop-policy.xml"
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # NOTE: this is not used by default!  <-----
[36malgo-1    |[0m # You can define variables right here and then re-use them later on.
[36malgo-1    |[0m # For example, it is common to use the same garbage collection settings
[36malgo-1    |[0m # for all the daemons.  So one could define:
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HADOOP_GC_SETTINGS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps"
[36malgo-1    |[0m #
[36malgo-1    |[0m # .. and then use it as per the b option under the namenode.
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Secure/privileged execution
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Out of the box, Hadoop uses jsvc from Apache Commons to launch daemons
[36malgo-1    |[0m # on privileged ports.  This functionality can be replaced by providing
[36malgo-1    |[0m # custom functions.  See hadoop-functions.sh for more information.
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m # The jsvc implementation to use. Jsvc is required to run secure datanodes
[36malgo-1    |[0m # that bind to privileged ports to provide authentication of data transfer
[36malgo-1    |[0m # protocol.  Jsvc is not required if SASL is configured for authentication of
[36malgo-1    |[0m # data transfer protocol using non-privileged ports.
[36malgo-1    |[0m # export JSVC_HOME=/usr/bin
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # This directory contains pids for secure and privileged processes.
[36malgo-1    |[0m #export HADOOP_SECURE_PID_DIR=${HADOOP_PID_DIR}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # This directory contains the logs for secure and privileged processes.
[36malgo-1    |[0m # Java property: hadoop.log.dir
[36malgo-1    |[0m # export HADOOP_SECURE_LOG=${HADOOP_LOG_DIR}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # When running a secure daemon, the default value of HADOOP_IDENT_STRING
[36malgo-1    |[0m # ends up being a bit bogus.  Therefore, by default, the code will
[36malgo-1    |[0m # replace HADOOP_IDENT_STRING with HADOOP_xx_SECURE_USER.  If one wants
[36malgo-1    |[0m # to keep HADOOP_IDENT_STRING untouched, then uncomment this line.
[36malgo-1    |[0m # export HADOOP_SECURE_IDENT_PRESERVE="true"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # NameNode specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Default log level and output location for file system related change
[36malgo-1    |[0m # messages. For non-namenode daemons, the Java property must be set in
[36malgo-1    |[0m # the appropriate _OPTS if one wants something other than INFO,NullAppender
[36malgo-1    |[0m # Java property: hdfs.audit.logger
[36malgo-1    |[0m # export HDFS_AUDIT_LOGGER=INFO,NullAppender
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the NameNode.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # a) Set JMX options
[36malgo-1    |[0m # export HDFS_NAMENODE_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
[36malgo-1    |[0m #
[36malgo-1    |[0m # b) Set garbage collection logs
[36malgo-1    |[0m # export HDFS_NAMENODE_OPTS="${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[36malgo-1    |[0m #
[36malgo-1    |[0m # c) ... or set them directly
[36malgo-1    |[0m # export HDFS_NAMENODE_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[36malgo-1    |[0m 
[36malgo-1    |[0m # this is the default:
[36malgo-1    |[0m # export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # SecondaryNameNode specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the SecondaryNameNode.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # This is the default:
[36malgo-1    |[0m # export HDFS_SECONDARYNAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # DataNode specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the DataNode.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # This is the default:
[36malgo-1    |[0m # export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS"
[36malgo-1    |[0m 
[36malgo-1    |[0m # On secure datanodes, user to run the datanode as after dropping privileges.
[36malgo-1    |[0m # This **MUST** be uncommented to enable secure HDFS if using privileged ports
[36malgo-1    |[0m # to provide authentication of data transfer protocol.  This **MUST NOT** be
[36malgo-1    |[0m # defined if SASL is configured for authentication of data transfer protocol
[36malgo-1    |[0m # using non-privileged ports.
[36malgo-1    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[36malgo-1    |[0m # export HDFS_DATANODE_SECURE_USER=hdfs
[36malgo-1    |[0m 
[36malgo-1    |[0m # Supplemental options for secure datanodes
[36malgo-1    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[36malgo-1    |[0m # server jvm.
[36malgo-1    |[0m # export HDFS_DATANODE_SECURE_EXTRA_OPTS="-jvm server"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # NFS3 Gateway specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the NFS3 Gateway.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_NFS3_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the Hadoop portmapper.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_PORTMAP_OPTS="-Xmx512m"
[36malgo-1    |[0m 
[36malgo-1    |[0m # Supplemental options for priviliged gateways
[36malgo-1    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[36malgo-1    |[0m # server jvm.
[36malgo-1    |[0m # export HDFS_NFS3_SECURE_EXTRA_OPTS="-jvm server"
[36malgo-1    |[0m 
[36malgo-1    |[0m # On privileged gateways, user to run the gateway as after dropping privileges
[36malgo-1    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[36malgo-1    |[0m # export HDFS_NFS3_SECURE_USER=nfsserver
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # ZKFailoverController specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the ZKFailoverController.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_ZKFC_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # QuorumJournalNode specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the QuorumJournalNode.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_JOURNALNODE_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # HDFS Balancer specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the HDFS Balancer.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_BALANCER_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # HDFS Mover specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the HDFS Mover.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_MOVER_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Router-based HDFS Federation specific parameters
[36malgo-1    |[0m # Specify the JVM options to be used when starting the RBF Routers.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_DFSROUTER_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # HDFS StorageContainerManager specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the HDFS Storage Container Manager.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # export HDFS_STORAGECONTAINERMANAGER_OPTS=""
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Advanced Users Only!
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # When building Hadoop, one can add the class paths to the commands
[36malgo-1    |[0m # via this special env var:
[36malgo-1    |[0m # export HADOOP_ENABLE_BUILD_PATHS="true"
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # To prevent accidents, shell commands be (superficially) locked
[36malgo-1    |[0m # to only allow certain users to execute certain subcommands.
[36malgo-1    |[0m # It uses the format of (command)_(subcommand)_USER.
[36malgo-1    |[0m #
[36malgo-1    |[0m # For example, to limit who can execute the namenode command,
[36malgo-1    |[0m # export HDFS_NAMENODE_USER=hdfs
[36malgo-1    |[0m export SPARK_MASTER_HOST=172.18.0.3
[36malgo-1    |[0m export AWS_CONTAINER_CREDENTIALS_RELATIVE_URI=
[36malgo-1    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[36malgo-1    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/core-site.xml
[36malgo-1    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/core-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0" encoding="UTF-8"?>
[36malgo-1    |[0m  <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[36malgo-1    |[0m  <!-- Put site-specific property overrides in this file. -->
[36malgo-1    |[0m 
[36malgo-1    |[0m  <configuration>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>fs.defaultFS</name>
[36malgo-1    |[0m          <value>hdfs://172.18.0.3/</value>
[36malgo-1    |[0m          <description>NameNode URI</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>fs.s3a.aws.credentials.provider</name>
[36malgo-1    |[0m          <value>com.amazonaws.auth.DefaultAWSCredentialsProviderChain</value>
[36malgo-1    |[0m          <description>AWS S3 credential provider</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>fs.s3.impl</name>
[36malgo-1    |[0m          <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
[36malgo-1    |[0m          <description>s3a filesystem implementation</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>fs.AbstractFileSystem.s3a.imp</name>
[36malgo-1    |[0m          <value>org.apache.hadoop.fs.s3a.S3A</value>
[36malgo-1    |[0m          <description>s3a filesystem implementation</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m  
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>spark.executor.memory</name>
[36malgo-1    |[0m     <value>2g</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>spark.executor.cores</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/log4j.properties
[36malgo-1    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/log4j.properties is: 
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[36malgo-1    |[0m # or more contributor license agreements.  See the NOTICE file
[36malgo-1    |[0m # distributed with this work for additional information
[36malgo-1    |[0m # regarding copyright ownership.  The ASF licenses this file
[36malgo-1    |[0m # to you under the Apache License, Version 2.0 (the
[36malgo-1    |[0m # "License"); you may not use this file except in compliance
[36malgo-1    |[0m # with the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m # Define some default values that can be overridden by system properties
[36malgo-1    |[0m hadoop.root.logger=INFO,console
[36malgo-1    |[0m hadoop.log.dir=.
[36malgo-1    |[0m hadoop.log.file=hadoop.log
[36malgo-1    |[0m 
[36malgo-1    |[0m # Define the root logger to the system property "hadoop.root.logger".
[36malgo-1    |[0m log4j.rootLogger=${hadoop.root.logger}, EventCounter
[36malgo-1    |[0m 
[36malgo-1    |[0m # Logging Threshold
[36malgo-1    |[0m log4j.threshold=ALL
[36malgo-1    |[0m 
[36malgo-1    |[0m # Null Appender
[36malgo-1    |[0m log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Rolling File Appender - cap space usage at 5gb.
[36malgo-1    |[0m #
[36malgo-1    |[0m hadoop.log.maxfilesize=256MB
[36malgo-1    |[0m hadoop.log.maxbackupindex=20
[36malgo-1    |[0m log4j.appender.RFA=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.RFA.MaxFileSize=${hadoop.log.maxfilesize}
[36malgo-1    |[0m log4j.appender.RFA.MaxBackupIndex=${hadoop.log.maxbackupindex}
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.RFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m 
[36malgo-1    |[0m # Pattern format: Date LogLevel LoggerName LogMessage
[36malgo-1    |[0m log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[36malgo-1    |[0m # Debugging Pattern format
[36malgo-1    |[0m #log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Daily Rolling File Appender
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.DRFA.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m 
[36malgo-1    |[0m # Rollver at midnight
[36malgo-1    |[0m #log4j.appender.DRFA.DatePattern=.yyyy-MM-dd
[36malgo-1    |[0m # Rollver at every hour
[36malgo-1    |[0m log4j.appender.DRFA.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m 
[36malgo-1    |[0m # Pattern format: Date LogLevel LoggerName LogMessage
[36malgo-1    |[0m log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m # Debugging Pattern format
[36malgo-1    |[0m #log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # console
[36malgo-1    |[0m # Add "console" to rootlogger above if you want to use this
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.console=org.apache.log4j.ConsoleAppender
[36malgo-1    |[0m log4j.appender.console.target=System.err
[36malgo-1    |[0m log4j.appender.console.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # TaskLog Appender
[36malgo-1    |[0m #
[36malgo-1    |[0m log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.TLA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # HDFS block state change log from block manager
[36malgo-1    |[0m #
[36malgo-1    |[0m # Uncomment the following to log normal block state change
[36malgo-1    |[0m # messages from BlockManager in NameNode.
[36malgo-1    |[0m #log4j.logger.BlockStateChange=DEBUG
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m #Security appender
[36malgo-1    |[0m #
[36malgo-1    |[0m hadoop.security.logger=INFO,NullAppender
[36malgo-1    |[0m hadoop.security.log.maxfilesize=256MB
[36malgo-1    |[0m hadoop.security.log.maxbackupindex=20
[36malgo-1    |[0m log4j.category.SecurityLogger=${hadoop.security.logger}
[36malgo-1    |[0m hadoop.security.log.file=SecurityAuth-${user.name}.audit
[36malgo-1    |[0m log4j.appender.RFAS=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.RFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
[36malgo-1    |[0m log4j.appender.RFAS.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[36malgo-1    |[0m log4j.appender.RFAS.MaxFileSize=${hadoop.security.log.maxfilesize}
[36malgo-1    |[0m log4j.appender.RFAS.MaxBackupIndex=${hadoop.security.log.maxbackupindex}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Daily Rolling Security appender
[36malgo-1    |[0m #
[36malgo-1    |[0m log4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.DRFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
[36malgo-1    |[0m log4j.appender.DRFAS.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.DRFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[36malgo-1    |[0m log4j.appender.DRFAS.DatePattern=.yyyy-MM-dd
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # hadoop configuration logging
[36malgo-1    |[0m #
[36malgo-1    |[0m 
[36malgo-1    |[0m # Uncomment the following line to turn off configuration deprecation warnings.
[36malgo-1    |[0m # log4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # hdfs audit logging
[36malgo-1    |[0m #
[36malgo-1    |[0m hdfs.audit.logger=INFO,NullAppender
[36malgo-1    |[0m hdfs.audit.log.maxfilesize=256MB
[36malgo-1    |[0m hdfs.audit.log.maxbackupindex=20
[36malgo-1    |[0m log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}
[36malgo-1    |[0m log4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false
[36malgo-1    |[0m log4j.appender.RFAAUDIT=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.RFAAUDIT.File=${hadoop.log.dir}/hdfs-audit.log
[36malgo-1    |[0m log4j.appender.RFAAUDIT.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.RFAAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[36malgo-1    |[0m log4j.appender.RFAAUDIT.MaxFileSize=${hdfs.audit.log.maxfilesize}
[36malgo-1    |[0m log4j.appender.RFAAUDIT.MaxBackupIndex=${hdfs.audit.log.maxbackupindex}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # NameNode metrics logging.
[36malgo-1    |[0m # The default is to retain two namenode-metrics.log files up to 64MB each.
[36malgo-1    |[0m #
[36malgo-1    |[0m namenode.metrics.logger=INFO,NullAppender
[36malgo-1    |[0m log4j.logger.NameNodeMetricsLog=${namenode.metrics.logger}
[36malgo-1    |[0m log4j.additivity.NameNodeMetricsLog=false
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.File=${hadoop.log.dir}/namenode-metrics.log
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.MaxBackupIndex=1
[36malgo-1    |[0m log4j.appender.NNMETRICSRFA.MaxFileSize=64MB
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # DataNode metrics logging.
[36malgo-1    |[0m # The default is to retain two datanode-metrics.log files up to 64MB each.
[36malgo-1    |[0m #
[36malgo-1    |[0m datanode.metrics.logger=INFO,NullAppender
[36malgo-1    |[0m log4j.logger.DataNodeMetricsLog=${datanode.metrics.logger}
[36malgo-1    |[0m log4j.additivity.DataNodeMetricsLog=false
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.File=${hadoop.log.dir}/datanode-metrics.log
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.MaxBackupIndex=1
[36malgo-1    |[0m log4j.appender.DNMETRICSRFA.MaxFileSize=64MB
[36malgo-1    |[0m 
[36malgo-1    |[0m # Custom Logging levels
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=DEBUG
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m # AWS SDK & S3A FileSystem
[36malgo-1    |[0m #log4j.logger.com.amazonaws=ERROR
[36malgo-1    |[0m log4j.logger.com.amazonaws.http.AmazonHttpClient=ERROR
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.fs.s3a.S3AFileSystem=WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Event Counter Appender
[36malgo-1    |[0m # Sends counts of logging messages at different severity levels to Hadoop Metrics.
[36malgo-1    |[0m #
[36malgo-1    |[0m log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Job Summary Appender
[36malgo-1    |[0m #
[36malgo-1    |[0m # Use following logger to send summary to separate file defined by
[36malgo-1    |[0m # hadoop.mapreduce.jobsummary.log.file :
[36malgo-1    |[0m # hadoop.mapreduce.jobsummary.logger=INFO,JSA
[36malgo-1    |[0m # 
[36malgo-1    |[0m hadoop.mapreduce.jobsummary.logger=${hadoop.root.logger}
[36malgo-1    |[0m hadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log
[36malgo-1    |[0m hadoop.mapreduce.jobsummary.log.maxfilesize=256MB
[36malgo-1    |[0m hadoop.mapreduce.jobsummary.log.maxbackupindex=20
[36malgo-1    |[0m log4j.appender.JSA=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.JSA.File=${hadoop.log.dir}/${hadoop.mapreduce.jobsummary.log.file}
[36malgo-1    |[0m log4j.appender.JSA.MaxFileSize=${hadoop.mapreduce.jobsummary.log.maxfilesize}
[36malgo-1    |[0m log4j.appender.JSA.MaxBackupIndex=${hadoop.mapreduce.jobsummary.log.maxbackupindex}
[36malgo-1    |[0m log4j.appender.JSA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.JSA.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
[36malgo-1    |[0m log4j.appender.JSA.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.logger.org.apache.hadoop.mapred.JobInProgress$JobSummary=${hadoop.mapreduce.jobsummary.logger}
[36malgo-1    |[0m log4j.additivity.org.apache.hadoop.mapred.JobInProgress$JobSummary=false
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # shuffle connection log from shuffleHandler
[36malgo-1    |[0m # Uncomment the following line to enable logging of shuffle connections
[36malgo-1    |[0m # log4j.logger.org.apache.hadoop.mapred.ShuffleHandler.audit=DEBUG
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Yarn ResourceManager Application Summary Log
[36malgo-1    |[0m #
[36malgo-1    |[0m # Set the ResourceManager summary log filename
[36malgo-1    |[0m yarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log
[36malgo-1    |[0m # Set the ResourceManager summary log level and appender
[36malgo-1    |[0m yarn.server.resourcemanager.appsummary.logger=${hadoop.root.logger}
[36malgo-1    |[0m #yarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY
[36malgo-1    |[0m 
[36malgo-1    |[0m # To enable AppSummaryLogging for the RM,
[36malgo-1    |[0m # set yarn.server.resourcemanager.appsummary.logger to
[36malgo-1    |[0m # <LEVEL>,RMSUMMARY in hadoop-env.sh
[36malgo-1    |[0m 
[36malgo-1    |[0m # Appender for ResourceManager Application Summary Log
[36malgo-1    |[0m # Requires the following properties to be set
[36malgo-1    |[0m #    - hadoop.log.dir (Hadoop Log directory)
[36malgo-1    |[0m #    - yarn.server.resourcemanager.appsummary.log.file (resource manager app summary log filename)
[36malgo-1    |[0m #    - yarn.server.resourcemanager.appsummary.logger (resource manager app summary log level and appender)
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=${yarn.server.resourcemanager.appsummary.logger}
[36malgo-1    |[0m log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=false
[36malgo-1    |[0m log4j.appender.RMSUMMARY=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m log4j.appender.RMSUMMARY.File=${hadoop.log.dir}/${yarn.server.resourcemanager.appsummary.log.file}
[36malgo-1    |[0m log4j.appender.RMSUMMARY.MaxFileSize=256MB
[36malgo-1    |[0m log4j.appender.RMSUMMARY.MaxBackupIndex=20
[36malgo-1    |[0m log4j.appender.RMSUMMARY.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.RMSUMMARY.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m # HS audit log configs
[36malgo-1    |[0m #mapreduce.hs.audit.logger=INFO,HSAUDIT
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=${mapreduce.hs.audit.logger}
[36malgo-1    |[0m #log4j.additivity.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=false
[36malgo-1    |[0m #log4j.appender.HSAUDIT=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m #log4j.appender.HSAUDIT.File=${hadoop.log.dir}/hs-audit.log
[36malgo-1    |[0m #log4j.appender.HSAUDIT.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m #log4j.appender.HSAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[36malgo-1    |[0m #log4j.appender.HSAUDIT.DatePattern=.yyyy-MM-dd
[36malgo-1    |[0m 
[36malgo-1    |[0m # Http Server Request Logs
[36malgo-1    |[0m #log4j.logger.http.requests.namenode=INFO,namenoderequestlog
[36malgo-1    |[0m #log4j.appender.namenoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.namenoderequestlog.Filename=${hadoop.log.dir}/jetty-namenode-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.namenoderequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.http.requests.datanode=INFO,datanoderequestlog
[36malgo-1    |[0m #log4j.appender.datanoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.datanoderequestlog.Filename=${hadoop.log.dir}/jetty-datanode-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.datanoderequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.http.requests.resourcemanager=INFO,resourcemanagerrequestlog
[36malgo-1    |[0m #log4j.appender.resourcemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.resourcemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-resourcemanager-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.resourcemanagerrequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.http.requests.jobhistory=INFO,jobhistoryrequestlog
[36malgo-1    |[0m #log4j.appender.jobhistoryrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.jobhistoryrequestlog.Filename=${hadoop.log.dir}/jetty-jobhistory-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.jobhistoryrequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.http.requests.nodemanager=INFO,nodemanagerrequestlog
[36malgo-1    |[0m #log4j.appender.nodemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[36malgo-1    |[0m #log4j.appender.nodemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-nodemanager-yyyy_mm_dd.log
[36malgo-1    |[0m #log4j.appender.nodemanagerrequestlog.RetainDays=3
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m # WebHdfs request log on datanodes
[36malgo-1    |[0m # Specify -Ddatanode.webhdfs.logger=INFO,HTTPDRFA on datanode startup to
[36malgo-1    |[0m # direct the log to a separate file.
[36malgo-1    |[0m #datanode.webhdfs.logger=INFO,console
[36malgo-1    |[0m #log4j.logger.datanode.webhdfs=${datanode.webhdfs.logger}
[36malgo-1    |[0m #log4j.appender.HTTPDRFA=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m #log4j.appender.HTTPDRFA.File=${hadoop.log.dir}/hadoop-datanode-webhdfs.log
[36malgo-1    |[0m #log4j.appender.HTTPDRFA.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m #log4j.appender.HTTPDRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[36malgo-1    |[0m #log4j.appender.HTTPDRFA.DatePattern=.yyyy-MM-dd
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m # Appender for viewing information for errors and warnings
[36malgo-1    |[0m yarn.ewma.cleanupInterval=300
[36malgo-1    |[0m yarn.ewma.messageAgeLimitSeconds=86400
[36malgo-1    |[0m yarn.ewma.maxUniqueMessages=250
[36malgo-1    |[0m log4j.appender.EWMA=org.apache.hadoop.yarn.util.Log4jWarningErrorMetricsAppender
[36malgo-1    |[0m log4j.appender.EWMA.cleanupInterval=${yarn.ewma.cleanupInterval}
[36malgo-1    |[0m log4j.appender.EWMA.messageAgeLimitSeconds=${yarn.ewma.messageAgeLimitSeconds}
[36malgo-1    |[0m log4j.appender.EWMA.maxUniqueMessages=${yarn.ewma.maxUniqueMessages}
[36malgo-1    |[0m 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Fair scheduler state dump
[36malgo-1    |[0m #
[36malgo-1    |[0m # Use following logger to dump the state to a separate file
[36malgo-1    |[0m 
[36malgo-1    |[0m #log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=DEBUG,FSSTATEDUMP
[36malgo-1    |[0m #log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=false
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP=org.apache.log4j.RollingFileAppender
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.File=${hadoop.log.dir}/fairscheduler-statedump.log
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.MaxFileSize=${hadoop.log.maxfilesize}
[36malgo-1    |[0m #log4j.appender.FSSTATEDUMP.MaxBackupIndex=${hadoop.log.maxbackupindex}
[36malgo-1    |[0m 
[36malgo-1    |[0m # Log levels of third-party libraries
[36malgo-1    |[0m log4j.logger.org.apache.commons.beanutils=WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m #AWS SDK Logging
[36malgo-1    |[0m log4j.logger.com.amazonaws=WARN
[36malgo-1    |[0m log4j.logger.org.apache.zookeeper=ERROR
[36malgo-1    |[0m log4j.logger.org.apache.hadoop.hbase=WARN
[36malgo-1    |[0m log4j.logger.amazon.emr.metrics=WARN
[36malgo-1    |[0m log4j.logger.emr=INFO
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.HADOOP=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.HADOOP.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m log4j.appender.HADOOP.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.appender.HADOOP.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.HADOOP.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.JHS=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.JHS.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m log4j.appender.JHS.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.appender.JHS.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.JHS.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.MAPRED=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.MAPRED.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m log4j.appender.MAPRED.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.appender.MAPRED.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.MAPRED.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m log4j.appender.YARN=org.apache.log4j.DailyRollingFileAppender
[36malgo-1    |[0m log4j.appender.YARN.File=${hadoop.log.dir}/${hadoop.log.file}
[36malgo-1    |[0m log4j.appender.YARN.DatePattern=.yyyy-MM-dd-HH
[36malgo-1    |[0m log4j.appender.YARN.layout=org.apache.log4j.PatternLayout
[36malgo-1    |[0m log4j.appender.YARN.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/hive/conf/hive-env.sh
[36malgo-1    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/hive/conf/hive-env.sh is: 
[36malgo-1    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[36malgo-1    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/hive/conf/hive-log4j2.properties
[36malgo-1    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/hive/conf/hive-log4j2.properties is: 
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[36malgo-1    |[0m # or more contributor license agreements.  See the NOTICE file
[36malgo-1    |[0m # distributed with this work for additional information
[36malgo-1    |[0m # regarding copyright ownership.  The ASF licenses this file
[36malgo-1    |[0m # to you under the Apache License, Version 2.0 (the
[36malgo-1    |[0m # "License"); you may not use this file except in compliance
[36malgo-1    |[0m # with the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m status = INFO
[36malgo-1    |[0m name = HiveLog4j2
[36malgo-1    |[0m packages = org.apache.hadoop.hive.ql.log
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of properties
[36malgo-1    |[0m property.hive.log.level = INFO
[36malgo-1    |[0m property.hive.root.logger = DRFA
[36malgo-1    |[0m property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
[36malgo-1    |[0m property.hive.log.file = hive.log
[36malgo-1    |[0m property.hive.perflogger.log.level = INFO
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of all appenders
[36malgo-1    |[0m appenders = console, DRFA
[36malgo-1    |[0m 
[36malgo-1    |[0m # console appender
[36malgo-1    |[0m appender.console.type = Console
[36malgo-1    |[0m appender.console.name = console
[36malgo-1    |[0m appender.console.target = SYSTEM_ERR
[36malgo-1    |[0m appender.console.layout.type = PatternLayout
[36malgo-1    |[0m appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m # daily rolling file appender
[36malgo-1    |[0m appender.DRFA.type = RollingRandomAccessFile
[36malgo-1    |[0m appender.DRFA.name = DRFA
[36malgo-1    |[0m appender.DRFA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
[36malgo-1    |[0m # Use %pid in the filePattern to append <process-id>@<host-name> to the filename if you want separate log files for different CLI session
[36malgo-1    |[0m appender.DRFA.filePattern = ${sys:hive.log.dir}/${sys:hive.log.file}.%d{yyyy-MM-dd}
[36malgo-1    |[0m appender.DRFA.layout.type = PatternLayout
[36malgo-1    |[0m appender.DRFA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[36malgo-1    |[0m appender.DRFA.policies.type = Policies
[36malgo-1    |[0m appender.DRFA.policies.time.type = TimeBasedTriggeringPolicy
[36malgo-1    |[0m appender.DRFA.policies.time.interval = 1
[36malgo-1    |[0m appender.DRFA.policies.time.modulate = true
[36malgo-1    |[0m appender.DRFA.strategy.type = DefaultRolloverStrategy
[36malgo-1    |[0m appender.DRFA.strategy.max = 30
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of all loggers
[36malgo-1    |[0m loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, PerfLogger, AmazonAws, ApacheHttp
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
[36malgo-1    |[0m logger.NIOServerCnxn.level = WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
[36malgo-1    |[0m logger.ClientCnxnSocketNIO.level = WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.DataNucleus.name = DataNucleus
[36malgo-1    |[0m logger.DataNucleus.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.Datastore.name = Datastore
[36malgo-1    |[0m logger.Datastore.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.JPOX.name = JPOX
[36malgo-1    |[0m logger.JPOX.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.AmazonAws.name=com.amazonaws
[36malgo-1    |[0m logger.AmazonAws.level = INFO
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.ApacheHttp.name=org.apache.http
[36malgo-1    |[0m logger.ApacheHttp.level = INFO
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.PerfLogger.name = org.apache.hadoop.hive.ql.log.PerfLogger
[36malgo-1    |[0m logger.PerfLogger.level = ${sys:hive.perflogger.log.level}
[36malgo-1    |[0m 
[36malgo-1    |[0m # root logger
[36malgo-1    |[0m rootLogger.level = ${sys:hive.log.level}
[36malgo-1    |[0m rootLogger.appenderRefs = root
[36malgo-1    |[0m rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/hive/conf/hive-exec-log4j2.properties
[36malgo-1    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/hive/conf/hive-exec-log4j2.properties is: 
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[36malgo-1    |[0m # or more contributor license agreements.  See the NOTICE file
[36malgo-1    |[0m # distributed with this work for additional information
[36malgo-1    |[0m # regarding copyright ownership.  The ASF licenses this file
[36malgo-1    |[0m # to you under the Apache License, Version 2.0 (the
[36malgo-1    |[0m # "License"); you may not use this file except in compliance
[36malgo-1    |[0m # with the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m status = INFO
[36malgo-1    |[0m name = HiveExecLog4j2
[36malgo-1    |[0m packages = org.apache.hadoop.hive.ql.log
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of properties
[36malgo-1    |[0m property.hive.log.level = INFO
[36malgo-1    |[0m property.hive.root.logger = FA
[36malgo-1    |[0m property.hive.query.id = hadoop
[36malgo-1    |[0m property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
[36malgo-1    |[0m property.hive.log.file = ${sys:hive.query.id}.log
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of all appenders
[36malgo-1    |[0m appenders = console, FA
[36malgo-1    |[0m 
[36malgo-1    |[0m # console appender
[36malgo-1    |[0m appender.console.type = Console
[36malgo-1    |[0m appender.console.name = console
[36malgo-1    |[0m appender.console.target = SYSTEM_ERR
[36malgo-1    |[0m appender.console.layout.type = PatternLayout
[36malgo-1    |[0m appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m # simple file appender
[36malgo-1    |[0m appender.FA.type = RandomAccessFile
[36malgo-1    |[0m appender.FA.name = FA
[36malgo-1    |[0m appender.FA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
[36malgo-1    |[0m appender.FA.layout.type = PatternLayout
[36malgo-1    |[0m appender.FA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[36malgo-1    |[0m 
[36malgo-1    |[0m # list of all loggers
[36malgo-1    |[0m loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
[36malgo-1    |[0m logger.NIOServerCnxn.level = WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
[36malgo-1    |[0m logger.ClientCnxnSocketNIO.level = WARN
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.DataNucleus.name = DataNucleus
[36malgo-1    |[0m logger.DataNucleus.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.Datastore.name = Datastore
[36malgo-1    |[0m logger.Datastore.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m logger.JPOX.name = JPOX
[36malgo-1    |[0m logger.JPOX.level = ERROR
[36malgo-1    |[0m 
[36malgo-1    |[0m # root logger
[36malgo-1    |[0m rootLogger.level = ${sys:hive.log.level}
[36malgo-1    |[0m rootLogger.appenderRefs = root
[36malgo-1    |[0m rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/hive/conf/hive-site.xml
[36malgo-1    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/hive/conf/hive-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0"?>
[36malgo-1    |[0m <!--
[36malgo-1    |[0m   Licensed to the Apache Software Foundation (ASF) under one or more
[36malgo-1    |[0m   contributor license agreements.  See the NOTICE file distributed with
[36malgo-1    |[0m   this work for additional information regarding copyright ownership.
[36malgo-1    |[0m   The ASF licenses this file to You under the Apache License, Version 2.0
[36malgo-1    |[0m   (the "License"); you may not use this file except in compliance with
[36malgo-1    |[0m   the License.  You may obtain a copy of the License at
[36malgo-1    |[0m 
[36malgo-1    |[0m       http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m 
[36malgo-1    |[0m   Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m   distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m   See the License for the specific language governing permissions and
[36malgo-1    |[0m   limitations under the License.
[36malgo-1    |[0m -->
[36malgo-1    |[0m <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[36malgo-1    |[0m 
[36malgo-1    |[0m <configuration>
[36malgo-1    |[0m 
[36malgo-1    |[0m <!-- Hive Configuration can either be stored in this file or in the hadoop configuration files  -->
[36malgo-1    |[0m <!-- that are implied by Hadoop setup variables.                                                -->
[36malgo-1    |[0m <!-- Aside from Hadoop setup variables - this file is provided as a convenience so that Hive    -->
[36malgo-1    |[0m <!-- users do not have to edit hadoop configuration files (that may be managed as a centralized -->
[36malgo-1    |[0m <!-- resource).                                                                                 -->
[36malgo-1    |[0m 
[36malgo-1    |[0m <!-- Hive Execution Parameters -->
[36malgo-1    |[0m 
[36malgo-1    |[0m <property>
[36malgo-1    |[0m   <name>javax.jdo.option.ConnectionURL</name>
[36malgo-1    |[0m   <value>jdbc:derby:;databaseName=/var/lib/hive/metastore/metastore_db;create=true</value>
[36malgo-1    |[0m   <description>JDBC connect string for a JDBC metastore</description>
[36malgo-1    |[0m </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m <property>
[36malgo-1    |[0m   <name>javax.jdo.option.ConnectionDriverName</name>
[36malgo-1    |[0m   <value>org.apache.derby.jdbc.EmbeddedDriver</value>
[36malgo-1    |[0m   <description>Driver class name for a JDBC metastore</description>
[36malgo-1    |[0m </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>key</name>
[36malgo-1    |[0m     <value>value</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[36malgo-1    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m spark.driver.host=172.18.0.3
[36malgo-1    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[36malgo-1    |[0m 
[36malgo-1    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[36malgo-1    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[36malgo-1    |[0m spark.rpc.askTimeout=300s
[36malgo-1    |[0m spark.driver.memory 2048m
[36malgo-1    |[0m spark.driver.memoryOverhead 204m
[36malgo-1    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[36malgo-1    |[0m spark.executor.memory 12399m
[36malgo-1    |[0m spark.executor.memoryOverhead 1239m
[36malgo-1    |[0m spark.executor.cores 4
[36malgo-1    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[36malgo-1    |[0m spark.executor.instances 2
[36malgo-1    |[0m spark.default.parallelism 16
[36malgo-1    |[0m spark.executor.memory 2g
[36malgo-1    |[0m spark.executor.cores 1
[36malgo-1    |[0m key value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/spark/conf/spark-env.sh
[36malgo-1    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/spark/conf/spark-env.sh is: 
[36malgo-1    |[0m #EMPTY FILE AVOID OVERRIDDING ENV VARS
[36malgo-1    |[0m # Specifically, without copying the empty file, SPARK_HISTORY_OPTS will be overriden, 
[36malgo-1    |[0m # spark.history.ui.port defaults to 18082, and spark.eventLog.dir defaults to local fs
[36malgo-1    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[36malgo-1    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/spark/conf/log4j.properties
[36malgo-1    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/spark/conf/log4j.properties is: 
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/spark/conf/hive-site.xml
[36malgo-1    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/spark/conf/hive-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0"?>
[36malgo-1    |[0m <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[36malgo-1    |[0m <configuration>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>key</name>
[36malgo-1    |[0m     <value>value</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/spark/conf/metrics.properties
[36malgo-1    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/spark/conf/metrics.properties is: 
[36malgo-1    |[0m key=value
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[36malgo-1    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: 
[36malgo-1    |[0m <?xml version="1.0"?>
[36malgo-1    |[0m <!-- Site specific YARN configuration properties -->
[36malgo-1    |[0m  <configuration>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.resourcemanager.hostname</name>
[36malgo-1    |[0m          <value>172.18.0.3</value>
[36malgo-1    |[0m          <description>The hostname of the RM.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.hostname</name>
[36malgo-1    |[0m          <value>algo-1</value>
[36malgo-1    |[0m          <description>The hostname of the NM.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.webapp.address</name>
[36malgo-1    |[0m          <value>algo-1:8042</value>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.vmem-pmem-ratio</name>
[36malgo-1    |[0m          <value>5</value>
[36malgo-1    |[0m          <description>Ratio between virtual memory to physical memory.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.resourcemanager.am.max-attempts</name>
[36malgo-1    |[0m          <value>1</value>
[36malgo-1    |[0m          <description>The maximum number of application attempts.</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m      <property>
[36malgo-1    |[0m          <name>yarn.nodemanager.env-whitelist</name>
[36malgo-1    |[0m          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>
[36malgo-1    |[0m          <description>Environment variable whitelist</description>
[36malgo-1    |[0m      </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m  
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.minimum-allocation-mb</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.maximum-allocation-mb</name>
[36malgo-1    |[0m     <value>15892</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.minimum-allocation-vcores</name>
[36malgo-1    |[0m     <value>1</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.scheduler.maximum-allocation-vcores</name>
[36malgo-1    |[0m     <value>4</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.nodemanager.resource.memory-mb</name>
[36malgo-1    |[0m     <value>15892</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>yarn.nodemanager.resource.cpu-vcores</name>
[36malgo-1    |[0m     <value>4</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m 
[36malgo-1    |[0m   <property>
[36malgo-1    |[0m     <name>key</name>
[36malgo-1    |[0m     <value>value</value>
[36malgo-1    |[0m   </property>
[36malgo-1    |[0m </configuration>
[36malgo-1    |[0m 
[36malgo-1    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/yarn-env.sh
[36malgo-1    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-env.sh is: 
[36malgo-1    |[0m #
[36malgo-1    |[0m # Licensed to the Apache Software Foundation (ASF) under one or more
[36malgo-1    |[0m # contributor license agreements.  See the NOTICE file distributed with
[36malgo-1    |[0m # this work for additional information regarding copyright ownership.
[36malgo-1    |[0m # The ASF licenses this file to You under the Apache License, Version 2.0
[36malgo-1    |[0m # (the "License"); you may not use this file except in compliance with
[36malgo-1    |[0m # the License.  You may obtain a copy of the License at
[36malgo-1    |[0m #
[36malgo-1    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[36malgo-1    |[0m #
[36malgo-1    |[0m # Unless required by applicable law or agreed to in writing, software
[36malgo-1    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36malgo-1    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[36malgo-1    |[0m # See the License for the specific language governing permissions and
[36malgo-1    |[0m # limitations under the License.
[36malgo-1    |[0m 
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## THIS FILE ACTS AS AN OVERRIDE FOR hadoop-env.sh FOR ALL
[36malgo-1    |[0m ## WORK DONE BY THE yarn AND RELATED COMMANDS.
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## Precedence rules:
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## yarn-env.sh > hadoop-env.sh > hard-coded defaults
[36malgo-1    |[0m ##
[36malgo-1    |[0m ## YARN_xyz > HADOOP_xyz > hard-coded defaults
[36malgo-1    |[0m ##
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Resource Manager specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the max heapsize for the ResourceManager.  If no units are
[36malgo-1    |[0m # given, it will be assumed to be in MB.
[36malgo-1    |[0m # This value will be overridden by an Xmx setting specified in either
[36malgo-1    |[0m # HADOOP_OPTS and/or YARN_RESOURCEMANAGER_OPTS.
[36malgo-1    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX
[36malgo-1    |[0m #export YARN_RESOURCEMANAGER_HEAPSIZE=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the ResourceManager.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # Examples for a Sun/Oracle JDK:
[36malgo-1    |[0m # a) override the appsummary log file:
[36malgo-1    |[0m # export YARN_RESOURCEMANAGER_OPTS="-Dyarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log -Dyarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY"
[36malgo-1    |[0m #
[36malgo-1    |[0m # b) Set JMX options
[36malgo-1    |[0m # export YARN_RESOURCEMANAGER_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
[36malgo-1    |[0m #
[36malgo-1    |[0m # c) Set garbage collection logs from hadoop-env.sh
[36malgo-1    |[0m # export YARN_RESOURCE_MANAGER_OPTS="${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[36malgo-1    |[0m #
[36malgo-1    |[0m # d) ... or set them directly
[36malgo-1    |[0m # export YARN_RESOURCEMANAGER_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[36malgo-1    |[0m #
[36malgo-1    |[0m #
[36malgo-1    |[0m # export YARN_RESOURCEMANAGER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Node Manager specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the max heapsize for the NodeManager.  If no units are
[36malgo-1    |[0m # given, it will be assumed to be in MB.
[36malgo-1    |[0m # This value will be overridden by an Xmx setting specified in either
[36malgo-1    |[0m # HADOOP_OPTS and/or YARN_NODEMANAGER_OPTS.
[36malgo-1    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[36malgo-1    |[0m #export YARN_NODEMANAGER_HEAPSIZE=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the NodeManager.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_NODEMANAGER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # TimeLineServer specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the max heapsize for the timelineserver.  If no units are
[36malgo-1    |[0m # given, it will be assumed to be in MB.
[36malgo-1    |[0m # This value will be overridden by an Xmx setting specified in either
[36malgo-1    |[0m # HADOOP_OPTS and/or YARN_TIMELINESERVER_OPTS.
[36malgo-1    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[36malgo-1    |[0m #export YARN_TIMELINE_HEAPSIZE=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the TimeLineServer.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_TIMELINESERVER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # TimeLineReader specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the TimeLineReader.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_TIMELINEREADER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Web App Proxy Server specifc parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the max heapsize for the web app proxy server.  If no units are
[36malgo-1    |[0m # given, it will be assumed to be in MB.
[36malgo-1    |[0m # This value will be overridden by an Xmx setting specified in either
[36malgo-1    |[0m # HADOOP_OPTS and/or YARN_PROXYSERVER_OPTS.
[36malgo-1    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[36malgo-1    |[0m #export YARN_PROXYSERVER_HEAPSIZE=
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the proxy server.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_PROXYSERVER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Shared Cache Manager specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Specify the JVM options to be used when starting the
[36malgo-1    |[0m # shared cache manager server.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_SHAREDCACHEMANAGER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Router specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m 
[36malgo-1    |[0m # Specify the JVM options to be used when starting the Router.
[36malgo-1    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[36malgo-1    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[36malgo-1    |[0m #
[36malgo-1    |[0m # See ResourceManager for some examples
[36malgo-1    |[0m #
[36malgo-1    |[0m #export YARN_ROUTER_OPTS=
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Registry DNS specific parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # For privileged registry DNS, user to run as after dropping privileges
[36malgo-1    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[36malgo-1    |[0m # export YARN_REGISTRYDNS_SECURE_USER=yarn
[36malgo-1    |[0m 
[36malgo-1    |[0m # Supplemental options for privileged registry DNS
[36malgo-1    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[36malgo-1    |[0m # server jvm.
[36malgo-1    |[0m # export YARN_REGISTRYDNS_SECURE_EXTRA_OPTS="-jvm server"
[36malgo-1    |[0m 
[36malgo-1    |[0m ###
[36malgo-1    |[0m # YARN Services parameters
[36malgo-1    |[0m ###
[36malgo-1    |[0m # Directory containing service examples
[36malgo-1    |[0m # export YARN_SERVICE_EXAMPLES_DIR = $HADOOP_YARN_HOME/share/hadoop/yarn/yarn-service-examples
[36malgo-1    |[0m # export YARN_CONTAINER_RUNTIME_DOCKER_RUN_OVERRIDE_DISABLE=true
[36malgo-1    |[0m export YARN_LOG_DIR=/var/log/yarn/export HADOOP_DATANODE_HEAPSIZE=2048
[36malgo-1    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[36malgo-1    |[0m 
[33malgo-2    |[0m 11-14 15:22 smspark-submit INFO     Found hadoop jar hadoop-aws.jar
[33malgo-2    |[0m 11-14 15:22 smspark-submit INFO     Optional jar jets3t-0.9.0.jar in /usr/lib/hadoop/lib does not exist
[33malgo-2    |[0m 11-14 15:22 smspark-submit INFO     copying cluster config
[33malgo-2    |[0m 11-14 15:22 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml
[33malgo-2    |[0m 11-14 15:22 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml
[33malgo-2    |[0m 11-14 15:22 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[33malgo-2    |[0m 11-14 15:22 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 15:22 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh
[33malgo-2    |[0m 11-14 15:22 root         INFO     Detected instance type: m5.xlarge with total memory: 16384M and total cores: 4
[33malgo-2    |[0m 11-14 15:22 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[33malgo-2    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0"?>
[33malgo-2    |[0m <!-- Site specific YARN configuration properties -->
[33malgo-2    |[0m  <configuration>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.resourcemanager.hostname</name>
[33malgo-2    |[0m          <value>172.18.0.3</value>
[33malgo-2    |[0m          <description>The hostname of the RM.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.hostname</name>
[33malgo-2    |[0m          <value>algo-2</value>
[33malgo-2    |[0m          <description>The hostname of the NM.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.webapp.address</name>
[33malgo-2    |[0m          <value>algo-2:8042</value>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.vmem-pmem-ratio</name>
[33malgo-2    |[0m          <value>5</value>
[33malgo-2    |[0m          <description>Ratio between virtual memory to physical memory.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.resourcemanager.am.max-attempts</name>
[33malgo-2    |[0m          <value>1</value>
[33malgo-2    |[0m          <description>The maximum number of application attempts.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.env-whitelist</name>
[33malgo-2    |[0m          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>
[33malgo-2    |[0m          <description>Environment variable whitelist</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m  
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.minimum-allocation-mb</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.maximum-allocation-mb</name>
[33malgo-2    |[0m     <value>15892</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.minimum-allocation-vcores</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.maximum-allocation-vcores</name>
[33malgo-2    |[0m     <value>4</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.nodemanager.resource.memory-mb</name>
[33malgo-2    |[0m     <value>15892</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.nodemanager.resource.cpu-vcores</name>
[33malgo-2    |[0m     <value>4</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:22 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[33malgo-2    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.driver.host=172.18.0.3
[33malgo-2    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[33malgo-2    |[0m 
[33malgo-2    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[33malgo-2    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[33malgo-2    |[0m spark.rpc.askTimeout=300s
[33malgo-2    |[0m spark.driver.memory 2048m
[33malgo-2    |[0m spark.driver.memoryOverhead 204m
[33malgo-2    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[33malgo-2    |[0m spark.executor.memory 12399m
[33malgo-2    |[0m spark.executor.memoryOverhead 1239m
[33malgo-2    |[0m spark.executor.cores 4
[33malgo-2    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[33malgo-2    |[0m spark.executor.instances 2
[33malgo-2    |[0m spark.default.parallelism 16
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:22 root         INFO     Finished Yarn configuration files setup.
[33malgo-2    |[0m 11-14 15:22 root         INFO     reading user configuration from /opt/ml/processing/input/conf/configuration.json
[33malgo-2    |[0m 11-14 15:22 root         INFO     User configuration list or dict: [{'Classification': 'spark-defaults', 'Properties': {'spark.executor.memory': '2g', 'spark.executor.cores': '1'}}, {'Classification': 'hadoop-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'core-site', 'Properties': {'spark.executor.memory': '2g', 'spark.executor.cores': '1'}}, {'Classification': 'hadoop-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'hive-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-exec-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'hive-site', 'Properties': {'key': 'value'}}, {'Classification': 'spark-defaults', 'Properties': {'key': 'value'}}, {'Classification': 'spark-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}, {'Classification': 'spark-log4j', 'Properties': {'key': 'value'}}, {'Classification': 'spark-hive-site', 'Properties': {'key': 'value'}}, {'Classification': 'spark-metrics', 'Properties': {'key': 'value'}}, {'Classification': 'yarn-site', 'Properties': {'key': 'value'}}, {'Classification': 'yarn-env', 'Properties': {}, 'Configurations': [{'Classification': 'export', 'Properties': {'HADOOP_DATANODE_HEAPSIZE': '2048', 'HADOOP_NAMENODE_OPTS': '-XX:GCTimeRatio=19'}, 'Configurations': []}]}] , type <class 'list'>
[33malgo-2    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[33malgo-2    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.driver.host=172.18.0.3
[33malgo-2    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[33malgo-2    |[0m 
[33malgo-2    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[33malgo-2    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[33malgo-2    |[0m spark.rpc.askTimeout=300s
[33malgo-2    |[0m spark.driver.memory 2048m
[33malgo-2    |[0m spark.driver.memoryOverhead 204m
[33malgo-2    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[33malgo-2    |[0m spark.executor.memory 12399m
[33malgo-2    |[0m spark.executor.memoryOverhead 1239m
[33malgo-2    |[0m spark.executor.cores 4
[33malgo-2    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[33malgo-2    |[0m spark.executor.instances 2
[33malgo-2    |[0m spark.default.parallelism 16
[33malgo-2    |[0m spark.executor.memory 2g
[33malgo-2    |[0m spark.executor.cores 1
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/hadoop-env.sh
[33malgo-2    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/hadoop-env.sh is: 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[33malgo-2    |[0m # or more contributor license agreements.  See the NOTICE file
[33malgo-2    |[0m # distributed with this work for additional information
[33malgo-2    |[0m # regarding copyright ownership.  The ASF licenses this file
[33malgo-2    |[0m # to you under the Apache License, Version 2.0 (the
[33malgo-2    |[0m # "License"); you may not use this file except in compliance
[33malgo-2    |[0m # with the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m # Set Hadoop-specific environment variables here.
[33malgo-2    |[0m 
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## THIS FILE ACTS AS THE MASTER FILE FOR ALL HADOOP PROJECTS.
[33malgo-2    |[0m ## SETTINGS HERE WILL BE READ BY ALL HADOOP COMMANDS.  THEREFORE,
[33malgo-2    |[0m ## ONE CAN USE THIS FILE TO SET YARN, HDFS, AND MAPREDUCE
[33malgo-2    |[0m ## CONFIGURATION OPTIONS INSTEAD OF xxx-env.sh.
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## Precedence rules:
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## {yarn-env.sh|hdfs-env.sh} > hadoop-env.sh > hard-coded defaults
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## {YARN_xyz|HDFS_xyz} > HADOOP_xyz > hard-coded defaults
[33malgo-2    |[0m ##
[33malgo-2    |[0m 
[33malgo-2    |[0m # Many of the options here are built from the perspective that users
[33malgo-2    |[0m # may want to provide OVERWRITING values on the command line.
[33malgo-2    |[0m # For example:
[33malgo-2    |[0m #
[33malgo-2    |[0m #  JAVA_HOME=/usr/java/testing hdfs dfs -ls
[33malgo-2    |[0m #
[33malgo-2    |[0m # Therefore, the vast majority (BUT NOT ALL!) of these defaults
[33malgo-2    |[0m # are configured for substitution and not append.  If append
[33malgo-2    |[0m # is preferable, modify this file accordingly.
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Generic settings for HADOOP
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Technically, the only required environment variable is JAVA_HOME.
[33malgo-2    |[0m # All others are optional.  However, the defaults are probably not
[33malgo-2    |[0m # preferred.  Many sites configure these options outside of Hadoop,
[33malgo-2    |[0m # such as in /etc/profile.d
[33malgo-2    |[0m 
[33malgo-2    |[0m # The java implementation to use. By default, this environment
[33malgo-2    |[0m # variable is REQUIRED on ALL platforms except OS X!
[33malgo-2    |[0m # export JAVA_HOME=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Location of Hadoop.  By default, Hadoop will attempt to determine
[33malgo-2    |[0m # this location based upon its execution path.
[33malgo-2    |[0m # export HADOOP_HOME=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Location of Hadoop's configuration information.  i.e., where this
[33malgo-2    |[0m # file is living. If this is not defined, Hadoop will attempt to
[33malgo-2    |[0m # locate it based upon its execution path.
[33malgo-2    |[0m #
[33malgo-2    |[0m # NOTE: It is recommend that this variable not be set here but in
[33malgo-2    |[0m # /etc/profile.d or equivalent.  Some options (such as
[33malgo-2    |[0m # --config) may react strangely otherwise.
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
[33malgo-2    |[0m 
[33malgo-2    |[0m # The maximum amount of heap to use (Java -Xmx).  If no unit
[33malgo-2    |[0m # is provided, it will be converted to MB.  Daemons will
[33malgo-2    |[0m # prefer any Xmx setting in their respective _OPT variable.
[33malgo-2    |[0m # There is no default; the JVM will autoscale based upon machine
[33malgo-2    |[0m # memory size.
[33malgo-2    |[0m # export HADOOP_HEAPSIZE_MAX=
[33malgo-2    |[0m 
[33malgo-2    |[0m # The minimum amount of heap to use (Java -Xms).  If no unit
[33malgo-2    |[0m # is provided, it will be converted to MB.  Daemons will
[33malgo-2    |[0m # prefer any Xms setting in their respective _OPT variable.
[33malgo-2    |[0m # There is no default; the JVM will autoscale based upon machine
[33malgo-2    |[0m # memory size.
[33malgo-2    |[0m # export HADOOP_HEAPSIZE_MIN=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Enable extra debugging of Hadoop's JAAS binding, used to set up
[33malgo-2    |[0m # Kerberos security.
[33malgo-2    |[0m # export HADOOP_JAAS_DEBUG=true
[33malgo-2    |[0m 
[33malgo-2    |[0m # Extra Java runtime options for all Hadoop commands. We don't support
[33malgo-2    |[0m # IPv6 yet/still, so by default the preference is set to IPv4.
[33malgo-2    |[0m # export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true"
[33malgo-2    |[0m # For Kerberos debugging, an extended option set logs more information
[33malgo-2    |[0m # export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug"
[33malgo-2    |[0m 
[33malgo-2    |[0m # Some parts of the shell code may do special things dependent upon
[33malgo-2    |[0m # the operating system.  We have to set this here. See the next
[33malgo-2    |[0m # section as to why....
[33malgo-2    |[0m #export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}
[33malgo-2    |[0m 
[33malgo-2    |[0m # Extra Java runtime options for some Hadoop commands
[33malgo-2    |[0m # and clients (i.e., hdfs dfs -blah).  These get appended to HADOOP_OPTS for
[33malgo-2    |[0m # such commands.  In most cases, # this should be left empty and
[33malgo-2    |[0m # let users supply it on the command line.
[33malgo-2    |[0m # export HADOOP_CLIENT_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # A note about classpaths.
[33malgo-2    |[0m #
[33malgo-2    |[0m # By default, Apache Hadoop overrides Java's CLASSPATH
[33malgo-2    |[0m # environment variable.  It is configured such
[33malgo-2    |[0m # that it starts out blank with new entries added after passing
[33malgo-2    |[0m # a series of checks (file/dir exists, not already listed aka
[33malgo-2    |[0m # de-deduplication).  During de-deduplication, wildcards and/or
[33malgo-2    |[0m # directories are *NOT* expanded to keep it simple. Therefore,
[33malgo-2    |[0m # if the computed classpath has two specific mentions of
[33malgo-2    |[0m # awesome-methods-1.0.jar, only the first one added will be seen.
[33malgo-2    |[0m # If two directories are in the classpath that both contain
[33malgo-2    |[0m # awesome-methods-1.0.jar, then Java will pick up both versions.
[33malgo-2    |[0m 
[33malgo-2    |[0m # An additional, custom CLASSPATH. Site-wide configs should be
[33malgo-2    |[0m # handled via the shellprofile functionality, utilizing the
[33malgo-2    |[0m # hadoop_add_classpath function for greater control and much
[33malgo-2    |[0m # harder for apps/end-users to accidentally override.
[33malgo-2    |[0m # Similarly, end users should utilize ${HOME}/.hadooprc .
[33malgo-2    |[0m # This variable should ideally only be used as a short-cut,
[33malgo-2    |[0m # interactive way for temporary additions on the command line.
[33malgo-2    |[0m # export HADOOP_CLASSPATH="/some/cool/path/on/your/machine"
[33malgo-2    |[0m 
[33malgo-2    |[0m # Should HADOOP_CLASSPATH be first in the official CLASSPATH?
[33malgo-2    |[0m # export HADOOP_USER_CLASSPATH_FIRST="yes"
[33malgo-2    |[0m 
[33malgo-2    |[0m # If HADOOP_USE_CLIENT_CLASSLOADER is set, the classpath along
[33malgo-2    |[0m # with the main jar are handled by a separate isolated
[33malgo-2    |[0m # client classloader when 'hadoop jar', 'yarn jar', or 'mapred job'
[33malgo-2    |[0m # is utilized. If it is set, HADOOP_CLASSPATH and
[33malgo-2    |[0m # HADOOP_USER_CLASSPATH_FIRST are ignored.
[33malgo-2    |[0m # export HADOOP_USE_CLIENT_CLASSLOADER=true
[33malgo-2    |[0m 
[33malgo-2    |[0m # HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES overrides the default definition of
[33malgo-2    |[0m # system classes for the client classloader when HADOOP_USE_CLIENT_CLASSLOADER
[33malgo-2    |[0m # is enabled. Names ending in '.' (period) are treated as package names, and
[33malgo-2    |[0m # names starting with a '-' are treated as negative matches. For example,
[33malgo-2    |[0m # export HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES="-org.apache.hadoop.UserClass,java.,javax.,org.apache.hadoop."
[33malgo-2    |[0m 
[33malgo-2    |[0m # Enable optional, bundled Hadoop features
[33malgo-2    |[0m # This is a comma delimited list.  It may NOT be overridden via .hadooprc
[33malgo-2    |[0m # Entries may be added/removed as needed.
[33malgo-2    |[0m # export HADOOP_OPTIONAL_TOOLS="hadoop-azure,hadoop-azure-datalake,hadoop-aws,hadoop-openstack,hadoop-aliyun,hadoop-kafka"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Options for remote shell connectivity
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # There are some optional components of hadoop that allow for
[33malgo-2    |[0m # command and control of remote hosts.  For example,
[33malgo-2    |[0m # start-dfs.sh will attempt to bring up all NNs, DNS, etc.
[33malgo-2    |[0m 
[33malgo-2    |[0m # Options to pass to SSH when one of the "log into a host and
[33malgo-2    |[0m # start/stop daemons" scripts is executed
[33malgo-2    |[0m # export HADOOP_SSH_OPTS="-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10s"
[33malgo-2    |[0m 
[33malgo-2    |[0m # The built-in ssh handler will limit itself to 10 simultaneous connections.
[33malgo-2    |[0m # For pdsh users, this sets the fanout size ( -f )
[33malgo-2    |[0m # Change this to increase/decrease as necessary.
[33malgo-2    |[0m # export HADOOP_SSH_PARALLEL=10
[33malgo-2    |[0m 
[33malgo-2    |[0m # Filename which contains all of the hosts for any remote execution
[33malgo-2    |[0m # helper scripts # such as workers.sh, start-dfs.sh, etc.
[33malgo-2    |[0m # export HADOOP_WORKERS="${HADOOP_CONF_DIR}/workers"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Options for all daemons
[33malgo-2    |[0m ###
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Many options may also be specified as Java properties.  It is
[33malgo-2    |[0m # very common, and in many cases, desirable, to hard-set these
[33malgo-2    |[0m # in daemon _OPTS variables.  Where applicable, the appropriate
[33malgo-2    |[0m # Java property is also identified.  Note that many are re-used
[33malgo-2    |[0m # or set differently in certain contexts (e.g., secure vs
[33malgo-2    |[0m # non-secure)
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m # Where (primarily) daemon log files are stored.
[33malgo-2    |[0m # ${HADOOP_HOME}/logs by default.
[33malgo-2    |[0m # Java property: hadoop.log.dir
[33malgo-2    |[0m # export HADOOP_LOG_DIR=${HADOOP_HOME}/logs
[33malgo-2    |[0m 
[33malgo-2    |[0m # A string representing this instance of hadoop. $USER by default.
[33malgo-2    |[0m # This is used in writing log and pid files, so keep that in mind!
[33malgo-2    |[0m # Java property: hadoop.id.str
[33malgo-2    |[0m # export HADOOP_IDENT_STRING=$USER
[33malgo-2    |[0m 
[33malgo-2    |[0m # How many seconds to pause after stopping a daemon
[33malgo-2    |[0m # export HADOOP_STOP_TIMEOUT=5
[33malgo-2    |[0m 
[33malgo-2    |[0m # Where pid files are stored.  /tmp by default.
[33malgo-2    |[0m # export HADOOP_PID_DIR=/tmp
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default log4j setting for interactive commands
[33malgo-2    |[0m # Java property: hadoop.root.logger
[33malgo-2    |[0m # export HADOOP_ROOT_LOGGER=INFO,console
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default log4j setting for daemons spawned explicitly by
[33malgo-2    |[0m # --daemon option of hadoop, hdfs, mapred and yarn command.
[33malgo-2    |[0m # Java property: hadoop.root.logger
[33malgo-2    |[0m # export HADOOP_DAEMON_ROOT_LOGGER=INFO,RFA
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default log level and output location for security-related messages.
[33malgo-2    |[0m # You will almost certainly want to change this on a per-daemon basis via
[33malgo-2    |[0m # the Java property (i.e., -Dhadoop.security.logger=foo). (Note that the
[33malgo-2    |[0m # defaults for the NN and 2NN override this by default.)
[33malgo-2    |[0m # Java property: hadoop.security.logger
[33malgo-2    |[0m # export HADOOP_SECURITY_LOGGER=INFO,NullAppender
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default process priority level
[33malgo-2    |[0m # Note that sub-processes will also run at this level!
[33malgo-2    |[0m # export HADOOP_NICENESS=0
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default name for the service level authorization file
[33malgo-2    |[0m # Java property: hadoop.policy.file
[33malgo-2    |[0m # export HADOOP_POLICYFILE="hadoop-policy.xml"
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # NOTE: this is not used by default!  <-----
[33malgo-2    |[0m # You can define variables right here and then re-use them later on.
[33malgo-2    |[0m # For example, it is common to use the same garbage collection settings
[33malgo-2    |[0m # for all the daemons.  So one could define:
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HADOOP_GC_SETTINGS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps"
[33malgo-2    |[0m #
[33malgo-2    |[0m # .. and then use it as per the b option under the namenode.
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Secure/privileged execution
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Out of the box, Hadoop uses jsvc from Apache Commons to launch daemons
[33malgo-2    |[0m # on privileged ports.  This functionality can be replaced by providing
[33malgo-2    |[0m # custom functions.  See hadoop-functions.sh for more information.
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m # The jsvc implementation to use. Jsvc is required to run secure datanodes
[33malgo-2    |[0m # that bind to privileged ports to provide authentication of data transfer
[33malgo-2    |[0m # protocol.  Jsvc is not required if SASL is configured for authentication of
[33malgo-2    |[0m # data transfer protocol using non-privileged ports.
[33malgo-2    |[0m # export JSVC_HOME=/usr/bin
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # This directory contains pids for secure and privileged processes.
[33malgo-2    |[0m #export HADOOP_SECURE_PID_DIR=${HADOOP_PID_DIR}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # This directory contains the logs for secure and privileged processes.
[33malgo-2    |[0m # Java property: hadoop.log.dir
[33malgo-2    |[0m # export HADOOP_SECURE_LOG=${HADOOP_LOG_DIR}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # When running a secure daemon, the default value of HADOOP_IDENT_STRING
[33malgo-2    |[0m # ends up being a bit bogus.  Therefore, by default, the code will
[33malgo-2    |[0m # replace HADOOP_IDENT_STRING with HADOOP_xx_SECURE_USER.  If one wants
[33malgo-2    |[0m # to keep HADOOP_IDENT_STRING untouched, then uncomment this line.
[33malgo-2    |[0m # export HADOOP_SECURE_IDENT_PRESERVE="true"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # NameNode specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Default log level and output location for file system related change
[33malgo-2    |[0m # messages. For non-namenode daemons, the Java property must be set in
[33malgo-2    |[0m # the appropriate _OPTS if one wants something other than INFO,NullAppender
[33malgo-2    |[0m # Java property: hdfs.audit.logger
[33malgo-2    |[0m # export HDFS_AUDIT_LOGGER=INFO,NullAppender
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the NameNode.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # a) Set JMX options
[33malgo-2    |[0m # export HDFS_NAMENODE_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
[33malgo-2    |[0m #
[33malgo-2    |[0m # b) Set garbage collection logs
[33malgo-2    |[0m # export HDFS_NAMENODE_OPTS="${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[33malgo-2    |[0m #
[33malgo-2    |[0m # c) ... or set them directly
[33malgo-2    |[0m # export HDFS_NAMENODE_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[33malgo-2    |[0m 
[33malgo-2    |[0m # this is the default:
[33malgo-2    |[0m # export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # SecondaryNameNode specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the SecondaryNameNode.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # This is the default:
[33malgo-2    |[0m # export HDFS_SECONDARYNAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # DataNode specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the DataNode.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # This is the default:
[33malgo-2    |[0m # export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS"
[33malgo-2    |[0m 
[33malgo-2    |[0m # On secure datanodes, user to run the datanode as after dropping privileges.
[33malgo-2    |[0m # This **MUST** be uncommented to enable secure HDFS if using privileged ports
[33malgo-2    |[0m # to provide authentication of data transfer protocol.  This **MUST NOT** be
[33malgo-2    |[0m # defined if SASL is configured for authentication of data transfer protocol
[33malgo-2    |[0m # using non-privileged ports.
[33malgo-2    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[33malgo-2    |[0m # export HDFS_DATANODE_SECURE_USER=hdfs
[33malgo-2    |[0m 
[33malgo-2    |[0m # Supplemental options for secure datanodes
[33malgo-2    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[33malgo-2    |[0m # server jvm.
[33malgo-2    |[0m # export HDFS_DATANODE_SECURE_EXTRA_OPTS="-jvm server"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # NFS3 Gateway specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the NFS3 Gateway.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_NFS3_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the Hadoop portmapper.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_PORTMAP_OPTS="-Xmx512m"
[33malgo-2    |[0m 
[33malgo-2    |[0m # Supplemental options for priviliged gateways
[33malgo-2    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[33malgo-2    |[0m # server jvm.
[33malgo-2    |[0m # export HDFS_NFS3_SECURE_EXTRA_OPTS="-jvm server"
[33malgo-2    |[0m 
[33malgo-2    |[0m # On privileged gateways, user to run the gateway as after dropping privileges
[33malgo-2    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[33malgo-2    |[0m # export HDFS_NFS3_SECURE_USER=nfsserver
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # ZKFailoverController specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the ZKFailoverController.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_ZKFC_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # QuorumJournalNode specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the QuorumJournalNode.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_JOURNALNODE_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # HDFS Balancer specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the HDFS Balancer.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_BALANCER_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # HDFS Mover specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the HDFS Mover.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_MOVER_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Router-based HDFS Federation specific parameters
[33malgo-2    |[0m # Specify the JVM options to be used when starting the RBF Routers.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_DFSROUTER_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # HDFS StorageContainerManager specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the HDFS Storage Container Manager.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # export HDFS_STORAGECONTAINERMANAGER_OPTS=""
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Advanced Users Only!
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # When building Hadoop, one can add the class paths to the commands
[33malgo-2    |[0m # via this special env var:
[33malgo-2    |[0m # export HADOOP_ENABLE_BUILD_PATHS="true"
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # To prevent accidents, shell commands be (superficially) locked
[33malgo-2    |[0m # to only allow certain users to execute certain subcommands.
[33malgo-2    |[0m # It uses the format of (command)_(subcommand)_USER.
[33malgo-2    |[0m #
[33malgo-2    |[0m # For example, to limit who can execute the namenode command,
[33malgo-2    |[0m # export HDFS_NAMENODE_USER=hdfs
[33malgo-2    |[0m export SPARK_MASTER_HOST=172.18.0.3
[33malgo-2    |[0m export AWS_CONTAINER_CREDENTIALS_RELATIVE_URI=
[33malgo-2    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[33malgo-2    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/core-site.xml
[33malgo-2    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/core-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0" encoding="UTF-8"?>
[33malgo-2    |[0m  <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[33malgo-2    |[0m  <!-- Put site-specific property overrides in this file. -->
[33malgo-2    |[0m 
[33malgo-2    |[0m  <configuration>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>fs.defaultFS</name>
[33malgo-2    |[0m          <value>hdfs://172.18.0.3/</value>
[33malgo-2    |[0m          <description>NameNode URI</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>fs.s3a.aws.credentials.provider</name>
[33malgo-2    |[0m          <value>com.amazonaws.auth.DefaultAWSCredentialsProviderChain</value>
[33malgo-2    |[0m          <description>AWS S3 credential provider</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>fs.s3.impl</name>
[33malgo-2    |[0m          <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
[33malgo-2    |[0m          <description>s3a filesystem implementation</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>fs.AbstractFileSystem.s3a.imp</name>
[33malgo-2    |[0m          <value>org.apache.hadoop.fs.s3a.S3A</value>
[33malgo-2    |[0m          <description>s3a filesystem implementation</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m  
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>spark.executor.memory</name>
[33malgo-2    |[0m     <value>2g</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>spark.executor.cores</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/log4j.properties
[33malgo-2    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/log4j.properties is: 
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[33malgo-2    |[0m # or more contributor license agreements.  See the NOTICE file
[33malgo-2    |[0m # distributed with this work for additional information
[33malgo-2    |[0m # regarding copyright ownership.  The ASF licenses this file
[33malgo-2    |[0m # to you under the Apache License, Version 2.0 (the
[33malgo-2    |[0m # "License"); you may not use this file except in compliance
[33malgo-2    |[0m # with the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m # Define some default values that can be overridden by system properties
[33malgo-2    |[0m hadoop.root.logger=INFO,console
[33malgo-2    |[0m hadoop.log.dir=.
[33malgo-2    |[0m hadoop.log.file=hadoop.log
[33malgo-2    |[0m 
[33malgo-2    |[0m # Define the root logger to the system property "hadoop.root.logger".
[33malgo-2    |[0m log4j.rootLogger=${hadoop.root.logger}, EventCounter
[33malgo-2    |[0m 
[33malgo-2    |[0m # Logging Threshold
[33malgo-2    |[0m log4j.threshold=ALL
[33malgo-2    |[0m 
[33malgo-2    |[0m # Null Appender
[33malgo-2    |[0m log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Rolling File Appender - cap space usage at 5gb.
[33malgo-2    |[0m #
[33malgo-2    |[0m hadoop.log.maxfilesize=256MB
[33malgo-2    |[0m hadoop.log.maxbackupindex=20
[33malgo-2    |[0m log4j.appender.RFA=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.RFA.MaxFileSize=${hadoop.log.maxfilesize}
[33malgo-2    |[0m log4j.appender.RFA.MaxBackupIndex=${hadoop.log.maxbackupindex}
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.RFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m 
[33malgo-2    |[0m # Pattern format: Date LogLevel LoggerName LogMessage
[33malgo-2    |[0m log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[33malgo-2    |[0m # Debugging Pattern format
[33malgo-2    |[0m #log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Daily Rolling File Appender
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.DRFA.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m 
[33malgo-2    |[0m # Rollver at midnight
[33malgo-2    |[0m #log4j.appender.DRFA.DatePattern=.yyyy-MM-dd
[33malgo-2    |[0m # Rollver at every hour
[33malgo-2    |[0m log4j.appender.DRFA.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m 
[33malgo-2    |[0m # Pattern format: Date LogLevel LoggerName LogMessage
[33malgo-2    |[0m log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m # Debugging Pattern format
[33malgo-2    |[0m #log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # console
[33malgo-2    |[0m # Add "console" to rootlogger above if you want to use this
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.console=org.apache.log4j.ConsoleAppender
[33malgo-2    |[0m log4j.appender.console.target=System.err
[33malgo-2    |[0m log4j.appender.console.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # TaskLog Appender
[33malgo-2    |[0m #
[33malgo-2    |[0m log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.TLA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # HDFS block state change log from block manager
[33malgo-2    |[0m #
[33malgo-2    |[0m # Uncomment the following to log normal block state change
[33malgo-2    |[0m # messages from BlockManager in NameNode.
[33malgo-2    |[0m #log4j.logger.BlockStateChange=DEBUG
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m #Security appender
[33malgo-2    |[0m #
[33malgo-2    |[0m hadoop.security.logger=INFO,NullAppender
[33malgo-2    |[0m hadoop.security.log.maxfilesize=256MB
[33malgo-2    |[0m hadoop.security.log.maxbackupindex=20
[33malgo-2    |[0m log4j.category.SecurityLogger=${hadoop.security.logger}
[33malgo-2    |[0m hadoop.security.log.file=SecurityAuth-${user.name}.audit
[33malgo-2    |[0m log4j.appender.RFAS=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.RFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
[33malgo-2    |[0m log4j.appender.RFAS.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[33malgo-2    |[0m log4j.appender.RFAS.MaxFileSize=${hadoop.security.log.maxfilesize}
[33malgo-2    |[0m log4j.appender.RFAS.MaxBackupIndex=${hadoop.security.log.maxbackupindex}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Daily Rolling Security appender
[33malgo-2    |[0m #
[33malgo-2    |[0m log4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.DRFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
[33malgo-2    |[0m log4j.appender.DRFAS.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.DRFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[33malgo-2    |[0m log4j.appender.DRFAS.DatePattern=.yyyy-MM-dd
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # hadoop configuration logging
[33malgo-2    |[0m #
[33malgo-2    |[0m 
[33malgo-2    |[0m # Uncomment the following line to turn off configuration deprecation warnings.
[33malgo-2    |[0m # log4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # hdfs audit logging
[33malgo-2    |[0m #
[33malgo-2    |[0m hdfs.audit.logger=INFO,NullAppender
[33malgo-2    |[0m hdfs.audit.log.maxfilesize=256MB
[33malgo-2    |[0m hdfs.audit.log.maxbackupindex=20
[33malgo-2    |[0m log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}
[33malgo-2    |[0m log4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false
[33malgo-2    |[0m log4j.appender.RFAAUDIT=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.RFAAUDIT.File=${hadoop.log.dir}/hdfs-audit.log
[33malgo-2    |[0m log4j.appender.RFAAUDIT.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.RFAAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[33malgo-2    |[0m log4j.appender.RFAAUDIT.MaxFileSize=${hdfs.audit.log.maxfilesize}
[33malgo-2    |[0m log4j.appender.RFAAUDIT.MaxBackupIndex=${hdfs.audit.log.maxbackupindex}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # NameNode metrics logging.
[33malgo-2    |[0m # The default is to retain two namenode-metrics.log files up to 64MB each.
[33malgo-2    |[0m #
[33malgo-2    |[0m namenode.metrics.logger=INFO,NullAppender
[33malgo-2    |[0m log4j.logger.NameNodeMetricsLog=${namenode.metrics.logger}
[33malgo-2    |[0m log4j.additivity.NameNodeMetricsLog=false
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.File=${hadoop.log.dir}/namenode-metrics.log
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.MaxBackupIndex=1
[33malgo-2    |[0m log4j.appender.NNMETRICSRFA.MaxFileSize=64MB
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # DataNode metrics logging.
[33malgo-2    |[0m # The default is to retain two datanode-metrics.log files up to 64MB each.
[33malgo-2    |[0m #
[33malgo-2    |[0m datanode.metrics.logger=INFO,NullAppender
[33malgo-2    |[0m log4j.logger.DataNodeMetricsLog=${datanode.metrics.logger}
[33malgo-2    |[0m log4j.additivity.DataNodeMetricsLog=false
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.File=${hadoop.log.dir}/datanode-metrics.log
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.MaxBackupIndex=1
[33malgo-2    |[0m log4j.appender.DNMETRICSRFA.MaxFileSize=64MB
[33malgo-2    |[0m 
[33malgo-2    |[0m # Custom Logging levels
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=DEBUG
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m # AWS SDK & S3A FileSystem
[33malgo-2    |[0m #log4j.logger.com.amazonaws=ERROR
[33malgo-2    |[0m log4j.logger.com.amazonaws.http.AmazonHttpClient=ERROR
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.fs.s3a.S3AFileSystem=WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Event Counter Appender
[33malgo-2    |[0m # Sends counts of logging messages at different severity levels to Hadoop Metrics.
[33malgo-2    |[0m #
[33malgo-2    |[0m log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Job Summary Appender
[33malgo-2    |[0m #
[33malgo-2    |[0m # Use following logger to send summary to separate file defined by
[33malgo-2    |[0m # hadoop.mapreduce.jobsummary.log.file :
[33malgo-2    |[0m # hadoop.mapreduce.jobsummary.logger=INFO,JSA
[33malgo-2    |[0m # 
[33malgo-2    |[0m hadoop.mapreduce.jobsummary.logger=${hadoop.root.logger}
[33malgo-2    |[0m hadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log
[33malgo-2    |[0m hadoop.mapreduce.jobsummary.log.maxfilesize=256MB
[33malgo-2    |[0m hadoop.mapreduce.jobsummary.log.maxbackupindex=20
[33malgo-2    |[0m log4j.appender.JSA=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.JSA.File=${hadoop.log.dir}/${hadoop.mapreduce.jobsummary.log.file}
[33malgo-2    |[0m log4j.appender.JSA.MaxFileSize=${hadoop.mapreduce.jobsummary.log.maxfilesize}
[33malgo-2    |[0m log4j.appender.JSA.MaxBackupIndex=${hadoop.mapreduce.jobsummary.log.maxbackupindex}
[33malgo-2    |[0m log4j.appender.JSA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.JSA.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
[33malgo-2    |[0m log4j.appender.JSA.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.logger.org.apache.hadoop.mapred.JobInProgress$JobSummary=${hadoop.mapreduce.jobsummary.logger}
[33malgo-2    |[0m log4j.additivity.org.apache.hadoop.mapred.JobInProgress$JobSummary=false
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # shuffle connection log from shuffleHandler
[33malgo-2    |[0m # Uncomment the following line to enable logging of shuffle connections
[33malgo-2    |[0m # log4j.logger.org.apache.hadoop.mapred.ShuffleHandler.audit=DEBUG
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Yarn ResourceManager Application Summary Log
[33malgo-2    |[0m #
[33malgo-2    |[0m # Set the ResourceManager summary log filename
[33malgo-2    |[0m yarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log
[33malgo-2    |[0m # Set the ResourceManager summary log level and appender
[33malgo-2    |[0m yarn.server.resourcemanager.appsummary.logger=${hadoop.root.logger}
[33malgo-2    |[0m #yarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY
[33malgo-2    |[0m 
[33malgo-2    |[0m # To enable AppSummaryLogging for the RM,
[33malgo-2    |[0m # set yarn.server.resourcemanager.appsummary.logger to
[33malgo-2    |[0m # <LEVEL>,RMSUMMARY in hadoop-env.sh
[33malgo-2    |[0m 
[33malgo-2    |[0m # Appender for ResourceManager Application Summary Log
[33malgo-2    |[0m # Requires the following properties to be set
[33malgo-2    |[0m #    - hadoop.log.dir (Hadoop Log directory)
[33malgo-2    |[0m #    - yarn.server.resourcemanager.appsummary.log.file (resource manager app summary log filename)
[33malgo-2    |[0m #    - yarn.server.resourcemanager.appsummary.logger (resource manager app summary log level and appender)
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=${yarn.server.resourcemanager.appsummary.logger}
[33malgo-2    |[0m log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=false
[33malgo-2    |[0m log4j.appender.RMSUMMARY=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m log4j.appender.RMSUMMARY.File=${hadoop.log.dir}/${yarn.server.resourcemanager.appsummary.log.file}
[33malgo-2    |[0m log4j.appender.RMSUMMARY.MaxFileSize=256MB
[33malgo-2    |[0m log4j.appender.RMSUMMARY.MaxBackupIndex=20
[33malgo-2    |[0m log4j.appender.RMSUMMARY.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.RMSUMMARY.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m # HS audit log configs
[33malgo-2    |[0m #mapreduce.hs.audit.logger=INFO,HSAUDIT
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=${mapreduce.hs.audit.logger}
[33malgo-2    |[0m #log4j.additivity.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=false
[33malgo-2    |[0m #log4j.appender.HSAUDIT=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m #log4j.appender.HSAUDIT.File=${hadoop.log.dir}/hs-audit.log
[33malgo-2    |[0m #log4j.appender.HSAUDIT.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m #log4j.appender.HSAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
[33malgo-2    |[0m #log4j.appender.HSAUDIT.DatePattern=.yyyy-MM-dd
[33malgo-2    |[0m 
[33malgo-2    |[0m # Http Server Request Logs
[33malgo-2    |[0m #log4j.logger.http.requests.namenode=INFO,namenoderequestlog
[33malgo-2    |[0m #log4j.appender.namenoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.namenoderequestlog.Filename=${hadoop.log.dir}/jetty-namenode-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.namenoderequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.http.requests.datanode=INFO,datanoderequestlog
[33malgo-2    |[0m #log4j.appender.datanoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.datanoderequestlog.Filename=${hadoop.log.dir}/jetty-datanode-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.datanoderequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.http.requests.resourcemanager=INFO,resourcemanagerrequestlog
[33malgo-2    |[0m #log4j.appender.resourcemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.resourcemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-resourcemanager-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.resourcemanagerrequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.http.requests.jobhistory=INFO,jobhistoryrequestlog
[33malgo-2    |[0m #log4j.appender.jobhistoryrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.jobhistoryrequestlog.Filename=${hadoop.log.dir}/jetty-jobhistory-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.jobhistoryrequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.http.requests.nodemanager=INFO,nodemanagerrequestlog
[33malgo-2    |[0m #log4j.appender.nodemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
[33malgo-2    |[0m #log4j.appender.nodemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-nodemanager-yyyy_mm_dd.log
[33malgo-2    |[0m #log4j.appender.nodemanagerrequestlog.RetainDays=3
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m # WebHdfs request log on datanodes
[33malgo-2    |[0m # Specify -Ddatanode.webhdfs.logger=INFO,HTTPDRFA on datanode startup to
[33malgo-2    |[0m # direct the log to a separate file.
[33malgo-2    |[0m #datanode.webhdfs.logger=INFO,console
[33malgo-2    |[0m #log4j.logger.datanode.webhdfs=${datanode.webhdfs.logger}
[33malgo-2    |[0m #log4j.appender.HTTPDRFA=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m #log4j.appender.HTTPDRFA.File=${hadoop.log.dir}/hadoop-datanode-webhdfs.log
[33malgo-2    |[0m #log4j.appender.HTTPDRFA.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m #log4j.appender.HTTPDRFA.layout.ConversionPattern=%d{ISO8601} %m%n
[33malgo-2    |[0m #log4j.appender.HTTPDRFA.DatePattern=.yyyy-MM-dd
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m # Appender for viewing information for errors and warnings
[33malgo-2    |[0m yarn.ewma.cleanupInterval=300
[33malgo-2    |[0m yarn.ewma.messageAgeLimitSeconds=86400
[33malgo-2    |[0m yarn.ewma.maxUniqueMessages=250
[33malgo-2    |[0m log4j.appender.EWMA=org.apache.hadoop.yarn.util.Log4jWarningErrorMetricsAppender
[33malgo-2    |[0m log4j.appender.EWMA.cleanupInterval=${yarn.ewma.cleanupInterval}
[33malgo-2    |[0m log4j.appender.EWMA.messageAgeLimitSeconds=${yarn.ewma.messageAgeLimitSeconds}
[33malgo-2    |[0m log4j.appender.EWMA.maxUniqueMessages=${yarn.ewma.maxUniqueMessages}
[33malgo-2    |[0m 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Fair scheduler state dump
[33malgo-2    |[0m #
[33malgo-2    |[0m # Use following logger to dump the state to a separate file
[33malgo-2    |[0m 
[33malgo-2    |[0m #log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=DEBUG,FSSTATEDUMP
[33malgo-2    |[0m #log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=false
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP=org.apache.log4j.RollingFileAppender
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.File=${hadoop.log.dir}/fairscheduler-statedump.log
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.MaxFileSize=${hadoop.log.maxfilesize}
[33malgo-2    |[0m #log4j.appender.FSSTATEDUMP.MaxBackupIndex=${hadoop.log.maxbackupindex}
[33malgo-2    |[0m 
[33malgo-2    |[0m # Log levels of third-party libraries
[33malgo-2    |[0m log4j.logger.org.apache.commons.beanutils=WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m #AWS SDK Logging
[33malgo-2    |[0m log4j.logger.com.amazonaws=WARN
[33malgo-2    |[0m log4j.logger.org.apache.zookeeper=ERROR
[33malgo-2    |[0m log4j.logger.org.apache.hadoop.hbase=WARN
[33malgo-2    |[0m log4j.logger.amazon.emr.metrics=WARN
[33malgo-2    |[0m log4j.logger.emr=INFO
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.HADOOP=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.HADOOP.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m log4j.appender.HADOOP.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.appender.HADOOP.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.HADOOP.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.JHS=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.JHS.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m log4j.appender.JHS.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.appender.JHS.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.JHS.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.MAPRED=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.MAPRED.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m log4j.appender.MAPRED.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.appender.MAPRED.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.MAPRED.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m log4j.appender.YARN=org.apache.log4j.DailyRollingFileAppender
[33malgo-2    |[0m log4j.appender.YARN.File=${hadoop.log.dir}/${hadoop.log.file}
[33malgo-2    |[0m log4j.appender.YARN.DatePattern=.yyyy-MM-dd-HH
[33malgo-2    |[0m log4j.appender.YARN.layout=org.apache.log4j.PatternLayout
[33malgo-2    |[0m log4j.appender.YARN.layout.ConversionPattern=%d{ISO8601} %p %c (%t): %m%n
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/hive/conf/hive-env.sh
[33malgo-2    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/hive/conf/hive-env.sh is: 
[33malgo-2    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[33malgo-2    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[33malgo-2    |[0m 
[36malgo-1    |[0m WARNING: HADOOP_NAMENODE_OPTS has been replaced by HDFS_NAMENODE_OPTS. Using value of HADOOP_NAMENODE_OPTS.
[33malgo-2    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/hive/conf/hive-log4j2.properties
[33malgo-2    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/hive/conf/hive-log4j2.properties is: 
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[33malgo-2    |[0m # or more contributor license agreements.  See the NOTICE file
[33malgo-2    |[0m # distributed with this work for additional information
[33malgo-2    |[0m # regarding copyright ownership.  The ASF licenses this file
[33malgo-2    |[0m # to you under the Apache License, Version 2.0 (the
[33malgo-2    |[0m # "License"); you may not use this file except in compliance
[33malgo-2    |[0m # with the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m status = INFO
[33malgo-2    |[0m name = HiveLog4j2
[33malgo-2    |[0m packages = org.apache.hadoop.hive.ql.log
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of properties
[33malgo-2    |[0m property.hive.log.level = INFO
[33malgo-2    |[0m property.hive.root.logger = DRFA
[33malgo-2    |[0m property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
[33malgo-2    |[0m property.hive.log.file = hive.log
[33malgo-2    |[0m property.hive.perflogger.log.level = INFO
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of all appenders
[33malgo-2    |[0m appenders = console, DRFA
[33malgo-2    |[0m 
[33malgo-2    |[0m # console appender
[33malgo-2    |[0m appender.console.type = Console
[33malgo-2    |[0m appender.console.name = console
[33malgo-2    |[0m appender.console.target = SYSTEM_ERR
[33malgo-2    |[0m appender.console.layout.type = PatternLayout
[33malgo-2    |[0m appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m # daily rolling file appender
[33malgo-2    |[0m appender.DRFA.type = RollingRandomAccessFile
[33malgo-2    |[0m appender.DRFA.name = DRFA
[33malgo-2    |[0m appender.DRFA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
[33malgo-2    |[0m # Use %pid in the filePattern to append <process-id>@<host-name> to the filename if you want separate log files for different CLI session
[33malgo-2    |[0m appender.DRFA.filePattern = ${sys:hive.log.dir}/${sys:hive.log.file}.%d{yyyy-MM-dd}
[33malgo-2    |[0m appender.DRFA.layout.type = PatternLayout
[33malgo-2    |[0m appender.DRFA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[33malgo-2    |[0m appender.DRFA.policies.type = Policies
[33malgo-2    |[0m appender.DRFA.policies.time.type = TimeBasedTriggeringPolicy
[33malgo-2    |[0m appender.DRFA.policies.time.interval = 1
[33malgo-2    |[0m appender.DRFA.policies.time.modulate = true
[33malgo-2    |[0m appender.DRFA.strategy.type = DefaultRolloverStrategy
[33malgo-2    |[0m appender.DRFA.strategy.max = 30
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of all loggers
[33malgo-2    |[0m loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, PerfLogger, AmazonAws, ApacheHttp
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
[33malgo-2    |[0m logger.NIOServerCnxn.level = WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
[33malgo-2    |[0m logger.ClientCnxnSocketNIO.level = WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.DataNucleus.name = DataNucleus
[33malgo-2    |[0m logger.DataNucleus.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.Datastore.name = Datastore
[33malgo-2    |[0m logger.Datastore.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.JPOX.name = JPOX
[33malgo-2    |[0m logger.JPOX.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.AmazonAws.name=com.amazonaws
[33malgo-2    |[0m logger.AmazonAws.level = INFO
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.ApacheHttp.name=org.apache.http
[33malgo-2    |[0m logger.ApacheHttp.level = INFO
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.PerfLogger.name = org.apache.hadoop.hive.ql.log.PerfLogger
[33malgo-2    |[0m logger.PerfLogger.level = ${sys:hive.perflogger.log.level}
[33malgo-2    |[0m 
[33malgo-2    |[0m # root logger
[33malgo-2    |[0m rootLogger.level = ${sys:hive.log.level}
[33malgo-2    |[0m rootLogger.appenderRefs = root
[33malgo-2    |[0m rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/hive/conf/hive-exec-log4j2.properties
[36malgo-1    |[0m WARNING: /usr/lib/hadoop/logs does not exist. Creating.
[33malgo-2    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/hive/conf/hive-exec-log4j2.properties is: 
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one
[33malgo-2    |[0m # or more contributor license agreements.  See the NOTICE file
[33malgo-2    |[0m # distributed with this work for additional information
[33malgo-2    |[0m # regarding copyright ownership.  The ASF licenses this file
[33malgo-2    |[0m # to you under the Apache License, Version 2.0 (the
[33malgo-2    |[0m # "License"); you may not use this file except in compliance
[33malgo-2    |[0m # with the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m status = INFO
[33malgo-2    |[0m name = HiveExecLog4j2
[33malgo-2    |[0m packages = org.apache.hadoop.hive.ql.log
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of properties
[33malgo-2    |[0m property.hive.log.level = INFO
[33malgo-2    |[0m property.hive.root.logger = FA
[33malgo-2    |[0m property.hive.query.id = hadoop
[33malgo-2    |[0m property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
[33malgo-2    |[0m property.hive.log.file = ${sys:hive.query.id}.log
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of all appenders
[33malgo-2    |[0m appenders = console, FA
[33malgo-2    |[0m 
[33malgo-2    |[0m # console appender
[33malgo-2    |[0m appender.console.type = Console
[33malgo-2    |[0m appender.console.name = console
[33malgo-2    |[0m appender.console.target = SYSTEM_ERR
[33malgo-2    |[0m appender.console.layout.type = PatternLayout
[33malgo-2    |[0m appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m # simple file appender
[33malgo-2    |[0m appender.FA.type = RandomAccessFile
[33malgo-2    |[0m appender.FA.name = FA
[33malgo-2    |[0m appender.FA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
[33malgo-2    |[0m appender.FA.layout.type = PatternLayout
[33malgo-2    |[0m appender.FA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
[33malgo-2    |[0m 
[33malgo-2    |[0m # list of all loggers
[33malgo-2    |[0m loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
[33malgo-2    |[0m logger.NIOServerCnxn.level = WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
[33malgo-2    |[0m logger.ClientCnxnSocketNIO.level = WARN
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.DataNucleus.name = DataNucleus
[33malgo-2    |[0m logger.DataNucleus.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.Datastore.name = Datastore
[33malgo-2    |[0m logger.Datastore.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m logger.JPOX.name = JPOX
[33malgo-2    |[0m logger.JPOX.level = ERROR
[33malgo-2    |[0m 
[33malgo-2    |[0m # root logger
[33malgo-2    |[0m rootLogger.level = ${sys:hive.log.level}
[33malgo-2    |[0m rootLogger.appenderRefs = root
[33malgo-2    |[0m rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/hive/conf/hive-site.xml
[33malgo-2    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/hive/conf/hive-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0"?>
[33malgo-2    |[0m <!--
[33malgo-2    |[0m   Licensed to the Apache Software Foundation (ASF) under one or more
[33malgo-2    |[0m   contributor license agreements.  See the NOTICE file distributed with
[33malgo-2    |[0m   this work for additional information regarding copyright ownership.
[33malgo-2    |[0m   The ASF licenses this file to You under the Apache License, Version 2.0
[33malgo-2    |[0m   (the "License"); you may not use this file except in compliance with
[33malgo-2    |[0m   the License.  You may obtain a copy of the License at
[33malgo-2    |[0m 
[33malgo-2    |[0m       http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m 
[33malgo-2    |[0m   Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m   distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m   See the License for the specific language governing permissions and
[33malgo-2    |[0m   limitations under the License.
[33malgo-2    |[0m -->
[33malgo-2    |[0m <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[33malgo-2    |[0m 
[33malgo-2    |[0m <configuration>
[33malgo-2    |[0m 
[33malgo-2    |[0m <!-- Hive Configuration can either be stored in this file or in the hadoop configuration files  -->
[33malgo-2    |[0m <!-- that are implied by Hadoop setup variables.                                                -->
[33malgo-2    |[0m <!-- Aside from Hadoop setup variables - this file is provided as a convenience so that Hive    -->
[33malgo-2    |[0m <!-- users do not have to edit hadoop configuration files (that may be managed as a centralized -->
[33malgo-2    |[0m <!-- resource).                                                                                 -->
[33malgo-2    |[0m 
[33malgo-2    |[0m <!-- Hive Execution Parameters -->
[33malgo-2    |[0m 
[33malgo-2    |[0m <property>
[33malgo-2    |[0m   <name>javax.jdo.option.ConnectionURL</name>
[33malgo-2    |[0m   <value>jdbc:derby:;databaseName=/var/lib/hive/metastore/metastore_db;create=true</value>
[33malgo-2    |[0m   <description>JDBC connect string for a JDBC metastore</description>
[33malgo-2    |[0m </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m <property>
[33malgo-2    |[0m   <name>javax.jdo.option.ConnectionDriverName</name>
[33malgo-2    |[0m   <value>org.apache.derby.jdbc.EmbeddedDriver</value>
[33malgo-2    |[0m   <description>Driver class name for a JDBC metastore</description>
[33malgo-2    |[0m </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m 
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>key</name>
[33malgo-2    |[0m     <value>value</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/spark/conf/spark-defaults.conf
[33malgo-2    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: 
[33malgo-2    |[0m spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[33malgo-2    |[0m spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[33malgo-2    |[0m spark.driver.host=172.18.0.3
[33malgo-2    |[0m spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[33malgo-2    |[0m 
[33malgo-2    |[0m # Fix for "Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot
[33malgo-2    |[0m # receive any reply from 10.0.109.30:35219 in 120 seconds.""
[33malgo-2    |[0m spark.rpc.askTimeout=300s
[33malgo-2    |[0m spark.driver.memory 2048m
[33malgo-2    |[0m spark.driver.memoryOverhead 204m
[33malgo-2    |[0m spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[33malgo-2    |[0m spark.executor.memory 12399m
[33malgo-2    |[0m spark.executor.memoryOverhead 1239m
[33malgo-2    |[0m spark.executor.cores 4
[33malgo-2    |[0m spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 
[33malgo-2    |[0m spark.executor.instances 2
[33malgo-2    |[0m spark.default.parallelism 16
[33malgo-2    |[0m spark.executor.memory 2g
[33malgo-2    |[0m spark.executor.cores 1
[33malgo-2    |[0m key value
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/spark/conf/spark-env.sh
[33malgo-2    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/spark/conf/spark-env.sh is: 
[33malgo-2    |[0m #EMPTY FILE AVOID OVERRIDDING ENV VARS
[33malgo-2    |[0m # Specifically, without copying the empty file, SPARK_HISTORY_OPTS will be overriden, 
[33malgo-2    |[0m # spark.history.ui.port defaults to 18082, and spark.eventLog.dir defaults to local fs
[33malgo-2    |[0m export HADOOP_DATANODE_HEAPSIZE=2048
[33malgo-2    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/spark/conf/log4j.properties
[33malgo-2    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/spark/conf/log4j.properties is: 
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/spark/conf/hive-site.xml
[33malgo-2    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/spark/conf/hive-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0"?>
[33malgo-2    |[0m <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
[33malgo-2    |[0m <configuration>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>key</name>
[33malgo-2    |[0m     <value>value</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/spark/conf/metrics.properties
[33malgo-2    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/spark/conf/metrics.properties is: 
[33malgo-2    |[0m key=value
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml
[33malgo-2    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: 
[33malgo-2    |[0m <?xml version="1.0"?>
[33malgo-2    |[0m <!-- Site specific YARN configuration properties -->
[33malgo-2    |[0m  <configuration>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.resourcemanager.hostname</name>
[33malgo-2    |[0m          <value>172.18.0.3</value>
[33malgo-2    |[0m          <description>The hostname of the RM.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.hostname</name>
[33malgo-2    |[0m          <value>algo-2</value>
[33malgo-2    |[0m          <description>The hostname of the NM.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.webapp.address</name>
[33malgo-2    |[0m          <value>algo-2:8042</value>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.vmem-pmem-ratio</name>
[33malgo-2    |[0m          <value>5</value>
[33malgo-2    |[0m          <description>Ratio between virtual memory to physical memory.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.resourcemanager.am.max-attempts</name>
[33malgo-2    |[0m          <value>1</value>
[33malgo-2    |[0m          <description>The maximum number of application attempts.</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m      <property>
[33malgo-2    |[0m          <name>yarn.nodemanager.env-whitelist</name>
[33malgo-2    |[0m          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>
[33malgo-2    |[0m          <description>Environment variable whitelist</description>
[33malgo-2    |[0m      </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m  
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.minimum-allocation-mb</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.maximum-allocation-mb</name>
[33malgo-2    |[0m     <value>15892</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.minimum-allocation-vcores</name>
[33malgo-2    |[0m     <value>1</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.scheduler.maximum-allocation-vcores</name>
[33malgo-2    |[0m     <value>4</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.nodemanager.resource.memory-mb</name>
[33malgo-2    |[0m     <value>15892</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>yarn.nodemanager.resource.cpu-vcores</name>
[33malgo-2    |[0m     <value>4</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m 
[33malgo-2    |[0m   <property>
[33malgo-2    |[0m     <name>key</name>
[33malgo-2    |[0m     <value>value</value>
[33malgo-2    |[0m   </property>
[33malgo-2    |[0m </configuration>
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:22 root         INFO     Writing user config to /usr/lib/hadoop/etc/hadoop/yarn-env.sh
[33malgo-2    |[0m 11-14 15:22 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-env.sh is: 
[33malgo-2    |[0m #
[33malgo-2    |[0m # Licensed to the Apache Software Foundation (ASF) under one or more
[33malgo-2    |[0m # contributor license agreements.  See the NOTICE file distributed with
[33malgo-2    |[0m # this work for additional information regarding copyright ownership.
[33malgo-2    |[0m # The ASF licenses this file to You under the Apache License, Version 2.0
[33malgo-2    |[0m # (the "License"); you may not use this file except in compliance with
[33malgo-2    |[0m # the License.  You may obtain a copy of the License at
[33malgo-2    |[0m #
[33malgo-2    |[0m #     http://www.apache.org/licenses/LICENSE-2.0
[33malgo-2    |[0m #
[33malgo-2    |[0m # Unless required by applicable law or agreed to in writing, software
[33malgo-2    |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[33malgo-2    |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[33malgo-2    |[0m # See the License for the specific language governing permissions and
[33malgo-2    |[0m # limitations under the License.
[33malgo-2    |[0m 
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## THIS FILE ACTS AS AN OVERRIDE FOR hadoop-env.sh FOR ALL
[33malgo-2    |[0m ## WORK DONE BY THE yarn AND RELATED COMMANDS.
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## Precedence rules:
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## yarn-env.sh > hadoop-env.sh > hard-coded defaults
[33malgo-2    |[0m ##
[33malgo-2    |[0m ## YARN_xyz > HADOOP_xyz > hard-coded defaults
[33malgo-2    |[0m ##
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Resource Manager specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the max heapsize for the ResourceManager.  If no units are
[33malgo-2    |[0m # given, it will be assumed to be in MB.
[33malgo-2    |[0m # This value will be overridden by an Xmx setting specified in either
[33malgo-2    |[0m # HADOOP_OPTS and/or YARN_RESOURCEMANAGER_OPTS.
[33malgo-2    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX
[33malgo-2    |[0m #export YARN_RESOURCEMANAGER_HEAPSIZE=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the ResourceManager.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # Examples for a Sun/Oracle JDK:
[33malgo-2    |[0m # a) override the appsummary log file:
[33malgo-2    |[0m # export YARN_RESOURCEMANAGER_OPTS="-Dyarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log -Dyarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY"
[33malgo-2    |[0m #
[33malgo-2    |[0m # b) Set JMX options
[33malgo-2    |[0m # export YARN_RESOURCEMANAGER_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026"
[33malgo-2    |[0m #
[33malgo-2    |[0m # c) Set garbage collection logs from hadoop-env.sh
[33malgo-2    |[0m # export YARN_RESOURCE_MANAGER_OPTS="${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[33malgo-2    |[0m #
[33malgo-2    |[0m # d) ... or set them directly
[33malgo-2    |[0m # export YARN_RESOURCEMANAGER_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')"
[33malgo-2    |[0m #
[33malgo-2    |[0m #
[33malgo-2    |[0m # export YARN_RESOURCEMANAGER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Node Manager specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the max heapsize for the NodeManager.  If no units are
[33malgo-2    |[0m # given, it will be assumed to be in MB.
[33malgo-2    |[0m # This value will be overridden by an Xmx setting specified in either
[33malgo-2    |[0m # HADOOP_OPTS and/or YARN_NODEMANAGER_OPTS.
[33malgo-2    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[33malgo-2    |[0m #export YARN_NODEMANAGER_HEAPSIZE=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the NodeManager.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_NODEMANAGER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # TimeLineServer specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the max heapsize for the timelineserver.  If no units are
[33malgo-2    |[0m # given, it will be assumed to be in MB.
[33malgo-2    |[0m # This value will be overridden by an Xmx setting specified in either
[33malgo-2    |[0m # HADOOP_OPTS and/or YARN_TIMELINESERVER_OPTS.
[33malgo-2    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[33malgo-2    |[0m #export YARN_TIMELINE_HEAPSIZE=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the TimeLineServer.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_TIMELINESERVER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # TimeLineReader specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the TimeLineReader.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_TIMELINEREADER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Web App Proxy Server specifc parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the max heapsize for the web app proxy server.  If no units are
[33malgo-2    |[0m # given, it will be assumed to be in MB.
[33malgo-2    |[0m # This value will be overridden by an Xmx setting specified in either
[33malgo-2    |[0m # HADOOP_OPTS and/or YARN_PROXYSERVER_OPTS.
[33malgo-2    |[0m # Default is the same as HADOOP_HEAPSIZE_MAX.
[33malgo-2    |[0m #export YARN_PROXYSERVER_HEAPSIZE=
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the proxy server.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_PROXYSERVER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Shared Cache Manager specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Specify the JVM options to be used when starting the
[33malgo-2    |[0m # shared cache manager server.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_SHAREDCACHEMANAGER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Router specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m 
[33malgo-2    |[0m # Specify the JVM options to be used when starting the Router.
[33malgo-2    |[0m # These options will be appended to the options specified as HADOOP_OPTS
[33malgo-2    |[0m # and therefore may override any similar flags set in HADOOP_OPTS
[33malgo-2    |[0m #
[33malgo-2    |[0m # See ResourceManager for some examples
[33malgo-2    |[0m #
[33malgo-2    |[0m #export YARN_ROUTER_OPTS=
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Registry DNS specific parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # For privileged registry DNS, user to run as after dropping privileges
[33malgo-2    |[0m # This will replace the hadoop.id.str Java property in secure mode.
[33malgo-2    |[0m # export YARN_REGISTRYDNS_SECURE_USER=yarn
[33malgo-2    |[0m 
[33malgo-2    |[0m # Supplemental options for privileged registry DNS
[33malgo-2    |[0m # By default, Hadoop uses jsvc which needs to know to launch a
[33malgo-2    |[0m # server jvm.
[33malgo-2    |[0m # export YARN_REGISTRYDNS_SECURE_EXTRA_OPTS="-jvm server"
[33malgo-2    |[0m 
[33malgo-2    |[0m ###
[33malgo-2    |[0m # YARN Services parameters
[33malgo-2    |[0m ###
[33malgo-2    |[0m # Directory containing service examples
[33malgo-2    |[0m # export YARN_SERVICE_EXAMPLES_DIR = $HADOOP_YARN_HOME/share/hadoop/yarn/yarn-service-examples
[33malgo-2    |[0m # export YARN_CONTAINER_RUNTIME_DOCKER_RUN_OVERRIDE_DISABLE=true
[33malgo-2    |[0m export YARN_LOG_DIR=/var/log/yarn/export HADOOP_DATANODE_HEAPSIZE=2048
[33malgo-2    |[0m export HADOOP_NAMENODE_OPTS=-XX:GCTimeRatio=19
[33malgo-2    |[0m 
[33malgo-2    |[0m 11-14 15:22 smspark-submit INFO     waiting for cluster to be up
[33malgo-2    |[0m WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.
[33malgo-2    |[0m WARNING: /usr/lib/hadoop/logs does not exist. Creating.
[33malgo-2    |[0m WARNING: /var/log/yarn/export does not exist. Creating.
[36malgo-1    |[0m 2021-11-14 15:22:27,799 INFO namenode.NameNode: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting NameNode
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/172.18.0.3
[36malgo-1    |[0m STARTUP_MSG:   args = [-format, -force]
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 15:22:27,811 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
[33malgo-2    |[0m 2021-11-14 15:22:27,850 INFO nodemanager.NodeManager: STARTUP_MSG: 
[33malgo-2    |[0m /************************************************************
[33malgo-2    |[0m STARTUP_MSG: Starting NodeManager
[33malgo-2    |[0m STARTUP_MSG:   host = algo-2/172.18.0.2
[33malgo-2    |[0m STARTUP_MSG:   args = []
[33malgo-2    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[33malgo-2    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar
[33malgo-2    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[33malgo-2    |[0m STARTUP_MSG:   java = 1.8.0_302
[33malgo-2    |[0m ************************************************************/
[33malgo-2    |[0m 2021-11-14 15:22:27,859 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]
[33malgo-2    |[0m 2021-11-14 15:22:27,879 INFO datanode.DataNode: STARTUP_MSG: 
[33malgo-2    |[0m /************************************************************
[33malgo-2    |[0m STARTUP_MSG: Starting DataNode
[33malgo-2    |[0m STARTUP_MSG:   host = algo-2/172.18.0.2
[33malgo-2    |[0m STARTUP_MSG:   args = []
[33malgo-2    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[33malgo-2    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar
[33malgo-2    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[33malgo-2    |[0m STARTUP_MSG:   java = 1.8.0_302
[33malgo-2    |[0m ************************************************************/
[33malgo-2    |[0m 2021-11-14 15:22:27,889 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 15:22:27,889 INFO namenode.NameNode: createNameNode [-format, -force]
[33malgo-2    |[0m 2021-11-14 15:22:28,223 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode
[33malgo-2    |[0m 2021-11-14 15:22:28,240 INFO resourceplugin.ResourcePluginManager: No Resource plugins found from configuration!
[33malgo-2    |[0m 2021-11-14 15:22:28,240 INFO resourceplugin.ResourcePluginManager: Found Resource plugins from configuration: null
[36malgo-1    |[0m Formatting using clusterid: CID-d1e64556-4dba-4d0e-a81a-97c9d3653265
[33malgo-2    |[0m 2021-11-14 15:22:28,307 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.
[33malgo-2    |[0m 2021-11-14 15:22:28,313 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m 2021-11-14 15:22:28,323 INFO namenode.FSEditLog: Edit logging is async:true
[36malgo-1    |[0m 2021-11-14 15:22:28,338 INFO namenode.FSNamesystem: KeyProvider: null
[36malgo-1    |[0m 2021-11-14 15:22:28,339 INFO namenode.FSNamesystem: fsLock is fair: true
[36malgo-1    |[0m 2021-11-14 15:22:28,340 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
[36malgo-1    |[0m 2021-11-14 15:22:28,345 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 15:22:28,345 INFO namenode.FSNamesystem: supergroup          = supergroup
[36malgo-1    |[0m 2021-11-14 15:22:28,345 INFO namenode.FSNamesystem: isPermissionEnabled = true
[36malgo-1    |[0m 2021-11-14 15:22:28,345 INFO namenode.FSNamesystem: HA Enabled: false
[33malgo-2    |[0m 2021-11-14 15:22:28,353 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher
[33malgo-2    |[0m 2021-11-14 15:22:28,354 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher
[33malgo-2    |[0m 2021-11-14 15:22:28,355 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper
[33malgo-2    |[0m 2021-11-14 15:22:28,356 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices
[33malgo-2    |[0m 2021-11-14 15:22:28,356 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl
[33malgo-2    |[0m 2021-11-14 15:22:28,357 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher
[33malgo-2    |[0m 2021-11-14 15:22:28,357 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler
[33malgo-2    |[0m 2021-11-14 15:22:28,359 INFO tracker.NMLogAggregationStatusTracker: the rolling interval seconds for the NodeManager Cached Log aggregation status is 600
[33malgo-2    |[0m 2021-11-14 15:22:28,377 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl
[33malgo-2    |[0m 2021-11-14 15:22:28,377 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager
[33malgo-2    |[0m 2021-11-14 15:22:28,385 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[33malgo-2    |[0m 2021-11-14 15:22:28,385 INFO impl.MetricsSystemImpl: DataNode metrics system started
[36malgo-1    |[0m 2021-11-14 15:22:28,390 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[36malgo-1    |[0m 2021-11-14 15:22:28,403 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
[36malgo-1    |[0m 2021-11-14 15:22:28,403 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
[36malgo-1    |[0m 2021-11-14 15:22:28,407 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
[36malgo-1    |[0m 2021-11-14 15:22:28,408 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Nov 14 15:22:28
[36malgo-1    |[0m 2021-11-14 15:22:28,409 INFO util.GSet: Computing capacity for map BlocksMap
[36malgo-1    |[0m 2021-11-14 15:22:28,409 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 15:22:28,410 INFO util.GSet: 2.0% max memory 6.5 GB = 133.6 MB
[36malgo-1    |[0m 2021-11-14 15:22:28,410 INFO util.GSet: capacity      = 2^24 = 16777216 entries
[33malgo-2    |[0m 2021-11-14 15:22:28,433 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m 2021-11-14 15:22:28,453 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
[36malgo-1    |[0m 2021-11-14 15:22:28,453 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
[36malgo-1    |[0m 2021-11-14 15:22:28,460 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
[36malgo-1    |[0m 2021-11-14 15:22:28,460 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
[36malgo-1    |[0m 2021-11-14 15:22:28,460 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
[36malgo-1    |[0m 2021-11-14 15:22:28,460 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
[36malgo-1    |[0m 2021-11-14 15:22:28,461 INFO blockmanagement.BlockManager: defaultReplication         = 3
[36malgo-1    |[0m 2021-11-14 15:22:28,461 INFO blockmanagement.BlockManager: maxReplication             = 512
[36malgo-1    |[0m 2021-11-14 15:22:28,461 INFO blockmanagement.BlockManager: minReplication             = 1
[36malgo-1    |[0m 2021-11-14 15:22:28,461 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
[36malgo-1    |[0m 2021-11-14 15:22:28,461 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
[36malgo-1    |[0m 2021-11-14 15:22:28,461 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
[36malgo-1    |[0m 2021-11-14 15:22:28,461 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
[36malgo-1    |[0m 2021-11-14 15:22:28,485 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
[36malgo-1    |[0m 2021-11-14 15:22:28,485 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 15:22:28,485 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 15:22:28,485 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 15:22:28,499 INFO util.GSet: Computing capacity for map INodeMap
[36malgo-1    |[0m 2021-11-14 15:22:28,499 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 15:22:28,500 INFO util.GSet: 1.0% max memory 6.5 GB = 66.8 MB
[36malgo-1    |[0m 2021-11-14 15:22:28,500 INFO util.GSet: capacity      = 2^23 = 8388608 entries
[33malgo-2    |[0m 2021-11-14 15:22:28,510 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[33malgo-2    |[0m 2021-11-14 15:22:28,510 INFO impl.MetricsSystemImpl: NodeManager metrics system started
[36malgo-1    |[0m 2021-11-14 15:22:28,519 INFO namenode.FSDirectory: ACLs enabled? false
[36malgo-1    |[0m 2021-11-14 15:22:28,519 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
[36malgo-1    |[0m 2021-11-14 15:22:28,519 INFO namenode.FSDirectory: XAttrs enabled? true
[36malgo-1    |[0m 2021-11-14 15:22:28,519 INFO namenode.NameNode: Caching file names occurring more than 10 times
[36malgo-1    |[0m 2021-11-14 15:22:28,524 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
[36malgo-1    |[0m 2021-11-14 15:22:28,526 INFO snapshot.SnapshotManager: SkipList is disabled
[33malgo-2    |[0m 2021-11-14 15:22:28,527 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.
[36malgo-1    |[0m 2021-11-14 15:22:28,530 INFO util.GSet: Computing capacity for map cachedBlocks
[36malgo-1    |[0m 2021-11-14 15:22:28,530 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 15:22:28,531 INFO util.GSet: 0.25% max memory 6.5 GB = 16.7 MB
[36malgo-1    |[0m 2021-11-14 15:22:28,531 INFO util.GSet: capacity      = 2^21 = 2097152 entries
[33malgo-2    |[0m 2021-11-14 15:22:28,535 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.
[36malgo-1    |[0m 2021-11-14 15:22:28,538 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
[36malgo-1    |[0m 2021-11-14 15:22:28,538 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
[36malgo-1    |[0m 2021-11-14 15:22:28,538 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
[33malgo-2    |[0m 2021-11-14 15:22:28,541 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[36malgo-1    |[0m 2021-11-14 15:22:28,542 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
[36malgo-1    |[0m 2021-11-14 15:22:28,542 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
[36malgo-1    |[0m 2021-11-14 15:22:28,543 INFO util.GSet: Computing capacity for map NameNodeRetryCache
[36malgo-1    |[0m 2021-11-14 15:22:28,543 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 15:22:28,543 INFO util.GSet: 0.029999999329447746% max memory 6.5 GB = 2.0 MB
[36malgo-1    |[0m 2021-11-14 15:22:28,543 INFO util.GSet: capacity      = 2^18 = 262144 entries
[33malgo-2    |[0m 2021-11-14 15:22:28,544 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
[33malgo-2    |[0m 2021-11-14 15:22:28,548 INFO datanode.DataNode: Configured hostname is algo-2
[33malgo-2    |[0m 2021-11-14 15:22:28,549 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[33malgo-2    |[0m 2021-11-14 15:22:28,552 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0
[33malgo-2    |[0m 2021-11-14 15:22:28,565 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@4c60d6e9
[33malgo-2    |[0m 2021-11-14 15:22:28,566 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler
[33malgo-2    |[0m 2021-11-14 15:22:28,568 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService
[33malgo-2    |[0m 2021-11-14 15:22:28,568 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled
[33malgo-2    |[0m 2021-11-14 15:22:28,568 INFO localizer.ResourceLocalizationService: per directory file limit = 8192
[36malgo-1    |[0m 2021-11-14 15:22:28,570 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1297552749-172.18.0.3-1636903348563
[33malgo-2    |[0m 2021-11-14 15:22:28,571 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866
[33malgo-2    |[0m 2021-11-14 15:22:28,573 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s
[33malgo-2    |[0m 2021-11-14 15:22:28,573 INFO datanode.DataNode: Number threads for balancing is 50
[33malgo-2    |[0m 2021-11-14 15:22:28,586 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker
[36malgo-1    |[0m 2021-11-14 15:22:28,587 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.
[33malgo-2    |[0m 2021-11-14 15:22:28,592 INFO resources.ResourceHandlerModule: Using traffic control bandwidth handler
[33malgo-2    |[0m 2021-11-14 15:22:28,594 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@221a3fa4
[33malgo-2    |[0m 2021-11-14 15:22:28,595 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null
[33malgo-2    |[0m 2021-11-14 15:22:28,596 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true
[33malgo-2    |[0m 2021-11-14 15:22:28,596 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true
[33malgo-2    |[0m 2021-11-14 15:22:28,596 INFO monitor.ContainersMonitorImpl: Elastic memory control enabled: false
[33malgo-2    |[0m 2021-11-14 15:22:28,596 INFO monitor.ContainersMonitorImpl: Strict memory control enabled: true
[33malgo-2    |[0m 2021-11-14 15:22:28,596 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true
[33malgo-2    |[0m 2021-11-14 15:22:28,598 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.
[36malgo-1    |[0m 2021-11-14 15:22:28,611 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression
[33malgo-2    |[0m 2021-11-14 15:22:28,613 INFO util.log: Logging initialized @1222ms to org.eclipse.jetty.util.log.Slf4jLog
[33malgo-2    |[0m 2021-11-14 15:22:28,626 INFO conf.Configuration: resource-types.xml not found
[33malgo-2    |[0m 2021-11-14 15:22:28,626 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
[33malgo-2    |[0m 2021-11-14 15:22:28,631 INFO conf.Configuration: node-resources.xml not found
[33malgo-2    |[0m 2021-11-14 15:22:28,631 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.
[33malgo-2    |[0m 2021-11-14 15:22:28,633 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:15892, vCores:4>
[33malgo-2    |[0m 2021-11-14 15:22:28,636 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=15892 virtual-memory=79460 virtual-cores=4
[33malgo-2    |[0m 2021-11-14 15:22:28,674 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[33malgo-2    |[0m 2021-11-14 15:22:28,690 INFO ipc.Server: Starting Socket Reader #1 for port 0
[36malgo-1    |[0m 2021-11-14 15:22:28,699 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .
[36malgo-1    |[0m 2021-11-14 15:22:28,708 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
[36malgo-1    |[0m 2021-11-14 15:22:28,713 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
[36malgo-1    |[0m 2021-11-14 15:22:28,714 INFO namenode.NameNode: SHUTDOWN_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m SHUTDOWN_MSG: Shutting down NameNode at algo-1/172.18.0.3
[36malgo-1    |[0m ************************************************************/
[33malgo-2    |[0m 2021-11-14 15:22:28,746 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[33malgo-2    |[0m 2021-11-14 15:22:28,752 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined
[36malgo-1    |[0m 11-14 15:22 smspark-submit INFO     waiting for cluster to be up
[33malgo-2    |[0m 2021-11-14 15:22:28,758 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[33malgo-2    |[0m 2021-11-14 15:22:28,760 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
[33malgo-2    |[0m 2021-11-14 15:22:28,760 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[33malgo-2    |[0m 2021-11-14 15:22:28,760 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[33malgo-2    |[0m 2021-11-14 15:22:28,784 INFO http.HttpServer2: Jetty bound to port 40477
[33malgo-2    |[0m 2021-11-14 15:22:28,785 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[33malgo-2    |[0m 2021-11-14 15:22:28,807 INFO server.session: DefaultSessionIdManager workerName=node0
[33malgo-2    |[0m 2021-11-14 15:22:28,807 INFO server.session: No SessionScavenger set, using defaults
[33malgo-2    |[0m 2021-11-14 15:22:28,808 INFO server.session: node0 Scavenging every 600000ms
[36malgo-1    |[0m WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.
[36malgo-1    |[0m WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.
[33malgo-2    |[0m 2021-11-14 15:22:28,817 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a3793c7{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}
[33malgo-2    |[0m 2021-11-14 15:22:28,818 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@bb9e6dc{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}
[36malgo-1    |[0m WARNING: HADOOP_NAMENODE_OPTS has been replaced by HDFS_NAMENODE_OPTS. Using value of HADOOP_NAMENODE_OPTS.
[36malgo-1    |[0m WARNING: /var/log/yarn/export does not exist. Creating.
[33malgo-2    |[0m 2021-11-14 15:22:28,877 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server
[33malgo-2    |[0m 2021-11-14 15:22:28,877 INFO ipc.Server: IPC Server Responder: starting
[33malgo-2    |[0m 2021-11-14 15:22:28,877 INFO util.TypeUtil: JVM Runtime does not support Modules
[33malgo-2    |[0m 2021-11-14 15:22:28,877 INFO ipc.Server: IPC Server listener on 0: starting
[33malgo-2    |[0m 2021-11-14 15:22:28,884 INFO security.NMContainerTokenSecretManager: Updating node address : algo-2:34827
[33malgo-2    |[0m 2021-11-14 15:22:28,887 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@338c99c8{datanode,/,file:///usr/lib/hadoop-hdfs/webapps/datanode/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/datanode}
[33malgo-2    |[0m 2021-11-14 15:22:28,889 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 500, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[33malgo-2    |[0m 2021-11-14 15:22:28,890 INFO ipc.Server: Starting Socket Reader #1 for port 8040
[33malgo-2    |[0m 2021-11-14 15:22:28,893 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server
[33malgo-2    |[0m 2021-11-14 15:22:28,893 INFO ipc.Server: IPC Server Responder: starting
[33malgo-2    |[0m 2021-11-14 15:22:28,893 INFO ipc.Server: IPC Server listener on 8040: starting
[33malgo-2    |[0m 2021-11-14 15:22:28,894 INFO localizer.ResourceLocalizationService: Localizer started on port 8040
[33malgo-2    |[0m 2021-11-14 15:22:28,896 INFO server.AbstractConnector: Started ServerConnector@6c372fe6{HTTP/1.1,[http/1.1]}{localhost:40477}
[33malgo-2    |[0m 2021-11-14 15:22:28,896 INFO server.Server: Started @1505ms
[33malgo-2    |[0m 2021-11-14 15:22:28,896 INFO containermanager.ContainerManagerImpl: ContainerManager started at /172.18.0.2:34827
[33malgo-2    |[0m 2021-11-14 15:22:28,897 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-2/172.18.0.2:0
[33malgo-2    |[0m 2021-11-14 15:22:28,897 WARN tracker.NMLogAggregationStatusTracker: Log Aggregation is disabled.So is the LogAggregationStatusTracker.
[33malgo-2    |[0m 2021-11-14 15:22:28,900 INFO webapp.WebServer: Instantiating NMWebApp at algo-2:8042
[33malgo-2    |[0m 2021-11-14 15:22:28,922 INFO util.log: Logging initialized @1530ms to org.eclipse.jetty.util.log.Slf4jLog
[33malgo-2    |[0m 2021-11-14 15:22:29,025 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[33malgo-2    |[0m 2021-11-14 15:22:29,028 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined
[33malgo-2    |[0m 2021-11-14 15:22:29,034 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[33malgo-2    |[0m 2021-11-14 15:22:29,036 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node
[33malgo-2    |[0m 2021-11-14 15:22:29,036 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[33malgo-2    |[0m 2021-11-14 15:22:29,036 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[33malgo-2    |[0m 2021-11-14 15:22:29,037 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context node
[33malgo-2    |[0m 2021-11-14 15:22:29,037 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
[33malgo-2    |[0m 2021-11-14 15:22:29,037 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
[33malgo-2    |[0m 2021-11-14 15:22:29,039 INFO http.HttpServer2: adding path spec: /node/*
[33malgo-2    |[0m 2021-11-14 15:22:29,039 INFO http.HttpServer2: adding path spec: /ws/*
[33malgo-2    |[0m 2021-11-14 15:22:29,044 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864
[33malgo-2    |[0m 2021-11-14 15:22:29,049 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[33malgo-2    |[0m 2021-11-14 15:22:29,050 INFO datanode.DataNode: dnUserName = root
[33malgo-2    |[0m 2021-11-14 15:22:29,050 INFO datanode.DataNode: supergroup = supergroup
[33malgo-2    |[0m 2021-11-14 15:22:29,092 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[33malgo-2    |[0m 2021-11-14 15:22:29,105 INFO ipc.Server: Starting Socket Reader #1 for port 9867
[33malgo-2    |[0m 2021-11-14 15:22:29,336 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:9867
[33malgo-2    |[0m 2021-11-14 15:22:29,356 INFO datanode.DataNode: Refresh request received for nameservices: null
[33malgo-2    |[0m 2021-11-14 15:22:29,367 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>
[33malgo-2    |[0m 2021-11-14 15:22:29,377 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1.spark-network/172.18.0.3:8020 starting to offer service
[33malgo-2    |[0m 2021-11-14 15:22:29,383 INFO ipc.Server: IPC Server Responder: starting
[33malgo-2    |[0m 2021-11-14 15:22:29,383 INFO ipc.Server: IPC Server listener on 9867: starting
[36malgo-1    |[0m 2021-11-14 15:22:29,385 INFO nodemanager.NodeManager: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting NodeManager
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/172.18.0.3
[36malgo-1    |[0m STARTUP_MSG:   args = []
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 15:22:29,397 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]
[33malgo-2    |[0m 2021-11-14 15:22:29,417 INFO webapp.WebApps: Registered webapp guice modules
[33malgo-2    |[0m 2021-11-14 15:22:29,420 INFO http.HttpServer2: Jetty bound to port 8042
[33malgo-2    |[0m 2021-11-14 15:22:29,422 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[36malgo-1    |[0m 2021-11-14 15:22:29,434 INFO namenode.NameNode: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting NameNode
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/172.18.0.3
[36malgo-1    |[0m STARTUP_MSG:   args = []
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 15:22:29,439 INFO datanode.DataNode: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting DataNode
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/172.18.0.3
[36malgo-1    |[0m STARTUP_MSG:   args = []
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 15:22:29,445 INFO resourcemanager.ResourceManager: STARTUP_MSG: 
[36malgo-1    |[0m /************************************************************
[36malgo-1    |[0m STARTUP_MSG: Starting ResourceManager
[36malgo-1    |[0m STARTUP_MSG:   host = algo-1/172.18.0.3
[36malgo-1    |[0m STARTUP_MSG:   args = []
[36malgo-1    |[0m STARTUP_MSG:   version = 3.2.1-amzn-1
[36malgo-1    |[0m STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar
[36malgo-1    |[0m STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z
[36malgo-1    |[0m STARTUP_MSG:   java = 1.8.0_302
[36malgo-1    |[0m ************************************************************/
[36malgo-1    |[0m 2021-11-14 15:22:29,451 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 15:22:29,454 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
[36malgo-1    |[0m 2021-11-14 15:22:29,455 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]
[33malgo-2    |[0m 2021-11-14 15:22:29,457 INFO server.session: DefaultSessionIdManager workerName=node0
[33malgo-2    |[0m 2021-11-14 15:22:29,457 INFO server.session: No SessionScavenger set, using defaults
[33malgo-2    |[0m 2021-11-14 15:22:29,459 INFO server.session: node0 Scavenging every 660000ms
[33malgo-2    |[0m 2021-11-14 15:22:29,469 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[33malgo-2    |[0m 2021-11-14 15:22:29,471 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4bff64c2{logs,/logs,file:///var/log/yarn/export/,AVAILABLE}
[33malgo-2    |[0m 2021-11-14 15:22:29,472 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3cc41abc{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/static,AVAILABLE}
[33malgo-2    |[0m 2021-11-14 15:22:29,483 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException
[36malgo-1    |[0m 2021-11-14 15:22:29,537 INFO namenode.NameNode: createNameNode []
[33malgo-2    |[0m 2021-11-14 15:22:29,601 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m 2021-11-14 15:22:29,703 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[33malgo-2    |[0m Nov 14, 2021 3:22:29 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[33malgo-2    |[0m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class
[33malgo-2    |[0m Nov 14, 2021 3:22:29 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[33malgo-2    |[0m INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
[33malgo-2    |[0m Nov 14, 2021 3:22:29 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[33malgo-2    |[0m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class
[33malgo-2    |[0m Nov 14, 2021 3:22:29 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
[33malgo-2    |[0m INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'
[33malgo-2    |[0m Nov 14, 2021 3:22:29 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[33malgo-2    |[0m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 15:22:29,828 INFO conf.Configuration: found resource core-site.xml at file:/etc/hadoop/conf.empty/core-site.xml
[36malgo-1    |[0m 2021-11-14 15:22:29,836 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m 2021-11-14 15:22:29,837 INFO impl.MetricsSystemImpl: NameNode metrics system started
[36malgo-1    |[0m 2021-11-14 15:22:29,850 INFO resourceplugin.ResourcePluginManager: No Resource plugins found from configuration!
[36malgo-1    |[0m 2021-11-14 15:22:29,850 INFO resourceplugin.ResourcePluginManager: Found Resource plugins from configuration: null
[36malgo-1    |[0m 2021-11-14 15:22:29,863 INFO namenode.NameNodeUtils: fs.defaultFS is hdfs://172.18.0.3/
[36malgo-1    |[0m 2021-11-14 15:22:29,876 INFO conf.Configuration: resource-types.xml not found
[36malgo-1    |[0m 2021-11-14 15:22:29,876 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
[36malgo-1    |[0m 2021-11-14 15:22:29,883 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode
[36malgo-1    |[0m 2021-11-14 15:22:29,925 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.
[36malgo-1    |[0m 2021-11-14 15:22:29,927 INFO conf.Configuration: found resource yarn-site.xml at file:/etc/hadoop/conf.empty/yarn-site.xml
[36malgo-1    |[0m 2021-11-14 15:22:29,946 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher
[33malgo-2    |[0m Nov 14, 2021 3:22:29 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[33malgo-2    |[0m INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 15:22:29,979 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms
[36malgo-1    |[0m 2021-11-14 15:22:29,983 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms
[36malgo-1    |[0m 2021-11-14 15:22:29,987 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher
[36malgo-1    |[0m 2021-11-14 15:22:29,988 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher
[36malgo-1    |[0m 2021-11-14 15:22:29,988 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms
[36malgo-1    |[0m 2021-11-14 15:22:29,989 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper
[36malgo-1    |[0m 2021-11-14 15:22:29,990 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices
[36malgo-1    |[0m 2021-11-14 15:22:29,991 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl
[36malgo-1    |[0m 2021-11-14 15:22:29,991 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m 2021-11-14 15:22:29,991 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher
[36malgo-1    |[0m 2021-11-14 15:22:29,992 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler
[36malgo-1    |[0m 2021-11-14 15:22:29,995 INFO tracker.NMLogAggregationStatusTracker: the rolling interval seconds for the NodeManager Cached Log aggregation status is 600
[36malgo-1    |[0m 2021-11-14 15:22:30,019 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl
[36malgo-1    |[0m 2021-11-14 15:22:30,019 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager
[36malgo-1    |[0m 2021-11-14 15:22:30,035 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler
[36malgo-1    |[0m 2021-11-14 15:22:30,039 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 15:22:30,039 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager
[36malgo-1    |[0m 2021-11-14 15:22:30,040 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
[36malgo-1    |[0m 2021-11-14 15:22:30,077 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:9870
[36malgo-1    |[0m 2021-11-14 15:22:30,081 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m 2021-11-14 15:22:30,083 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m 2021-11-14 15:22:30,083 INFO impl.MetricsSystemImpl: DataNode metrics system started
[36malgo-1    |[0m 2021-11-14 15:22:30,087 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher
[36malgo-1    |[0m 2021-11-14 15:22:30,088 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher
[36malgo-1    |[0m 2021-11-14 15:22:30,090 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher
[36malgo-1    |[0m 2021-11-14 15:22:30,091 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher
[36malgo-1    |[0m 2021-11-14 15:22:30,095 INFO util.log: Logging initialized @1243ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m 2021-11-14 15:22:30,183 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m 2021-11-14 15:22:30,183 INFO impl.MetricsSystemImpl: NodeManager metrics system started
[36malgo-1    |[0m 2021-11-14 15:22:30,183 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m 2021-11-14 15:22:30,206 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.
[36malgo-1    |[0m 2021-11-14 15:22:30,218 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.
[36malgo-1    |[0m 2021-11-14 15:22:30,245 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 15:22:30,260 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined
[36malgo-1    |[0m 2021-11-14 15:22:30,268 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@4c60d6e9
[36malgo-1    |[0m 2021-11-14 15:22:30,269 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[36malgo-1    |[0m 2021-11-14 15:22:30,270 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler
[36malgo-1    |[0m 2021-11-14 15:22:30,272 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService
[36malgo-1    |[0m 2021-11-14 15:22:30,272 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
[36malgo-1    |[0m 2021-11-14 15:22:30,272 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled
[36malgo-1    |[0m 2021-11-14 15:22:30,272 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[36malgo-1    |[0m 2021-11-14 15:22:30,272 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 2021-11-14 15:22:30,272 INFO localizer.ResourceLocalizationService: per directory file limit = 8192
[36malgo-1    |[0m 2021-11-14 15:22:30,295 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m 2021-11-14 15:22:30,296 INFO impl.MetricsSystemImpl: ResourceManager metrics system started
[36malgo-1    |[0m 2021-11-14 15:22:30,306 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
[36malgo-1    |[0m 2021-11-14 15:22:30,307 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
[36malgo-1    |[0m 2021-11-14 15:22:30,323 INFO http.HttpServer2: Jetty bound to port 9870
[36malgo-1    |[0m 2021-11-14 15:22:30,325 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[36malgo-1    |[0m 2021-11-14 15:22:30,330 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker
[36malgo-1    |[0m 2021-11-14 15:22:30,337 INFO resources.ResourceHandlerModule: Using traffic control bandwidth handler
[36malgo-1    |[0m 2021-11-14 15:22:30,341 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@221a3fa4
[36malgo-1    |[0m 2021-11-14 15:22:30,341 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null
[36malgo-1    |[0m 2021-11-14 15:22:30,342 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[36malgo-1    |[0m 2021-11-14 15:22:30,343 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true
[36malgo-1    |[0m 2021-11-14 15:22:30,343 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true
[36malgo-1    |[0m 2021-11-14 15:22:30,343 INFO monitor.ContainersMonitorImpl: Elastic memory control enabled: false
[36malgo-1    |[0m 2021-11-14 15:22:30,343 INFO monitor.ContainersMonitorImpl: Strict memory control enabled: true
[36malgo-1    |[0m 2021-11-14 15:22:30,343 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true
[36malgo-1    |[0m 2021-11-14 15:22:30,344 INFO security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.
[36malgo-1    |[0m 2021-11-14 15:22:30,345 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
[36malgo-1    |[0m 2021-11-14 15:22:30,347 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.
[36malgo-1    |[0m 2021-11-14 15:22:30,351 INFO datanode.DataNode: Configured hostname is algo-1
[36malgo-1    |[0m 2021-11-14 15:22:30,352 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[36malgo-1    |[0m 2021-11-14 15:22:30,352 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager
[36malgo-1    |[0m 2021-11-14 15:22:30,356 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0
[36malgo-1    |[0m 2021-11-14 15:22:30,361 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher
[36malgo-1    |[0m 2021-11-14 15:22:30,364 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean
[36malgo-1    |[0m 2021-11-14 15:22:30,364 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.
[36malgo-1    |[0m 2021-11-14 15:22:30,372 INFO placement.MultiNodeSortingManager: Initializing NodeSortingService=MultiNodeSortingManager
[36malgo-1    |[0m 2021-11-14 15:22:30,374 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list
[33malgo-2    |[0m Nov 14, 2021 3:22:30 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[33malgo-2    |[0m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 15:22:30,385 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866
[36malgo-1    |[0m 2021-11-14 15:22:30,388 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s
[36malgo-1    |[0m 2021-11-14 15:22:30,388 INFO datanode.DataNode: Number threads for balancing is 50
[36malgo-1    |[0m 2021-11-14 15:22:30,389 INFO server.session: DefaultSessionIdManager workerName=node0
[36malgo-1    |[0m 2021-11-14 15:22:30,389 INFO server.session: No SessionScavenger set, using defaults
[36malgo-1    |[0m 2021-11-14 15:22:30,391 INFO server.session: node0 Scavenging every 660000ms
[36malgo-1    |[0m 2021-11-14 15:22:30,392 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/etc/hadoop/conf.empty/capacity-scheduler.xml
[36malgo-1    |[0m 2021-11-14 15:22:30,401 INFO conf.Configuration: resource-types.xml not found
[36malgo-1    |[0m 2021-11-14 15:22:30,401 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
[36malgo-1    |[0m 2021-11-14 15:22:30,407 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6d60fe40{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 15:22:30,407 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@73eb439a{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 15:22:30,410 INFO conf.Configuration: node-resources.xml not found
[36malgo-1    |[0m 2021-11-14 15:22:30,410 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.
[36malgo-1    |[0m 2021-11-14 15:22:30,413 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 15:22:30,416 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1, vCores:1>
[36malgo-1    |[0m 2021-11-14 15:22:30,416 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 15:22:30,419 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=15892 virtual-memory=79460 virtual-cores=4
[33malgo-2    |[0m 2021-11-14 15:22:30,427 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5d5b5fa7{node,/,file:///tmp/jetty-algo-2-8042-_-any-5630430791228920172.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/node}
[36malgo-1    |[0m 2021-11-14 15:22:30,436 INFO util.log: Logging initialized @1592ms to org.eclipse.jetty.util.log.Slf4jLog
[33malgo-2    |[0m 2021-11-14 15:22:30,438 INFO server.AbstractConnector: Started ServerConnector@3f390d63{HTTP/1.1,[http/1.1]}{algo-2:8042}
[33malgo-2    |[0m 2021-11-14 15:22:30,438 INFO server.Server: Started @3045ms
[33malgo-2    |[0m 2021-11-14 15:22:30,438 INFO webapp.WebApps: Web app node started at 8042
[33malgo-2    |[0m 2021-11-14 15:22:30,439 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-2:34827
[33malgo-2    |[0m 2021-11-14 15:22:30,440 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[33malgo-2    |[0m 2021-11-14 15:22:30,447 INFO client.RMProxy: Connecting to ResourceManager at /172.18.0.3:8031
[36malgo-1    |[0m 2021-11-14 15:22:30,462 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined
[36malgo-1    |[0m 2021-11-14 15:22:30,462 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined
[33malgo-2    |[0m 2021-11-14 15:22:30,463 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/172.18.0.3:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 15:22:30,476 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=ADMINISTER_QUEUE:*SUBMIT_APP:*, labels=*,
[36malgo-1    |[0m , reservationsContinueLooking=true, orderingPolicy=utilization, priority=0
[36malgo-1    |[0m 2021-11-14 15:22:30,477 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root
[36malgo-1    |[0m 2021-11-14 15:22:30,478 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 15:22:30,481 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m 2021-11-14 15:22:30,495 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4dd6fd0a{hdfs,/,file:///usr/lib/hadoop-hdfs/webapps/hdfs/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/hdfs}
[36malgo-1    |[0m 2021-11-14 15:22:30,497 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined
[36malgo-1    |[0m 2021-11-14 15:22:30,497 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined
[36malgo-1    |[0m 2021-11-14 15:22:30,499 INFO ipc.Server: Starting Socket Reader #1 for port 0
[36malgo-1    |[0m 2021-11-14 15:22:30,501 INFO capacity.LeafQueue: Initializing default
[36malgo-1    |[0m capacity = 1.0 [= (float) configuredCapacity / 100 ]
[36malgo-1    |[0m absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]
[36malgo-1    |[0m maxCapacity = 1.0 [= configuredMaxCapacity ]
[36malgo-1    |[0m absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]
[36malgo-1    |[0m effectiveMinResource=<memory:0, vCores:0>
[36malgo-1    |[0m  , effectiveMaxResource=<memory:0, vCores:0>
[36malgo-1    |[0m userLimit = 100 [= configuredUserLimit ]
[36malgo-1    |[0m userLimitFactor = 1.0 [= configuredUserLimitFactor ]
[36malgo-1    |[0m maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]
[36malgo-1    |[0m maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]
[36malgo-1    |[0m usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]
[36malgo-1    |[0m absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]
[36malgo-1    |[0m maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]
[36malgo-1    |[0m minimumAllocationFactor = 0.99993706 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]
[36malgo-1    |[0m maximumAllocation = <memory:15892, vCores:4> [= configuredMaxAllocation ]
[36malgo-1    |[0m numContainers = 0 [= currentNumContainers ]
[36malgo-1    |[0m state = RUNNING [= configuredState ]
[36malgo-1    |[0m acls = ADMINISTER_QUEUE:*SUBMIT_APP:* [= configuredAcls ]
[36malgo-1    |[0m nodeLocalityDelay = 40
[36malgo-1    |[0m rackLocalityAdditionalDelay = -1
[36malgo-1    |[0m labels=*,
[36malgo-1    |[0m reservationsContinueLooking = true
[36malgo-1    |[0m preemptionDisabled = true
[36malgo-1    |[0m defaultAppPriorityPerQueue = 0
[36malgo-1    |[0m priority = 0
[36malgo-1    |[0m maxLifetime = -1 seconds
[36malgo-1    |[0m defaultLifetime = -1 seconds
[36malgo-1    |[0m 2021-11-14 15:22:30,501 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>
[36malgo-1    |[0m 2021-11-14 15:22:30,502 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
[33malgo-2    |[0m 2021-11-14 15:22:30,504 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []
[36malgo-1    |[0m 2021-11-14 15:22:30,504 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
[36malgo-1    |[0m 2021-11-14 15:22:30,505 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false
[36malgo-1    |[0m 2021-11-14 15:22:30,506 INFO placement.MultiNodeSortingManager: MultiNode scheduling is 'false', and configured policies are 
[36malgo-1    |[0m 2021-11-14 15:22:30,506 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1, vCores:1>>, maximumAllocation=<<memory:15892, vCores:4>>, asynchronousScheduling=false, asyncScheduleInterval=5ms,multiNodePlacementEnabled=false
[36malgo-1    |[0m 2021-11-14 15:22:30,510 INFO conf.Configuration: dynamic-resources.xml not found
[36malgo-1    |[0m 2021-11-14 15:22:30,512 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].
[36malgo-1    |[0m 2021-11-14 15:22:30,512 INFO resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.
[36malgo-1    |[0m 2021-11-14 15:22:30,514 INFO resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. 
[33malgo-2    |[0m 2021-11-14 15:22:30,516 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]
[36malgo-1    |[0m 2021-11-14 15:22:30,521 INFO resourcemanager.ResourceManager: TimelineServicePublisher is not configured
[36malgo-1    |[0m 2021-11-14 15:22:30,556 INFO server.AbstractConnector: Started ServerConnector@4d14b6c2{HTTP/1.1,[http/1.1]}{0.0.0.0:9870}
[36malgo-1    |[0m 2021-11-14 15:22:30,557 INFO server.Server: Started @1705ms
[36malgo-1    |[0m 2021-11-14 15:22:30,574 INFO util.log: Logging initialized @1720ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m 2021-11-14 15:22:30,710 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 15:22:30,715 INFO http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined
[36malgo-1    |[0m 2021-11-14 15:22:30,722 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[36malgo-1    |[0m 2021-11-14 15:22:30,723 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 15:22:30,725 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster
[36malgo-1    |[0m 2021-11-14 15:22:30,725 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs
[36malgo-1    |[0m 2021-11-14 15:22:30,725 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static
[36malgo-1    |[0m 2021-11-14 15:22:30,725 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster
[36malgo-1    |[0m 2021-11-14 15:22:30,725 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[36malgo-1    |[0m 2021-11-14 15:22:30,725 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 2021-11-14 15:22:30,727 INFO http.HttpServer2: adding path spec: /cluster/*
[36malgo-1    |[0m 2021-11-14 15:22:30,727 INFO http.HttpServer2: adding path spec: /ws/*
[36malgo-1    |[0m 2021-11-14 15:22:30,727 INFO http.HttpServer2: adding path spec: /app/*
[36malgo-1    |[0m 2021-11-14 15:22:30,730 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined
[36malgo-1    |[0m 2021-11-14 15:22:30,738 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[36malgo-1    |[0m 2021-11-14 15:22:30,739 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
[36malgo-1    |[0m 2021-11-14 15:22:30,740 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[36malgo-1    |[0m 2021-11-14 15:22:30,740 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 2021-11-14 15:22:30,768 INFO http.HttpServer2: Jetty bound to port 35357
[36malgo-1    |[0m 2021-11-14 15:22:30,770 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[36malgo-1    |[0m 2021-11-14 15:22:30,781 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 15:22:30,781 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 15:22:30,781 INFO ipc.Server: IPC Server listener on 0: starting
[36malgo-1    |[0m 2021-11-14 15:22:30,791 INFO security.NMContainerTokenSecretManager: Updating node address : algo-1:35251
[36malgo-1    |[0m 2021-11-14 15:22:30,799 INFO server.session: DefaultSessionIdManager workerName=node0
[36malgo-1    |[0m 2021-11-14 15:22:30,799 INFO server.session: No SessionScavenger set, using defaults
[36malgo-1    |[0m 2021-11-14 15:22:30,799 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 500, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 15:22:30,800 INFO ipc.Server: Starting Socket Reader #1 for port 8040
[36malgo-1    |[0m 2021-11-14 15:22:30,801 INFO server.session: node0 Scavenging every 660000ms
[36malgo-1    |[0m 2021-11-14 15:22:30,806 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 15:22:30,806 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 15:22:30,806 INFO ipc.Server: IPC Server listener on 8040: starting
[36malgo-1    |[0m 2021-11-14 15:22:30,807 INFO localizer.ResourceLocalizationService: Localizer started on port 8040
[33malgo-2    |[0m 11-14 15:22 smspark-submit INFO     cluster is up
[33malgo-2    |[0m 11-14 15:22 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING
[33malgo-2    |[0m 11-14 15:22 smspark-submit INFO     starting executor logs watcher
[33malgo-2    |[0m 11-14 15:22 smspark-submit INFO     waiting for the primary to come up
[33malgo-2    |[0m Starting executor logs watcher on log_dir: /var/log/yarn
[36malgo-1    |[0m 2021-11-14 15:22:30,810 INFO containermanager.ContainerManagerImpl: ContainerManager started at /172.18.0.3:35251
[36malgo-1    |[0m 2021-11-14 15:22:30,810 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-1/172.18.0.3:0
[36malgo-1    |[0m 2021-11-14 15:22:30,810 WARN tracker.NMLogAggregationStatusTracker: Log Aggregation is disabled.So is the LogAggregationStatusTracker.
[36malgo-1    |[0m 2021-11-14 15:22:30,814 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a3793c7{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 15:22:30,814 INFO webapp.WebServer: Instantiating NMWebApp at algo-1:8042
[36malgo-1    |[0m 2021-11-14 15:22:30,815 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@bb9e6dc{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}
[33malgo-2    |[0m 11-14 15:22 smspark-submit INFO     waiting for the primary to go down
[36malgo-1    |[0m 2021-11-14 15:22:30,847 INFO util.log: Logging initialized @1995ms to org.eclipse.jetty.util.log.Slf4jLog
[36malgo-1    |[0m 2021-11-14 15:22:30,878 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m 2021-11-14 15:22:30,888 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@338c99c8{datanode,/,file:///usr/lib/hadoop-hdfs/webapps/datanode/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/datanode}
[36malgo-1    |[0m 2021-11-14 15:22:30,888 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
[36malgo-1    |[0m 2021-11-14 15:22:30,888 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
[36malgo-1    |[0m 2021-11-14 15:22:30,896 INFO server.AbstractConnector: Started ServerConnector@6c372fe6{HTTP/1.1,[http/1.1]}{localhost:35357}
[36malgo-1    |[0m 2021-11-14 15:22:30,896 INFO server.Server: Started @2052ms
[36malgo-1    |[0m 2021-11-14 15:22:30,949 INFO namenode.FSEditLog: Edit logging is async:true
[36malgo-1    |[0m 2021-11-14 15:22:30,964 INFO namenode.FSNamesystem: KeyProvider: null
[36malgo-1    |[0m 2021-11-14 15:22:30,966 INFO namenode.FSNamesystem: fsLock is fair: true
[36malgo-1    |[0m 2021-11-14 15:22:30,966 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
[36malgo-1    |[0m 2021-11-14 15:22:30,966 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 15:22:30,970 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined
[36malgo-1    |[0m 2021-11-14 15:22:30,973 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 15:22:30,973 INFO namenode.FSNamesystem: supergroup          = supergroup
[36malgo-1    |[0m 2021-11-14 15:22:30,973 INFO namenode.FSNamesystem: isPermissionEnabled = true
[36malgo-1    |[0m 2021-11-14 15:22:30,973 INFO namenode.FSNamesystem: HA Enabled: false
[36malgo-1    |[0m 2021-11-14 15:22:30,978 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[36malgo-1    |[0m 2021-11-14 15:22:30,979 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node
[36malgo-1    |[0m 2021-11-14 15:22:30,979 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36malgo-1    |[0m 2021-11-14 15:22:30,979 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[36malgo-1    |[0m 2021-11-14 15:22:30,980 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context node
[36malgo-1    |[0m 2021-11-14 15:22:30,980 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
[36malgo-1    |[0m 2021-11-14 15:22:30,980 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
[36malgo-1    |[0m 2021-11-14 15:22:30,982 INFO http.HttpServer2: adding path spec: /node/*
[36malgo-1    |[0m 2021-11-14 15:22:30,982 INFO http.HttpServer2: adding path spec: /ws/*
[36malgo-1    |[0m 2021-11-14 15:22:31,015 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
[36malgo-1    |[0m 2021-11-14 15:22:31,024 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
[36malgo-1    |[0m 2021-11-14 15:22:31,024 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
[36malgo-1    |[0m 2021-11-14 15:22:31,027 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
[36malgo-1    |[0m 2021-11-14 15:22:31,028 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Nov 14 15:22:31
[36malgo-1    |[0m 2021-11-14 15:22:31,029 INFO util.GSet: Computing capacity for map BlocksMap
[36malgo-1    |[0m 2021-11-14 15:22:31,029 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 15:22:31,031 INFO util.GSet: 2.0% max memory 6.5 GB = 133.6 MB
[36malgo-1    |[0m 2021-11-14 15:22:31,031 INFO util.GSet: capacity      = 2^24 = 16777216 entries
[36malgo-1    |[0m 2021-11-14 15:22:31,039 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864
[36malgo-1    |[0m 2021-11-14 15:22:31,044 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 15:22:31,045 INFO datanode.DataNode: dnUserName = root
[36malgo-1    |[0m 2021-11-14 15:22:31,045 INFO datanode.DataNode: supergroup = supergroup
[36malgo-1    |[0m 2021-11-14 15:22:31,051 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
[36malgo-1    |[0m 2021-11-14 15:22:31,051 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
[36malgo-1    |[0m 2021-11-14 15:22:31,057 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
[36malgo-1    |[0m 2021-11-14 15:22:31,057 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
[36malgo-1    |[0m 2021-11-14 15:22:31,057 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
[36malgo-1    |[0m 2021-11-14 15:22:31,057 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
[36malgo-1    |[0m 2021-11-14 15:22:31,058 INFO blockmanagement.BlockManager: defaultReplication         = 3
[36malgo-1    |[0m 2021-11-14 15:22:31,058 INFO blockmanagement.BlockManager: maxReplication             = 512
[36malgo-1    |[0m 2021-11-14 15:22:31,058 INFO blockmanagement.BlockManager: minReplication             = 1
[36malgo-1    |[0m 2021-11-14 15:22:31,058 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
[36malgo-1    |[0m 2021-11-14 15:22:31,058 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
[36malgo-1    |[0m 2021-11-14 15:22:31,058 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
[36malgo-1    |[0m 2021-11-14 15:22:31,058 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
[36malgo-1    |[0m 2021-11-14 15:22:31,080 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
[36malgo-1    |[0m 2021-11-14 15:22:31,080 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 15:22:31,080 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 15:22:31,080 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
[36malgo-1    |[0m 2021-11-14 15:22:31,081 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 15:22:31,094 INFO ipc.Server: Starting Socket Reader #1 for port 9867
[36malgo-1    |[0m 2021-11-14 15:22:31,094 INFO util.GSet: Computing capacity for map INodeMap
[36malgo-1    |[0m 2021-11-14 15:22:31,094 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 15:22:31,095 INFO util.GSet: 1.0% max memory 6.5 GB = 66.8 MB
[36malgo-1    |[0m 2021-11-14 15:22:31,095 INFO util.GSet: capacity      = 2^23 = 8388608 entries
[36malgo-1    |[0m 2021-11-14 15:22:31,232 INFO namenode.FSDirectory: ACLs enabled? false
[36malgo-1    |[0m 2021-11-14 15:22:31,232 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
[36malgo-1    |[0m 2021-11-14 15:22:31,232 INFO namenode.FSDirectory: XAttrs enabled? true
[36malgo-1    |[0m 2021-11-14 15:22:31,232 INFO namenode.NameNode: Caching file names occurring more than 10 times
[36malgo-1    |[0m 2021-11-14 15:22:31,238 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
[36malgo-1    |[0m 2021-11-14 15:22:31,240 INFO snapshot.SnapshotManager: SkipList is disabled
[36malgo-1    |[0m 2021-11-14 15:22:31,246 INFO util.GSet: Computing capacity for map cachedBlocks
[36malgo-1    |[0m 2021-11-14 15:22:31,246 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 15:22:31,246 INFO util.GSet: 0.25% max memory 6.5 GB = 16.7 MB
[36malgo-1    |[0m 2021-11-14 15:22:31,246 INFO util.GSet: capacity      = 2^21 = 2097152 entries
[36malgo-1    |[0m 2021-11-14 15:22:31,256 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
[36malgo-1    |[0m 2021-11-14 15:22:31,256 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
[36malgo-1    |[0m 2021-11-14 15:22:31,256 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
[36malgo-1    |[0m 2021-11-14 15:22:31,260 INFO webapp.WebApps: Registered webapp guice modules
[36malgo-1    |[0m 2021-11-14 15:22:31,260 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
[36malgo-1    |[0m 2021-11-14 15:22:31,260 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
[36malgo-1    |[0m 2021-11-14 15:22:31,262 INFO util.GSet: Computing capacity for map NameNodeRetryCache
[36malgo-1    |[0m 2021-11-14 15:22:31,262 INFO util.GSet: VM type       = 64-bit
[36malgo-1    |[0m 2021-11-14 15:22:31,263 INFO util.GSet: 0.029999999329447746% max memory 6.5 GB = 2.0 MB
[36malgo-1    |[0m 2021-11-14 15:22:31,263 INFO util.GSet: capacity      = 2^18 = 262144 entries
[36malgo-1    |[0m 2021-11-14 15:22:31,267 INFO http.HttpServer2: Jetty bound to port 8088
[36malgo-1    |[0m 2021-11-14 15:22:31,268 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[36malgo-1    |[0m 2021-11-14 15:22:31,283 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/namenode/in_use.lock acquired by nodename 134@algo-1
[36malgo-1    |[0m 2021-11-14 15:22:31,294 INFO server.session: DefaultSessionIdManager workerName=node0
[36malgo-1    |[0m 2021-11-14 15:22:31,294 INFO server.session: No SessionScavenger set, using defaults
[36malgo-1    |[0m 2021-11-14 15:22:31,296 INFO server.session: node0 Scavenging every 600000ms
[36malgo-1    |[0m 2021-11-14 15:22:31,305 INFO namenode.FileJournalManager: Recovering unfinalized segments in /opt/amazon/hadoop/hdfs/namenode/current
[36malgo-1    |[0m 2021-11-14 15:22:31,305 INFO namenode.FSImage: No edit log streams selected.
[36malgo-1    |[0m 2021-11-14 15:22:31,305 INFO namenode.FSImage: Planning to load image: FSImageFile(file=/opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
[36malgo-1    |[0m 2021-11-14 15:22:31,307 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 15:22:31,314 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
[36malgo-1    |[0m 2021-11-14 15:22:31,316 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
[36malgo-1    |[0m 2021-11-14 15:22:31,316 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
[36malgo-1    |[0m 2021-11-14 15:22:31,335 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5a9f4771{logs,/logs,file:///var/log/yarn/export/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 15:22:31,335 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@68d279ec{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/static,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 15:22:31,345 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException
[36malgo-1    |[0m 2021-11-14 15:22:31,347 INFO webapp.WebApps: Registered webapp guice modules
[36malgo-1    |[0m 2021-11-14 15:22:31,350 INFO http.HttpServer2: Jetty bound to port 8042
[36malgo-1    |[0m 2021-11-14 15:22:31,351 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_302-b08
[36malgo-1    |[0m 2021-11-14 15:22:31,355 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:9867
[36malgo-1    |[0m 2021-11-14 15:22:31,364 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.
[36malgo-1    |[0m 2021-11-14 15:22:31,375 INFO datanode.DataNode: Refresh request received for nameservices: null
[36malgo-1    |[0m 2021-11-14 15:22:31,381 INFO server.session: DefaultSessionIdManager workerName=node0
[36malgo-1    |[0m 2021-11-14 15:22:31,381 INFO server.session: No SessionScavenger set, using defaults
[36malgo-1    |[0m 2021-11-14 15:22:31,382 INFO server.session: node0 Scavenging every 600000ms
[36malgo-1    |[0m 2021-11-14 15:22:31,383 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>
[36malgo-1    |[0m 2021-11-14 15:22:31,392 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
[36malgo-1    |[0m 2021-11-14 15:22:31,392 INFO namenode.FSImage: Loaded image for txid 0 from /opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000
[36malgo-1    |[0m 2021-11-14 15:22:31,393 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1/172.18.0.3:8020 starting to offer service
[36malgo-1    |[0m 2021-11-14 15:22:31,394 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36malgo-1    |[0m 2021-11-14 15:22:31,397 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
[36malgo-1    |[0m 2021-11-14 15:22:31,397 INFO namenode.FSEditLog: Starting log segment at 1
[36malgo-1    |[0m 2021-11-14 15:22:31,397 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4bff64c2{logs,/logs,file:///var/log/yarn/export/,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 15:22:31,398 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 15:22:31,398 INFO ipc.Server: IPC Server listener on 9867: starting
[36malgo-1    |[0m 2021-11-14 15:22:31,398 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3cc41abc{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/static,AVAILABLE}
[36malgo-1    |[0m 2021-11-14 15:22:31,411 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException
[36malgo-1    |[0m 2021-11-14 15:22:31,438 INFO util.TypeUtil: JVM Runtime does not support Modules
[33malgo-2    |[0m 2021-11-14 15:22:31,463 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/172.18.0.3:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 15:22:31,507 INFO namenode.NameCache: initialized with 0 entries 0 lookups
[36malgo-1    |[0m 2021-11-14 15:22:31,507 INFO namenode.FSNamesystem: Finished loading FSImage in 241 msecs
[36malgo-1    |[0m 2021-11-14 15:22:31,508 INFO util.TypeUtil: JVM Runtime does not support Modules
[36malgo-1    |[0m Nov 14, 2021 3:22:31 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class
[36malgo-1    |[0m Nov 14, 2021 3:22:31 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class
[36malgo-1    |[0m Nov 14, 2021 3:22:31 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
[36malgo-1    |[0m Nov 14, 2021 3:22:31 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
[36malgo-1    |[0m INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'
[36malgo-1    |[0m Nov 14, 2021 3:22:31 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m Nov 14, 2021 3:22:31 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class
[36malgo-1    |[0m Nov 14, 2021 3:22:31 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
[36malgo-1    |[0m Nov 14, 2021 3:22:31 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
[36malgo-1    |[0m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class
[36malgo-1    |[0m Nov 14, 2021 3:22:31 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
[36malgo-1    |[0m INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'
[33malgo-2    |[0m 2021-11-14 15:22:31,631 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/172.18.0.3:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m Nov 14, 2021 3:22:31 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 15:22:31,732 INFO namenode.NameNode: RPC server is binding to algo-1:8020
[36malgo-1    |[0m 2021-11-14 15:22:31,764 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 15:22:31,774 INFO ipc.Server: Starting Socket Reader #1 for port 8020
[36malgo-1    |[0m Nov 14, 2021 3:22:31 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m Nov 14, 2021 3:22:31 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 15:22:31,947 INFO namenode.NameNode: Clients are to use algo-1:8020 to access this namenode/service.
[36malgo-1    |[0m 2021-11-14 15:22:31,950 INFO namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
[36malgo-1    |[0m 2021-11-14 15:22:31,958 INFO namenode.LeaseManager: Number of blocks under construction: 0
[36malgo-1    |[0m 2021-11-14 15:22:31,968 INFO blockmanagement.BlockManager: initializing replication queues
[36malgo-1    |[0m 2021-11-14 15:22:31,968 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs
[36malgo-1    |[0m 2021-11-14 15:22:31,968 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
[36malgo-1    |[0m 2021-11-14 15:22:31,968 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
[36malgo-1    |[0m 2021-11-14 15:22:31,985 INFO blockmanagement.BlockManager: Total number of blocks            = 0
[36malgo-1    |[0m 2021-11-14 15:22:31,985 INFO blockmanagement.BlockManager: Number of invalid blocks          = 0
[36malgo-1    |[0m 2021-11-14 15:22:31,985 INFO blockmanagement.BlockManager: Number of under-replicated blocks = 0
[36malgo-1    |[0m 2021-11-14 15:22:31,985 INFO blockmanagement.BlockManager: Number of  over-replicated blocks = 0
[36malgo-1    |[0m 2021-11-14 15:22:31,985 INFO blockmanagement.BlockManager: Number of blocks being written    = 0
[36malgo-1    |[0m 2021-11-14 15:22:31,985 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 17 msec
[36malgo-1    |[0m 2021-11-14 15:22:31,991 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 15:22:31,991 INFO ipc.Server: IPC Server listener on 8020: starting
[36malgo-1    |[0m 2021-11-14 15:22:31,993 INFO namenode.NameNode: NameNode RPC up at: algo-1/172.18.0.3:8020
[36malgo-1    |[0m 2021-11-14 15:22:31,996 INFO namenode.FSNamesystem: Starting services required for active state
[36malgo-1    |[0m 2021-11-14 15:22:31,996 INFO namenode.FSDirectory: Initializing quota with 4 thread(s)
[36malgo-1    |[0m 2021-11-14 15:22:32,010 INFO namenode.FSDirectory: Quota initialization completed in 14 milliseconds
[36malgo-1    |[0m name space=1
[36malgo-1    |[0m storage space=0
[36malgo-1    |[0m storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
[36malgo-1    |[0m 2021-11-14 15:22:32,016 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
[36malgo-1    |[0m Nov 14, 2021 3:22:32 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 15:22:32,334 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5d5b5fa7{node,/,file:///tmp/jetty-algo-1-8042-_-any-2441240717407572113.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/node}
[36malgo-1    |[0m 2021-11-14 15:22:32,385 INFO server.AbstractConnector: Started ServerConnector@3f390d63{HTTP/1.1,[http/1.1]}{algo-1:8042}
[36malgo-1    |[0m 2021-11-14 15:22:32,386 INFO server.Server: Started @3533ms
[36malgo-1    |[0m 2021-11-14 15:22:32,386 INFO webapp.WebApps: Web app node started at 8042
[36malgo-1    |[0m 2021-11-14 15:22:32,390 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-1:35251
[36malgo-1    |[0m 2021-11-14 15:22:32,392 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 15:22:32,399 INFO client.RMProxy: Connecting to ResourceManager at /172.18.0.3:8031
[33malgo-2    |[0m 2021-11-14 15:22:32,464 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/172.18.0.3:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 15:22:32,466 INFO ipc.Client: Retrying connect to server: algo-1/172.18.0.3:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m Nov 14, 2021 3:22:32 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
[36malgo-1    |[0m INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope "Singleton"
[36malgo-1    |[0m 2021-11-14 15:22:32,469 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []
[36malgo-1    |[0m 2021-11-14 15:22:32,508 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4efc25fc{cluster,/,file:///tmp/jetty-172_18_0_3-8088-_-any-2315309268842665944.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/cluster}
[36malgo-1    |[0m 2021-11-14 15:22:32,514 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]
[36malgo-1    |[0m 2021-11-14 15:22:32,521 INFO server.AbstractConnector: Started ServerConnector@73d983ea{HTTP/1.1,[http/1.1]}{172.18.0.3:8088}
[36malgo-1    |[0m 2021-11-14 15:22:32,521 INFO server.Server: Started @3667ms
[36malgo-1    |[0m 2021-11-14 15:22:32,522 INFO webapp.WebApps: Web app cluster started at 8088
[36malgo-1    |[0m 2021-11-14 15:22:32,600 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 15:22:32,612 INFO ipc.Server: Starting Socket Reader #1 for port 8033
[33malgo-2    |[0m 2021-11-14 15:22:32,632 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/172.18.0.3:8031. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 15:22:32,798 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 15:22:32,798 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 15:22:32,798 INFO ipc.Server: IPC Server listener on 8033: starting
[36malgo-1    |[0m 2021-11-14 15:22:32,799 INFO resourcemanager.ResourceManager: Transitioning to active state
[36malgo-1    |[0m 11-14 15:22 smspark-submit INFO     cluster is up
[36malgo-1    |[0m 11-14 15:22 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING
[36malgo-1    |[0m 11-14 15:22 smspark-submit INFO     starting executor logs watcher
[36malgo-1    |[0m 11-14 15:22 smspark-submit INFO     start log event log publisher
[36malgo-1    |[0m Starting executor logs watcher on log_dir: /var/log/yarn
[36malgo-1    |[0m 11-14 15:22 sagemaker-spark-event-logs-publisher INFO     Spark event log not enabled.
[36malgo-1    |[0m 11-14 15:22 smspark-submit INFO     Waiting for hosts to bootstrap: ['algo-1', 'algo-2']
[36malgo-1    |[0m 11-14 15:22 smspark-submit INFO     Received host statuses: dict_items([('algo-1', StatusMessage(status='WAITING', timestamp='2021-11-14T15:22:32.811986')), ('algo-2', StatusMessage(status='WAITING', timestamp='2021-11-14T15:22:32.818404'))])
[36malgo-1    |[0m 2021-11-14 15:22:32,834 INFO recovery.RMStateStore: Updating AMRMToken
[36malgo-1    |[0m 2021-11-14 15:22:32,835 INFO security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
[36malgo-1    |[0m 2021-11-14 15:22:32,835 INFO security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
[36malgo-1    |[0m 2021-11-14 15:22:32,835 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
[36malgo-1    |[0m 2021-11-14 15:22:32,835 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 1
[36malgo-1    |[0m 2021-11-14 15:22:32,835 INFO recovery.RMStateStore: Storing RMDTMasterKey.
[36malgo-1    |[0m 2021-11-14 15:22:32,836 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
[36malgo-1    |[0m 2021-11-14 15:22:32,836 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
[36malgo-1    |[0m 2021-11-14 15:22:32,836 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 2
[36malgo-1    |[0m 2021-11-14 15:22:32,836 INFO recovery.RMStateStore: Storing RMDTMasterKey.
[36malgo-1    |[0m 2021-11-14 15:22:32,838 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler
[36malgo-1    |[0m 2021-11-14 15:22:32,852 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1/172.18.0.3:8020
[36malgo-1    |[0m 2021-11-14 15:22:32,854 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
[33malgo-2    |[0m 2021-11-14 15:22:32,857 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1.spark-network/172.18.0.3:8020
[33malgo-2    |[0m 2021-11-14 15:22:32,858 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
[36malgo-1    |[0m 2021-11-14 15:22:32,861 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 135@algo-1
[36malgo-1    |[0m 2021-11-14 15:22:32,862 INFO common.Storage: Storage directory with location [DISK]file:/opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 489834641. Formatting...
[36malgo-1    |[0m 2021-11-14 15:22:32,863 INFO common.Storage: Generated new storageID DS-d4dc21a0-eed3-401e-83f6-79dcb4726fca for directory /opt/amazon/hadoop/hdfs/datanode 
[33malgo-2    |[0m 2021-11-14 15:22:32,865 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 18@algo-2
[33malgo-2    |[0m 2021-11-14 15:22:32,866 INFO common.Storage: Storage directory with location [DISK]file:/opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 489834641. Formatting...
[33malgo-2    |[0m 2021-11-14 15:22:32,866 INFO common.Storage: Generated new storageID DS-8652abdf-247c-447f-b32e-97f9abf902d5 for directory /opt/amazon/hadoop/hdfs/datanode 
[33malgo-2    |[0m 2021-11-14 15:22:32,892 INFO common.Storage: Analyzing storage directories for bpid BP-1297552749-172.18.0.3-1636903348563
[33malgo-2    |[0m 2021-11-14 15:22:32,892 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-1297552749-172.18.0.3-1636903348563
[33malgo-2    |[0m 2021-11-14 15:22:32,893 INFO common.Storage: Block pool storage directory for location [DISK]file:/opt/amazon/hadoop/hdfs/datanode and block pool id BP-1297552749-172.18.0.3-1636903348563 is not formatted. Formatting ...
[33malgo-2    |[0m 2021-11-14 15:22:32,893 INFO common.Storage: Formatting block pool BP-1297552749-172.18.0.3-1636903348563 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-1297552749-172.18.0.3-1636903348563/current
[33malgo-2    |[0m 2021-11-14 15:22:32,902 INFO datanode.DataNode: Setting up storage: nsid=489834641;bpid=BP-1297552749-172.18.0.3-1636903348563;lv=-57;nsInfo=lv=-65;cid=CID-d1e64556-4dba-4d0e-a81a-97c9d3653265;nsid=489834641;c=1636903348563;bpid=BP-1297552749-172.18.0.3-1636903348563;dnuuid=null
[36malgo-1    |[0m 2021-11-14 15:22:32,905 INFO common.Storage: Analyzing storage directories for bpid BP-1297552749-172.18.0.3-1636903348563
[36malgo-1    |[0m 2021-11-14 15:22:32,905 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-1297552749-172.18.0.3-1636903348563
[33malgo-2    |[0m 2021-11-14 15:22:32,905 INFO datanode.DataNode: Generated and persisted new Datanode UUID 4d2edd13-f33b-493b-92f7-3a6154df0d91
[36malgo-1    |[0m 2021-11-14 15:22:32,905 INFO common.Storage: Block pool storage directory for location [DISK]file:/opt/amazon/hadoop/hdfs/datanode and block pool id BP-1297552749-172.18.0.3-1636903348563 is not formatted. Formatting ...
[36malgo-1    |[0m 2021-11-14 15:22:32,905 INFO common.Storage: Formatting block pool BP-1297552749-172.18.0.3-1636903348563 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-1297552749-172.18.0.3-1636903348563/current
[36malgo-1    |[0m 2021-11-14 15:22:32,911 INFO datanode.DataNode: Setting up storage: nsid=489834641;bpid=BP-1297552749-172.18.0.3-1636903348563;lv=-57;nsInfo=lv=-65;cid=CID-d1e64556-4dba-4d0e-a81a-97c9d3653265;nsid=489834641;c=1636903348563;bpid=BP-1297552749-172.18.0.3-1636903348563;dnuuid=null
[36malgo-1    |[0m 2021-11-14 15:22:32,913 INFO datanode.DataNode: Generated and persisted new Datanode UUID 01942069-d63b-4f4e-a9f0-67b3238ac9bb
[33malgo-2    |[0m 2021-11-14 15:22:33,000 INFO impl.FsDatasetImpl: Added new volume: DS-8652abdf-247c-447f-b32e-97f9abf902d5
[33malgo-2    |[0m 2021-11-14 15:22:33,001 INFO impl.FsDatasetImpl: Added volume - [DISK]file:/opt/amazon/hadoop/hdfs/datanode, StorageType: DISK
[33malgo-2    |[0m 2021-11-14 15:22:33,005 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean
[33malgo-2    |[0m 2021-11-14 15:22:33,011 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode
[33malgo-2    |[0m 2021-11-14 15:22:33,020 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode
[33malgo-2    |[0m 2021-11-14 15:22:33,022 INFO impl.FsDatasetImpl: Adding block pool BP-1297552749-172.18.0.3-1636903348563
[33malgo-2    |[0m 2021-11-14 15:22:33,022 INFO impl.FsDatasetImpl: Scanning block pool BP-1297552749-172.18.0.3-1636903348563 on volume /opt/amazon/hadoop/hdfs/datanode...
[36malgo-1    |[0m 2021-11-14 15:22:33,028 INFO impl.FsDatasetImpl: Added new volume: DS-d4dc21a0-eed3-401e-83f6-79dcb4726fca
[36malgo-1    |[0m 2021-11-14 15:22:33,028 INFO impl.FsDatasetImpl: Added volume - [DISK]file:/opt/amazon/hadoop/hdfs/datanode, StorageType: DISK
[36malgo-1    |[0m 2021-11-14 15:22:33,031 INFO store.AbstractFSNodeStore: Created store directory :file:/tmp/hadoop-yarn-root/node-attribute
[36malgo-1    |[0m 2021-11-14 15:22:33,033 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean
[36malgo-1    |[0m 2021-11-14 15:22:33,041 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode
[36malgo-1    |[0m 2021-11-14 15:22:33,049 INFO store.AbstractFSNodeStore: Finished write mirror at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.mirror
[36malgo-1    |[0m 2021-11-14 15:22:33,049 INFO store.AbstractFSNodeStore: Finished create editlog file at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.editlog
[33malgo-2    |[0m 2021-11-14 15:22:33,050 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-1297552749-172.18.0.3-1636903348563 on /opt/amazon/hadoop/hdfs/datanode: 28ms
[33malgo-2    |[0m 2021-11-14 15:22:33,050 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1297552749-172.18.0.3-1636903348563: 29ms
[33malgo-2    |[0m 2021-11-14 15:22:33,052 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-1297552749-172.18.0.3-1636903348563 on volume /opt/amazon/hadoop/hdfs/datanode...
[33malgo-2    |[0m 2021-11-14 15:22:33,052 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-1297552749-172.18.0.3-1636903348563/current/replicas doesn't exist 
[36malgo-1    |[0m 2021-11-14 15:22:33,052 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode
[33malgo-2    |[0m 2021-11-14 15:22:33,053 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1297552749-172.18.0.3-1636903348563 on volume /opt/amazon/hadoop/hdfs/datanode: 2ms
[33malgo-2    |[0m 2021-11-14 15:22:33,054 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-1297552749-172.18.0.3-1636903348563: 2ms
[36malgo-1    |[0m 2021-11-14 15:22:33,055 INFO impl.FsDatasetImpl: Adding block pool BP-1297552749-172.18.0.3-1636903348563
[33malgo-2    |[0m 2021-11-14 15:22:33,055 INFO datanode.VolumeScanner: Now scanning bpid BP-1297552749-172.18.0.3-1636903348563 on volume /opt/amazon/hadoop/hdfs/datanode
[36malgo-1    |[0m 2021-11-14 15:22:33,055 INFO impl.FsDatasetImpl: Scanning block pool BP-1297552749-172.18.0.3-1636903348563 on volume /opt/amazon/hadoop/hdfs/datanode...
[33malgo-2    |[0m 2021-11-14 15:22:33,057 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-8652abdf-247c-447f-b32e-97f9abf902d5): finished scanning block pool BP-1297552749-172.18.0.3-1636903348563
[36malgo-1    |[0m 2021-11-14 15:22:33,062 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl$ForwardingEventHandler
[36malgo-1    |[0m 2021-11-14 15:22:33,062 INFO placement.MultiNodeSortingManager: Starting NodeSortingService=MultiNodeSortingManager
[33malgo-2    |[0m 2021-11-14 15:22:33,071 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-8652abdf-247c-447f-b32e-97f9abf902d5): no suitable block pools found to scan.  Waiting 1814399984 ms.
[33malgo-2    |[0m 2021-11-14 15:22:33,072 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 11/14/21 6:59 PM with interval of 21600000ms
[36malgo-1    |[0m 2021-11-14 15:22:33,073 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 15:22:33,074 INFO ipc.Server: Starting Socket Reader #1 for port 8031
[36malgo-1    |[0m 2021-11-14 15:22:33,076 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server
[36malgo-1    |[0m 2021-11-14 15:22:33,077 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 15:22:33,077 INFO ipc.Server: IPC Server listener on 8031: starting
[33malgo-2    |[0m 2021-11-14 15:22:33,078 INFO datanode.DataNode: Block pool BP-1297552749-172.18.0.3-1636903348563 (Datanode Uuid 4d2edd13-f33b-493b-92f7-3a6154df0d91) service to algo-1.spark-network/172.18.0.3:8020 beginning handshake with NN
[36malgo-1    |[0m 2021-11-14 15:22:33,085 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-1297552749-172.18.0.3-1636903348563 on /opt/amazon/hadoop/hdfs/datanode: 29ms
[36malgo-1    |[0m 2021-11-14 15:22:33,085 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1297552749-172.18.0.3-1636903348563: 31ms
[36malgo-1    |[0m 2021-11-14 15:22:33,087 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-1297552749-172.18.0.3-1636903348563 on volume /opt/amazon/hadoop/hdfs/datanode...
[36malgo-1    |[0m 2021-11-14 15:22:33,087 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-1297552749-172.18.0.3-1636903348563/current/replicas doesn't exist 
[36malgo-1    |[0m 2021-11-14 15:22:33,088 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1297552749-172.18.0.3-1636903348563 on volume /opt/amazon/hadoop/hdfs/datanode: 2ms
[36malgo-1    |[0m 2021-11-14 15:22:33,088 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-1297552749-172.18.0.3-1636903348563: 2ms
[36malgo-1    |[0m 2021-11-14 15:22:33,089 INFO util.JvmPauseMonitor: Starting JVM pause monitor
[36malgo-1    |[0m 2021-11-14 15:22:33,090 INFO datanode.VolumeScanner: Now scanning bpid BP-1297552749-172.18.0.3-1636903348563 on volume /opt/amazon/hadoop/hdfs/datanode
[36malgo-1    |[0m 2021-11-14 15:22:33,092 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-d4dc21a0-eed3-401e-83f6-79dcb4726fca): finished scanning block pool BP-1297552749-172.18.0.3-1636903348563
[36malgo-1    |[0m 2021-11-14 15:22:33,101 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 15:22:33,103 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-d4dc21a0-eed3-401e-83f6-79dcb4726fca): no suitable block pools found to scan.  Waiting 1814399987 ms.
[36malgo-1    |[0m 2021-11-14 15:22:33,104 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 11/14/21 8:54 PM with interval of 21600000ms
[36malgo-1    |[0m 2021-11-14 15:22:33,107 INFO ipc.Server: Starting Socket Reader #1 for port 8030
[36malgo-1    |[0m 2021-11-14 15:22:33,110 INFO datanode.DataNode: Block pool BP-1297552749-172.18.0.3-1636903348563 (Datanode Uuid 01942069-d63b-4f4e-a9f0-67b3238ac9bb) service to algo-1/172.18.0.3:8020 beginning handshake with NN
[36malgo-1    |[0m 2021-11-14 15:22:33,115 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.2:9866, datanodeUuid=4d2edd13-f33b-493b-92f7-3a6154df0d91, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-d1e64556-4dba-4d0e-a81a-97c9d3653265;nsid=489834641;c=1636903348563) storage 4d2edd13-f33b-493b-92f7-3a6154df0d91
[36malgo-1    |[0m 2021-11-14 15:22:33,115 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 15:22:33,116 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 15:22:33,116 INFO net.NetworkTopology: Adding a new node: /default-rack/172.18.0.2:9866
[36malgo-1    |[0m 2021-11-14 15:22:33,116 INFO blockmanagement.BlockReportLeaseManager: Registered DN 4d2edd13-f33b-493b-92f7-3a6154df0d91 (172.18.0.2:9866).
[36malgo-1    |[0m 2021-11-14 15:22:33,116 INFO ipc.Server: IPC Server listener on 8030: starting
[33malgo-2    |[0m 2021-11-14 15:22:33,126 INFO datanode.DataNode: Block pool Block pool BP-1297552749-172.18.0.3-1636903348563 (Datanode Uuid 4d2edd13-f33b-493b-92f7-3a6154df0d91) service to algo-1.spark-network/172.18.0.3:8020 successfully registered with NN
[33malgo-2    |[0m 2021-11-14 15:22:33,126 INFO datanode.DataNode: For namenode algo-1.spark-network/172.18.0.3:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
[36malgo-1    |[0m 2021-11-14 15:22:33,131 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:9866, datanodeUuid=01942069-d63b-4f4e-a9f0-67b3238ac9bb, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-d1e64556-4dba-4d0e-a81a-97c9d3653265;nsid=489834641;c=1636903348563) storage 01942069-d63b-4f4e-a9f0-67b3238ac9bb
[36malgo-1    |[0m 2021-11-14 15:22:33,132 INFO net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:9866
[36malgo-1    |[0m 2021-11-14 15:22:33,132 INFO blockmanagement.BlockReportLeaseManager: Registered DN 01942069-d63b-4f4e-a9f0-67b3238ac9bb (172.18.0.3:9866).
[36malgo-1    |[0m 2021-11-14 15:22:33,135 INFO datanode.DataNode: Block pool Block pool BP-1297552749-172.18.0.3-1636903348563 (Datanode Uuid 01942069-d63b-4f4e-a9f0-67b3238ac9bb) service to algo-1/172.18.0.3:8020 successfully registered with NN
[36malgo-1    |[0m 2021-11-14 15:22:33,135 INFO datanode.DataNode: For namenode algo-1/172.18.0.3:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
[36malgo-1    |[0m 2021-11-14 15:22:33,178 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-8652abdf-247c-447f-b32e-97f9abf902d5 for DN 172.18.0.2:9866
[36malgo-1    |[0m 2021-11-14 15:22:33,184 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-d4dc21a0-eed3-401e-83f6-79dcb4726fca for DN 172.18.0.3:9866
[36malgo-1    |[0m 2021-11-14 15:22:33,200 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
[36malgo-1    |[0m 2021-11-14 15:22:33,201 INFO ipc.Server: Starting Socket Reader #1 for port 8032
[36malgo-1    |[0m 2021-11-14 15:22:33,204 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server
[36malgo-1    |[0m 2021-11-14 15:22:33,204 INFO ipc.Server: IPC Server Responder: starting
[36malgo-1    |[0m 2021-11-14 15:22:33,204 INFO ipc.Server: IPC Server listener on 8032: starting
[36malgo-1    |[0m 2021-11-14 15:22:33,223 INFO resourcemanager.ResourceManager: Transitioned to active state
[36malgo-1    |[0m 2021-11-14 15:22:33,297 INFO BlockStateChange: BLOCK* processReport 0x702f5dc48c630969: Processing first storage report for DS-8652abdf-247c-447f-b32e-97f9abf902d5 from datanode 4d2edd13-f33b-493b-92f7-3a6154df0d91
[36malgo-1    |[0m 2021-11-14 15:22:33,299 INFO BlockStateChange: BLOCK* processReport 0x702f5dc48c630969: from storage DS-8652abdf-247c-447f-b32e-97f9abf902d5 node DatanodeRegistration(172.18.0.2:9866, datanodeUuid=4d2edd13-f33b-493b-92f7-3a6154df0d91, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-d1e64556-4dba-4d0e-a81a-97c9d3653265;nsid=489834641;c=1636903348563), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
[36malgo-1    |[0m 2021-11-14 15:22:33,302 INFO BlockStateChange: BLOCK* processReport 0xd26b8681e4bf3c86: Processing first storage report for DS-d4dc21a0-eed3-401e-83f6-79dcb4726fca from datanode 01942069-d63b-4f4e-a9f0-67b3238ac9bb
[36malgo-1    |[0m 2021-11-14 15:22:33,303 INFO BlockStateChange: BLOCK* processReport 0xd26b8681e4bf3c86: from storage DS-d4dc21a0-eed3-401e-83f6-79dcb4726fca node DatanodeRegistration(172.18.0.3:9866, datanodeUuid=01942069-d63b-4f4e-a9f0-67b3238ac9bb, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-d1e64556-4dba-4d0e-a81a-97c9d3653265;nsid=489834641;c=1636903348563), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
[33malgo-2    |[0m 2021-11-14 15:22:33,332 INFO datanode.DataNode: Successfully sent block report 0x702f5dc48c630969,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 132 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
[33malgo-2    |[0m 2021-11-14 15:22:33,332 INFO datanode.DataNode: Got finalize command for block pool BP-1297552749-172.18.0.3-1636903348563
[36malgo-1    |[0m 2021-11-14 15:22:33,333 INFO datanode.DataNode: Successfully sent block report 0xd26b8681e4bf3c86,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 5 msec to generate and 128 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
[36malgo-1    |[0m 2021-11-14 15:22:33,333 INFO datanode.DataNode: Got finalize command for block pool BP-1297552749-172.18.0.3-1636903348563
[33malgo-2    |[0m 2021-11-14 15:22:33,632 INFO ipc.Client: Retrying connect to server: algo-1.spark-network/172.18.0.3:8031. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 15:22:33,655 INFO ipc.Client: Retrying connect to server: algo-1/172.18.0.3:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
[36malgo-1    |[0m 2021-11-14 15:22:33,815 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-1(cmPort: 35251 httpPort: 8042) registered with capability: <memory:15892, vCores:4>, assigned nodeId algo-1:35251
[36malgo-1    |[0m 2021-11-14 15:22:33,815 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-2(cmPort: 34827 httpPort: 8042) registered with capability: <memory:15892, vCores:4>, assigned nodeId algo-2:34827
[36malgo-1    |[0m 2021-11-14 15:22:33,820 INFO rmnode.RMNodeImpl: algo-2:34827 Node Transitioned from NEW to RUNNING
[36malgo-1    |[0m 2021-11-14 15:22:33,820 INFO rmnode.RMNodeImpl: algo-1:35251 Node Transitioned from NEW to RUNNING
[33malgo-2    |[0m 2021-11-14 15:22:33,832 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id -2010304081
[33malgo-2    |[0m 2021-11-14 15:22:33,833 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id 1212690658
[33malgo-2    |[0m 2021-11-14 15:22:33,833 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-2:34827 with total resource of <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 15:22:33,834 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id -2010304081
[36malgo-1    |[0m 2021-11-14 15:22:33,835 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id 1212690658
[36malgo-1    |[0m 2021-11-14 15:22:33,836 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-1:35251 with total resource of <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 15:22:33,839 INFO capacity.CapacityScheduler: Added node algo-2:34827 clusterResource: <memory:15892, vCores:4>
[36malgo-1    |[0m 2021-11-14 15:22:33,841 INFO capacity.CapacityScheduler: Added node algo-1:35251 clusterResource: <memory:31784, vCores:8>
[36malgo-1    |[0m Using properties file: /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m Adding default property: spark.driver.host=172.18.0.3
[36malgo-1    |[0m Adding default property: spark.executor.memoryOverhead=1239m
[36malgo-1    |[0m Adding default property: spark.driver.extraLibraryPath=/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m Adding default property: spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
[36malgo-1    |[0m Adding default property: spark.rpc.askTimeout=300s
[36malgo-1    |[0m Adding default property: spark.driver.memory=2048m
[36malgo-1    |[0m Adding default property: spark.executor.instances=2
[36malgo-1    |[0m Adding default property: spark.driver.memoryOverhead=204m
[36malgo-1    |[0m Adding default property: key=value
[36malgo-1    |[0m Adding default property: spark.default.parallelism=16
[36malgo-1    |[0m Adding default property: spark.executor.defaultJavaOptions=-verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3
[36malgo-1    |[0m Adding default property: spark.driver.defaultJavaOptions=-XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled
[36malgo-1    |[0m Adding default property: spark.executor.extraLibraryPath=/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m Adding default property: spark.executor.memory=2g
[36malgo-1    |[0m Adding default property: spark.driver.extraClassPath=/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m Adding default property: spark.executor.extraClassPath=/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m Adding default property: spark.executor.cores=1
[36malgo-1    |[0m Warning: Ignoring non-Spark config property: key
[36malgo-1    |[0m Parsed arguments:
[36malgo-1    |[0m   master                  yarn
[36malgo-1    |[0m   deployMode              client
[36malgo-1    |[0m   executorMemory          2g
[36malgo-1    |[0m   executorCores           1
[36malgo-1    |[0m   totalExecutorCores      null
[36malgo-1    |[0m   propertiesFile          /usr/lib/spark/conf/spark-defaults.conf
[36malgo-1    |[0m   driverMemory            2048m
[36malgo-1    |[0m   driverCores             null
[36malgo-1    |[0m   driverExtraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
[36malgo-1    |[0m   driverExtraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
[36malgo-1    |[0m   driverExtraJavaOptions  null
[36malgo-1    |[0m   supervise               false
[36malgo-1    |[0m   queue                   null
[36malgo-1    |[0m   numExecutors            2
[36malgo-1    |[0m   files                   null
[36malgo-1    |[0m   pyFiles                 null
[36malgo-1    |[0m   archives                null
[36malgo-1    |[0m   mainClass               com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp
[36malgo-1    |[0m   primaryResource         file:/opt/ml/processing/input/code/java/hello-java-spark/target/hello-java-spark-1.0-SNAPSHOT.jar
[36malgo-1    |[0m   name                    com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp
[36malgo-1    |[0m   childArgs               [--input file:///opt/ml/processing/input/data/data.jsonl --output file:///opt/ml/processing/output/data]
[36malgo-1    |[0m   jars                    null
[36malgo-1    |[0m   packages                null
[36malgo-1    |[0m   packagesExclusions      null
[36malgo-1    |[0m   repositories            null
[36malgo-1    |[0m   verbose                 true
[36malgo-1    |[0m 
[36malgo-1    |[0m Spark properties used, including those specified through
[36malgo-1    |[0m  --conf and those from the properties file /usr/lib/spark/conf/spark-defaults.conf:
[36malgo-1    |[0m   (spark.executor.defaultJavaOptions,-verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3)
[36malgo-1    |[0m   (spark.default.parallelism,16)
[36malgo-1    |[0m   (spark.executor.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)
[36malgo-1    |[0m   (spark.executor.memory,2g)
[36malgo-1    |[0m   (spark.driver.memory,2048m)
[36malgo-1    |[0m   (spark.driver.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)
[36malgo-1    |[0m   (spark.executor.instances,2)
[36malgo-1    |[0m   (spark.executor.memoryOverhead,1239m)
[36malgo-1    |[0m   (spark.rpc.askTimeout,300s)
[36malgo-1    |[0m   (spark.driver.memoryOverhead,204m)
[36malgo-1    |[0m   (spark.driver.host,172.18.0.3)
[36malgo-1    |[0m   (spark.driver.defaultJavaOptions,-XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled)
[36malgo-1    |[0m   (spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version,2)
[36malgo-1    |[0m   (spark.executor.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)
[36malgo-1    |[0m   (spark.executor.cores,1)
[36malgo-1    |[0m   (spark.driver.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)
[36malgo-1    |[0m 
[36malgo-1    |[0m     
[36malgo-1    |[0m Main class:
[36malgo-1    |[0m com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp
[36malgo-1    |[0m Arguments:
[36malgo-1    |[0m --input
[36malgo-1    |[0m file:///opt/ml/processing/input/data/data.jsonl
[36malgo-1    |[0m --output
[36malgo-1    |[0m file:///opt/ml/processing/output/data
[36malgo-1    |[0m Spark config:
[36malgo-1    |[0m (spark.driver.host,172.18.0.3)
[36malgo-1    |[0m (spark.executor.memoryOverhead,1239m)
[36malgo-1    |[0m (spark.driver.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)
[36malgo-1    |[0m (spark.jars,file:/opt/ml/processing/input/code/java/hello-java-spark/target/hello-java-spark-1.0-SNAPSHOT.jar)
[36malgo-1    |[0m (spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version,2)
[36malgo-1    |[0m (spark.app.name,com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp)
[36malgo-1    |[0m (spark.rpc.askTimeout,300s)
[36malgo-1    |[0m (spark.driver.memory,2048m)
[36malgo-1    |[0m (spark.executor.instances,2)
[36malgo-1    |[0m (spark.submit.pyFiles,)
[36malgo-1    |[0m (spark.driver.memoryOverhead,204m)
[36malgo-1    |[0m (spark.default.parallelism,16)
[36malgo-1    |[0m (spark.executor.defaultJavaOptions,-verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3)
[36malgo-1    |[0m (spark.submit.deployMode,client)
[36malgo-1    |[0m (spark.master,yarn)
[36malgo-1    |[0m (spark.driver.defaultJavaOptions,-XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled)
[36malgo-1    |[0m (spark.executor.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)
[36malgo-1    |[0m (spark.executor.memory,2g)
[36malgo-1    |[0m (spark.driver.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)
[36malgo-1    |[0m (spark.executor.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)
[36malgo-1    |[0m (spark.executor.cores,1)
[36malgo-1    |[0m Classpath elements:
[36malgo-1    |[0m file:/opt/ml/processing/input/code/java/hello-java-spark/target/hello-java-spark-1.0-SNAPSHOT.jar
[36malgo-1    |[0m 
[36malgo-1    |[0m 
[36malgo-1    |[0m Hello World, this is Java-Spark!
[36malgo-1    |[0m Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[36malgo-1    |[0m 21/11/14 15:22:34 INFO SparkContext: Running Spark version 3.0.0-amzn-0
[36malgo-1    |[0m 21/11/14 15:22:35 INFO ResourceUtils: ==============================================================
[36malgo-1    |[0m 21/11/14 15:22:35 INFO ResourceUtils: Resources for spark.driver:
[36malgo-1    |[0m 
[36malgo-1    |[0m 21/11/14 15:22:35 INFO ResourceUtils: ==============================================================
[36malgo-1    |[0m 21/11/14 15:22:35 INFO SparkContext: Submitted application: Hello Spark App
[36malgo-1    |[0m 21/11/14 15:22:35 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m 21/11/14 15:22:35 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m 21/11/14 15:22:35 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m 21/11/14 15:22:35 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m 21/11/14 15:22:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m 21/11/14 15:22:35 INFO Utils: Successfully started service 'sparkDriver' on port 46495.
[36malgo-1    |[0m 21/11/14 15:22:35 INFO SparkEnv: Registering MapOutputTracker
[36malgo-1    |[0m 21/11/14 15:22:35 INFO SparkEnv: Registering BlockManagerMaster
[36malgo-1    |[0m 21/11/14 15:22:35 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[36malgo-1    |[0m 21/11/14 15:22:35 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[36malgo-1    |[0m 21/11/14 15:22:35 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[36malgo-1    |[0m 21/11/14 15:22:35 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f0004ebf-a6ea-4268-8bc8-099aeaccc97e
[36malgo-1    |[0m 21/11/14 15:22:35 INFO MemoryStore: MemoryStore started with capacity 1007.8 MiB
[36malgo-1    |[0m 21/11/14 15:22:35 INFO SparkEnv: Registering OutputCommitCoordinator
[36malgo-1    |[0m 21/11/14 15:22:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[36malgo-1    |[0m 21/11/14 15:22:35 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.18.0.3:4040
[36malgo-1    |[0m 21/11/14 15:22:35 INFO SparkContext: Added JAR file:/opt/ml/processing/input/code/java/hello-java-spark/target/hello-java-spark-1.0-SNAPSHOT.jar at spark://172.18.0.3:46495/jars/hello-java-spark-1.0-SNAPSHOT.jar with timestamp 1636903355855
[36malgo-1    |[0m 21/11/14 15:22:36 INFO RMProxy: Connecting to ResourceManager at /172.18.0.3:8032
[36malgo-1    |[0m 21/11/14 15:22:36 INFO Client: Requesting a new application from cluster with 2 NodeManagers
[36malgo-1    |[0m 2021-11-14 15:22:36,391 INFO resourcemanager.ClientRMService: Allocated new applicationId: 1
[36malgo-1    |[0m 21/11/14 15:22:36 INFO Configuration: resource-types.xml not found
[36malgo-1    |[0m 21/11/14 15:22:36 INFO ResourceUtils: Unable to find 'resource-types.xml'.
[36malgo-1    |[0m 21/11/14 15:22:36 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15892 MB per container)
[36malgo-1    |[0m 21/11/14 15:22:36 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
[36malgo-1    |[0m 21/11/14 15:22:36 INFO Client: Setting up container launch context for our AM
[36malgo-1    |[0m 21/11/14 15:22:36 INFO Client: Setting up the launch environment for our AM container
[36malgo-1    |[0m 21/11/14 15:22:36 INFO Client: Preparing resources for our AM container
[36malgo-1    |[0m 21/11/14 15:22:37 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
[36malgo-1    |[0m 21/11/14 15:22:42 INFO Client: Uploading resource file:/tmp/spark-1d6fa094-53dd-4f75-9949-8921c57c4a2f/__spark_libs__4680140580001336232.zip -> hdfs://172.18.0.3/user/root/.sparkStaging/application_1636903352801_0001/__spark_libs__4680140580001336232.zip
[36malgo-1    |[0m 2021-11-14 15:22:42,961 INFO hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=172.18.0.3:9866, 172.18.0.2:9866 for /user/root/.sparkStaging/application_1636903352801_0001/__spark_libs__4680140580001336232.zip
[36malgo-1    |[0m 2021-11-14 15:22:43,088 INFO datanode.DataNode: Receiving BP-1297552749-172.18.0.3-1636903348563:blk_1073741825_1001 src: /172.18.0.3:35264 dest: /172.18.0.3:9866
[33malgo-2    |[0m 2021-11-14 15:22:43,150 INFO datanode.DataNode: Receiving BP-1297552749-172.18.0.3-1636903348563:blk_1073741825_1001 src: /172.18.0.3:43886 dest: /172.18.0.2:9866
[33malgo-2    |[0m 2021-11-14 15:22:43,574 INFO DataNode.clienttrace: src: /172.18.0.3:43886, dest: /172.18.0.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1629465286_1, offset: 0, srvID: 4d2edd13-f33b-493b-92f7-3a6154df0d91, blockid: BP-1297552749-172.18.0.3-1636903348563:blk_1073741825_1001, duration(ns): 400811973
[33malgo-2    |[0m 2021-11-14 15:22:43,574 INFO datanode.DataNode: PacketResponder: BP-1297552749-172.18.0.3-1636903348563:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 15:22:43,577 INFO DataNode.clienttrace: src: /172.18.0.3:35264, dest: /172.18.0.3:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1629465286_1, offset: 0, srvID: 01942069-d63b-4f4e-a9f0-67b3238ac9bb, blockid: BP-1297552749-172.18.0.3-1636903348563:blk_1073741825_1001, duration(ns): 399356739
[36malgo-1    |[0m 2021-11-14 15:22:43,577 INFO datanode.DataNode: PacketResponder: BP-1297552749-172.18.0.3-1636903348563:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.18.0.2:9866] terminating
[36malgo-1    |[0m 2021-11-14 15:22:43,584 INFO hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=172.18.0.3:9866, 172.18.0.2:9866 for /user/root/.sparkStaging/application_1636903352801_0001/__spark_libs__4680140580001336232.zip
[36malgo-1    |[0m 2021-11-14 15:22:43,587 INFO datanode.DataNode: Receiving BP-1297552749-172.18.0.3-1636903348563:blk_1073741826_1002 src: /172.18.0.3:35268 dest: /172.18.0.3:9866
[33malgo-2    |[0m 2021-11-14 15:22:43,589 INFO datanode.DataNode: Receiving BP-1297552749-172.18.0.3-1636903348563:blk_1073741826_1002 src: /172.18.0.3:43890 dest: /172.18.0.2:9866
[33malgo-2    |[0m 2021-11-14 15:22:43,927 INFO DataNode.clienttrace: src: /172.18.0.3:43890, dest: /172.18.0.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1629465286_1, offset: 0, srvID: 4d2edd13-f33b-493b-92f7-3a6154df0d91, blockid: BP-1297552749-172.18.0.3-1636903348563:blk_1073741826_1002, duration(ns): 336888001
[33malgo-2    |[0m 2021-11-14 15:22:43,928 INFO datanode.DataNode: PacketResponder: BP-1297552749-172.18.0.3-1636903348563:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 15:22:43,929 INFO DataNode.clienttrace: src: /172.18.0.3:35268, dest: /172.18.0.3:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1629465286_1, offset: 0, srvID: 01942069-d63b-4f4e-a9f0-67b3238ac9bb, blockid: BP-1297552749-172.18.0.3-1636903348563:blk_1073741826_1002, duration(ns): 337964270
[36malgo-1    |[0m 2021-11-14 15:22:43,929 INFO datanode.DataNode: PacketResponder: BP-1297552749-172.18.0.3-1636903348563:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.18.0.2:9866] terminating
[36malgo-1    |[0m 2021-11-14 15:22:43,931 INFO hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=172.18.0.3:9866, 172.18.0.2:9866 for /user/root/.sparkStaging/application_1636903352801_0001/__spark_libs__4680140580001336232.zip
[36malgo-1    |[0m 2021-11-14 15:22:43,934 INFO datanode.DataNode: Receiving BP-1297552749-172.18.0.3-1636903348563:blk_1073741827_1003 src: /172.18.0.3:35272 dest: /172.18.0.3:9866
[33malgo-2    |[0m 2021-11-14 15:22:43,936 INFO datanode.DataNode: Receiving BP-1297552749-172.18.0.3-1636903348563:blk_1073741827_1003 src: /172.18.0.3:43894 dest: /172.18.0.2:9866
[33malgo-2    |[0m 2021-11-14 15:22:44,258 INFO DataNode.clienttrace: src: /172.18.0.3:43894, dest: /172.18.0.2:9866, bytes: 128628638, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1629465286_1, offset: 0, srvID: 4d2edd13-f33b-493b-92f7-3a6154df0d91, blockid: BP-1297552749-172.18.0.3-1636903348563:blk_1073741827_1003, duration(ns): 321146731
[33malgo-2    |[0m 2021-11-14 15:22:44,258 INFO datanode.DataNode: PacketResponder: BP-1297552749-172.18.0.3-1636903348563:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 15:22:44,260 INFO DataNode.clienttrace: src: /172.18.0.3:35272, dest: /172.18.0.3:9866, bytes: 128628638, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1629465286_1, offset: 0, srvID: 01942069-d63b-4f4e-a9f0-67b3238ac9bb, blockid: BP-1297552749-172.18.0.3-1636903348563:blk_1073741827_1003, duration(ns): 322330094
[36malgo-1    |[0m 2021-11-14 15:22:44,260 INFO datanode.DataNode: PacketResponder: BP-1297552749-172.18.0.3-1636903348563:blk_1073741827_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.18.0.2:9866] terminating
[36malgo-1    |[0m 2021-11-14 15:22:44,265 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1636903352801_0001/__spark_libs__4680140580001336232.zip is closed by DFSClient_NONMAPREDUCE_1629465286_1
[36malgo-1    |[0m 21/11/14 15:22:44 INFO Client: Uploading resource file:/tmp/spark-1d6fa094-53dd-4f75-9949-8921c57c4a2f/__spark_conf__623074989735376732.zip -> hdfs://172.18.0.3/user/root/.sparkStaging/application_1636903352801_0001/__spark_conf__.zip
[36malgo-1    |[0m 2021-11-14 15:22:44,480 INFO hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=172.18.0.3:9866, 172.18.0.2:9866 for /user/root/.sparkStaging/application_1636903352801_0001/__spark_conf__.zip
[36malgo-1    |[0m 2021-11-14 15:22:44,484 INFO datanode.DataNode: Receiving BP-1297552749-172.18.0.3-1636903348563:blk_1073741828_1004 src: /172.18.0.3:35276 dest: /172.18.0.3:9866
[33malgo-2    |[0m 2021-11-14 15:22:44,485 INFO datanode.DataNode: Receiving BP-1297552749-172.18.0.3-1636903348563:blk_1073741828_1004 src: /172.18.0.3:43898 dest: /172.18.0.2:9866
[33malgo-2    |[0m 2021-11-14 15:22:44,489 INFO DataNode.clienttrace: src: /172.18.0.3:43898, dest: /172.18.0.2:9866, bytes: 251856, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1629465286_1, offset: 0, srvID: 4d2edd13-f33b-493b-92f7-3a6154df0d91, blockid: BP-1297552749-172.18.0.3-1636903348563:blk_1073741828_1004, duration(ns): 2527943
[33malgo-2    |[0m 2021-11-14 15:22:44,489 INFO datanode.DataNode: PacketResponder: BP-1297552749-172.18.0.3-1636903348563:blk_1073741828_1004, type=LAST_IN_PIPELINE terminating
[36malgo-1    |[0m 2021-11-14 15:22:44,490 INFO DataNode.clienttrace: src: /172.18.0.3:35276, dest: /172.18.0.3:9866, bytes: 251856, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1629465286_1, offset: 0, srvID: 01942069-d63b-4f4e-a9f0-67b3238ac9bb, blockid: BP-1297552749-172.18.0.3-1636903348563:blk_1073741828_1004, duration(ns): 3497238
[36malgo-1    |[0m 2021-11-14 15:22:44,490 INFO datanode.DataNode: PacketResponder: BP-1297552749-172.18.0.3-1636903348563:blk_1073741828_1004, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.18.0.2:9866] terminating
[36malgo-1    |[0m 2021-11-14 15:22:44,492 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1636903352801_0001/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_1629465286_1
[36malgo-1    |[0m 21/11/14 15:22:44 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m 21/11/14 15:22:44 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m 21/11/14 15:22:44 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m 21/11/14 15:22:44 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m 21/11/14 15:22:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m 21/11/14 15:22:44 INFO Client: Submitting application application_1636903352801_0001 to ResourceManager
[36malgo-1    |[0m 2021-11-14 15:22:44,618 INFO capacity.CapacityScheduler: Application 'application_1636903352801_0001' is submitted without priority hence considering default queue/cluster priority: 0
[36malgo-1    |[0m 2021-11-14 15:22:44,618 INFO capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1636903352801_0001
[36malgo-1    |[0m 2021-11-14 15:22:44,633 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 1]. Use the global max attempts instead.
[36malgo-1    |[0m 2021-11-14 15:22:44,635 INFO resourcemanager.ClientRMService: Application with id 1 submitted by user root
[36malgo-1    |[0m 2021-11-14 15:22:44,635 INFO rmapp.RMAppImpl: Storing application with id application_1636903352801_0001
[36malgo-1    |[0m 2021-11-14 15:22:44,637 INFO resourcemanager.RMAuditLogger: USER=root	IP=172.18.0.3	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1636903352801_0001	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 15:22:44,642 INFO recovery.RMStateStore: Storing info for app: application_1636903352801_0001
[36malgo-1    |[0m 2021-11-14 15:22:44,642 INFO rmapp.RMAppImpl: application_1636903352801_0001 State change from NEW to NEW_SAVING on event = START
[36malgo-1    |[0m 2021-11-14 15:22:44,643 INFO rmapp.RMAppImpl: application_1636903352801_0001 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
[36malgo-1    |[0m 2021-11-14 15:22:44,644 INFO capacity.ParentQueue: Application added - appId: application_1636903352801_0001 user: root leaf-queue of parent: root #applications: 1
[36malgo-1    |[0m 2021-11-14 15:22:44,645 INFO capacity.CapacityScheduler: Accepted application application_1636903352801_0001 from user: root, in queue: default
[36malgo-1    |[0m 2021-11-14 15:22:44,656 INFO rmapp.RMAppImpl: application_1636903352801_0001 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
[36malgo-1    |[0m 2021-11-14 15:22:44,682 INFO resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1636903352801_0001_000001
[36malgo-1    |[0m 2021-11-14 15:22:44,683 INFO attempt.RMAppAttemptImpl: appattempt_1636903352801_0001_000001 State change from NEW to SUBMITTED on event = START
[36malgo-1    |[0m 21/11/14 15:22:44 INFO YarnClientImpl: Submitted application application_1636903352801_0001
[36malgo-1    |[0m 2021-11-14 15:22:44,735 INFO capacity.LeafQueue: Application application_1636903352801_0001 from user: root activated in queue: default
[36malgo-1    |[0m 2021-11-14 15:22:44,735 INFO capacity.LeafQueue: Application added - appId: application_1636903352801_0001 user: root, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
[36malgo-1    |[0m 2021-11-14 15:22:44,735 INFO capacity.CapacityScheduler: Added Application Attempt appattempt_1636903352801_0001_000001 to scheduler from user root in queue default
[36malgo-1    |[0m 2021-11-14 15:22:44,742 INFO attempt.RMAppAttemptImpl: appattempt_1636903352801_0001_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
[36malgo-1    |[0m 2021-11-14 15:22:44,937 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1636903352801_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=
[36malgo-1    |[0m 2021-11-14 15:22:44,941 INFO rmcontainer.RMContainerImpl: container_1636903352801_0001_01_000001 Container Transitioned from NEW to ALLOCATED
[36malgo-1    |[0m 2021-11-14 15:22:44,941 INFO fica.FiCaSchedulerNode: Assigned container container_1636903352801_0001_01_000001 of capacity <memory:896, vCores:1> on host algo-1:35251, which has 1 containers, <memory:896, vCores:1> used and <memory:14996, vCores:3> available after allocation
[36malgo-1    |[0m 2021-11-14 15:22:44,941 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636903352801_0001	CONTAINERID=container_1636903352801_0001_01_000001	RESOURCE=<memory:896, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 15:22:44,959 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:35251 for container : container_1636903352801_0001_01_000001
[36malgo-1    |[0m 2021-11-14 15:22:44,967 INFO rmcontainer.RMContainerImpl: container_1636903352801_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
[36malgo-1    |[0m 2021-11-14 15:22:44,967 INFO security.NMTokenSecretManagerInRM: Clear node set for appattempt_1636903352801_0001_000001
[36malgo-1    |[0m 2021-11-14 15:22:44,967 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.028190285 absoluteUsedCapacity=0.028190285 used=<memory:896, vCores:1> cluster=<memory:31784, vCores:8>
[36malgo-1    |[0m 2021-11-14 15:22:44,967 INFO attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1636903352801_0001 AttemptId: appattempt_1636903352801_0001_000001 MasterContainer: Container: [ContainerId: container_1636903352801_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-1:35251, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 172.18.0.3:35251 }, ExecutionType: GUARANTEED, ]
[36malgo-1    |[0m 2021-11-14 15:22:44,967 INFO capacity.CapacityScheduler: Allocation proposal accepted
[36malgo-1    |[0m 2021-11-14 15:22:44,978 INFO attempt.RMAppAttemptImpl: appattempt_1636903352801_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
[36malgo-1    |[0m 2021-11-14 15:22:44,982 INFO attempt.RMAppAttemptImpl: appattempt_1636903352801_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
[36malgo-1    |[0m 2021-11-14 15:22:44,994 INFO amlauncher.AMLauncher: Launching masterappattempt_1636903352801_0001_000001
[36malgo-1    |[0m 2021-11-14 15:22:45,052 INFO amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1636903352801_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-1:35251, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 172.18.0.3:35251 }, ExecutionType: GUARANTEED, ] for AM appattempt_1636903352801_0001_000001
[36malgo-1    |[0m 2021-11-14 15:22:45,053 INFO security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1636903352801_0001_000001
[36malgo-1    |[0m 2021-11-14 15:22:45,056 INFO security.AMRMTokenSecretManager: Creating password for appattempt_1636903352801_0001_000001
[36malgo-1    |[0m 2021-11-14 15:22:45,176 INFO ipc.Server: Auth successful for appattempt_1636903352801_0001_000001 (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 15:22:45,258 INFO containermanager.ContainerManagerImpl: Start request for container_1636903352801_0001_01_000001 by user root
[36malgo-1    |[0m 2021-11-14 15:22:45,305 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1636903352801_0001
[36malgo-1    |[0m 2021-11-14 15:22:45,312 INFO application.ApplicationImpl: Application application_1636903352801_0001 transitioned from NEW to INITING
[36malgo-1    |[0m 2021-11-14 15:22:45,313 INFO application.ApplicationImpl: Adding container_1636903352801_0001_01_000001 to application application_1636903352801_0001
[36malgo-1    |[0m 2021-11-14 15:22:45,313 INFO nodemanager.NMAuditLogger: USER=root	IP=172.18.0.3	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1636903352801_0001	CONTAINERID=container_1636903352801_0001_01_000001
[36malgo-1    |[0m 2021-11-14 15:22:45,316 INFO application.ApplicationImpl: Application application_1636903352801_0001 transitioned from INITING to RUNNING
[36malgo-1    |[0m 2021-11-14 15:22:45,319 INFO container.ContainerImpl: Container container_1636903352801_0001_01_000001 transitioned from NEW to LOCALIZING
[36malgo-1    |[0m 2021-11-14 15:22:45,319 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1636903352801_0001
[36malgo-1    |[0m 2021-11-14 15:22:45,328 INFO localizer.ResourceLocalizationService: Created localizer for container_1636903352801_0001_01_000001
[36malgo-1    |[0m 2021-11-14 15:22:45,336 INFO amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1636903352801_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-1:35251, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 172.18.0.3:35251 }, ExecutionType: GUARANTEED, ] for AM appattempt_1636903352801_0001_000001
[36malgo-1    |[0m 2021-11-14 15:22:45,338 INFO attempt.RMAppAttemptImpl: appattempt_1636903352801_0001_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
[36malgo-1    |[0m 2021-11-14 15:22:45,340 INFO rmapp.RMAppImpl: update the launch time for applicationId: application_1636903352801_0001, attemptId: appattempt_1636903352801_0001_000001launchTime: 1636903365336
[36malgo-1    |[0m 2021-11-14 15:22:45,340 INFO recovery.RMStateStore: Updating info for app: application_1636903352801_0001
[36malgo-1    |[0m 2021-11-14 15:22:45,393 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1636903352801_0001_01_000001.tokens
[36malgo-1    |[0m 2021-11-14 15:22:45,403 INFO nodemanager.DefaultContainerExecutor: Initializing user root
[36malgo-1    |[0m 2021-11-14 15:22:45,408 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1636903352801_0001_01_000001.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/container_1636903352801_0001_01_000001.tokens
[36malgo-1    |[0m 2021-11-14 15:22:45,408 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001
[36malgo-1    |[0m 21/11/14 15:22:45 INFO Client: Application report for application_1636903352801_0001 (state: ACCEPTED)
[36malgo-1    |[0m 21/11/14 15:22:45 INFO Client: 
[36malgo-1    |[0m 	 client token: N/A
[36malgo-1    |[0m 	 diagnostics: AM container is launched, waiting for AM container to Register with RM
[36malgo-1    |[0m 	 ApplicationMaster host: N/A
[36malgo-1    |[0m 	 ApplicationMaster RPC port: -1
[36malgo-1    |[0m 	 queue: default
[36malgo-1    |[0m 	 start time: 1636903364633
[36malgo-1    |[0m 	 final status: UNDEFINED
[36malgo-1    |[0m 	 tracking URL: http://algo-1:8088/proxy/application_1636903352801_0001/
[36malgo-1    |[0m 	 user: root
[36malgo-1    |[0m 2021-11-14 15:22:45,933 INFO rmcontainer.RMContainerImpl: container_1636903352801_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING
[36malgo-1    |[0m 21/11/14 15:22:46 INFO Client: Application report for application_1636903352801_0001 (state: ACCEPTED)
[36malgo-1    |[0m 21/11/14 15:22:47 INFO Client: Application report for application_1636903352801_0001 (state: ACCEPTED)
[36malgo-1    |[0m 2021-11-14 15:22:48,743 INFO container.ContainerImpl: Container container_1636903352801_0001_01_000001 transitioned from LOCALIZING to SCHEDULED
[36malgo-1    |[0m 2021-11-14 15:22:48,744 INFO scheduler.ContainerScheduler: Starting container [container_1636903352801_0001_01_000001]
[36malgo-1    |[0m 21/11/14 15:22:48 INFO Client: Application report for application_1636903352801_0001 (state: ACCEPTED)
[36malgo-1    |[0m 2021-11-14 15:22:48,768 INFO container.ContainerImpl: Container container_1636903352801_0001_01_000001 transitioned from SCHEDULED to RUNNING
[36malgo-1    |[0m 2021-11-14 15:22:48,769 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1636903352801_0001_01_000001
[36malgo-1    |[0m 2021-11-14 15:22:48,772 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/container_1636903352801_0001_01_000001/default_container_executor.sh]
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/prelaunch.out
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/prelaunch.err
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/launch_container.sh
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/directory.info
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stdout
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr
[36malgo-1    |[0m 2021-11-14 15:22:48,815 INFO monitor.ContainersMonitorImpl: container_1636903352801_0001_01_000001's ip = 172.18.0.3, and hostname = algo-1
[36malgo-1    |[0m 2021-11-14 15:22:48,820 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1636903352801_0001_01_000001 since CPU usage is not yet available.
[36malgo-1    |[0m 21/11/14 15:22:49 INFO Client: Application report for application_1636903352801_0001 (state: ACCEPTED)
[36malgo-1    |[0m 21/11/14 15:22:50 INFO Client: Application report for application_1636903352801_0001 (state: ACCEPTED)
[36malgo-1    |[0m 2021-11-14 15:22:50,996 INFO ipc.Server: Auth successful for appattempt_1636903352801_0001_000001 (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 15:22:51,021 INFO resourcemanager.DefaultAMSProcessor: AM registration appattempt_1636903352801_0001_000001
[36malgo-1    |[0m 2021-11-14 15:22:51,022 INFO resourcemanager.RMAuditLogger: USER=root	IP=172.18.0.3	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1636903352801_0001	APPATTEMPTID=appattempt_1636903352801_0001_000001
[36malgo-1    |[0m 2021-11-14 15:22:51,022 INFO attempt.RMAppAttemptImpl: appattempt_1636903352801_0001_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
[36malgo-1    |[0m 2021-11-14 15:22:51,022 INFO rmapp.RMAppImpl: application_1636903352801_0001 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
[36malgo-1    |[0m 21/11/14 15:22:51 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1636903352801_0001), /proxy/application_1636903352801_0001
[36malgo-1    |[0m 21/11/14 15:22:51 INFO Client: Application report for application_1636903352801_0001 (state: RUNNING)
[36malgo-1    |[0m 21/11/14 15:22:51 INFO Client: 
[36malgo-1    |[0m 	 client token: N/A
[36malgo-1    |[0m 	 diagnostics: N/A
[36malgo-1    |[0m 	 ApplicationMaster host: 172.18.0.3
[36malgo-1    |[0m 	 ApplicationMaster RPC port: -1
[36malgo-1    |[0m 	 queue: default
[36malgo-1    |[0m 	 start time: 1636903364633
[36malgo-1    |[0m 	 final status: UNDEFINED
[36malgo-1    |[0m 	 tracking URL: http://algo-1:8088/proxy/application_1636903352801_0001/
[36malgo-1    |[0m 	 user: root
[36malgo-1    |[0m 21/11/14 15:22:51 INFO YarnClientSchedulerBackend: Application application_1636903352801_0001 has started running.
[36malgo-1    |[0m 21/11/14 15:22:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46871.
[36malgo-1    |[0m 21/11/14 15:22:51 INFO NettyBlockTransferService: Server created on 172.18.0.3:46871
[36malgo-1    |[0m 21/11/14 15:22:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[36malgo-1    |[0m 21/11/14 15:22:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.18.0.3, 46871, None)
[36malgo-1    |[0m 21/11/14 15:22:51 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:46871 with 1007.8 MiB RAM, BlockManagerId(driver, 172.18.0.3, 46871, None)
[36malgo-1    |[0m 21/11/14 15:22:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.18.0.3, 46871, None)
[36malgo-1    |[0m 21/11/14 15:22:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.18.0.3, 46871, None)
[36malgo-1    |[0m 21/11/14 15:22:51 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 15:22:52 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[36malgo-1    |[0m Got a Spark session with version: 3.0.0-amzn-0
[36malgo-1    |[0m Reading input from: file:///opt/ml/processing/input/data/data.jsonl
[36malgo-1    |[0m 21/11/14 15:22:52 INFO SharedState: loading hive config file: file:/etc/spark/conf.dist/hive-site.xml
[36malgo-1    |[0m 21/11/14 15:22:52 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/lib/spark/spark-warehouse').
[36malgo-1    |[0m 21/11/14 15:22:52 INFO SharedState: Warehouse path is 'file:/usr/lib/spark/spark-warehouse'.
[36malgo-1    |[0m 21/11/14 15:22:52 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
[36malgo-1    |[0m 21/11/14 15:22:52 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 15:22:52 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 15:22:52 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 15:22:52 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m 21/11/14 15:22:52 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] 21/11/14 15:22:49 INFO SignalUtils: Registered signal handler for TERM
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] 21/11/14 15:22:49 INFO SignalUtils: Registered signal handler for HUP
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] 21/11/14 15:22:49 INFO SignalUtils: Registered signal handler for INT
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] 21/11/14 15:22:50 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] 21/11/14 15:22:50 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] 21/11/14 15:22:50 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] 21/11/14 15:22:50 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] 21/11/14 15:22:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] 21/11/14 15:22:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] 21/11/14 15:22:50 INFO ApplicationMaster: ApplicationAttemptId: appattempt_1636903352801_0001_000001
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] 21/11/14 15:22:50 INFO RMProxy: Connecting to ResourceManager at /172.18.0.3:8030
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] 21/11/14 15:22:50 INFO YarnRMClient: Registering the ApplicationMaster
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] 21/11/14 15:22:51 INFO TransportClientFactory: Successfully created connection to /172.18.0.3:46495 after 85 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] 21/11/14 15:22:51 INFO ApplicationMaster: Preparing Local resources
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] 21/11/14 15:22:52 INFO ApplicationMaster: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] ===============================================================================
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] Default YARN executor launch context:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]   env:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]     CLASSPATH -> /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]     SPARK_YARN_STAGING_DIR -> hdfs://172.18.0.3/user/root/.sparkStaging/application_1636903352801_0001
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]     SPARK_USER -> root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]   command:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]     LD_LIBRARY_PATH=\"/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:$LD_LIBRARY_PATH\" \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       {{JAVA_HOME}}/bin/java \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       -server \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       -Xmx2048m \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       '-verbose:gc' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       '-XX:OnOutOfMemoryError=kill -9 %p' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       '-XX:+PrintGCDetails' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       '-XX:+PrintGCDateStamps' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       '-XX:+UseParallelGC' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       '-XX:InitiatingHeapOccupancyPercent=70' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       '-XX:ConcGCThreads=1' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       '-XX:ParallelGCThreads=3' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       -Djava.io.tmpdir={{PWD}}/tmp \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       '-Dspark.rpc.askTimeout=300s' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       '-Dspark.driver.port=46495' \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       -Dspark.yarn.app.container.log.dir=<LOG_DIR> \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       org.apache.spark.executor.YarnCoarseGrainedExecutorBackend \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       --driver-url \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       spark://CoarseGrainedScheduler@172.18.0.3:46495 \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       --executor-id \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       <executorId> \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       --hostname \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       <hostname> \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       --cores \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       1 \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       --app-id \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       application_1636903352801_0001 \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       --resourceProfileId \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       0 \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       --user-class-path \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       file:$PWD/__app__.jar \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       1><LOG_DIR>/stdout \ 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]       2><LOG_DIR>/stderr
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]   resources:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]     __spark_libs__ -> resource { scheme: "hdfs" host: "172.18.0.3" port: -1 file: "/user/root/.sparkStaging/application_1636903352801_0001/__spark_libs__4680140580001336232.zip" } size: 397064094 timestamp: 1636903364264 type: ARCHIVE visibility: PRIVATE
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr]     __spark_conf__ -> resource { scheme: "hdfs" host: "172.18.0.3" port: -1 file: "/user/root/.sparkStaging/application_1636903352801_0001/__spark_conf__.zip" } size: 251856 timestamp: 1636903364492 type: ARCHIVE visibility: PRIVATE
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] ===============================================================================
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] 21/11/14 15:22:52 INFO Configuration: resource-types.xml not found
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] 21/11/14 15:22:52 INFO ResourceUtils: Unable to find 'resource-types.xml'.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] 21/11/14 15:22:52 INFO YarnAllocator: Will request 2 executor container(s), each with 1 core(s) and 3287 MB memory (including 1239 MB of overhead)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] 21/11/14 15:22:52 INFO YarnAllocator: Submitted 2 unlocalized container requests.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000001/stderr] 21/11/14 15:22:52 INFO2021-11-14 15:22:52,932 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1636903352801_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=
[36malgo-1    |[0m 2021-11-14 15:22:52,933 INFO rmcontainer.RMContainerImpl: container_1636903352801_0001_01_000002 Container Transitioned from NEW to ALLOCATED
[36malgo-1    |[0m 2021-11-14 15:22:52,933 INFO fica.FiCaSchedulerNode: Assigned container container_1636903352801_0001_01_000002 of capacity <memory:3287, vCores:1> on host algo-2:34827, which has 1 containers, <memory:3287, vCores:1> used and <memory:12605, vCores:3> available after allocation
[36malgo-1    |[0m 2021-11-14 15:22:52,933 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636903352801_0001	CONTAINERID=container_1636903352801_0001_01_000002	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 15:22:52,934 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.1316071 absoluteUsedCapacity=0.1316071 used=<memory:4183, vCores:2> cluster=<memory:31784, vCores:8>
[36malgo-1    |[0m 2021-11-14 15:22:52,934 INFO capacity.CapacityScheduler: Allocation proposal accepted
[36malgo-1    |[0m 2021-11-14 15:22:52,950 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1636903352801_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=
[36malgo-1    |[0m 2021-11-14 15:22:52,951 INFO rmcontainer.RMContainerImpl: container_1636903352801_0001_01_000003 Container Transitioned from NEW to ALLOCATED
[36malgo-1    |[0m 2021-11-14 15:22:52,951 INFO fica.FiCaSchedulerNode: Assigned container container_1636903352801_0001_01_000003 of capacity <memory:3287, vCores:1> on host algo-1:35251, which has 2 containers, <memory:4183, vCores:2> used and <memory:11709, vCores:2> available after allocation
[36malgo-1    |[0m 2021-11-14 15:22:52,951 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636903352801_0001	CONTAINERID=container_1636903352801_0001_01_000003	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 15:22:52,951 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.23502392 absoluteUsedCapacity=0.23502392 used=<memory:7470, vCores:3> cluster=<memory:31784, vCores:8>
[36malgo-1    |[0m 2021-11-14 15:22:52,951 INFO capacity.CapacityScheduler: Allocation proposal accepted
[36malgo-1    |[0m 21/11/14 15:22:52 INFO InMemoryFileIndex: It took 28 ms to list leaf files for 1 paths.
[36malgo-1    |[0m 21/11/14 15:22:53 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[36malgo-1    |[0m 2021-11-14 15:22:53,049 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-2:34827 for container : container_1636903352801_0001_01_000002
[36malgo-1    |[0m 2021-11-14 15:22:53,050 INFO rmcontainer.RMContainerImpl: container_1636903352801_0001_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
[36malgo-1    |[0m 2021-11-14 15:22:53,051 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:35251 for container : container_1636903352801_0001_01_000003
[36malgo-1    |[0m 2021-11-14 15:22:53,052 INFO rmcontainer.RMContainerImpl: container_1636903352801_0001_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
[36malgo-1    |[0m 2021-11-14 15:22:53,171 INFO ipc.Server: Auth successful for appattempt_1636903352801_0001_000001 (auth:SIMPLE)
[36malgo-1    |[0m 2021-11-14 15:22:53,175 INFO containermanager.ContainerManagerImpl: Start request for container_1636903352801_0001_01_000003 by user root
[36malgo-1    |[0m 2021-11-14 15:22:53,177 INFO nodemanager.NMAuditLogger: USER=root	IP=172.18.0.3	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1636903352801_0001	CONTAINERID=container_1636903352801_0001_01_000003
[36malgo-1    |[0m 2021-11-14 15:22:53,177 INFO application.ApplicationImpl: Adding container_1636903352801_0001_01_000003 to application application_1636903352801_0001
[36malgo-1    |[0m 2021-11-14 15:22:53,178 INFO container.ContainerImpl: Container container_1636903352801_0001_01_000003 transitioned from NEW to LOCALIZING
[36malgo-1    |[0m 2021-11-14 15:22:53,178 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1636903352801_0001
[36malgo-1    |[0m 2021-11-14 15:22:53,178 INFO container.ContainerImpl: Container container_1636903352801_0001_01_000003 transitioned from LOCALIZING to SCHEDULED
[36malgo-1    |[0m 2021-11-14 15:22:53,178 INFO scheduler.ContainerScheduler: Starting container [container_1636903352801_0001_01_000003]
[36malgo-1    |[0m 2021-11-14 15:22:53,192 INFO container.ContainerImpl: Container container_1636903352801_0001_01_000003 transitioned from SCHEDULED to RUNNING
[36malgo-1    |[0m 2021-11-14 15:22:53,192 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1636903352801_0001_01_000003
[36malgo-1    |[0m 2021-11-14 15:22:53,196 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/container_1636903352801_0001_01_000003/default_container_executor.sh]
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/prelaunch.out
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/prelaunch.err
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/launch_container.sh
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/directory.info
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stdout
[36malgo-1    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr
[33malgo-2    |[0m 2021-11-14 15:22:53,243 INFO ipc.Server: Auth successful for appattempt_1636903352801_0001_000001 (auth:SIMPLE)
[33malgo-2    |[0m 2021-11-14 15:22:53,322 INFO containermanager.ContainerManagerImpl: Start request for container_1636903352801_0001_01_000002 by user root
[33malgo-2    |[0m 2021-11-14 15:22:53,369 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1636903352801_0001
[33malgo-2    |[0m 2021-11-14 15:22:53,377 INFO nodemanager.NMAuditLogger: USER=root	IP=172.18.0.3	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1636903352801_0001	CONTAINERID=container_1636903352801_0001_01_000002
[33malgo-2    |[0m 2021-11-14 15:22:53,377 INFO application.ApplicationImpl: Application application_1636903352801_0001 transitioned from NEW to INITING
[33malgo-2    |[0m 2021-11-14 15:22:53,378 INFO application.ApplicationImpl: Adding container_1636903352801_0001_01_000002 to application application_1636903352801_0001
[33malgo-2    |[0m 2021-11-14 15:22:53,383 INFO application.ApplicationImpl: Application application_1636903352801_0001 transitioned from INITING to RUNNING
[33malgo-2    |[0m 2021-11-14 15:22:53,387 INFO container.ContainerImpl: Container container_1636903352801_0001_01_000002 transitioned from NEW to LOCALIZING
[33malgo-2    |[0m 2021-11-14 15:22:53,387 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1636903352801_0001
[33malgo-2    |[0m 2021-11-14 15:22:53,399 INFO localizer.ResourceLocalizationService: Created localizer for container_1636903352801_0001_01_000002
[33malgo-2    |[0m 2021-11-14 15:22:53,465 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1636903352801_0001_01_000002.tokens
[33malgo-2    |[0m 2021-11-14 15:22:53,476 INFO nodemanager.DefaultContainerExecutor: Initializing user root
[33malgo-2    |[0m 2021-11-14 15:22:53,481 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1636903352801_0001_01_000002.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/container_1636903352801_0001_01_000002.tokens
[33malgo-2    |[0m 2021-11-14 15:22:53,481 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001
[36malgo-1    |[0m 2021-11-14 15:22:53,945 INFO rmcontainer.RMContainerImpl: container_1636903352801_0001_01_000002 Container Transitioned from ACQUIRED to RUNNING
[36malgo-1    |[0m 2021-11-14 15:22:53,952 INFO rmcontainer.RMContainerImpl: container_1636903352801_0001_01_000003 Container Transitioned from ACQUIRED to RUNNING
[36malgo-1    |[0m 2021-11-14 15:22:54,827 INFO monitor.ContainersMonitorImpl: container_1636903352801_0001_01_000003's ip = 172.18.0.3, and hostname = algo-1
[36malgo-1    |[0m 2021-11-14 15:22:54,833 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1636903352801_0001_01_000003 since CPU usage is not yet available.
[36malgo-1    |[0m 21/11/14 15:22:55 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 15:22:55 INFO FileSourceStrategy: Pushed Filters: 
[36malgo-1    |[0m 21/11/14 15:22:55 INFO FileSourceStrategy: Post-Scan Filters: 
[36malgo-1    |[0m 21/11/14 15:22:55 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[36malgo-1    |[0m 21/11/14 15:22:55 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 1239, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[36malgo-1    |[0m 21/11/14 15:22:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 317.1 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:22:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:22:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.3:46871 (size: 29.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:55 INFO SparkContext: Created broadcast 0 from json at HelloJavaSparkApp.java:46
[36malgo-1    |[0m 21/11/14 15:22:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 15:22:55 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 15:22:55 INFO SparkContext: Starting job: json at HelloJavaSparkApp.java:46
[36malgo-1    |[0m 21/11/14 15:22:55 INFO DAGScheduler: Got job 0 (json at HelloJavaSparkApp.java:46) with 1 output partitions
[36malgo-1    |[0m 21/11/14 15:22:55 INFO DAGScheduler: Final stage: ResultStage 0 (json at HelloJavaSparkApp.java:46)
[36malgo-1    |[0m 21/11/14 15:22:55 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:22:55 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:22:55 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at HelloJavaSparkApp.java:46), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:22:56 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.4 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:22:56 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:22:56 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.3:46871 (size: 7.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:56 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 2021-11-14 15:22:56,075 INFO scheduler.AppSchedulingInfo: checking for deactivate of application :application_1636903352801_0001
[36malgo-1    |[0m 21/11/14 15:22:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at HelloJavaSparkApp.java:46) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 15:22:56 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:54 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 1268@algo-1
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:54 INFO SignalUtils: Registered signal handler for TERM
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:54 INFO SignalUtils: Registered signal handler for HUP
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:54 INFO SignalUtils: Registered signal handler for INT
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:54 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:54 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:54 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:54 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:55 INFO TransportClientFactory: Successfully created connection to /172.18.0.3:46495 after 97 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:55 INFO SecurityManager: Changing view acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:55 INFO SecurityManager: Changing modify acls to: root
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:55 INFO SecurityManager: Changing view acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:55 INFO SecurityManager: Changing modify acls groups to: 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:55 INFO TransportClientFactory: Successfully created connection to /172.18.0.3:46495 after 1 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:55 INFO DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/blockmgr-f2769ef5-ae6f-46cb-8b06-69ee914d2b6f
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:55 INFO MemoryStore: MemoryStore started with capacity 912.3 M21/11/14 15:22:56 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:43116) with ID 2
[36malgo-1    |[0m 21/11/14 15:22:56 INFO BlockManagerMasterEndpoint: Registering block manager algo-1:34059 with 912.3 MiB RAM, BlockManagerId(2, algo-1, 34059, None)
[36malgo-1    |[0m 21/11/14 15:22:56 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7755 bytes)
[36malgo-1    |[0m 21/11/14 15:22:56 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:34059 (size: 7.2 KiB, free: 912.3 MiB)
[33malgo-2    |[0m 2021-11-14 15:22:57,013 INFO container.ContainerImpl: Container container_1636903352801_0001_01_000002 transitioned from LOCALIZING to SCHEDULED
[33malgo-2    |[0m 2021-11-14 15:22:57,014 INFO scheduler.ContainerScheduler: Starting container [container_1636903352801_0001_01_000002]
[33malgo-2    |[0m 2021-11-14 15:22:57,045 INFO container.ContainerImpl: Container container_1636903352801_0001_01_000002 transitioned from SCHEDULED to RUNNING
[33malgo-2    |[0m 2021-11-14 15:22:57,046 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1636903352801_0001_01_000002
[33malgo-2    |[0m 2021-11-14 15:22:57,051 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/container_1636903352801_0001_01_000002/default_container_executor.sh]
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/prelaunch.out
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/prelaunch.err
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/launch_container.sh
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/directory.info
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stdout
[33malgo-2    |[0m Handling create event for file: /var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr
[36malgo-1    |[0m iB
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@172.18.0.3:46495
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO ResourceUtils: ==============================================================
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO ResourceUtils: Resources for spark.executor:
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO ResourceUtils: ==============================================================
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO YarnCoarseGrainedExecutorBackend: Successfully registered with driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO Executor: Starting executor ID 2 on host algo-1
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34059.
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO NettyBlockTransferService: Server created on algo-1:34059
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(2, algo-1, 34059, None)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(2, algo-1, 34059, None)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(2, algo-1, 34059, None)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO MetricsConfig: Loaded properties from hadoop-metrics2.properties
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 0
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO MetricsSystemImpl: s3a-file-system metrics system started
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO Executor: Fetching spark://172.18.0.3:46495/jars/hello-java-spark-1.0-SNAPSHOT.jar with timestamp 1636903355855
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO TransportClientFactory: Successfully created connection to /172.18.0.3:46495 after 1 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m [/var/log/yarn21/11/14 15:22:58 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:34059 (size: 29.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:22:58 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1821 ms on algo-1 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 15:22:58 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:22:58 INFO DAGScheduler: ResultStage 0 (json at HelloJavaSparkApp.java:46) finished in 2.377 s
[36malgo-1    |[0m 21/11/14 15:22:58 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:22:58 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished
[36malgo-1    |[0m 21/11/14 15:22:58 INFO DAGScheduler: Job 0 finished: json at HelloJavaSparkApp.java:46, took 2.425610 s
[36malgo-1    |[0m root
[36malgo-1    |[0m  |-- date: string (nullable = true)
[36malgo-1    |[0m  |-- sale: long (nullable = true)
[36malgo-1    |[0m 
[36malgo-1    |[0m 21/11/14 15:22:58 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.3:46871 in memory (size: 7.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:58 INFO BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:34059 in memory (size: 7.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:22:58 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.3:46871 in memory (size: 29.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:58 INFO BlockManagerInfo: Removed broadcast_0_piece0 on algo-1:34059 in memory (size: 29.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:22:58 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 15:22:58 INFO FileSourceStrategy: Pushed Filters: IsNotNull(sale),GreaterThan(sale,750)
[36malgo-1    |[0m 21/11/14 15:22:58 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(sale#8L),(sale#8L > 750)
[36malgo-1    |[0m 21/11/14 15:22:58 INFO FileSourceStrategy: Output Data Schema: struct<date: string, sale: bigint>
[33malgo-2    |[0m 2021-11-14 15:22:58,902 INFO monitor.ContainersMonitorImpl: container_1636903352801_0001_01_000002's ip = 172.18.0.2, and hostname = algo-2
[33malgo-2    |[0m 2021-11-14 15:22:58,908 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1636903352801_0001_01_000002 since CPU usage is not yet available.
[36malgo-1    |[0m 21/11/14 15:22:59 INFO CodeGenerator: Code generated in 280.449127 ms
[36malgo-1    |[0m 21/11/14 15:22:59 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 317.1 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:22:59 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:22:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.3:46871 (size: 29.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:59 INFO SparkContext: Created broadcast 2 from show at HelloJavaSparkApp.java:52
[36malgo-1    |[0m 21/11/14 15:22:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 15:22:59 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 15:22:59 INFO SparkContext: Starting job: show at HelloJavaSparkApp.java:52
[36malgo-1    |[0m 21/11/14 15:22:59 INFO DAGScheduler: Got job 1 (show at HelloJavaSparkApp.java:52) with 1 output partitions
[36malgo-1    |[0m 21/11/14 15:22:59 INFO DAGScheduler: Final stage: ResultStage 1 (show at HelloJavaSparkApp.java:52)
[36malgo-1    |[0m 21/11/14 15:22:59 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:22:59 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:22:59 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at show at HelloJavaSparkApp.java:52), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:22:59 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 18.4 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:22:59 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.1 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:22:59 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.3:46871 (size: 9.1 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:22:59 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:22:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at show at HelloJavaSparkApp.java:52) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 15:22:59 INFO YarnScheduler: Adding task set 1.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 15:22:59 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7755 bytes)
[36malgo-1    |[0m 21/11/14 15:22:59 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:34059 (size: 9.1 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:22:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:34059 (size: 29.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:22:59 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 362 ms on algo-1 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 15:22:59 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:22:59 INFO DAGScheduler: ResultStage 1 (show at HelloJavaSparkApp.java:52) finished in 0.371 s
[36malgo-1    |[0m 21/11/14 15:22:59 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:22:59 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
[36malgo-1    |[0m 21/11/14 15:22:59 INFO DAGScheduler: Job 1 finished: show at HelloJavaSparkApp.java:52, took 0.375358 s
[36malgo-1    |[0m 21/11/14 15:22:59 INFO CodeGenerator: Code generated in 16.872265 ms
[36malgo-1    |[0m +----------+----+
[36malgo-1    |[0m |      date|sale|
[36malgo-1    |[0m +----------+----+
[36malgo-1    |[0m |2020-01-03| 999|
[36malgo-1    |[0m |2020-01-02| 999|
[36malgo-1    |[0m |2020-01-06| 998|
[36malgo-1    |[0m |2020-01-05| 998|
[36malgo-1    |[0m |2020-01-07| 996|
[36malgo-1    |[0m |2020-01-07| 994|
[36malgo-1    |[0m |2020-01-07| 993|
[36malgo-1    |[0m |2020-01-01| 993|
[36malgo-1    |[0m |2020-01-02| 991|
[36malgo-1    |[0m |2020-01-05| 990|
[36malgo-1    |[0m |2020-01-07| 990|
[36malgo-1    |[0m |2020-01-03| 989|
[36malgo-1    |[0m |2020-01-04| 988|
[36malgo-1    |[0m |2020-01-05| 988|
[36malgo-1    |[0m |2020-01-07| 985|
[36malgo-1    |[0m |2020-01-03| 985|
[36malgo-1    |[0m |2020-01-04| 982|
[36malgo-1    |[0m |2020-01-02| 981|
[36malgo-1    |[0m |2020-01-03| 980|
[36malgo-1    |[0m |2020-01-05| 979|
[36malgo-1    |[0m +----------+----+
[36malgo-1    |[0m only showing top 20 rows
[36malgo-1    |[0m 
[36malgo-1    |[0m 21/11/14 15:22:59 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 15:22:59 INFO FileSourceStrategy: Pushed Filters: 
[36malgo-1    |[0m 21/11/14 15:22:59 INFO FileSourceStrategy: Post-Scan Filters: 
[36malgo-1    |[0m 21/11/14 15:22:59 INFO FileSourceStrategy: Output Data Schema: struct<date: string, sale: bigint>
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:22:58 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 420@algo-2
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:22:58 INFO SignalUtils: Registered signal handler for TERM
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:22:58 INFO SignalUtils: Registered signal handler for HUP
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:22:58 INFO SignalUtils: Registered signal handler for INT
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:22:58 INFO SecurityManager: Changing view acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:22:58 INFO SecurityManager: Changing modify acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:22:58 INFO SecurityManager: Changing view acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:22:58 INFO SecurityManager: Changing modify acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:22:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:22:59 INFO TransportClientFactory: Successfully created connection to /172.18.0.3:46495 after 94 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:22:59 INFO SecurityManager: Changing view acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:22:59 INFO SecurityManager: Changing modify acls to: root
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:22:59 INFO SecurityManager: Changing view acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:22:59 INFO SecurityManager: Changing modify acls groups to: 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:22:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:22:59 INFO TransportClientFactory: Successfully created connection to /172.18.0.3:46495 after 1 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:22:59 INFO DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/blockmgr-9f856b88-5e95-4436-811e-118c66d48105
[36malgo-1    |[0m 21/11/14 15:23:00 INFO CodeGenerator: Code generated in 61.297373 ms
[36malgo-1    |[0m 21/11/14 15:23:00 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 317.1 KiB, free 1007.2 MiB)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.3:46871 (size: 29.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO SparkContext: Created broadcast 4 from collectAsList at HelloJavaSparkApp.java:55
[36malgo-1    |[0m 21/11/14 15:23:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 15:23:00 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 15:23:00 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:36336) with ID 1
[36malgo-1    |[0m 21/11/14 15:23:00 INFO DAGScheduler: Registering RDD 11 (collectAsList at HelloJavaSparkApp.java:55) as input to shuffle 0
[36malgo-1    |[0m 21/11/14 15:23:00 INFO DAGScheduler: Got map stage job 2 (collectAsList at HelloJavaSparkApp.java:55) with 1 output partitions
[36malgo-1    |[0m 21/11/14 15:23:00 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (collectAsList at HelloJavaSparkApp.java:55)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:23:00 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:23:00 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at collectAsList at HelloJavaSparkApp.java:55), which has no missing parents
[36malgo-1    |[0m /export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO Utils: Fetching spark://172.18.0.3:46495/jars/hello-java-spark-1.0-SNAPSHOT.jar to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/spark-94134763-9d09-4f07-b4c6-f80c0cf3fc76/fetchFileTemp6018766574596039215.tmp
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO Utils: Copying /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/spark-94134763-9d09-4f07-b4c6-f80c0cf3fc76/-4409432941636903355855_cache to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/container_1636903352801_0001_01_000003/./hello-java-spark-1.0-SNAPSHOT.jar
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO Executor: Adding file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/container_1636903352801_0001_01_000003/./hello-java-spark-1.0-SNAPSHOT.jar to class loader
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO TorrentBroadcast: Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO TransportClientFactory: Successfully created connection to /172.18.0.3:46871 after 1 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:56 INFO TorrentBroadcast: Reading broadcast variable 1 took 146 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.4 KiB, free 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:57 INFO FileScanRDD: TID: 0 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:58 INFO CodeGenerator: Code generated in 363.792267 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:58 INFO TorrentBroadcast: Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:58 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 912.2 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:58 INFO TorrentBroadcast: Reading broadcast variable 0 took 9 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:58 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 445.1 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:58 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2013 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:59 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 1
[36malgo-1    |[0m [/var/log/yarn/export/21/11/14 15:23:00 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 29.7 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.9 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.3:46871 (size: 13.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:23:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at collectAsList at HelloJavaSparkApp.java:55) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 15:23:00 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 15:23:00 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7744 bytes)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:34059 (size: 13.9 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO BlockManagerMasterEndpoint: Registering block manager algo-2:44685 with 912.3 MiB RAM, BlockManagerId(1, algo-2, 44685, None)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:34059 (size: 29.9 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 330 ms on algo-1 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:23:00 INFO DAGScheduler: ShuffleMapStage 2 (collectAsList at HelloJavaSparkApp.java:55) finished in 0.351 s
[36malgo-1    |[0m 21/11/14 15:23:00 INFO DAGScheduler: looking for newly runnable stages
[36malgo-1    |[0m 21/11/14 15:23:00 INFO DAGScheduler: running: Set()
[36malgo-1    |[0m 21/11/14 15:23:00 INFO DAGScheduler: waiting: Set()
[36malgo-1    |[0m 21/11/14 15:23:00 INFO DAGScheduler: failed: Set()
[36malgo-1    |[0m 21/11/14 15:23:00 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 16.
[36malgo-1    |[0m 21/11/14 15:23:00 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.18.0.3:46871 in memory (size: 13.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:34059 in memory (size: 13.9 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.18.0.3:46871 in memory (size: 9.1 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:34059 in memory (size: 9.1 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.18.0.3:46871 in memory (size: 29.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO BlockManagerInfo: Removed broadcast_2_piece0 on algo-1:34059 in memory (size: 29.9 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO CodeGenerator: Code generated in 24.449947 ms
[36malgo-1    |[0m 21/11/14 15:23:00 INFO CodeGenerator: Code generated in 12.446294 ms
[36malgo-1    |[0m 21/11/14 15:23:00 INFO SparkContext: Starting job: collectAsList at HelloJavaSparkApp.java:55
[36malgo-1    |[0m 21/11/14 15:23:00 INFO DAGScheduler: Got job 3 (collectAsList at HelloJavaSparkApp.java:55) with 15 output partitions
[36malgo-1    |[0m 21/11/14 15:23:00 INFO DAGScheduler: Final stage: ResultStage 4 (collectAsList at HelloJavaSparkApp.java:55)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:23:00 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[16] at collectAsList at HelloJavaSparkApp.java:55), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:23:00 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 31.1 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.8 KiB, free 1007.5 MiB)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.3:46871 (size: 14.8 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:23:00 INFO DAGScheduler: Submitting 15 missing tasks from ResultStage 4 (MapPartitionsRDD[16] at collectAsList at HelloJavaSparkApp.java:55) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[36malgo-1    |[0m 21/11/14 15:23:00 INFO YarnScheduler: Adding task set 4.0 with 15 tasks
[36malgo-1    |[0m 21/11/14 15:23:00 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3, algo-2, executor 1, partition 0, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 4, algo-1, executor 2, partition 1, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:34059 (size: 14.8 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:23:00 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.3:43116
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 5, algo-1, executor 2, partition 2, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 4) in 248 ms on algo-1 (executor 2) (1/15)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 6, algo-1, executor 2, partition 3, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 5) in 29 ms on algo-1 (executor 2) (2/15)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 7, algo-1, executor 2, partition 4, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 6) in 26 ms on algo-1 (executor 2) (3/15)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:22:59 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:00 INFO YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@172.18.0.3:46495
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:00 INFO ResourceUtils: ==============================================================
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:00 INFO ResourceUtils: Resources for spark.executor:
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:00 INFO ResourceUtils: ==============================================================
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:00 INFO YarnCoarseGrainedExecutorBackend: Successfully registered with driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:00 INFO Executor: Starting executor ID 1 on host algo-2
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44685.
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:00 INFO NettyBlockTransferService: Server created on algo-2:44685
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(1, algo-2, 44685, None)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(1, algo-2, 44685, None)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(1, algo-2, 44685, None)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:00 INFO MetricsConfig: Loaded properties from hadoop-metrics2.properties
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:00 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:00 INFO MetricsSystemImpl: s3a-file-system metrics system started
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:00 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 3
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:00 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:00 INFO Executor: Fetching spark://172.18.0.3:46495/jars/hello-java-spark-1.0-SNAPSHOT.jar with timestamp 1636903355855
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:00 INFO TransportClientFactory: Successfully created connection to /172.18.0.3:46495 after 1 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 8, algo-1, executor 2, partition 5, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 7) in 21 ms on algo-1 (executor 2) (4/15)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-2:44685 (size: 14.8 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 9, algo-1, executor 2, partition 6, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 8) in 22 ms on algo-1 (executor 2) (5/15)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 10, algo-1, executor 2, partition 7, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 9) in 18 ms on algo-1 (executor 2) (6/15)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 11, algo-1, executor 2, partition 8, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 10) in 20 ms on algo-1 (executor 2) (7/15)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 12, algo-1, executor 2, partition 9, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 11) in 18 ms on algo-1 (executor 2) (8/15)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 13, algo-1, executor 2, partition 10, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 12) in 25 ms on algo-1 (executor 2) (9/15)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 14, algo-1, executor 2, partition 11, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 13) in 22 ms on algo-1 (executor 2) (10/15)
[36malgo-1    |[0m userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:59 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:59 INFO TorrentBroadcast: Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:59 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.1 KiB, free 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:59 INFO TorrentBroadcast: Reading broadcast variable 3 took 10 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:59 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 18.4 KiB, free 912.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:59 INFO CodeGenerator: Code generated in 44.902724 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:59 INFO CodeGenerator: Code generated in 12.836199 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:59 INFO CodeGenerator: Code generated in 9.415189 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:59 INFO FileScanRDD: TID: 1 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:59 INFO CodeGenerator: Code generated in 10.889708 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:59 INFO TorrentBroadcast: Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:59 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 912.2 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:59 INFO TorrentBroadcast: Reading broadcast variable 2 took 10 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:59 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 445.1 KiB, free 911.7 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:22:59 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2327 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 2
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO TorrentBroadcast: Started reading broadcast variable 5 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.9 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO TorrentBroadcast: Reading broadcast variable 5 took 7 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 29.7 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO CodeGenerator: Code generated in 58.285938 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO CodeGenerator: Code generated in 19.955884 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO CodeGenerator: Code generated in 6.722879 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO CodeGenerator: Code generated in 8.909372 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO FileScanRDD: TID: 2 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO TorrentBroadcast: Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 879.5 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO TorrentBroadcast: Reading broadcast variable 4 took 10 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 445.1 KiB, free 879.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 4531 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 4
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO Executor: Running task 1.0 in stage 4.0 (TID 4)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO TorrentBroadcast: Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.8 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO TorrentBroadcast: Reading broadcast variable 6 took 11 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 31.1 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@172.18.0.3:46495)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO MapOutputTrackerWorker: Got the output locations
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:00 INFO CodeGenerator: Code generated in 25.340307 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Finished task 1.0 in stage 4.0 (TID 4). 3890 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 5
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Running task 2.0 in stage 4.0 (TID 5)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Finished task 2.0 in stage 4.0 (TID 5). 3723 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 6
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Running task 3.0 in stage 4.0 (TID 6)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Finished task 3.0 in stage 4.0 (TID 6). 3890 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 7
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Running task 4.0 in stage 4.0 (TID 7)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Finished task 4.0 in stage 4.0 (TID 7). 3723 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 8
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Running task 5.0 in stage 4.0 (TID 8)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Finished task 5.0 in stage 4.0 (TID 8). 3890 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 9
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Running task 6.0 in stage 4.0 (TID 9)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Finished task 6.0 in stage 4.0 (TID 9). 3723 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 10
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Running task 7.0 in stage 4.0 (TID 10)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Finished task 7.0 in stage 4.0 (TID 10). 3890 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 11
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Running task 8.0 in stage 4.0 (TID 11)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks includin21/11/14 15:23:01 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 15, algo-1, executor 2, partition 12, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 14) in 22 ms on algo-1 (executor 2) (11/15)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 16, algo-1, executor 2, partition 13, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 15) in 19 ms on algo-1 (executor 2) (12/15)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 17, algo-1, executor 2, partition 14, PROCESS_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 16) in 23 ms on algo-1 (executor 2) (13/15)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 17) in 18 ms on algo-1 (executor 2) (14/15)
[36malgo-1    |[0m 21/11/14 15:23:01 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.2:36336
[36malgo-1    |[0m g 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Finished task 8.0 in stage 4.0 (TID 11). 3723 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 12
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Running task 9.0 in stage 4.0 (TID 12)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Finished task 9.0 in stage 4.0 (TID 12). 3890 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 13
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Running task 10.0 in stage 4.0 (TID 13)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Finished task 10.0 in stage 4.0 (TID 13). 3723 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 14
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Running task 11.0 in stage 4.0 (TID 14)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Finished task 11.0 in stage 4.0 (TID 14). 3890 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 15
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Running task 12.0 in stage 4.0 (TID 15)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_163690321/11/14 15:23:02 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 1775 ms on algo-2 (executor 1) (15/15)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:23:02 INFO DAGScheduler: ResultStage 4 (collectAsList at HelloJavaSparkApp.java:55) finished in 1.784 s
[36malgo-1    |[0m 21/11/14 15:23:02 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:23:02 INFO YarnScheduler: Killing all running tasks in stage 4: Stage finished
[36malgo-1    |[0m 21/11/14 15:23:02 INFO DAGScheduler: Job 3 finished: collectAsList at HelloJavaSparkApp.java:55, took 1.793767 s
[36malgo-1    |[0m 21/11/14 15:23:02 INFO DAGScheduler: Registering RDD 17 (collectAsList at HelloJavaSparkApp.java:55) as input to shuffle 1
[36malgo-1    |[0m 21/11/14 15:23:02 INFO DAGScheduler: Got map stage job 4 (collectAsList at HelloJavaSparkApp.java:55) with 15 output partitions
[36malgo-1    |[0m 21/11/14 15:23:02 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (collectAsList at HelloJavaSparkApp.java:55)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:23:02 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[17] at collectAsList at HelloJavaSparkApp.java:55), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:23:02 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 31.9 KiB, free 1007.4 MiB)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 15.2 KiB, free 1007.4 MiB)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.3:46871 (size: 15.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:23:02 INFO DAGScheduler: Submitting 15 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[17] at collectAsList at HelloJavaSparkApp.java:55) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[36malgo-1    |[0m 21/11/14 15:23:02 INFO YarnScheduler: Adding task set 6.0 with 15 tasks
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 18, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 19, algo-2, executor 1, partition 1, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:34059 (size: 15.2 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-2:44685 (size: 15.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Starting task 2.0 in stage 6.0 (TID 20, algo-1, executor 2, partition 2, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 18) in 64 ms on algo-1 (executor 2) (1/15)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 21, algo-1, executor 2, partition 3, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Finished task 2.0 in stage 6.0 (TID 20) in 19 ms on algo-1 (executor 2) (2/15)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Starting task 4.0 in stage 6.0 (TID 22, algo-1, executor 2, partition 4, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Finished task 3.0 in stage 6.0 (TID 21) in 27 ms on algo-1 (executor 2) (3/15)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Starting task 5.0 in stage 6.0 (TID 23, algo-1, executor 2, partition 5, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Finished task 4.0 in stage 6.0 (TID 22) in 22 ms on algo-1 (executor 2) (4/15)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Starting task 6.0 in stage 6.0 (TID 24, algo-1, executor 2, partition 6, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Finished task 5.0 in stage 6.0 (TID 23) in 24 ms on algo-1 (executor 2) (5/15)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Starting task 7.0 in stage 6.0 (TID 25, algo-1, executor 2, partition 7, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Finished task 6.0 in stage 6.0 (TID 24) in 21 ms on algo-1 (executor 2) (6/15)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Starting task 8.0 in stage 6.0 (TID 26, algo-2, executor 1, partition 8, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 19) in 188 ms on algo-2 (executor 1) (7/15)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Starting task 9.0 in stage 6.0 (TID 27, algo-1, executor 2, partition 9, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Finished task 7.0 in stage 6.0 (TID 25) in 24 ms on algo-1 (executor 2) (8/15)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Starting task 10.0 in stage 6.0 (TID 28, algo-1, executor 2, partition 10, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Finished task 9.0 in stage 6.0 (TID 27) in 20 ms on algo-1 (executor 2) (9/15)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Starting task 11.0 in stage 6.0 (TID 29, algo-2, executor 1, partition 11, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Finished task 8.0 in stage 6.0 (TID 26) in 32 ms on algo-2 (executor 1) (10/15)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Starting task 12.0 in stage 6.0 (TID 30, algo-1, executor 2, partition 12, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Finished task 10.0 in stage 6.0 (TID 28) in 16 ms on algo-1 (executor 2) (11/15)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Starting task 13.0 in stage 6.0 (TID 31, algo-1, executor 2, partition 13, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Finished task 12.0 in stage 6.0 (TID 30) in 20 ms on algo-1 (executor 2) (12/15)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Starting task 14.0 in stage 6.0 (TID 32, algo-2, executor 1, partition 14, PROCESS_LOCAL, 7325 bytes)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Finished task 11.0 in stage 6.0 (TID 29) in 36 ms on algo-2 (executor 1) (13/15)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Finished task 13.0 in stage 6.0 (TID 31) in 18 ms on algo-1 (executor 2) (14/15)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Finished task 14.0 in stage 6.0 (TID 32) in 28 ms on algo-2 (executor 1) (15/15)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:23:02 INFO DAGScheduler: ShuffleMapStage 6 (collectAsList at HelloJavaSparkApp.java:55) finished in 0.296 s
[36malgo-1    |[0m 21/11/14 15:23:02 INFO DAGScheduler: looking for newly runnable stages
[36malgo-1    |[0m 21/11/14 15:23:02 INFO DAGScheduler: running: Set()
[36malgo-1    |[0m 21/11/14 15:23:02 INFO DAGScheduler: waiting: Set()
[36malgo-1    |[0m 21/11/14 15:23:02 INFO DAGScheduler: failed: Set()
[36malgo-1    |[0m 21/11/14 15:23:02 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 16.
[36malgo-1    |[0m 21/11/14 15:23:02 INFO CodeGenerator: Code generated in 14.05896 ms
[36malgo-1    |[0m 21/11/14 15:23:02 INFO SparkContext: Starting job: collectAsList at HelloJavaSparkApp.java:55
[36malgo-1    |[0m 21/11/14 15:23:02 INFO DAGScheduler: Got job 5 (collectAsList at HelloJavaSparkApp.java:55) with 7 output partitions
[36malgo-1    |[0m 21/11/14 15:23:02 INFO DAGScheduler: Final stage: ResultStage 9 (collectAsList at HelloJavaSparkApp.java:55)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:23:02 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[20] at collectAsList at HelloJavaSparkApp.java:55), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:23:02 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 26.7 KiB, free 1007.4 MiB)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 1007.4 MiB)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.3:46871 (size: 13.4 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:23:02 INFO DAGScheduler: Submitting 7 missing tasks from ResultStage 9 (MapPartitionsRDD[20] at collectAsList at HelloJavaSparkApp.java:55) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))
[36malgo-1    |[0m 21/11/14 15:23:02 INFO YarnScheduler: Adding task set 9.0 with 7 tasks
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 33, algo-1, executor 2, partition 0, NODE_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 34, algo-2, executor 1, partition 1, NODE_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:34059 (size: 13.4 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-2:44685 (size: 13.4 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:23:02 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.18.0.3:43116
[36malgo-1    |[0m 21/11/14 15:23:03 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.18.0.2:36336
[36malgo-1    |[0m 21/11/14 15:23:03 INFO TaskSetManager: Starting task 3.0 in stage 9.0 (TID 35, algo-1, executor 2, partition 3, NODE_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 33) in 77 ms on algo-1 (executor 2) (1/7)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO TaskSetManager: Starting task 4.0 in stage 9.0 (TID 36, algo-1, executor 2, partition 4, NODE_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO TaskSetManager: Finished task 3.0 in stage 9.0 (TID 35) in 14 ms on algo-1 (executor 2) (2/7)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO TaskSetManager: Starting task 5.0 in stage 9.0 (TID 37, algo-1, executor 2, partition 5, NODE_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO TaskSetManager: Finished task 4.0 in stage 9.0 (TID 36) in 18 ms on algo-1 (executor 2) (3/7)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:00 INFO Utils: Fetching spark://172.18.0.3:46495/jars/hello-java-spark-1.0-SNAPSHOT.jar to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/spark-7d41c8a1-d533-432c-84f1-3e7ff5ebaece/fetchFileTemp2971049183620591693.tmp
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:00 INFO Utils: Copying /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/spark-7d41c8a1-d533-432c-84f1-3e7ff5ebaece/-4409432941636903355855_cache to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/container_1636903352801_0001_01_000002/./hello-java-spark-1.0-SNAPSHOT.jar
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:00 INFO Executor: Adding file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/container_1636903352801_0001_01_000002/./hello-java-spark-1.0-SNAPSHOT.jar to class loader
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:00 INFO MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:01 INFO TorrentBroadcast: Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:01 INFO TransportClientFactory: Successfully created connection to /172.18.0.3:46871 after 1 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:01 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.8 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:01 INFO TorrentBroadcast: Reading broadcast variable 6 took 116 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:01 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 31.1 KiB, free 912.3 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:01 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:01 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@172.18.0.3:46495)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:01 INFO MapOutputTrackerWorker: Got the output locations
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO CodeGenerator: Code generated in 389.833005 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO CodeGenerator: Code generated in 11.737109 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO CodeGenerator: Code generated in 7.338174 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO CodeGenerator: Code generated in 8.679894 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 3766 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 19
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO Executor: Running task 1.0 in stage 6.0 (TID 19)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO TorrentBroadcast: Started reading broadcast variable 7 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 15.2 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO TorrentBroadcast: Reading broadcast variable 7 took 8 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 31.9 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO CodeGenerator: Code generated in 11.47307 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO TransportClientFactory: Successfully created connection to algo-1/172.18.0.3:34059 after 1 ms (0 ms spent in bootstraps)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 14 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO Executor: Finished task 1.0 in stage 6.0 (TID 19). 3902 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 26
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO Executor: Running task 8.0 in stage 6.0 (TID 26)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO Executor: Finished task 8.0 in stage 6.0 (TID 26). 3766 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 29
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO Executor: Running task 11.0 in stage 6.0 (TID 29)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO Executor: Finished task 11.0 in stage 6.0 (TID 29). 3902 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 32
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO Executor: Running task 14.0 in stage 6.0 (TID 32)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO Executor: Finished task 14.0 in stage 6.0 (TID 32). 3766 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 34
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO Executor: Running task 1.0 in stage 9.0 (TID 34)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO MapOutputTrackerWorker: Updating epoch to 2 and clearing cache
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO TorrentBroadcast: Started reading broadcast variable 8 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO TorrentBroadcast: Reading broadcast variable 8 took 10 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:02 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 26.7 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:03 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:03 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@172.18.0.3:46495)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:03 INFO MapOutputTrackerWorker: Got the output locations
[36malgo-1    |[0m 21/11/14 15:23:03 INFO TaskSetManager: Starting task 6.0 in stage 9.0 (TID 38, algo-1, executor 2, partition 6, NODE_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO TaskSetManager: Finished task 5.0 in stage 9.0 (TID 37) in 18 ms on algo-1 (executor 2) (4/7)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 39, algo-1, executor 2, partition 2, RACK_LOCAL, 7336 bytes)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO TaskSetManager: Finished task 6.0 in stage 9.0 (TID 38) in 17 ms on algo-1 (executor 2) (5/7)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 34) in 167 ms on algo-2 (executor 1) (6/7)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO TaskSetManager: Finished task 2.0 in stage 9.0 (TID 39) in 44 ms on algo-1 (executor 2) (7/7)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:23:03 INFO DAGScheduler: ResultStage 9 (collectAsList at HelloJavaSparkApp.java:55) finished in 0.235 s
[36malgo-1    |[0m 21/11/14 15:23:03 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:23:03 INFO YarnScheduler: Killing all running tasks in stage 9: Stage finished
[36malgo-1    |[0m 21/11/14 15:23:03 INFO DAGScheduler: Job 5 finished: collectAsList at HelloJavaSparkApp.java:55, took 0.246082 s
[36malgo-1    |[0m 21/11/14 15:23:03 INFO CodeGenerator: Code generated in 8.86187 ms
[36malgo-1    |[0m Collected average sales: [[2020-01-01,490.1985294117647], [2020-01-02,472.625], [2020-01-03,474.6470588235294], [2020-01-04,491.67973856209153], [2020-01-05,471.24691358024694], [2020-01-06,518.7692307692307], [2020-01-07,533.9855072463768]]
[36malgo-1    |[0m 21/11/14 15:23:03 WARN SimpleFunctionRegistry: The function double replaced a previously registered function.
[36malgo-1    |[0m 352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Finished task 12.0 in stage 4.0 (TID 15). 3723 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 16
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Running task 13.0 in stage 4.0 (TID 16)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Finished task 13.0 in stage 4.0 (TID 16). 3890 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 17
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Running task 14.0 in stage 4.0 (TID 17)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:01 INFO Executor: Finished task 14.0 in stage 4.0 (TID 17). 3723 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 18
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO Executor: Running task 0.0 in stage 6.0 (TID 18)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO TorrentBroadcast: Started reading broadcast variable 7 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 15.2 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO TorrentBroadcast: Reading broadcast variable 7 took 7 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 31.9 KiB, free 911.7 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO CodeGenerator: Code generated in 8.582389 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO Executor: Finished task 0.0 in stage 6.0 (TID 18). 3766 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 20
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO Executor: Running task 2.0 in stage 6.0 (TID 20)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO Executor: Finished task 2.0 in stage 6.0 (TID 20). 3766 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 21
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO Executor: Running task 3.0 in stage 6.0 (TID 21)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO Executor: Finished task 3.0 in stage 6.0 (TID 21). 3902 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 22
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO Executor: Running task 4.0 in stage 6.0 (TID 22)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO Executor: Finished task 4.0 in stage 6.0 (TID 22). 3766 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 23
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO Executor: Running task 5.0 in stage 6.0 (TID 23)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO Executor: Finished task 5.0 in stage 6.0 (TID 23). 3902 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 24
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO Executor: Running task 6.0 in stage 6.0 (TID 24)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO Executor: Finished task 6.0 in stage 6.0 (TID 24). 3766 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 25
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO Executor: Running task 7.0 in stage 6.0 (TID 25)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO Executor: Finished task 7.0 in stage 6.0 (TID 25). 3902 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 27
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO Executor: Running task 9.0 in stage 6.0 (TID 27)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO Executor: Finished task 9.0 in stage 6.0 (TID 27). 3902 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 28
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO Executor: Running task 10.0 in stage 6.0 (TID 28)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO Executor: Finished task 10.0 in stage 6.0 (TID 28). 3766 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 30
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO Executor: Running task 12.0 in stage 6.0 (TID 30)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO Executor: Finished task 12.0 in stage 6.0 (TID 30). 3766 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 31
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO Executor: Running task 13.0 in stage 6.0 (TID 31)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO Executor: Finished task 13.0 in stage 6.0 (TID 31). 3902 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 33
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO Executor: Running task 0.0 in stage 9.0 (TID 33)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO MapOutputTrackerWorker: Updating epoch to 2 and clearing cache
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO TorrentBroadcast: Started reading broadcast variable 8 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 911.7 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO TorrentBroadcast: Reading broadcast variable 8 took 10 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 26.7 KiB, free 911.7 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@172.18.0.3:46495)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO MapOutputTrackerWorker: Got the output locations
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO CodeGenerator: Code generated in 15.962176 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO CodeGenerator: Code generated in 18.952816 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO Executor: Finished task 0.0 in stage 9.0 (TID 33). 4645 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 35
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO Executor: Running task 3.0 in stage 9.0 (TID 35)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO Executor: Finished task 3.0 in stage 9.0 (TID 35). 4646 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 36
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO Executor: Running task 4.0 in stage 9.0 (TID 36)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO Executor: Finished task 4.0 in stage 9.0 (TID 36). 4646 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 37
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO Executor: Running task 5.0 in stage 9.0 (TID 37)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO Executor: Finished task 5.0 in stage 9.0 (TID 37). 4646 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 38
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO Executor: Running task 6.0 in stage 9.0 (TID 38)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO Executor: Finished task 6.0 in stage 9.0 (TID 38). 4646 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 39
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO Executor: Running task 2.0 in stage 9.0 (TID 39)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 1 (88.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO TransportClientFactory: Successfully created connection to algo-2/172.18.0.2:44685 after 1 ms (0 ms spent in bootstraps)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 14 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 INFO Executor: Finished task 2.0 in stage 9.0 (TID 39). 4645 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:03 WARN YarnCoarseGrainedExecutorBackend: eagerFSInit: Unable to eagerly init filesystem s3://does/not/exist
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] java.nio.file.AccessDeniedException: does: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@2a962436: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@5e7f90ad: Failed to connect to service endpoint: ]
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:187)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:111)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:380)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:314)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3358)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:123)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3407)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3375)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:486)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at org.apache.spark.executor.CoarseGrainedExecutorBackend.$anonfun$eagerFSInit$1(CoarseGrainedExecutorBackend.scala:274)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at scala.collection.parallel.mutable.ParArray$ParArrayIterator.flatmap2combiner(ParArray.scala:419)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at scala.collection.parallel.ParIterableLike$FlatMap.leaf(ParIterableLike.scala:1074)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at scala.collection.parallel.ParIterableLike$FlatMap.tryLeaf(ParIterableLike.scala:1070)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at scala.collection.parallel.FutureTasks.$anonfun$exec$5(Tasks.scala:499)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at scala.util.Success.$anonfun$map$1(Try.scala:255)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at scala.util.Success.map(Try.scala:213)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at java.lang.Thread.run(Thread.java:748)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] Caused by: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@2a962436: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@5e7f90ad: Failed to connect to service endpoint: ]
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:159)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1257)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:833)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:783)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5062)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:5850)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:5823)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5046)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5008)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1416)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1352)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$verifyBucketExists$1(S3AFileSystem.java:381)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	... 32 more
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] Caused by: com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and ro21/11/14 15:23:03 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 15:23:03 INFO FileSourceStrategy: Pushed Filters: 
[36malgo-1    |[0m 21/11/14 15:23:03 INFO FileSourceStrategy: Post-Scan Filters: 
[36malgo-1    |[0m 21/11/14 15:23:03 INFO FileSourceStrategy: Output Data Schema: struct<date: string, sale: bigint>
[36malgo-1    |[0m 21/11/14 15:23:03 INFO CodeGenerator: Code generated in 10.514269 ms
[36malgo-1    |[0m 21/11/14 15:23:03 INFO CodeGenerator: Code generated in 16.220317 ms
[36malgo-1    |[0m 21/11/14 15:23:03 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 317.1 KiB, free 1007.1 MiB)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 1007.0 MiB)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.18.0.3:46871 (size: 29.9 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO SparkContext: Created broadcast 9 from show at HelloJavaSparkApp.java:63
[36malgo-1    |[0m 21/11/14 15:23:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 15:23:03 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 15:23:03 INFO SparkContext: Starting job: show at HelloJavaSparkApp.java:63
[36malgo-1    |[0m 21/11/14 15:23:03 INFO DAGScheduler: Got job 6 (show at HelloJavaSparkApp.java:63) with 1 output partitions
[36malgo-1    |[0m 21/11/14 15:23:03 INFO DAGScheduler: Final stage: ResultStage 10 (show at HelloJavaSparkApp.java:63)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:23:03 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:23:03 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[25] at show at HelloJavaSparkApp.java:63), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:23:03 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 15.8 KiB, free 1007.0 MiB)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 1007.0 MiB)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.18.0.3:46871 (size: 7.6 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:23:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[25] at show at HelloJavaSparkApp.java:63) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 15:23:03 INFO YarnScheduler: Adding task set 10.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 15:23:03 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 40, algo-2, executor 1, partition 0, PROCESS_LOCAL, 7755 bytes)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-2:44685 (size: 7.6 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-2:44685 (size: 29.9 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 40) in 514 ms on algo-2 (executor 1) (1/1)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:23:03 INFO DAGScheduler: ResultStage 10 (show at HelloJavaSparkApp.java:63) finished in 0.523 s
[36malgo-1    |[0m 21/11/14 15:23:03 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:23:03 INFO YarnScheduler: Killing all running tasks in stage 10: Stage finished
[36malgo-1    |[0m 21/11/14 15:23:03 INFO DAGScheduler: Job 6 finished: show at HelloJavaSparkApp.java:63, took 0.526314 s
[36malgo-1    |[0m 21/11/14 15:23:03 INFO CodeGenerator: Code generated in 10.026263 ms
[36malgo-1    |[0m 21/11/14 15:23:03 INFO CodeGenerator: Code generated in 7.617696 ms
[36malgo-1    |[0m +----------+----+-----------+
[36malgo-1    |[0m |      date|sale|sale_double|
[36malgo-1    |[0m +----------+----+-----------+
[36malgo-1    |[0m |2020-01-01|   5|         10|
[36malgo-1    |[0m |2020-01-01|   7|         14|
[36malgo-1    |[0m |2020-01-01|  15|         30|
[36malgo-1    |[0m |2020-01-01|  15|         30|
[36malgo-1    |[0m |2020-01-01|  23|         46|
[36malgo-1    |[0m |2020-01-01|  33|         66|
[36malgo-1    |[0m |2020-01-01|  34|         68|
[36malgo-1    |[0m |2020-01-01|  39|         78|
[36malgo-1    |[0m |2020-01-01|  40|         80|
[36malgo-1    |[0m |2020-01-01|  43|         86|
[36malgo-1    |[0m |2020-01-01|  48|         96|
[36malgo-1    |[0m |2020-01-01|  59|        118|
[36malgo-1    |[0m |2020-01-01|  64|        128|
[36malgo-1    |[0m |2020-01-01|  79|        158|
[36malgo-1    |[0m |2020-01-01|  82|        164|
[36malgo-1    |[0m |2020-01-01|  85|        170|
[36malgo-1    |[0m |2020-01-01| 103|        206|
[36malgo-1    |[0m |2020-01-01| 105|        210|
[36malgo-1    |[0m |2020-01-01| 105|        210|
[36malgo-1    |[0m |2020-01-01| 122|        244|
[36malgo-1    |[0m +----------+----+-----------+
[36malgo-1    |[0m only showing top 20 rows
[36malgo-1    |[0m 
[36malgo-1    |[0m Writing output to: file:///opt/ml/processing/output/data
[36malgo-1    |[0m 21/11/14 15:23:03 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.18.0.3:46871 in memory (size: 13.4 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:34059 in memory (size: 13.4 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO FileSourceStrategy: Pruning directories with: 
[36malgo-1    |[0m 21/11/14 15:23:03 INFO FileSourceStrategy: Pushed Filters: 
[36malgo-1    |[0m 21/11/14 15:23:03 INFO FileSourceStrategy: Post-Scan Filters: 
[36malgo-1    |[0m 21/11/14 15:23:03 INFO FileSourceStrategy: Output Data Schema: struct<date: string, sale: bigint>
[36malgo-1    |[0m 21/11/14 15:23:03 INFO BlockManagerInfo: Removed broadcast_8_piece0 on algo-2:44685 in memory (size: 13.4 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.18.0.3:46871 in memory (size: 14.8 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:34059 in memory (size: 14.8 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO BlockManagerInfo: Removed broadcast_6_piece0 on algo-2:44685 in memory (size: 14.8 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.18.0.3:46871 in memory (size: 15.2 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:34059 in memory (size: 15.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO BlockManagerInfo: Removed broadcast_7_piece0 on algo-2:44685 in memory (size: 15.2 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO BlockManagerInfo: Removed broadcast_10_piece0 on algo-2:44685 in memory (size: 7.6 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.18.0.3:46871 in memory (size: 7.6 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:23:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
[36malgo-1    |[0m 21/11/14 15:23:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[36malgo-1    |[0m 21/11/14 15:23:03 INFO DirectFileOutputCommitter: Direct Write: DISABLED
[36malgo-1    |[0m 21/11/14 15:23:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter
[36malgo-1    |[0m 21/11/14 15:23:04 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 317.1 KiB, free 1006.9 MiB)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.18.0.3:46871 (size: 29.9 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO SparkContext: Created broadcast 11 from json at HelloJavaSparkApp.java:66
[36malgo-1    |[0m 21/11/14 15:23:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
[36malgo-1    |[0m 21/11/14 15:23:04 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
[36malgo-1    |[0m 21/11/14 15:23:04 INFO SparkContext: Starting job: json at HelloJavaSparkApp.java:66
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: Got job 7 (json at HelloJavaSparkApp.java:66) with 1 output partitions
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: Final stage: ResultStage 11 (json at HelloJavaSparkApp.java:66)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[31] at json at HelloJavaSparkApp.java:66), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:23:04 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 15.1 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 1006.8 MiB)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.18.0.3:46871 (size: 7.5 KiB, free: 1007.8 MiB)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[31] at json at HelloJavaSparkApp.java:66) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 15:23:04 INFO YarnScheduler: Adding task set 11.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 15:23:04 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 41, algo-2, executor 1, partition 0, PROCESS_LOCAL, 7755 bytes)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-2:44685 (size: 7.5 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-2:44685 (size: 29.9 KiB, free: 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:03 INFO ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:03 INFO CodeGenerator: Code generated in 21.600944 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:03 INFO CodeGenerator: Code generated in 17.042586 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:03 INFO Executor: Finished task 1.0 in stage 9.0 (TID 34). 4642 bytes result sent to driver
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:03 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 40
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:03 INFO Executor: Running task 0.0 in stage 10.0 (TID 40)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:03 INFO TorrentBroadcast: Started reading broadcast variable 10 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:03 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 912.2 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:03 INFO TorrentBroadcast: Reading broadcast variable 10 took 6 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:03 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 15.8 KiB, free 912.1 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:03 INFO CodeGenerator: Code generated in 10.226241 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:03 INFO CodeGenerator: Code generated in 15.339449 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:03 INFO FileScanRDD: TID: 40 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:03 INFO CodeGenerator: Code generated in 9.801691 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:03 INFO TorrentBroadcast: Started reading broadcast variable 9 with 1 pieces (estimated total size 4.0 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:03 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 912.1 MiB)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:03 INFO TorrentBroadcast: Reading broadcast variable 9 took 7 ms
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:03 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 445.1 KiB, free 911.7 MiB)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 41) in 53 ms on algo-2 (executor 1) (1/1)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: ResultStage 11 (json at HelloJavaSparkApp.java:66) finished in 0.059 s
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:23:04 INFO YarnScheduler: Killing all running tasks in stage 11: Stage finished
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: Job 7 finished: json at HelloJavaSparkApp.java:66, took 0.061459 s
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: Registering RDD 32 (json at HelloJavaSparkApp.java:66) as input to shuffle 2
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: Got map stage job 8 (json at HelloJavaSparkApp.java:66) with 1 output partitions
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (json at HelloJavaSparkApp.java:66)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: Parents of final stage: List()
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[32] at json at HelloJavaSparkApp.java:66), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:23:04 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 70.4 KiB, free 1006.7 MiB)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 15.1 KiB, free 1006.7 MiB)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.18.0.3:46871 (size: 15.1 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[32] at json at HelloJavaSparkApp.java:66) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 15:23:04 INFO YarnScheduler: Adding task set 12.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 15:23:04 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 42, algo-1, executor 2, partition 0, PROCESS_LOCAL, 7744 bytes)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:34059 (size: 15.1 KiB, free: 912.3 MiB)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:34059 (size: 29.9 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 42) in 347 ms on algo-1 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO YarnScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: ShuffleMapStage 12 (json at HelloJavaSparkApp.java:66) finished in 0.354 s
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: looking for newly runnable stages
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: running: Set()
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: waiting: Set()
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: failed: Set()
[36malgo-1    |[0m 21/11/14 15:23:04 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 2155.
[36malgo-1    |[0m 21/11/14 15:23:04 INFO CodeGenerator: Code generated in 11.894284 ms
[36malgo-1    |[0m 21/11/14 15:23:04 INFO SparkContext: Starting job: json at HelloJavaSparkApp.java:66
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: Got job 9 (json at HelloJavaSparkApp.java:66) with 1 output partitions
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: Final stage: ResultStage 14 (json at HelloJavaSparkApp.java:66)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: Missing parents: List()
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: Submitting ResultStage 14 (CoalescedRDD[36] at json at HelloJavaSparkApp.java:66), which has no missing parents
[36malgo-1    |[0m 21/11/14 15:23:04 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 193.4 KiB, free 1006.5 MiB)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 71.6 KiB, free 1006.5 MiB)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.18.0.3:46871 (size: 71.6 KiB, free: 1007.7 MiB)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1240
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (CoalescedRDD[36] at json at HelloJavaSparkApp.java:66) (first 15 tasks are for partitions Vector(0))
[36malgo-1    |[0m 21/11/14 15:23:04 INFO YarnScheduler: Adding task set 14.0 with 1 tasks
[36malgo-1    |[0m 21/11/14 15:23:04 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 43, algo-1, executor 2, partition 0, NODE_LOCAL, 8312 bytes)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:34059 (size: 71.6 KiB, free: 912.2 MiB)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.18.0.3:43116
[36malgo-1    |[0m 21/11/14 15:23:04 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 43) in 275 ms on algo-1 (executor 2) (1/1)
[36malgo-1    |[0m 21/11/14 15:23:04 INFO YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool 
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: ResultStage 14 (json at HelloJavaSparkApp.java:66) finished in 0.295 s
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[36malgo-1    |[0m 21/11/14 15:23:04 INFO YarnScheduler: Killing all running tasks in stage 14: Stage finished
[36malgo-1    |[0m 21/11/14 15:23:04 INFO DAGScheduler: Job 9 finished: json at HelloJavaSparkApp.java:66, took 0.300233 s
[36malgo-1    |[0m 21/11/14 15:23:04 INFO FileFormatWriter: Write Job e0dc226e-513c-4dee-a4fa-cb2dd16df6f6 committed.
[36malgo-1    |[0m 21/11/14 15:23:04 INFO FileFormatWriter: Finished processing stats for write job e0dc226e-513c-4dee-a4fa-cb2dd16df6f6.
[36malgo-1    |[0m 21/11/14 15:23:04 INFO SparkUI: Stopped Spark web UI at http://172.18.0.3:4040
[36malgo-1    |[0m 21/11/14 15:23:04 INFO YarnClientSchedulerBackend: Interrupting monitor thread
[36malgo-1    |[0m 21/11/14 15:23:04 INFO YarnClientSchedulerBackend: Shutting down all executors
[36malgo-1    |[0m 21/11/14 15:23:04 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
[36malgo-1    |[0m 21/11/14 15:23:04 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
[36malgo-1    |[0m 21/11/14 15:23:04 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[36malgo-1    |[0m 21/11/14 15:23:04 INFO MemoryStore: MemoryStore cleared
[36malgo-1    |[0m 21/11/14 15:23:04 INFO BlockManager: BlockManager stopped
[36malgo-1    |[0m 21/11/14 15:23:04 INFO BlockManagerMaster: BlockManagerMaster stopped
[36malgo-1    |[0m 21/11/14 15:23:04 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[36malgo-1    |[0m 21/11/14 15:23:04 INFO SparkContext: Successfully stopped SparkContext
[36malgo-1    |[0m 21/11/14 15:23:04 INFO ShutdownHookManager: Shutdown hook called
[36malgo-1    |[0m 21/11/14 15:23:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-0ef40bb0-07a7-4f96-8d65-3f9f33365916
[36malgo-1    |[0m 21/11/14 15:23:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-1d6fa094-53dd-4f75-9949-8921c57c4a2f
[36malgo-1    |[0m 2021-11-14 15:23:04,976 INFO attempt.RMAppAttemptImpl: Updating application attempt appattempt_1636903352801_0001_000001 with final state: FINISHING, and exit status: -1000
[36malgo-1    |[0m 2021-11-14 15:23:04,977 INFO attempt.RMAppAttemptImpl: appattempt_1636903352801_0001_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
[36malgo-1    |[0m 2021-11-14 15:23:04,977 INFO rmapp.RMAppImpl: Updating application application_1636903352801_0001 with final state: FINISHING
[36malgo-1    |[0m 2021-11-14 15:23:04,977 INFO recovery.RMStateStore: Updating info for app: application_1636903352801_0001
[36malgo-1    |[0m 2021-11-14 15:23:04,977 INFO rmapp.RMAppImpl: application_1636903352801_0001 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
[36malgo-1    |[0m 2021-11-14 15:23:04,978 INFO attempt.RMAppAttemptImpl: appattempt_1636903352801_0001_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
[36malgo-1    |[0m 2021-11-14 15:23:04,978 INFO rmapp.RMAppImpl: application_1636903352801_0001 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
[36malgo-1    |[0m 2021-11-14 15:23:05,019 INFO launcher.ContainerLaunch: Container container_1636903352801_0001_01_000003 succeeded 
[36malgo-1    |[0m 2021-11-14 15:23:05,027 INFO container.ContainerImpl: Container container_1636903352801_0001_01_000003 transitioned from RUNNING to EXITED_WITH_SUCCESS
[36malgo-1    |[0m 2021-11-14 15:23:05,028 INFO launcher.ContainerCleanup: Cleaning up container container_1636903352801_0001_01_000003
[36malgo-1    |[0m 2021-11-14 15:23:05,030 INFO nodemanager.NMAuditLogger: USER=root	OPERATION=Container Finished - Succeeded	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1636903352801_0001	CONTAINERID=container_1636903352801_0001_01_000003
[36malgo-1    |[0m 2021-11-14 15:23:05,031 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/container_1636903352801_0001_01_000003
[36malgo-1    |[0m 2021-11-14 15:23:05,032 INFO container.ContainerImpl: Container container_1636903352801_0001_01_000003 transitioned from EXITED_WITH_SUCCESS to DONE
[36malgo-1    |[0m 2021-11-14 15:23:05,032 INFO application.ApplicationImpl: Removing container_1636903352801_0001_01_000003 from application application_1636903352801_0001
[36malgo-1    |[0m 2021-11-14 15:23:05,033 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1636903352801_0001_01_000003
[36malgo-1    |[0m 2021-11-14 15:23:05,033 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1636903352801_0001
[36malgo-1    |[0m 2021-11-14 15:23:05,035 INFO rmcontainer.RMContainerImpl: container_1636903352801_0001_01_000003 Container Transitioned from RUNNING to COMPLETED
[36malgo-1    |[0m 2021-11-14 15:23:05,035 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636903352801_0001	CONTAINERID=container_1636903352801_0001_01_000003	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[36malgo-1    |[0m 2021-11-14 15:23:05,046 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/container_1636903352801_0001_01_000003/launch_container.sh
[36malgo-1    |[0m 2021-11-14 15:23:05,046 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/container_1636903352801_0001_01_000003/launch_container.sh]
[36malgo-1    |[0m 2021-11-14 15:23:05,046 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/container_1636903352801_0001_01_000003/container_tokens
[36malgo-1    |[0m 2021-11-14 15:23:05,046 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/container_1636903352801_0001_01_000003/container_tokens]
[36malgo-1    |[0m 2021-11-14 15:23:05,046 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/container_1636903352801_0001_01_000003/sysfs
[36malgo-1    |[0m 2021-11-14 15:23:05,046 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/container_1636903352801_0001_01_000003/sysfs]
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stderr] 21/11/14 15:23:03 INFO Executor: Finished task 0.0 in stage 10.0 ([/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stdout] 2021-11-14T15:22:58.336+0000: [GC (Allocation Failure) [PSYoungGen: 120320K->9019K(140288K)] 120320K->9027K(461312K), 0.0074518 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stdout] 2021-11-14T15:22:58.825+0000: [GC (Allocation Failure) [PSYoungGen: 129339K->7740K(140288K)] 129347K->7756K(461312K), 0.0061442 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stdout] 2021-11-14T15:22:59.199+0000: [GC (Metadata GC Threshold) [PSYoungGen: 109367K->8661K(140288K)] 109383K->8685K(461312K), 0.0049636 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stdout] 2021-11-14T15:22:59.204+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 8661K->0K(140288K)] [ParOldGen: 24K->8402K(190464K)] 8685K->8402K(330752K), [Metaspace: 20371K->20371K(1067008K)], 0.0227066 secs] [Times: user=0.04 sys=0.01, real=0.02 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stdout] 2021-11-14T15:22:59.726+0000: [GC (Allocation Failure) [PSYoungGen: 120320K->4576K(184832K)] 128722K->12987K(375296K), 0.0059780 secs] [Times: user=0.00 sys=0.01, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stdout] 2021-11-14T15:23:00.214+0000: [GC (GCLocker Initiated GC) [PSYoungGen: 184800K->7594K(245248K)] 193211K->16005K(435712K), 0.0067453 secs] [Times: user=0.01 sys=0.01, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stdout] 2021-11-14T15:23:00.314+0000: [GC (Metadata GC Threshold) [PSYoungGen: 46371K->3942K(283136K)] 54783K->12361K(473600K), 0.0045999 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stdout] 2021-11-14T15:23:00.319+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 3942K->0K(283136K)] [ParOldGen: 8419K->10052K(277504K)] 12361K->10052K(560640K), [Metaspace: 33956K->33954K(1079296K)], 0.0270855 secs] [Times: user=0.06 sys=0.00, real=0.02 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stdout] 2021-11-14T15:23:01.300+0000: [GC (Allocation Failure) [PSYoungGen: 272896K->10739K(283648K)] 282957K->22089K(561152K), 0.0111079 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stdout] 2021-11-14T15:23:02.160+0000: [GC (Metadata GC Threshold) [PSYoungGen: 256456K->12796K(419328K)] 267806K->26725K(696832K), 0.0119479 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stdout] 2021-11-14T15:23:02.172+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 12796K->0K(419328K)] [ParOldGen: 13929K->23709K(390656K)] 26725K->23709K(809984K), [Metaspace: 54390K->54390K(1099776K)], 0.0803949 secs] [Times: user=0.18 sys=0.01, real=0.08 secs] 
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stdout] Heap
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stdout]  PSYoungGen      total 419328K, used 351056K [0x00000000d5580000, 0x00000000f0080000, 0x0000000100000000)
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stdout]   eden space 406528K, 86% used [0x00000000d5580000,0x00000000eac54170,0x00000000ee280000)
[36malgo-1    |[0m 2021-11-14 15:23:05,078 INFO resourcemanager.ApplicationMasterService: application_1636903352801_0001 unregistered successfully. 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stdout] 2021-11-14T15:22:54.437+0000: [GC (Allocation Failure) [PSYoungGen: 120320K->9045K(140288K)] 120320K->9053K(461312K), 0.0072735 secs] [Times: user=0.01 sys=0.01, real=0.00 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stdout] 2021-11-14T15:22:54.910+0000: [GC (Allocation Failure) [PSYoungGen: 129365K->7718K(140288K)] 129373K->7734K(461312K), 0.0058963 secs] [Times: user=0.01 sys=0.01, real=0.00 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stdout] 2021-11-14T15:22:55.265+0000: [GC (Metadata GC Threshold) [PSYoungGen: 108559K->8706K(140288K)] 108575K->8730K(461312K), 0.0053274 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stdout] 2021-11-14T15:22:55.271+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 8706K->0K(140288K)] [ParOldGen: 24K->8405K(189440K)] 8730K->8405K(329728K), [Metaspace: 20376K->20376K(1067008K)], 0.0225046 secs] [Times: user=0.04 sys=0.01, real=0.02 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stdout] 2021-11-14T15:22:55.853+0000: [GC (Allocation Failure) [PSYoungGen: 120320K->4548K(183296K)] 128725K->12962K(372736K), 0.0049403 secs] [Times: user=0.01 sys=0.01, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stdout] 2021-11-14T15:22:56.308+0000: [GC (Allocation Failure) [PSYoungGen: 183236K->7370K(245248K)] 191650K->15784K(434688K), 0.0074918 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stdout] 2021-11-14T15:22:56.422+0000: [GC (Metadata GC Threshold) [PSYoungGen: 51601K->4154K(281088K)] 60015K->12576K(470528K), 0.0047724 secs] [Times: user=0.02 sys=0.00, real=0.00 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stdout] 2021-11-14T15:22:56.427+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 4154K->0K(281088K)] [ParOldGen: 8421K->10295K(275456K)] 12576K->10295K(556544K), [Metaspace: 33975K->33973K(1081344K)], 0.0255247 secs] [Times: user=0.04 sys=0.00, real=0.03 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stdout] 2021-11-14T15:22:57.123+0000: [GC (Allocation Failure) [PSYoungGen: 270848K->10741K(281600K)] 281143K->22119K(557056K), 0.0128136 secs] [Times: user=0.02 sys=0.01, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stdout] 2021-11-14T15:22:58.009+0000: [GC (Metadata GC Threshold) [PSYoungGen: 273292K->12266K(423936K)] 284670K->26039K(699392K), 0.0127478 secs] [Times: user=0.04 sys=0.00, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stdout] 2021-11-14T15:22:58.022+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 12266K->0K(423936K)] [ParOldGen: 13773K->23184K(388096K)] 26039K->23184K(812032K), [Metaspace: 54339K->54339K(1099776K)], 0.0856510 secs] [Times: user=0.22 sys=0.00, real=0.08 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stdout] 2021-11-14T15:23:04.284+0000: [GC (Allocation Failure) [PSYoungGen: 411648K->10355K(426496K)] 434832K->33547K(814592K), 0.0096681 secs] [Times: user=0.02 sys=0.01, real=0.01 secs] 
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stdout] Heap
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stdout]  PSYoungGen      total 426496K, used 221577K [0x00000000d5580000, 0x00000000f5380000, 0x0000000100000000)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/coleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@2a962436: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@5e7f90ad: Failed to connect to service endpoint: ]
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:136)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:137)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 	... 50 more
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 42
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO Executor: Running task 0.0 in stage 12.0 (TID 42)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO TorrentBroadcast: Started reading broadcast variable 13 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 15.1 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO TorrentBroadcast: Reading broadcast variable 13 took 5 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 70.4 KiB, free 911.8 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO CodeGenerator: Code generated in 7.082599 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO CodeGenerator: Code generated in 10.202614 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO FileScanRDD: TID: 42 - Reading current file: path: file:///opt/ml/processing/input/data/data.jsonl, range: 0-32882, partition values: [empty row], isDataPresent: false
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO TorrentBroadcast: Started reading broadcast variable 11 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 911.7 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO TorrentBroadcast: Reading broadcast variable 11 took 10 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 445.1 KiB, free 911.3 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO Executor: Finished task 0.0 in stage 12.0 (TID 42). 3742 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO YarnCoarseGrainedExecutorBackend: Got assigned task 43
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO Executor: Running task 0.0 in stage 14.0 (TID 43)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO MapOutputTrackerWorker: Updating epoch to 3 and clearing cache
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO TorrentBroadcast: Started reading broadcast variable 14 with 1 pieces (estimated total size 4.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 71.6 KiB, free 911.2 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO TorrentBroadcast: Reading broadcast variable 14 took 5 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 193.4 KiB, free 911.0 MiB)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO DirectFileOutputCommitter: Direct Write: DISABLED
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@172.18.0.3:46495)
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO MapOutputTrackerWorker: Got the output locations
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO CodeGenerator: Code generated in 12.521095 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.1 KiB) non-empty blocks including 1 (3.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.1 KiB) non-empty blocks including 1 (3.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.0 KiB) non-empty blocks including 1 (3.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.1 KiB) non-empty blocks including 1 (3.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.0 KiB) non-empty blocks including 1 (3.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.0 KiB) non-empty blocks including 1 (3.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.0 KiB) non-empty blocks including 1 (3.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.3 KiB) non-empty blocks including 1 (3.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (1408.0 B) non-empty blocks including 1 (1408.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO FileOutputCommitter: Saved output of task 'attempt_20211114152304_0014_m_000000_43' to file:/opt/ml/processing/output/data
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO SparkHadoopMapRedUtil: attempt_20211114152304_0014_m_000000_43: Committed
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO Executor: Finished task 0.0 in stage 14.0 (TID 43). 4023 bytes result sent to driver
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO YarnCoarseGrainedExecutorBackend: Driver commanded a shutdown
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO MemoryStore: MemoryStore cleared
[33malgo-2    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000002/stdout]   from space 12800K, 0% used [0x00000000ef32021-11-14 15:23:05,291 INFO launcher.ContainerLaunch: Container container_1636903352801_0001_01_000002 succeeded 
[33malgo-2    |[0m 2021-11-14 15:23:05,299 INFO container.ContainerImpl: Container container_1636903352801_0001_01_000002 transitioned from RUNNING to EXITED_WITH_SUCCESS
[33malgo-2    |[0m 2021-11-14 15:23:05,300 INFO launcher.ContainerCleanup: Cleaning up container container_1636903352801_0001_01_000002
[33malgo-2    |[0m 2021-11-14 15:23:05,301 INFO nodemanager.NMAuditLogger: USER=root	OPERATION=Container Finished - Succeeded	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1636903352801_0001	CONTAINERID=container_1636903352801_0001_01_000002
[33malgo-2    |[0m 2021-11-14 15:23:05,302 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/container_1636903352801_0001_01_000002
[33malgo-2    |[0m 2021-11-14 15:23:05,304 INFO container.ContainerImpl: Container container_1636903352801_0001_01_000002 transitioned from EXITED_WITH_SUCCESS to DONE
[33malgo-2    |[0m 2021-11-14 15:23:05,304 INFO application.ApplicationImpl: Removing container_1636903352801_0001_01_000002 from application application_1636903352801_0001
[33malgo-2    |[0m 2021-11-14 15:23:05,304 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1636903352801_0001_01_000002
[33malgo-2    |[0m 2021-11-14 15:23:05,304 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1636903352801_0001
[36malgo-1    |[0m [/var/log/yarn/export/userlogs/application_1636903352801_0001/container_1636903352801_0001_01_000003/stderr] 21/11/14 15:23:04 INFO BlockManager: BlockMana2021-11-14 15:23:05,306 INFO rmcontainer.RMContainerImpl: container_1636903352801_0001_01_000002 Container Transitioned from RUNNING to COMPLETED
[36malgo-1    |[0m 2021-11-14 15:23:05,306 INFO resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1636903352801_0001	CONTAINERID=container_1636903352801_0001_01_000002	RESOURCE=<memory:3287, vCores:1>	QUEUENAME=default
[33malgo-2    |[0m 2021-11-14 15:23:05,316 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/container_1636903352801_0001_01_000002/launch_container.sh
[33malgo-2    |[0m 2021-11-14 15:23:05,316 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/container_1636903352801_0001_01_000002/launch_container.sh]
[33malgo-2    |[0m 2021-11-14 15:23:05,316 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/container_1636903352801_0001_01_000002/container_tokens
[33malgo-2    |[0m 2021-11-14 15:23:05,316 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/container_1636903352801_0001_01_000002/container_tokens]
[33malgo-2    |[0m 2021-11-14 15:23:05,316 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/container_1636903352801_0001_01_000002/sysfs
[33malgo-2    |[0m 2021-11-14 15:23:05,316 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1636903352801_0001/container_1636903352801_0001_01_000002/sysfs]
[36malgo-1    |[0m 11-14 15:23 smspark-submit INFO     spark submit was successful. primary node exiting.
[33malgo-2    |[0m 11-14 15:23 urllib3.connectionpool WARNING  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcc5db8d2d0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m 2021-11-14 15:23:06,129 WARN datanode.DataNode: IOException in offerService
[33malgo-2    |[0m java.io.EOFException: End of File Exception between local host is: "algo-2/172.18.0.2"; destination host is: "algo-1.spark-network":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
[33malgo-2    |[0m 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[33malgo-2    |[0m 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[33malgo-2    |[0m 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[33malgo-2    |[0m 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
[33malgo-2    |[0m 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
[33malgo-2    |[0m 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:791)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
[33malgo-2    |[0m 	at com.sun.proxy.$Proxy16.sendHeartbeat(Unknown Source)
[33malgo-2    |[0m 	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:168)
[33malgo-2    |[0m 	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:517)
[33malgo-2    |[0m 	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
[33malgo-2    |[0m 	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
[33malgo-2    |[0m 	at java.lang.Thread.run(Thread.java:748)
[33malgo-2    |[0m Caused by: java.io.EOFException
[33malgo-2    |[0m 	at java.io.DataInputStream.readInt(DataInputStream.java:392)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
[33malgo-2    |[0m 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)
[33malgo-2    |[0m 2021-11-14 15:23:06,310 INFO retry.RetryInvocationHandler: java.io.EOFException: End of File Exception between local host is: "algo-2/172.18.0.2"; destination host is: "algo-1.spark-network":8031; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException, while invoking ResourceTrackerPBClientImpl.nodeHeartbeat over null. Retrying after sleeping for 30000ms.
[33malgo-2    |[0m 11-14 15:23 urllib3.connectionpool WARNING  Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcc5db8d450>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m 11-14 15:23 urllib3.connectionpool WARNING  Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcc5db8d810>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m 11-14 15:23 urllib3.connectionpool WARNING  Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcc5db8de50>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[36malgo-1 exited with code 0
[0m[33malgo-2    |[0m 11-14 15:23 urllib3.connectionpool WARNING  Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcc5db70290>: Failed to establish a new connection: [Errno -2] Name or service not known')': /
[33malgo-2    |[0m 11-14 15:23 smspark-submit INFO     primary is down, worker now exiting
[33malgo-2 exited with code 0
[0mThe CMD variable is not set. Defaulting to a blank string.
Removing algo-1 ... 
Removing algo-2 ... 
[1A[2KRemoving algo-2 ... [32mdone[0m[1B[2A[2KRemoving algo-1 ... [32mdone[0m[2BRemoving network spark-network


Running docker-compose down ...
PASSED

=================================== FAILURES ===================================
____________________________ test_pyspark_multinode ____________________________

input_data = 'file:///opt/ml/processing/input/data/data.jsonl'
output_data = 'file:///opt/ml/processing/output/data', verbose_opt = '--verbose'

    def test_pyspark_multinode(input_data: str, output_data: str, verbose_opt: str) -> None:
        input = "--input {}".format(input_data)
        output = "--output {}".format(output_data)
        py_files_arg = "--py-files /opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_udfs.py"
        app_py = "/opt/ml/processing/input/code/python/hello_py_spark/hello_py_spark_app.py"
        docker_compose_cmd = (
            f"CMD='{py_files_arg} {verbose_opt} {app_py} {input} {output}' docker-compose up --force-recreate"
        )
    
        print(docker_compose_cmd)
        docker_compose_proc = subprocess.run(
            docker_compose_cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True, encoding="utf-8",
        )
    
        stdout = docker_compose_proc.stdout
        print(stdout)
    
        # assert exit code of docker compose command.
        assert 0 == docker_compose_proc.returncode
    
        # assert the expected Python script executed
        assert "Hello World, this is PySpark" in stdout
    
        # Application code does "df.show()"
        assert "only showing top 20 rows" in stdout
    
        # from spark-defaults configuration in configuration.json
        assert "(spark.executor.memory,2g)" in stdout
        assert "(spark.executor.cores,1)" in stdout
    
        # assert on stdout from docker compose to get exit code of each container.
>       assert 2 == stdout.count("exited with code 0")
E       assert 2 == 1
E         +2
E         -1

test/integration/local/test_multinode_container.py:64: AssertionError
============================ slowest test durations ============================
140.74s call     test/integration/local/test_multinode_container.py::test_pyspark_multinode
61.16s call     test/integration/local/test_multinode_container.py::test_scala_spark_multinode
56.31s call     test/integration/local/test_multinode_container.py::test_java_spark_multinode
0.00s teardown test/integration/local/test_multinode_container.py::test_java_spark_multinode
0.00s setup    test/integration/local/test_multinode_container.py::test_pyspark_multinode
0.00s setup    test/integration/local/test_multinode_container.py::test_java_spark_multinode
0.00s setup    test/integration/local/test_multinode_container.py::test_scala_spark_multinode
0.00s teardown test/integration/local/test_multinode_container.py::test_scala_spark_multinode
0.00s teardown test/integration/local/test_multinode_container.py::test_pyspark_multinode
=========================== short test summary info ============================
FAILED test/integration/local/test_multinode_container.py::test_pyspark_multinode
=================== 1 failed, 2 passed in 258.30s (0:04:18) ====================
make: *** [Makefile:81: test-local] Error 1
